[
{"key": "achtibat2022where", "year": "2022", "title":"From \"Where\" to \"What\": Towards Human-Understandable Explanations through Concept Relevance Propagation", "abstract": "<p>The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring\ntransparency to today’s powerful but opaque deep learning models. While local\nXAI methods explain individual predictions in form of attribution maps, thereby\nidentifying where important features occur (but not providing information about\nwhat they represent), global explanation techniques visualize what concepts a\nmodel has generally learned to encode. Both types of methods thus only provide\npartial insights and leave the burden of interpreting the model’s reasoning to\nthe user. Only few contemporary techniques aim at combining the principles\nbehind both local and global XAI for obtaining more informative explanations.\nThose methods, however, are often limited to specific model architectures or\nimpose additional requirements on training regimes or data and label\navailability, which renders the post-hoc application to arbitrarily pre-trained\nmodels practically impossible. In this work we introduce the Concept Relevance\nPropagation (CRP) approach, which combines the local and global perspectives of\nXAI and thus allows answering both the “where” and “what” questions for\nindividual predictions, without additional constraints imposed. We further\nintroduce the principle of Relevance Maximization for finding representative\nexamples of encoded concepts based on their usefulness to the model. We thereby\nlift the dependency on the common practice of Activation Maximization and its\nlimitations. We demonstrate the capabilities of our methods in various\nsettings, showcasing that Concept Relevance Propagation and Relevance\nMaximization lead to more human interpretable explanations and provide deep\ninsights into the model’s representations and reasoning through concept\natlases, concept composition analyses, and quantitative investigations of\nconcept subspaces and their role in fine-grained decision making.</p>\n", "tags": [] },
{"key": "adebayo2018sanity", "year": "2018", "title":"Sanity Checks for Saliency Maps", "abstract": "<p>Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings.</p>\n", "tags": [] },
{"key": "adler2016auditing", "year": "2016", "title":"Auditing Black-box Models for Indirect Influence", "abstract": "<p>Data-trained predictive models see widespread use, but for the most part they\nare used as black boxes which output a prediction or score. It is therefore\nhard to acquire a deeper understanding of model behavior, and in particular how\ndifferent features influence the model prediction. This is important when\ninterpreting the behavior of complex models, or asserting that certain\nproblematic attributes (like race or gender) are not unduly influencing\ndecisions.\n  In this paper, we present a technique for auditing black-box models, which\nlets us study the extent to which existing models take advantage of particular\nfeatures in the dataset, without knowing how the models work. Our work focuses\non the problem of indirect influence: how some features might indirectly\ninfluence outcomes via other, related features. As a result, we can find\nattribute influences even in cases where, upon further direct examination of\nthe model, the attribute is not referred to by the model at all.\n  Our approach does not require the black-box model to be retrained. This is\nimportant if (for example) the model is only accessible via an API, and\ncontrasts our work with other methods that investigate feature influence like\nfeature selection. We present experimental evidence for the effectiveness of\nour procedure using a variety of publicly available datasets and models. We\nalso validate our procedure using techniques from interpretable learning and\nfeature selection, as well as against other black-box auditing procedures.</p>\n", "tags": [] },
{"key": "ai2022explanatory", "year": "2022", "title":"Explanatory machine learning for sequential human teaching", "abstract": "<p>The topic of comprehensibility of machine-learned theories has recently drawn\nincreasing attention. Inductive Logic Programming (ILP) uses logic programming\nto derive logic theories from small data based on abduction and induction\ntechniques. Learned theories are represented in the form of rules as\ndeclarative descriptions of obtained knowledge. In earlier work, the authors\nprovided the first evidence of a measurable increase in human comprehension\nbased on machine-learned logic rules for simple classification tasks. In a\nlater study, it was found that the presentation of machine-learned explanations\nto humans can produce both beneficial and harmful effects in the context of\ngame learning. We continue our investigation of comprehensibility by examining\nthe effects of the ordering of concept presentations on human comprehension. In\nthis work, we examine the explanatory effects of curriculum order and the\npresence of machine-learned explanations for sequential problem-solving. We\nshow that 1) there exist tasks A and B such that learning A before B has a\nbetter human comprehension with respect to learning B before A and 2) there\nexist tasks A and B such that the presence of explanations when learning A\ncontributes to improved human comprehension when subsequently learning B. We\npropose a framework for the effects of sequential teaching on comprehension\nbased on an existing definition of comprehensibility and provide evidence for\nsupport from data collected in human trials. Empirical results show that\nsequential teaching of concepts with increasing complexity a) has a beneficial\neffect on human comprehension and b) leads to human re-discovery of\ndivide-and-conquer problem-solving strategies, and c) studying machine-learned\nexplanations allows adaptations of human problem-solving strategy with better\nperformance.</p>\n", "tags": [] },
{"key": "akhtar2021attack", "year": "2021", "title":"Attack to Fool and Explain Deep Networks", "abstract": "<p>Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n<code class=\"language-plaintext highlighter-rouge\">adversarial' objective of our attack to use it as a tool to </code>explain’ deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model’s understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers’.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.</p>\n", "tags": [] },
{"key": "alregib2022explanatory", "year": "2022", "title":"Explanatory Paradigms in Neural Networks", "abstract": "<p>In this article, we present a leap-forward expansion to the study of\nexplainability in neural networks by considering explanations as answers to\nabstract reasoning-based questions. With $P$ as the prediction from a neural\nnetwork, these questions are <code class=\"language-plaintext highlighter-rouge\">Why P?', </code>What if not P?’, and `Why P, rather\nthan Q?’ for a given contrast prediction $Q$. The answers to these questions\nare observed correlations, observed counterfactuals, and observed contrastive\nexplanations respectively. Together, these explanations constitute the\nabductive reasoning scheme. We term the three explanatory schemes as observed\nexplanatory paradigms. The term observed refers to the specific case of\npost-hoc explainability, when an explanatory technique explains the decision\n$P$ after a trained neural network has made the decision $P$. The primary\nadvantage of viewing explanations through the lens of abductive reasoning-based\nquestions is that explanations can be used as reasons while making decisions.\nThe post-hoc field of explainability, that previously only justified decisions,\nbecomes active by being involved in the decision making process and providing\nlimited, but relevant and contextual interventions. The contributions of this\narticle are: ($i$) realizing explanations as reasoning paradigms, ($ii$)\nproviding a probabilistic definition of observed explanations and their\ncompleteness, ($iii$) creating a taxonomy for evaluation of explanations, and\n($iv$) positioning gradient-based complete explanainability’s replicability and\nreproducibility across multiple applications and data modalities, ($v$) code\nrepositories, publicly available at\nhttps://github.com/olivesgatech/Explanatory-Paradigms.</p>\n", "tags": [] },
{"key": "alvarez-melis2018robustness", "year": "2018", "title":"On the Robustness of Interpretability Methods", "abstract": "<p>We argue that robustness of explanations—i.e., that similar inputs should\ngive rise to similar explanations—is a key desideratum for interpretability.\nWe introduce metrics to quantify robustness and demonstrate that current\nmethods do not perform well according to these metrics. Finally, we propose\nways that robustness can be enforced on existing interpretability approaches.</p>\n", "tags": [] },
{"key": "arras2019evaluating", "year": "2019", "title":"Evaluating Recurrent Neural Network Explanations", "abstract": "<p>Recently, several methods have been proposed to explain the predictions of\nrecurrent neural networks (RNNs), in particular of LSTMs. The goal of these\nmethods is to understand the network’s decisions by assigning to each input\nvariable, e.g., a word, a relevance indicating to which extent it contributed\nto a particular prediction. In previous works, some of these methods were not\nyet compared to one another, or were evaluated only qualitatively. We close\nthis gap by systematically and quantitatively comparing these methods in\ndifferent settings, namely (1) a toy arithmetic task which we use as a sanity\ncheck, (2) a five-class sentiment prediction of movie reviews, and besides (3)\nwe explore the usefulness of word relevances to build sentence-level\nrepresentations. Lastly, using the method that performed best in our\nexperiments, we show how specific linguistic phenomena such as the negation in\nsentiment analysis reflect in terms of relevance patterns, and how the\nrelevance visualization can help to understand the misclassification of\nindividual samples.</p>\n", "tags": [] },
{"key": "arras2020ground", "year": "2020", "title":"Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI", "abstract": "<p>The rise of deep learning in today’s applications entailed an increasing need\nin explaining the model’s decisions beyond prediction performances in order to\nfoster trust and accountability. Recently, the field of explainable AI (XAI)\nhas developed methods that provide such explanations for already trained neural\nnetworks. In computer vision tasks such explanations, termed heatmaps,\nvisualize the contributions of individual pixels to the prediction. So far XAI\nmethods along with their heatmaps were mainly validated qualitatively via\nhuman-based assessment, or evaluated through auxiliary proxy tasks such as\npixel perturbation, weak object localization or randomization tests. Due to the\nlack of an objective and commonly accepted quality measure for heatmaps, it was\ndebatable which XAI method performs best and whether explanations can be\ntrusted at all. In the present work, we tackle the problem by proposing a\nground truth based evaluation framework for XAI methods based on the CLEVR\nvisual question answering task. Our framework provides a (1) selective, (2)\ncontrolled and (3) realistic testbed for the evaluation of neural network\nexplanations. We compare ten different explanation methods, resulting in new\ninsights about the quality and properties of XAI methods, sometimes\ncontradicting with conclusions from previous comparative studies. The CLEVR-XAI\ndataset and the benchmarking code can be found at\nhttps://github.com/ahmedmagdiosman/clevr-xai.</p>\n", "tags": [] },
{"key": "artelt2021evaluating", "year": "2021", "title":"Evaluating Robustness of Counterfactual Explanations", "abstract": "<p>Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system’s behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions – as changes to the input\n– that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput – i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n  In this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.</p>\n", "tags": [] },
{"key": "arun2020assessing", "year": "2020", "title":"Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging", "abstract": "<p>Saliency maps have become a widely used method to make deep learning models\nmore interpretable by providing post-hoc explanations of classifiers through\nidentification of the most pertinent areas of the input medical image. They are\nincreasingly being used in medical imaging to provide clinically plausible\nexplanations for the decisions the neural network makes. However, the utility\nand robustness of these visualization maps has not yet been rigorously examined\nin the context of medical imaging. We posit that trustworthiness in this\ncontext requires 1) localization utility, 2) sensitivity to model weight\nrandomization, 3) repeatability, and 4) reproducibility. Using the localization\ninformation available in two large public radiology datasets, we quantify the\nperformance of eight commonly used saliency map approaches for the above\ncriteria using area under the precision-recall curves (AUPRC) and structural\nsimilarity index (SSIM), comparing their performance to various baseline\nmeasures. Using our framework to quantify the trustworthiness of saliency maps,\nwe show that all eight saliency map techniques fail at least one of the\ncriteria and are, in most cases, less trustworthy when compared to the\nbaselines. We suggest that their usage in the high-risk domain of medical\nimaging warrants additional scrutiny and recommend that detection or\nsegmentation models be used if localization is the desired output of the\nnetwork. Additionally, to promote reproducibility of our findings, we provide\nthe code we used for all tests performed in this work at this link:\nhttps://github.com/QTIM-Lab/Assessing-Saliency-Maps.</p>\n", "tags": [] },
{"key": "asher2020adequate", "year": "2020", "title":"Adequate and fair explanations", "abstract": "<p>Explaining sophisticated machine-learning based systems is an important issue\nat the foundations of AI. Recent efforts have shown various methods for\nproviding explanations. These approaches can be broadly divided into two\nschools: those that provide a local and human interpreatable approximation of a\nmachine learning algorithm, and logical approaches that exactly characterise\none aspect of the decision. In this paper we focus upon the second school of\nexact explanations with a rigorous logical foundation. There is an\nepistemological problem with these exact methods. While they can furnish\ncomplete explanations, such explanations may be too complex for humans to\nunderstand or even to write down in human readable form. Interpretability\nrequires epistemically accessible explanations, explanations humans can grasp.\nYet what is a sufficiently complete epistemically accessible explanation still\nneeds clarification. We do this here in terms of counterfactuals, following\n[Wachter et al., 2017]. With counterfactual explanations, many of the\nassumptions needed to provide a complete explanation are left implicit. To do\nso, counterfactual explanations exploit the properties of a particular data\npoint or sample, and as such are also local as well as partial explanations. We\nexplore how to move from local partial explanations to what we call complete\nlocal explanations and then to global ones. But to preserve accessibility we\nargue for the need for partiality. This partiality makes it possible to hide\nexplicit biases present in the algorithm that may be injurious or unfair.We\ninvestigate how easy it is to uncover these biases in providing complete and\nfair explanations by exploiting the structure of the set of counterfactuals\nproviding a complete local explanation.</p>\n", "tags": [] },
{"key": "atanasova2020generating", "year": "2020", "title":"Generating Fact Checking Explanations", "abstract": "<p>Most existing work on automated fact checking is concerned with predicting\nthe veracity of claims based on metadata, social network spread, language used\nin claims, and, more recently, evidence supporting or denying claims. A crucial\npiece of the puzzle that is still missing is to understand how to automate the\nmost elaborate part of the process – generating justifications for verdicts on\nclaims. This paper provides the first study of how these explanations can be\ngenerated automatically based on available claim context, and how this task can\nbe modelled jointly with veracity prediction. Our results indicate that\noptimising both objectives at the same time, rather than training them\nseparately, improves the performance of a fact checking system. The results of\na manual evaluation further suggest that the informativeness, coverage and\noverall quality of the generated explanations are also improved in the\nmulti-task model.</p>\n", "tags": [] },
{"key": "aubry2015understanding", "year": "2015", "title":"Understanding deep features with computer-generated imagery", "abstract": "<p>We introduce an approach for analyzing the variation of features generated by\nconvolutional neural networks (CNNs) with respect to scene factors that occur\nin natural images. Such factors may include object style, 3D viewpoint, color,\nand scene lighting configuration. Our approach analyzes CNN feature responses\ncorresponding to different scene factors by controlling for them via rendering\nusing a large database of 3D CAD models. The rendered images are presented to a\ntrained CNN and responses for different layers are studied with respect to the\ninput scene factors. We perform a decomposition of the responses based on\nknowledge of the input scene factors and analyze the resulting components. In\nparticular, we quantify their relative importance in the CNN responses and\nvisualize them using principal component analysis. We show qualitative and\nquantitative results of our study on three CNNs trained on large image\ndatasets: AlexNet, Places, and Oxford VGG. We observe important differences\nacross the networks and CNN layers for different scene factors and object\ncategories. Finally, we demonstrate that our analysis based on\ncomputer-generated imagery translates to the network representation of natural\nimages.</p>\n", "tags": [] },
{"key": "augenstein2021towards", "year": "2021", "title":"Towards Explainable Fact Checking", "abstract": "<p>The past decade has seen a substantial rise in the amount of mis- and\ndisinformation online, from targeted disinformation campaigns to influence\npolitics, to the unintentional spreading of misinformation about public health.\nThis development has spurred research in the area of automatic fact checking,\nfrom approaches to detect check-worthy claims and determining the stance of\ntweets towards claims, to methods to determine the veracity of claims given\nevidence documents. These automatic methods are often content-based, using\nnatural language processing methods, which in turn utilise deep neural networks\nto learn higher-order features from text in order to make predictions. As deep\nneural networks are black-box models, their inner workings cannot be easily\nexplained. At the same time, it is desirable to explain how they arrive at\ncertain decisions, especially if they are to be used for decision making. While\nthis has been known for some time, the issues this raises have been exacerbated\nby models increasing in size, and by EU legislation requiring models to be used\nfor decision making to provide explanations, and, very recently, by legislation\nrequiring online platforms operating in the EU to provide transparent reporting\non their services. Despite this, current solutions for explainability are still\nlacking in the area of fact checking. This thesis presents my research on\nautomatic fact checking, including claim check-worthiness detection, stance\ndetection and veracity prediction. Its contributions go beyond fact checking,\nwith the thesis proposing more general machine learning solutions for natural\nlanguage processing in the area of learning with limited labelled data.\nFinally, the thesis presents some first solutions for explainable fact\nchecking.</p>\n", "tags": [] },
{"key": "bai2020attentions", "year": "2020", "title":"Why Attentions May Not Be Interpretable?", "abstract": "<p>Attention-based methods have played important roles in model interpretations,\nwhere the calculated attention weights are expected to highlight the critical\nparts of inputs~(e.g., keywords in sentences). However, recent research found\nthat attention-as-importance interpretations often do not work as we expected.\nFor example, learned attention weights sometimes highlight less meaningful\ntokens like “[SEP]”, “,”, and “.”, and are frequently uncorrelated with other\nfeature importance indicators like gradient-based measures. A recent debate\nover whether attention is an explanation or not has drawn considerable\ninterest. In this paper, we demonstrate that one root cause of this phenomenon\nis the combinatorial shortcuts, which means that, in addition to the\nhighlighted parts, the attention weights themselves may carry extra information\nthat could be utilized by downstream models after attention layers. As a\nresult, the attention weights are no longer pure importance indicators. We\ntheoretically analyze combinatorial shortcuts, design one intuitive experiment\nto show their existence, and propose two methods to mitigate this issue. We\nconduct empirical studies on attention-based interpretation models. The results\nshow that the proposed methods can effectively improve the interpretability of\nattention mechanisms.</p>\n", "tags": [] },
{"key": "bansal2019case", "year": "2019", "title":"A Case for Backward Compatibility for Human-AI Teams", "abstract": "<p>AI systems are being deployed to support human decision making in high-stakes\ndomains. In many cases, the human and AI form a team, in which the human makes\ndecisions after reviewing the AI’s inferences. A successful partnership\nrequires that the human develops insights into the performance of the AI\nsystem, including its failures. We study the influence of updates to an AI\nsystem in this setting. While updates can increase the AI’s predictive\nperformance, they may also lead to changes that are at odds with the user’s\nprior experiences and confidence in the AI’s inferences, hurting therefore the\noverall team performance. We introduce the notion of the compatibility of an AI\nupdate with prior user experience and present methods for studying the role of\ncompatibility in human-AI teams. Empirical results on three high-stakes domains\nshow that current machine learning algorithms do not produce compatible\nupdates. We propose a re-training objective to improve the compatibility of an\nupdate by penalizing new errors. The objective offers full leverage of the\nperformance/compatibility tradeoff, enabling more compatible yet accurate\nupdates.</p>\n", "tags": [] },
{"key": "barr2020towards", "year": "2020", "title":"Towards Ground Truth Explainability on Tabular Data", "abstract": "<p>In data science, there is a long history of using synthetic data for method\ndevelopment, feature selection and feature engineering. Our current interest in\nsynthetic data comes from recent work in explainability. Today’s datasets are\ntypically larger and more complex - requiring less interpretable models. In the\nsetting of \\textit{post hoc} explainability, there is no ground truth for\nexplanations. Inspired by recent work in explaining image classifiers that does\nprovide ground truth, we propose a similar solution for tabular data. Using\ncopulas, a concise specification of the desired statistical properties of a\ndataset, users can build intuition around explainability using controlled data\nsets and experimentation. The current capabilities are demonstrated on three\nuse cases: one dimensional logistic regression, impact of correlation from\ninformative features, impact of correlation from redundant variables.</p>\n", "tags": [] },
{"key": "bastings2020elephant", "year": "2020", "title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?", "abstract": "<p>There is a recent surge of interest in using attention as explanation of\nmodel predictions, with mixed evidence on whether attention can be used as\nsuch. While attention conveniently gives us one weight per input token and is\neasily extracted, it is often unclear toward what goal it is used as\nexplanation. We find that often that goal, whether explicitly stated or not, is\nto find out what input tokens are the most relevant to a prediction, and that\nthe implied user for the explanation is a model developer. For this goal and\nuser, we argue that input saliency methods are better suited, and that there\nare no compelling reasons to use attention, despite the coincidence that it\nprovides a weight for each input. With this position paper, we hope to shift\nsome of the recent focus on attention to saliency methods, and for authors to\nclearly state the goal and user for their explanations.</p>\n", "tags": [] },
{"key": "bau2017network", "year": "2017", "title":"Network Dissection: Quantifying Interpretability of Deep Visual Representations", "abstract": "<p>We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower.</p>\n", "tags": [] },
{"key": "benchekroun2020need", "year": "2020", "title":"The Need for Standardized Explainability", "abstract": "<p>Explainable AI (XAI) is paramount in industry-grade AI; however existing\nmethods fail to address this necessity, in part due to a lack of\nstandardisation of explainability methods. The purpose of this paper is to\noffer a perspective on the current state of the area of explainability, and to\nprovide novel definitions for Explainability and Interpretability to begin\nstandardising this area of research. To do so, we provide an overview of the\nliterature on explainability, and of the existing methods that are already\nimplemented. Finally, we offer a tentative taxonomy of the different\nexplainability methods, opening the door to future research.</p>\n", "tags": [] },
{"key": "benshmuel2022meet", "year": "2022", "title":"Meet You Halfway: Explaining Deep Learning Mysteries", "abstract": "<p>Deep neural networks perform exceptionally well on various learning tasks\nwith state-of-the-art results. While these models are highly expressive and\nachieve impressively accurate solutions with excellent generalization\nabilities, they are susceptible to minor perturbations. Samples that suffer\nsuch perturbations are known as “adversarial examples”. Even though deep\nlearning is an extensively researched field, many questions about the nature of\ndeep learning models remain unanswered. In this paper, we introduce a new\nconceptual framework attached with a formal description that aims to shed light\non the network’s behavior and interpret the behind-the-scenes of the learning\nprocess. Our framework provides an explanation for inherent questions\nconcerning deep learning. Particularly, we clarify: (1) Why do neural networks\nacquire generalization abilities? (2) Why do adversarial examples transfer\nbetween different models?. We provide a comprehensive set of experiments that\nsupport this new framework, as well as its underlying theory.</p>\n", "tags": [] },
{"key": "berger2020visually", "year": "2020", "title":"Visually Analyzing Contextualized Embeddings", "abstract": "<p>In this paper we introduce a method for visually analyzing contextualized\nembeddings produced by deep neural network-based language models. Our approach\nis inspired by linguistic probes for natural language processing, where tasks\nare designed to probe language models for linguistic structure, such as\nparts-of-speech and named entities. These approaches are largely confirmatory,\nhowever, only enabling a user to test for information known a priori. In this\nwork, we eschew supervised probing tasks, and advocate for unsupervised probes,\ncoupled with visual exploration techniques, to assess what is learned by\nlanguage models. Specifically, we cluster contextualized embeddings produced\nfrom a large text corpus, and introduce a visualization design based on this\nclustering and textual structure - cluster co-occurrences, cluster spans, and\ncluster-word membership - to help elicit the functionality of, and relationship\nbetween, individual clusters. User feedback highlights the benefits of our\ndesign in discovering different types of linguistic structures.</p>\n", "tags": [] },
{"key": "besold2018what", "year": "2018", "title":"The What, the Why, and the How of Artificial Explanations in Automated Decision-Making", "abstract": "<p>The increasing incorporation of Artificial Intelligence in the form of\nautomated systems into decision-making procedures highlights not only the\nimportance of decision theory for automated systems but also the need for these\ndecision procedures to be explainable to the people involved in them.\nTraditional realist accounts of explanation, wherein explanation is a relation\nthat holds (or does not hold) eternally between an explanans and an\nexplanandum, are not adequate to account for the notion of explanation required\nfor artificial decision procedures. We offer an alternative account of\nexplanation as used in the context of automated decision-making that makes\nexplanation an epistemic phenomenon, and one that is dependent on context. This\naccount of explanation better accounts for the way that we talk about, and use,\nexplanations and derived concepts, such as `explanatory power’, and also allows\nus to differentiate between reasons or causes on the one hand, which do not\nneed to have an epistemic aspect, and explanations on the other, which do have\nsuch an aspect. Against this theoretical backdrop we then review existing\napproaches to explanation in Artificial Intelligence and Machine Learning, and\nsuggest desiderata which truly explainable decision systems should fulfill.</p>\n", "tags": [] },
{"key": "bhatt2019explainable", "year": "2019", "title":"Explainable Machine Learning in Deployment", "abstract": "<p>Explainable machine learning offers the potential to provide stakeholders\nwith insights into model behavior by using various methods such as feature\nimportance scores, counterfactual explanations, or influential training data.\nYet there is little understanding of how organizations use these methods in\npractice. This study explores how organizations view and use explainability for\nstakeholder consumption. We find that, currently, the majority of deployments\nare not for end users affected by the model but rather for machine learning\nengineers, who use explainability to debug the model itself. There is thus a\ngap between explainability in practice and the goal of transparency, since\nexplanations primarily serve internal stakeholders rather than external ones.\nOur study synthesizes the limitations of current explainability techniques that\nhamper their use for end users. To facilitate end user interaction, we develop\na framework for establishing clear goals for explainability. We end by\ndiscussing concerns raised regarding explainability.</p>\n", "tags": [] },
{"key": "bhatt2020evaluating", "year": "2020", "title":"Evaluating and Aggregating Feature-based Model Explanations", "abstract": "<p>A feature-based model explanation denotes how much each input feature\ncontributes to a model’s output for a given data point. As the number of\nproposed explanation functions grows, we lack quantitative evaluation criteria\nto help practitioners know when to use which explanation function. This paper\nproposes quantitative evaluation criteria for feature-based explanations: low\nsensitivity, high faithfulness, and low complexity. We devise a framework for\naggregating explanation functions. We develop a procedure for learning an\naggregate explanation function with lower complexity and then derive a new\naggregate Shapley value explanation function that minimizes sensitivity.</p>\n", "tags": [] },
{"key": "bodell2019interpretable", "year": "2019", "title":"Interpretable Word Embeddings via Informative Priors", "abstract": "<p>Word embeddings have demonstrated strong performance on NLP tasks. However,\nlack of interpretability and the unsupervised nature of word embeddings have\nlimited their use within computational social science and digital humanities.\nWe propose the use of informative priors to create interpretable and\ndomain-informed dimensions for probabilistic word embeddings. Experimental\nresults show that sensible priors can capture latent semantic concepts better\nthan or on-par with the current state of the art, while retaining the\nsimplicity and generalizability of using priors.</p>\n", "tags": [] },
{"key": "bodria2021benchmarking", "year": "2021", "title":"Benchmarking and Survey of Explanation Methods for Black Box Models", "abstract": "<p>The widespread adoption of black-box models in Artificial Intelligence has\nenhanced the need for explanation methods to reveal how these obscure models\nreach specific decisions. Retrieving explanations is fundamental to unveil\npossible biases and to resolve practical or ethical issues. Nowadays, the\nliterature is full of methods with different explanations. We provide a\ncategorization of explanation methods based on the type of explanation\nreturned. We present the most recent and widely used explainers, and we show a\nvisual comparison among explanations and a quantitative benchmarking.</p>\n", "tags": [] },
{"key": "braşoveanu2022visualizing", "year": "2022", "title":"Visualizing and Explaining Language Models", "abstract": "<p>During the last decade, Natural Language Processing has become, after\nComputer Vision, the second field of Artificial Intelligence that was massively\nchanged by the advent of Deep Learning. Regardless of the architecture, the\nlanguage models of the day need to be able to process or generate text, as well\nas predict missing words, sentences or relations depending on the task. Due to\ntheir black-box nature, such models are difficult to interpret and explain to\nthird parties. Visualization is often the bridge that language model designers\nuse to explain their work, as the coloring of the salient words and phrases,\nclustering or neuron activations can be used to quickly understand the\nunderlying models. This paper showcases the techniques used in some of the most\npopular Deep Learning for NLP visualizations, with a special focus on\ninterpretability and explainability.</p>\n", "tags": [] },
{"key": "buchholz2022means-end", "year": "2022", "title":"A Means-End Account of Explainable Artificial Intelligence", "abstract": "<p>Explainable artificial intelligence (XAI) seeks to produce explanations for\nthose machine learning methods which are deemed opaque. However, there is\nconsiderable disagreement about what this means and how to achieve it. Authors\ndisagree on what should be explained (topic), to whom something should be\nexplained (stakeholder), how something should be explained (instrument), and\nwhy something should be explained (goal). In this paper, I employ insights from\nmeans-end epistemology to structure the field. According to means-end\nepistemology, different means ought to be rationally adopted to achieve\ndifferent epistemic ends. Applied to XAI, different topics, stakeholders, and\ngoals thus require different instruments. I call this the means-end account of\nXAI. The means-end account has a descriptive and a normative component: on the\none hand, I show how the specific means-end relations give rise to a taxonomy\nof existing contributions to the field of XAI; on the other hand, I argue that\nthe suitability of XAI methods can be assessed by analyzing whether they are\nprescribed by a given topic, stakeholder, and goal.</p>\n", "tags": [] },
{"key": "bäuerle2020explornn", "year": "2020", "title":"exploRNN: Understanding Recurrent Neural Networks through Visual Exploration", "abstract": "<p>Due to the success of deep learning (DL) and its growing job market, students\nand researchers from many areas are interested in learning about DL\ntechnologies. Visualization has proven to be of great help during this learning\nprocess. While most current educational visualizations are targeted towards one\nspecific architecture or use case, recurrent neural networks (RNNs), which are\ncapable of processing sequential data, are not covered yet. This is despite the\nfact that tasks on sequential data, such as text and function analysis, are at\nthe forefront of DL research. Therefore, we propose exploRNN, the first\ninteractively explorable educational visualization for RNNs. On the basis of\nmaking learning easier and more fun, we define educational objectives targeted\ntowards understanding RNNs. We use these objectives to form guidelines for the\nvisual design process. By means of exploRNN, which is accessible online, we\nprovide an overview of the training process of RNNs at a coarse level, while\nalso allowing a detailed inspection of the data flow within LSTM cells. In an\nempirical study, we assessed 37 subjects in a between-subjects design to\ninvestigate the learning outcomes and cognitive load of exploRNN compared to a\nclassic text-based learning environment. While learners in the text group are\nahead in superficial knowledge acquisition, exploRNN is particularly helpful\nfor deeper understanding of the learning content. In addition, the complex\ncontent in exploRNN is perceived as significantly easier and causes less\nextraneous load than in the text group. The study shows that for difficult\nlearning material such as recurrent networks, where deep understanding is\nimportant, interactive visualizations such as exploRNN can be helpful.</p>\n", "tags": [] },
{"key": "böhle2021convolutional", "year": "2021", "title":"Convolutional Dynamic Alignment Networks for Interpretable Classifications", "abstract": "<p>We introduce a new family of neural network models called Convolutional\nDynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a\nhigh degree of inherent interpretability. Their core building blocks are\nDynamic Alignment Units (DAUs), which linearly transform their input with\nweight vectors that dynamically align with task-relevant patterns. As a result,\nCoDA-Nets model the classification prediction through a series of\ninput-dependent linear transformations, allowing for linear decomposition of\nthe output into individual input contributions. Given the alignment of the\nDAUs, the resulting contribution maps align with discriminative input patterns.\nThese model-inherent decompositions are of high visual quality and outperform\nexisting attribution methods under quantitative metrics. Further, CoDA-Nets\nconstitute performant classifiers, achieving on par results to ResNet and VGG\nmodels on e.g. CIFAR-10 and TinyImagenet.</p>\n", "tags": [] },
{"key": "böhle2021optimising", "year": "2021", "title":"Optimising for Interpretability: Convolutional Dynamic Alignment Networks", "abstract": "<p>We introduce a new family of neural network models called Convolutional\nDynamic Alignment Networks (CoDA Nets), which are performant classifiers with a\nhigh degree of inherent interpretability. Their core building blocks are\nDynamic Alignment Units (DAUs), which are optimised to transform their inputs\nwith dynamically computed weight vectors that align with task-relevant\npatterns. As a result, CoDA Nets model the classification prediction through a\nseries of input-dependent linear transformations, allowing for linear\ndecomposition of the output into individual input contributions. Given the\nalignment of the DAUs, the resulting contribution maps align with\ndiscriminative input patterns. These model-inherent decompositions are of high\nvisual quality and outperform existing attribution methods under quantitative\nmetrics. Further, CoDA Nets constitute performant classifiers, achieving on par\nresults to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,\nCoDA Nets can be combined with conventional neural network models to yield\npowerful classifiers that more easily scale to complex datasets such as\nImagenet whilst exhibiting an increased interpretable depth, i.e., the output\ncan be explained well in terms of contributions from intermediate layers within\nthe network.</p>\n", "tags": [] },
{"key": "camburu2020explaining", "year": "2020", "title":"Explaining Deep Neural Networks", "abstract": "<p>Deep neural networks are becoming more and more popular due to their\nrevolutionary success in diverse areas, such as computer vision, natural\nlanguage processing, and speech recognition. However, the decision-making\nprocesses of these models are generally not interpretable to users. In various\ndomains, such as healthcare, finance, or law, it is critical to know the\nreasons behind a decision made by an artificial intelligence system. Therefore,\nseveral directions for explaining neural models have recently been explored. In\nthis thesis, I investigate two major directions for explaining deep neural\nnetworks. The first direction consists of feature-based post-hoc explanatory\nmethods, that is, methods that aim to explain an already trained and fixed\nmodel (post-hoc), and that provide explanations in terms of input features,\nsuch as tokens for text and superpixels for images (feature-based). The second\ndirection consists of self-explanatory neural models that generate natural\nlanguage explanations, that is, models that have a built-in module that\ngenerates explanations for the predictions of the model.</p>\n", "tags": [] },
{"key": "chen2018classifier", "year": "2018", "title":"Why Is My Classifier Discriminatory?", "abstract": "<p>Recent attempts to achieve fairness in predictive models focus on the balance\nbetween fairness and accuracy. In sensitive applications such as healthcare or\ncriminal justice, this trade-off is often undesirable as any increase in\nprediction error could have devastating consequences. In this work, we argue\nthat the fairness of predictions should be evaluated in context of the data,\nand that unfairness induced by inadequate samples sizes or unmeasured\npredictive variables should be addressed through data collection, rather than\nby constraining the model. We decompose cost-based metrics of discrimination\ninto bias, variance, and noise, and propose actions aimed at estimating and\nreducing each term. Finally, we perform case-studies on prediction of income,\nmortality, and review ratings, confirming the value of this analysis. We find\nthat data collection is often a means to reduce discrimination without\nsacrificing accuracy.</p>\n", "tags": [] },
{"key": "chen2018interpretable", "year": "2018", "title":"An Interpretable Model with Globally Consistent Explanations for Credit Risk", "abstract": "<p>We propose a possible solution to a public challenge posed by the Fair Isaac\nCorporation (FICO), which is to provide an explainable model for credit risk\nassessment. Rather than present a black box model and explain it afterwards, we\nprovide a globally interpretable model that is as accurate as other neural\nnetworks. Our “two-layer additive risk model” is decomposable into subscales,\nwhere each node in the second layer represents a meaningful subscale, and all\nof the nonlinearities are transparent. We provide three types of explanations\nthat are simpler than, but consistent with, the global model. One of these\nexplanation methods involves solving a minimum set cover problem to find\nhigh-support globally-consistent explanations. We present a new online\nvisualization tool to allow users to explore the global model and its\nexplanations.</p>\n", "tags": [] },
{"key": "chen2020generating", "year": "2020", "title":"Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection", "abstract": "<p>Generating explanations for neural networks has become crucial for their\napplications in real-world with respect to reliability and trustworthiness. In\nnatural language processing, existing methods usually provide important\nfeatures which are words or phrases selected from an input text as an\nexplanation, but ignore the interactions between them. It poses challenges for\nhumans to interpret an explanation and connect it to model prediction. In this\nwork, we build hierarchical explanations by detecting feature interactions.\nSuch explanations visualize how words and phrases are combined at different\nlevels of the hierarchy, which can help users understand the decision-making of\nblack-box models. The proposed method is evaluated with three neural text\nclassifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic\nand human evaluations. Experiments show the effectiveness of the proposed\nmethod in providing explanations that are both faithful to models and\ninterpretable to humans.</p>\n", "tags": [] },
{"key": "cheng2021tsgb", "year": "2021", "title":"TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency", "abstract": "<p>The explanation for deep neural networks has drawn extensive attention in the\ndeep learning community over the past few years. In this work, we study the\nvisual saliency, a.k.a. visual explanation, to interpret convolutional neural\nnetworks. Compared to iteration based saliency methods, single backward pass\nbased saliency methods benefit from faster speed, and they are widely used in\ndownstream visual tasks. Thus, we focus on single backward pass based methods.\nHowever, existing methods in this category struggle to uccessfully produce\nfine-grained saliency maps concentrating on specific target classes. That said,\nproducing faithful saliency maps satisfying both target-selectiveness and\nfine-grainedness using a single backward pass is a challenging problem in the\nfield. To mitigate this problem, we revisit the gradient flow inside the\nnetwork, and find that the entangled semantics and original weights may disturb\nthe propagation of target-relevant saliency. Inspired by those observations, we\npropose a novel visual saliency method, termed Target-Selective Gradient\nBackprop (TSGB), which leverages rectification operations to effectively\nemphasize target classes and further efficiently propagate the saliency to the\nimage space, thereby generating target-selective and fine-grained saliency\nmaps. The proposed TSGB consists of two components, namely, TSGB-Conv and\nTSGB-FC, which rectify the gradients for convolutional layers and\nfully-connected layers, respectively. Extensive qualitative and quantitative\nexperiments on the ImageNet and Pascal VOC datasets show that the proposed\nmethod achieves more accurate and reliable results than the other competitive\nmethods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB.</p>\n", "tags": [] },
{"key": "chrysostomou2021enjoy", "year": "2021", "title":"Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience", "abstract": "<p>Pretrained transformer-based models such as BERT have demonstrated\nstate-of-the-art predictive performance when adapted into a range of natural\nlanguage processing tasks. An open problem is how to improve the faithfulness\nof explanations (rationales) for the predictions of these models. In this\npaper, we hypothesize that salient information extracted a priori from the\ntraining data can complement the task-specific information learned by the model\nduring fine-tuning on a downstream task. In this way, we aim to help BERT not\nto forget assigning importance to informative input tokens when making\npredictions by proposing SaLoss; an auxiliary loss function for guiding the\nmulti-head attention mechanism during training to be close to salient\ninformation extracted a priori using TextRank. Experiments for explanation\nfaithfulness across five datasets, show that models trained with SaLoss\nconsistently provide more faithful explanations across four different feature\nattribution methods compared to vanilla BERT. Using the rationales extracted\nfrom vanilla BERT and SaLoss models to train inherently faithful classifiers,\nwe further show that the latter result in higher predictive performance in\ndownstream tasks.</p>\n", "tags": [] },
{"key": "chyung2019extracting", "year": "2019", "title":"Extracting Interpretable Concept-Based Decision Trees from CNNs", "abstract": "<p>In an attempt to gather a deeper understanding of how convolutional neural\nnetworks (CNNs) reason about human-understandable concepts, we present a method\nto infer labeled concept data from hidden layer activations and interpret the\nconcepts through a shallow decision tree. The decision tree can provide\ninformation about which concepts a model deems important, as well as provide an\nunderstanding of how the concepts interact with each other. Experiments\ndemonstrate that the extracted decision tree is capable of accurately\nrepresenting the original CNN’s classifications at low tree depths, thus\nencouraging human-in-the-loop understanding of discriminative concepts.</p>\n", "tags": [] },
{"key": "ciocan2018interpretable", "year": "2018", "title":"Interpretable Optimal Stopping", "abstract": "<p>Optimal stopping is the problem of deciding when to stop a stochastic system\nto obtain the greatest reward, arising in numerous application areas such as\nfinance, healthcare and marketing. State-of-the-art methods for\nhigh-dimensional optimal stopping involve approximating the value function or\nthe continuation value, and then using that approximation within a greedy\npolicy. Although such policies can perform very well, they are generally not\nguaranteed to be interpretable; that is, a decision maker may not be able to\neasily see the link between the current system state and the policy’s action.\nIn this paper, we propose a new approach to optimal stopping, wherein the\npolicy is represented as a binary tree, in the spirit of naturally\ninterpretable tree models commonly used in machine learning. We show that the\nclass of tree policies is rich enough to approximate the optimal policy. We\nformulate the problem of learning such policies from observed trajectories of\nthe stochastic system as a sample average approximation (SAA) problem. We prove\nthat the SAA problem converges under mild conditions as the sample size\nincreases, but that computationally even immediate simplifications of the SAA\nproblem are theoretically intractable. We thus propose a tractable heuristic\nfor approximately solving the SAA problem, by greedily constructing the tree\nfrom the top down. We demonstrate the value of our approach by applying it to\nthe canonical problem of option pricing, using both synthetic instances and\ninstances using real S&amp;P-500 data. Our method obtains policies that (1)\noutperform state-of-the-art non-interpretable methods, based on\nsimulation-regression and martingale duality, and (2) possess a remarkably\nsimple and intuitive structure.</p>\n", "tags": [] },
{"key": "craye2018exploring", "year": "2018", "title":"Exploring to learn visual saliency: The RL-IAC approach", "abstract": "<p>The problem of object localization and recognition on autonomous mobile\nrobots is still an active topic. In this context, we tackle the problem of\nlearning a model of visual saliency directly on a robot. This model, learned\nand improved on-the-fly during the robot’s exploration provides an efficient\ntool for localizing relevant objects within their environment. The proposed\napproach includes two intertwined components. On the one hand, we describe a\nmethod for learning and incrementally updating a model of visual saliency from\na depth-based object detector. This model of saliency can also be exploited to\nproduce bounding box proposals around objects of interest. On the other hand,\nwe investigate an autonomous exploration technique to efficiently learn such a\nsaliency model. The proposed exploration, called Reinforcement\nLearning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot’s\nexploration so that samples selected by the robot are likely to improve the\ncurrent model of saliency. We then demonstrate that such a saliency model\nlearned directly on a robot outperforms several state-of-the-art saliency\ntechniques, and that RL-IAC can drastically decrease the required time for\nlearning a reliable saliency model.</p>\n", "tags": [] },
{"key": "dalvi2022towards", "year": "2022", "title":"Towards Teachable Reasoning Systems", "abstract": "<p>Our goal is a teachable reasoning system for question-answering (QA), where a\nuser can interact with faithful answer explanations, and correct errors so that\nthe system improves over time. Our approach is three-fold: First, generated\nchains of reasoning show how answers are implied by the system’s own internal\nbeliefs. Second, users can interact with the explanations to identify erroneous\nmodel beliefs and provide corrections. Third, we augment the model with a\ndynamic memory of such corrections. Retrievals from memory are used as\nadditional context for QA, to help avoid previous mistakes in similar new\nsituations - a novel type of memory-based continuous learning. To our\nknowledge, this is the first system to generate chains that are both faithful\n(the answer follows from the reasoning) and truthful (the chain reflects the\nsystem’s own beliefs, as ascertained by self-querying). In evaluation, users\njudge that a majority (65%+) of generated chains clearly show how an answer\nfollows from a set of facts - substantially better than a high-performance\nbaseline. We also find that using simulated feedback, our system (called\nEntailmentWriter) continually improves with time, requiring feedback on only\n25% of training examples to reach within 1% of the upper-bound (feedback on all\nexamples). We observe a similar trend with real users. This suggests new\nopportunities for using language models in an interactive setting where users\ncan inspect, debug, correct, and improve a system’s performance over time.</p>\n", "tags": [] },
{"key": "danilevsky2020survey", "year": "2020", "title":"A Survey of the State of Explainable AI for Natural Language Processing", "abstract": "<p>Recent years have seen important advances in the quality of state-of-the-art\nmodels, but this has come at the expense of models becoming less interpretable.\nThis survey presents an overview of the current state of Explainable AI (XAI),\nconsidered within the domain of Natural Language Processing (NLP). We discuss\nthe main categorization of explanations, as well as the various ways\nexplanations can be arrived at and visualized. We detail the operations and\nexplainability techniques currently available for generating explanations for\nNLP model predictions, to serve as a resource for model developers in the\ncommunity. Finally, we point out the current gaps and encourage directions for\nfuture work in this important research area.</p>\n", "tags": [] },
{"key": "das2016human", "year": "2016", "title":"Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "abstract": "<p>We conduct large-scale studies on `human attention’ in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.</p>\n", "tags": [] },
{"key": "das2020leveraging", "year": "2020", "title":"Leveraging Rationales to Improve Human Task Performance", "abstract": "<p>Machine learning (ML) systems across many application areas are increasingly\ndemonstrating performance that is beyond that of humans. In response to the\nproliferation of such models, the field of Explainable AI (XAI) has sought to\ndevelop techniques that enhance the transparency and interpretability of\nmachine learning methods. In this work, we consider a question not previously\nexplored within the XAI and ML communities: Given a computational system whose\nperformance exceeds that of its human user, can explainable AI capabilities be\nleveraged to improve the performance of the human? We study this question in\nthe context of the game of Chess, for which computational game engines that\nsurpass the performance of the average player are widely available. We\nintroduce the Rationale-Generating Algorithm, an automated technique for\ngenerating rationales for utility-based computational methods, which we\nevaluate with a multi-day user study against two baselines. The results show\nthat our approach produces rationales that lead to statistically significant\nimprovement in human task performance, demonstrating that rationales\nautomatically generated from an AI’s internal task model can be used not only\nto explain what the system is doing, but also to instruct the user and\nultimately improve their task performance.</p>\n", "tags": [] },
{"key": "dieber2020model", "year": "2020", "title":"Why model why? Assessing the strengths and limitations of LIME", "abstract": "<p>When it comes to complex machine learning models, commonly referred to as\nblack boxes, understanding the underlying decision making process is crucial\nfor domains such as healthcare and financial services, and also when it is used\nin connection with safety critical systems such as autonomous vehicles. As such\ninterest in explainable artificial intelligence (xAI) tools and techniques has\nincreased in recent years. However, the effectiveness of existing xAI\nframeworks, especially concerning algorithms that work with data as opposed to\nimages, is still an open research question. In order to address this gap, in\nthis paper we examine the effectiveness of the Local Interpretable\nModel-Agnostic Explanations (LIME) xAI framework, one of the most popular model\nagnostic frameworks found in the literature, with a specific focus on its\nperformance in terms of making tabular models more interpretable. In\nparticular, we apply several state of the art machine learning algorithms on a\ntabular dataset, and demonstrate how LIME can be used to supplement\nconventional performance assessment methods. In addition, we evaluate the\nunderstandability of the output produced by LIME both via a usability study,\ninvolving participants who are not familiar with LIME, and its overall\nusability via an assessment framework, which is derived from the International\nOrganisation for Standardisation 9241-11:1998 standard.</p>\n", "tags": [] },
{"key": "ding2021evaluating", "year": "2021", "title":"Evaluating Saliency Methods for Neural Language Models", "abstract": "<p>Saliency methods are widely used to interpret neural network predictions, but\ndifferent variants of saliency methods often disagree even on the\ninterpretations of the same prediction made by the same model. In these cases,\nhow do we identify when are these interpretations trustworthy enough to be used\nin analyses? To address this question, we conduct a comprehensive and\nquantitative evaluation of saliency methods on a fundamental category of NLP\nmodels: neural language models. We evaluate the quality of prediction\ninterpretations from two perspectives that each represents a desirable property\nof these interpretations: plausibility and faithfulness. Our evaluation is\nconducted on four different datasets constructed from the existing human\nannotation of syntactic and semantic agreements, on both sentence-level and\ndocument-level. Through our evaluation, we identified various ways saliency\nmethods could yield interpretations of low quality. We recommend that future\nwork deploying such methods to neural language models should carefully validate\ntheir interpretations before drawing insights.</p>\n", "tags": [] },
{"key": "dong2017towards", "year": "2017", "title":"Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples", "abstract": "<p>Deep neural networks (DNNs) have demonstrated impressive performance on a\nwide array of tasks, but they are usually considered opaque since internal\nstructure and learned parameters are not interpretable. In this paper, we\nre-examine the internal representations of DNNs using adversarial images, which\nare generated by an ensemble-optimization algorithm. We find that: (1) the\nneurons in DNNs do not truly detect semantic objects/parts, but respond to\nobjects/parts only as recurrent discriminative patches; (2) deep visual\nrepresentations are not robust distributed codes of visual concepts because the\nrepresentations of adversarial images are largely not consistent with those of\nreal images, although they have similar visual appearance, both of which are\ndifferent from previous findings. To further improve the interpretability of\nDNNs, we propose an adversarial training scheme with a consistent loss such\nthat the neurons are endowed with human-interpretable concepts. The induced\ninterpretable representations enable us to trace eventual outcomes back to\ninfluential neurons. Therefore, human users can know how the models make\npredictions, as well as when and why they make errors.</p>\n", "tags": [] },
{"key": "doshi-velez2017towards", "year": "2017", "title":"Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "<p>As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.</p>\n", "tags": [] },
{"key": "dosovitskiy2015inverting", "year": "2015", "title":"Inverting Visual Representations with Convolutional Networks", "abstract": "<p>Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities.</p>\n", "tags": [] },
{"key": "du2018towards", "year": "2018", "title":"Towards Explanation of DNN-based Prediction with Guided Feature Inversion", "abstract": "<p>While deep neural networks (DNN) have become an effective computational tool,\nthe prediction results are often criticized by the lack of interpretability,\nwhich is essential in many real-world applications such as health informatics.\nExisting attempts based on local interpretations aim to identify relevant\nfeatures contributing the most to the prediction of DNN by monitoring the\nneighborhood of a given input. They usually simply ignore the intermediate\nlayers of the DNN that might contain rich information for interpretation. To\nbridge the gap, in this paper, we propose to investigate a guided feature\ninversion framework for taking advantage of the deep architectures towards\neffective interpretation. The proposed framework not only determines the\ncontribution of each feature in the input but also provides insights into the\ndecision-making process of DNN models. By further interacting with the neuron\nof the target category at the output layer of the DNN, we enforce the\ninterpretation result to be class-discriminative. We apply the proposed\ninterpretation model to different CNN architectures to provide explanations for\nimage data and conduct extensive experiments on ImageNet and PASCAL VOC07\ndatasets. The interpretation results demonstrate the effectiveness of our\nproposed framework in providing class-discriminative interpretation for\nDNN-based prediction.</p>\n", "tags": [] },
{"key": "etheredge2020decontextualized", "year": "2020", "title":"Decontextualized learning for interpretable hierarchical representations of visual patterns", "abstract": "<p>Apart from discriminative models for classification and object detection\ntasks, the application of deep convolutional neural networks to basic research\nutilizing natural imaging data has been somewhat limited; particularly in cases\nwhere a set of interpretable features for downstream analysis is needed, a key\nrequirement for many scientific investigations. We present an algorithm and\ntraining paradigm designed specifically to address this: decontextualized\nhierarchical representation learning (DHRL). By combining a generative model\nchaining procedure with a ladder network architecture and latent space\nregularization for inference, DHRL address the limitations of small datasets\nand encourages a disentangled set of hierarchically organized features. In\naddition to providing a tractable path for analyzing complex hierarchal\npatterns using variation inference, this approach is generative and can be\ndirectly combined with empirical and theoretical approaches. To highlight the\nextensibility and usefulness of DHRL, we demonstrate this method in application\nto a question from evolutionary biology.</p>\n", "tags": [] },
{"key": "fabbri2011explanation-based", "year": "2011", "title":"Explanation-Based Auditing", "abstract": "<p>To comply with emerging privacy laws and regulations, it has become common\nfor applications like electronic health records systems (EHRs) to collect\naccess logs, which record each time a user (e.g., a hospital employee) accesses\na piece of sensitive data (e.g., a patient record). Using the access log, it is\neasy to answer simple queries (e.g., Who accessed Alice’s medical record?), but\nthis often does not provide enough information. In addition to learning who\naccessed their medical records, patients will likely want to understand why\neach access occurred. In this paper, we introduce the problem of generating\nexplanations for individual records in an access log. The problem is motivated\nby user-centric auditing applications, and it also provides a novel approach to\nmisuse detection. We develop a framework for modeling explanations which is\nbased on a fundamental observation: For certain classes of databases, including\nEHRs, the reason for most data accesses can be inferred from data stored\nelsewhere in the database. For example, if Alice has an appointment with Dr.\nDave, this information is stored in the database, and it explains why Dr. Dave\nlooked at Alice’s record. Large numbers of data accesses can be explained using\ngeneral forms called explanation templates. Rather than requiring an\nadministrator to manually specify explanation templates, we propose a set of\nalgorithms for automatically discovering frequent templates from the database\n(i.e., those that explain a large number of accesses). We also propose\ntechniques for inferring collaborative user groups, which can be used to\nenhance the quality of the discovered explanations. Finally, we have evaluated\nour proposed techniques using an access log and data from the University of\nMichigan Health System. Our results demonstrate that in practice we can provide\nexplanations for over 94% of data accesses in the log.</p>\n", "tags": [] },
{"key": "fan2020trust", "year": "2020", "title":"Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis", "abstract": "<p>With the rapid growth of Android malware, many machine learning-based malware\nanalysis approaches are proposed to mitigate the severe phenomenon. However,\nsuch classifiers are opaque, non-intuitive, and difficult for analysts to\nunderstand the inner decision reason. For this reason, a variety of explanation\napproaches are proposed to interpret predictions by providing important\nfeatures. Unfortunately, the explanation results obtained in the malware\nanalysis domain cannot achieve a consensus in general, which makes the analysts\nconfused about whether they can trust such results. In this work, we propose\nprincipled guidelines to assess the quality of five explanation approaches by\ndesigning three critical quantitative metrics to measure their stability,\nrobustness, and effectiveness. Furthermore, we collect five widely-used malware\ndatasets and apply the explanation approaches on them in two tasks, including\nmalware detection and familial identification. Based on the generated\nexplanation results, we conduct a sanity check of such explanation approaches\nin terms of the three metrics. The results demonstrate that our metrics can\nassess the explanation approaches and help us obtain the knowledge of most\ntypical malicious behaviors for malware analysis.</p>\n", "tags": [] },
{"key": "fong2017interpretable", "year": "2017", "title":"Interpretable Explanations of Black Boxes by Meaningful Perturbation", "abstract": "<p>As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks “look” in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations.</p>\n", "tags": [] },
{"key": "främling2020explainable", "year": "2020", "title":"Explainable AI without Interpretable Model", "abstract": "<p>Explainability has been a challenge in AI for as long as AI has existed. With\nthe recently increased use of AI in society, it has become more important than\never that AI systems would be able to explain the reasoning behind their\nresults also to end-users in situations such as being eliminated from a\nrecruitment process or having a bank loan application refused by an AI system.\nEspecially if the AI system has been trained using Machine Learning, it tends\nto contain too many parameters for them to be analysed and understood, which\nhas caused them to be called `black-box’ systems. Most Explainable AI (XAI)\nmethods are based on extracting an interpretable model that can be used for\nproducing explanations. However, the interpretable model does not necessarily\nmap accurately to the original black-box model. Furthermore, the\nunderstandability of interpretable models for an end-user remains questionable.\nThe notions of Contextual Importance and Utility (CIU) presented in this paper\nmake it possible to produce human-like explanations of black-box outcomes\ndirectly, without creating an interpretable model. Therefore, CIU explanations\nmap accurately to the black-box model itself. CIU is completely model-agnostic\nand can be used with any black-box system. In addition to feature importance,\nthe utility concept that is well-known in Decision Theory provides a new\ndimension to explanations compared to most existing XAI methods. Finally, CIU\ncan produce explanations at any level of abstraction and using different\nvocabularies and other means of interaction, which makes it possible to adjust\nexplanations and interaction according to the context and to the target users.</p>\n", "tags": [] },
{"key": "fu2020interactive", "year": "2020", "title":"Interactive Knowledge Distillation", "abstract": "<p>Knowledge distillation is a standard teacher-student learning framework to\ntrain a light-weight student network under the guidance of a well-trained large\nteacher network. As an effective teaching strategy, interactive teaching has\nbeen widely employed at school to motivate students, in which teachers not only\nprovide knowledge but also give constructive feedback to students upon their\nresponses, to improve their learning performance. In this work, we propose an\nInterActive Knowledge Distillation (IAKD) scheme to leverage the interactive\nteaching strategy for efficient knowledge distillation. In the distillation\nprocess, the interaction between teacher and student networks is implemented by\na swapping-in operation: randomly replacing the blocks in the student network\nwith the corresponding blocks in the teacher network. In the way, we directly\ninvolve the teacher’s powerful feature transformation ability to largely boost\nthe student’s performance. Experiments with typical settings of teacher-student\nnetworks demonstrate that the student networks trained by our IAKD achieve\nbetter performance than those trained by conventional knowledge distillation\nmethods on diverse image classification datasets.</p>\n", "tags": [] },
{"key": "ge2022explainable", "year": "2022", "title":"Explainable Fairness in Recommendation", "abstract": "<p>Existing research on fairness-aware recommendation has mainly focused on the\nquantification of fairness and the development of fair recommendation models,\nneither of which studies a more substantial problem–identifying the underlying\nreason of model disparity in recommendation. This information is critical for\nrecommender system designers to understand the intrinsic recommendation\nmechanism and provides insights on how to improve model fairness to decision\nmakers. Fortunately, with the rapid development of Explainable AI, we can use\nmodel explainability to gain insights into model (un)fairness. In this paper,\nwe study the problem of explainable fairness, which helps to gain insights\nabout why a system is fair or unfair, and guides the design of fair recommender\nsystems with a more informed and unified methodology. Particularly, we focus on\na common setting with feature-aware recommendation and exposure unfairness, but\nthe proposed explainable fairness framework is general and can be applied to\nother recommendation settings and fairness definitions. We propose a\nCounterfactual Explainable Fairness framework, called CEF, which generates\nexplanations about model fairness that can improve the fairness without\nsignificantly hurting the performance.The CEF framework formulates an\noptimization problem to learn the “minimal” change of the input features that\nchanges the recommendation results to a certain level of fairness. Based on the\ncounterfactual recommendation result of each feature, we calculate an\nexplainability score in terms of the fairness-utility trade-off to rank all the\nfeature-based explanations, and select the top ones as fairness explanations.</p>\n", "tags": [] },
{"key": "gerlings2021explainable", "year": "2021", "title":"Explainable AI, but explainable to whom?", "abstract": "<p>Advances in AI technologies have resulted in superior levels of AI-based\nmodel performance. However, this has also led to a greater degree of model\ncomplexity, resulting in ‘black box’ models. In response to the AI black box\nproblem, the field of explainable AI (xAI) has emerged with the aim of\nproviding explanations catered to human understanding, trust, and transparency.\nYet, we still have a limited understanding of how xAI addresses the need for\nexplainable AI in the context of healthcare. Our research explores the\ndiffering explanation needs amongst stakeholders during the development of an\nAI-system for classifying COVID-19 patients for the ICU. We demonstrate that\nthere is a constellation of stakeholders who have different explanation needs,\nnot just the ‘user’. Further, the findings demonstrate how the need for xAI\nemerges through concerns associated with specific stakeholder groups i.e., the\ndevelopment team, subject matter experts, decision makers, and the audience.\nOur findings contribute to the expansion of xAI by highlighting that different\nstakeholders have different explanation needs. From a practical perspective,\nthe study provides insights on how AI systems can be adjusted to support\ndifferent stakeholders needs, ensuring better implementation and operation in a\nhealthcare context.</p>\n", "tags": [] },
{"key": "ghosh2017towards", "year": "2017", "title":"Towards a New Interpretation of Separable Convolutions", "abstract": "<p>In recent times, the use of separable convolutions in deep convolutional\nneural network architectures has been explored. Several researchers, most\nnotably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in\ntheir deep architectures and have demonstrated state of the art or close to\nstate of the art performance. However, the underlying mechanism of action of\nseparable convolutions are still not fully understood. Although their\nmathematical definition is well understood as a depthwise convolution followed\nby a pointwise convolution, deeper interpretations such as the extreme\nInception hypothesis (Chollet, 2016) have failed to provide a thorough\nexplanation of their efficacy. In this paper, we propose a hybrid\ninterpretation that we believe is a better model for explaining the efficacy of\nseparable convolutions.</p>\n", "tags": [] },
{"key": "gilpin2019explaining", "year": "2019", "title":"Explaining Explanations to Society", "abstract": "<p>There is a disconnect between explanatory artificial intelligence (XAI)\nmethods and the types of explanations that are useful for and demanded by\nsociety (policy makers, government officials, etc.) Questions that experts in\nartificial intelligence (AI) ask opaque systems provide inside explanations,\nfocused on debugging, reliability, and validation. These are different from\nthose that society will ask of these systems to build trust and confidence in\ntheir decisions. Although explanatory AI systems can answer many questions that\nexperts desire, they often don’t explain why they made decisions in a way that\nis precise (true to the model) and understandable to humans. These outside\nexplanations can be used to build trust, comply with regulatory and policy\nchanges, and act as external validation. In this paper, we focus on XAI methods\nfor deep neural networks (DNNs) because of DNNs’ use in decision-making and\ninherent opacity. We explore the types of questions that explanatory DNN\nsystems can answer and discuss challenges in building explanatory systems that\nprovide outside explanations for societal requirements and benefit.</p>\n", "tags": [] },
{"key": "ginsparg2004information", "year": "2004", "title":"Information, please... ?", "abstract": "<p>Stephen Hawking’s recent concession that black holes do not irretrievably\neradicate information after all has garnered much attention. It is refreshing\nto see the public focused, if just for a moment, on an important conundrum that\nhas fascinated theoretical physicists for three decades, and prompted much\nconceptual progress. The scientific issues, however, remain much less settled\nthan Dr. Hawking’s celebrated wager on the question.</p>\n", "tags": [] },
{"key": "gosiewska2019trust", "year": "2019", "title":"Do Not Trust Additive Explanations", "abstract": "<p>Explainable Artificial Intelligence (XAI)has received a great deal of\nattention recently. Explainability is being presented as a remedy for the\ndistrust of complex and opaque models. Model agnostic methods such as LIME,\nSHAP, or Break Down promise instance-level interpretability for any complex\nmachine learning model. But how faithful are these additive explanations? Can\nwe rely on additive explanations for non-additive models?\n  In this paper, we (1) examine the behavior of the most popular instance-level\nexplanations under the presence of interactions, (2) introduce a new method\nthat detects interactions for instance-level explanations, (3) perform a large\nscale benchmark to see how frequently additive explanations may be misleading.</p>\n", "tags": [] },
{"key": "graziotin2015feel", "year": "2015", "title":"How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance", "abstract": "<p>Affects—emotions and moods—have an impact on cognitive activities and the\nworking performance of individuals. Development tasks are undertaken through\ncognitive processes, yet software engineering research lacks theory on affects\nand their impact on software development activities. In this paper, we report\non an interpretive study aimed at broadening our understanding of the\npsychology of programming in terms of the experience of affects while\nprogramming, and the impact of affects on programming performance. We conducted\na qualitative interpretive study based on: face-to-face open-ended interviews,\nin-field observations, and e-mail exchanges. This enabled us to construct a\nnovel explanatory theory of the impact of affects on development performance.\nThe theory is explicated using an established taxonomy framework. The proposed\ntheory builds upon the concepts of events, affects, attractors, focus, goals,\nand performance. Theoretical and practical implications are given.</p>\n", "tags": [] },
{"key": "gu2020interpretable", "year": "2020", "title":"Interpretable Graph Capsule Networks for Object Recognition", "abstract": "<p>Capsule Networks, as alternatives to Convolutional Neural Networks, have been\nproposed to recognize objects from images. The current literature demonstrates\nmany advantages of CapsNets over CNNs. However, how to create explanations for\nindividual classifications of CapsNets has not been well explored. The widely\nused saliency methods are mainly proposed for explaining CNN-based\nclassifications; they create saliency map explanations by combining activation\nvalues and the corresponding gradients, e.g., Grad-CAM. These saliency methods\nrequire a specific architecture of the underlying classifiers and cannot be\ntrivially applied to CapsNets due to the iterative routing mechanism therein.\nTo overcome the lack of interpretability, we can either propose new post-hoc\ninterpretation methods for CapsNets or modifying the model to have build-in\nexplanations. In this work, we explore the latter. Specifically, we propose\ninterpretable Graph Capsule Networks (GraCapsNets), where we replace the\nrouting part with a multi-head attention-based Graph Pooling approach. In the\nproposed model, individual classification explanations can be created\neffectively and efficiently. Our model also demonstrates some unexpected\nbenefits, even though it replaces the fundamental part of CapsNets. Our\nGraCapsNets achieve better classification performance with fewer parameters and\nbetter adversarial robustness, when compared to CapsNets. Besides, GraCapsNets\nalso keep other advantages of CapsNets, namely, disentangled representations\nand affine transformation robustness.</p>\n", "tags": [] },
{"key": "guo2018visualizing", "year": "2018", "title":"Visualizing and Understanding Deep Neural Networks in CTR Prediction", "abstract": "<p>Although deep learning techniques have been successfully applied to many\ntasks, interpreting deep neural network models is still a big challenge to us.\nRecently, many works have been done on visualizing and analyzing the mechanism\nof deep neural networks in the areas of image processing and natural language\nprocessing. In this paper, we present our approaches to visualize and\nunderstand deep neural networks for a very important commercial task–CTR\n(Click-through rate) prediction. We conduct experiments on the productive data\nfrom our online advertising system with daily varying distribution. To\nunderstand the mechanism and the performance of the model, we inspect the\nmodel’s inner status at neuron level. Also, a probe approach is implemented to\nmeasure the layer-wise performance of the model. Moreover, to measure the\ninfluence from the input features, we calculate saliency scores based on the\nback-propagated gradients. Practical applications are also discussed, for\nexample, in understanding, monitoring, diagnosing and refining models and\nalgorithms.</p>\n", "tags": [] },
{"key": "gupta2019simple", "year": "2019", "title":"A Simple Saliency Method That Passes the Sanity Checks", "abstract": "<p>There is great interest in “saliency methods” (also called “attribution\nmethods”), which give “explanations” for a deep net’s decision, by assigning a\n“score” to each feature/pixel in the input. Their design usually involves\ncredit-assignment via the gradient of the output with respect to input.\nRecently Adebayo et al. [arXiv:1810.03292] questioned the validity of many of\nthese methods since they do not pass simple <em>sanity checks</em> which test whether\nthe scores shift/vanish when layers of the trained net are randomized, or when\nthe net is retrained using random labels for inputs.\n  We propose a simple fix to existing saliency methods that helps them pass\nsanity checks, which we call “competition for pixels”. This involves computing\nsaliency maps for all possible labels in the classification task, and using a\nsimple competition among them to identify and remove less relevant pixels from\nthe map. The simplest variant of this is “Competitive Gradient $\\odot$ Input\n(CGI)”: it is efficient, requires no additional training, and uses only the\ninput and gradient. Some theoretical justification is provided for it\n(especially for ReLU networks) and its performance is empirically demonstrated.</p>\n", "tags": [] },
{"key": "halpern2002causes", "year": "2002", "title":"Causes and Explanations: A Structural-Model Approach. Part II: Explanations", "abstract": "<p>We propose new definitions of (causal) explanation, using structural\nequations to model counterfactuals. The definition is based on the notion of\nactual cause, as defined and motivated in a companion paper. Essentially, an\nexplanation is a fact that is not known for certain but, if found to be true,\nwould constitute an actual cause of the fact to be explained, regardless of the\nagent’s initial uncertainty. We show that the definition handles well a number\nof problematic examples from the literature.</p>\n", "tags": [] },
{"key": "hase2020evaluating", "year": "2020", "title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?", "abstract": "<p>Algorithmic approaches to interpreting machine learning models have\nproliferated in recent years. We carry out human subject tests that are the\nfirst of their kind to isolate the effect of algorithmic explanations on a key\naspect of model interpretability, simulatability, while avoiding important\nconfounding experimental factors. A model is simulatable when a person can\npredict its behavior on new inputs. Through two kinds of simulation tests\ninvolving text and tabular data, we evaluate five explanations methods: (1)\nLIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a\nComposite approach that combines explanations from each method. Clear evidence\nof method effectiveness is found in very few cases: LIME improves\nsimulatability in tabular classification, and our Prototype method is effective\nin counterfactual simulation tests. We also collect subjective ratings of\nexplanations, but we do not find that ratings are predictive of how helpful\nexplanations are. Our results provide the first reliable and comprehensive\nestimates of how explanations influence simulatability across a variety of\nexplanation methods and data domains. We show that (1) we need to be careful\nabout the metrics we use to evaluate explanation methods, and (2) there is\nsignificant room for improvement in current methods. All our supporting code,\ndata, and models are publicly available at:\nhttps://github.com/peterbhase/InterpretableNLP-ACL2020</p>\n", "tags": [] },
{"key": "hase2021models", "year": "2021", "title":"When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data", "abstract": "<p>Many methods now exist for conditioning model outputs on task instructions,\nretrieved documents, and user-provided explanations and feedback. Rather than\nrelying solely on examples of task inputs and outputs, these approaches use\nvaluable additional data for improving model correctness and aligning learned\nmodels with human priors. Meanwhile, a growing body of evidence suggests that\nsome language models can (1) store a large amount of knowledge in their\nparameters, and (2) perform inference over tasks in textual inputs at test\ntime. These results raise the possibility that, for some tasks, humans cannot\nexplain to a model any more about the task than it already knows or could infer\non its own. In this paper, we study the circumstances under which explanations\nof individual data points can (or cannot) improve modeling performance. In\norder to carefully control important properties of the data and explanations,\nwe introduce a synthetic dataset for experiments, and we also make use of three\nexisting datasets with explanations: e-SNLI, TACRED, and SemEval. We first give\na formal framework for the available modeling approaches, in which explanation\ndata can be used as model inputs, as targets, or as a prior. After arguing that\nthe most promising role for explanation data is as model inputs, we propose to\nuse a retrieval-based method and show that it solves our synthetic task with\naccuracies upwards of 95%, while baselines without explanation data achieve\nbelow 65% accuracy. We then identify properties of datasets for which\nretrieval-based modeling fails. With the three existing datasets, we find no\nimprovements from explanation retrieval. Drawing on findings from our synthetic\ntask, we suggest that at least one of six preconditions for successful modeling\nfails to hold with these datasets. Our code is publicly available at\nhttps://github.com/peterbhase/ExplanationRoles</p>\n", "tags": [] },
{"key": "he2019understanding", "year": "2019", "title":"Understanding and Visualizing Deep Visual Saliency Models", "abstract": "<p>Recently, data-driven deep saliency models have achieved high performance and\nhave outperformed classical saliency models, as demonstrated by results on\ndatasets such as the MIT300 and SALICON. Yet, there remains a large gap between\nthe performance of these models and the inter-human baseline. Some outstanding\nquestions include what have these models learned, how and where they fail, and\nhow they can be improved. This article attempts to answer these questions by\nanalyzing the representations learned by individual neurons located at the\nintermediate layers of deep saliency models. To this end, we follow the steps\nof existing deep saliency models, that is borrowing a pre-trained model of\nobject recognition to encode the visual features and learning a decoder to\ninfer the saliency. We consider two cases when the encoder is used as a fixed\nfeature extractor and when it is fine-tuned, and compare the inner\nrepresentations of the network. To study how the learned representations depend\non the task, we fine-tune the same network using the same image set but for two\ndifferent tasks: saliency prediction versus scene classification. Our analyses\nreveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are\nalready encoded within various layers of the network pre-trained for object\nrecognition, 2) using modern datasets, we find that fine-tuning pre-trained\nmodels for saliency prediction makes them favor some categories (e.g. head)\nover some others (e.g. text), 3) although deep models of saliency outperform\nclassical models on natural images, the converse is true for synthetic stimuli\n(e.g. pop-out search arrays), an evidence of significant difference between\nhuman and data-driven saliency models, and 4) we confirm that, after-fine\ntuning, the change in inner-representations is mostly due to the task and not\nthe domain shift in the data.</p>\n", "tags": [] },
{"key": "herm2022stop", "year": "2022", "title":"Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability", "abstract": "<p>Machine learning algorithms enable advanced decision making in contemporary\nintelligent systems. Research indicates that there is a tradeoff between their\nmodel performance and explainability. Machine learning models with higher\nperformance are often based on more complex algorithms and therefore lack\nexplainability and vice versa. However, there is little to no empirical\nevidence of this tradeoff from an end user perspective. We aim to provide\nempirical evidence by conducting two user experiments. Using two distinct\ndatasets, we first measure the tradeoff for five common classes of machine\nlearning algorithms. Second, we address the problem of end user perceptions of\nexplainable artificial intelligence augmentations aimed at increasing the\nunderstanding of the decision logic of high-performing complex models. Our\nresults diverge from the widespread assumption of a tradeoff curve and indicate\nthat the tradeoff between model performance and explainability is much less\ngradual in the end user’s perception. This is a stark contrast to assumed\ninherent model interpretability. Further, we found the tradeoff to be\nsituational for example due to data complexity. Results of our second\nexperiment show that while explainable artificial intelligence augmentations\ncan be used to increase explainability, the type of explanation plays an\nessential role in end user perception.</p>\n", "tags": [] },
{"key": "hohman2018visual", "year": "2018", "title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers", "abstract": "<p>Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W’s and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.</p>\n", "tags": [] },
{"key": "honeycutt2020soliciting", "year": "2020", "title":"Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy", "abstract": "<p>Mixed-initiative systems allow users to interactively provide feedback to\npotentially improve system performance. Human feedback can correct model errors\nand update model parameters to dynamically adapt to changing data.\nAdditionally, many users desire the ability to have a greater level of control\nand fix perceived flaws in systems they rely on. However, how the ability to\nprovide feedback to autonomous systems influences user trust is a largely\nunexplored area of research. Our research investigates how the act of providing\nfeedback can affect user understanding of an intelligent system and its\naccuracy. We present a controlled experiment using a simulated object detection\nsystem with image data to study the effects of interactive feedback collection\non user impressions. The results show that providing human-in-the-loop feedback\nlowered both participants’ trust in the system and their perception of system\naccuracy, regardless of whether the system accuracy improved in response to\ntheir feedback. These results highlight the importance of considering the\neffects of allowing end-user feedback on user trust when designing intelligent\nsystems.</p>\n", "tags": [] },
{"key": "hsieh2020evaluations", "year": "2020", "title":"Evaluations and Methods for Explanation through Robustness Analysis", "abstract": "<p>Feature based explanations, that provide importance of each feature towards\nthe model prediction, is arguably one of the most intuitive ways to explain a\nmodel. In this paper, we establish a novel set of evaluation criteria for such\nfeature based explanations by robustness analysis. In contrast to existing\nevaluations which require us to specify some way to “remove” features that\ncould inevitably introduces biases and artifacts, we make use of the subtler\nnotion of smaller adversarial perturbations. By optimizing towards our proposed\nevaluation criteria, we obtain new explanations that are loosely necessary and\nsufficient for a prediction. We further extend the explanation to extract the\nset of features that would move the current prediction to a target class by\nadopting targeted adversarial attack for the robustness analysis. Through\nexperiments across multiple domains and a user study, we validate the\nusefulness of our evaluation criteria and our derived explanations.</p>\n", "tags": [] },
{"key": "hu2016harnessing", "year": "2016", "title":"Harnessing Deep Neural Networks with Logic Rules", "abstract": "<p>Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.</p>\n", "tags": [] },
{"key": "huang2019understanding", "year": "2019", "title":"Understanding Generalization through Visualizations", "abstract": "<p>The power of neural networks lies in their ability to generalize to unseen\ndata, yet the underlying reasons for this phenomenon remain elusive. Numerous\nrigorous attempts have been made to explain generalization, but available\nbounds are still quite loose, and analysis does not always lead to true\nunderstanding. The goal of this work is to make generalization more intuitive.\nUsing visualization methods, we discuss the mystery of generalization, the\ngeometry of loss landscapes, and how the curse (or, rather, the blessing) of\ndimensionality causes optimizers to settle into minima that generalize well.</p>\n", "tags": [] },
{"key": "huang2021physically", "year": "2021", "title":"Physically Explainable CNN for SAR Image Classification", "abstract": "<p>Integrating the special electromagnetic characteristics of Synthetic Aperture\nRadar (SAR) in deep neural networks is essential in order to enhance the\nexplainability and physics awareness of deep learning. In this paper, we first\npropose a novel physically explainable convolutional neural network for SAR\nimage classification, namely physics guided and injected learning (PGIL). It\ncomprises three parts: (1) explainable models (XM) to provide prior physics\nknowledge, (2) physics guided network (PGN) to encode the knowledge into\nphysics-aware features, and (3) physics injected network (PIN) to adaptively\nintroduce the physics-aware features into classification pipeline for label\nprediction. A hybrid Image-Physics SAR dataset format is proposed for\nevaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The\nresults show that the proposed PGIL substantially improve the classification\nperformance in case of limited labeled data compared with the counterpart\ndata-driven CNN and other pre-training methods. Additionally, the physics\nexplanations are discussed to indicate the interpretability and the physical\nconsistency preserved in the predictions. We deem the proposed method would\npromote the development of physically explainable deep learning in SAR image\ninterpretation field.</p>\n", "tags": [] },
{"key": "hvilshøj2021quantitative", "year": "2021", "title":"On Quantitative Evaluations of Counterfactuals", "abstract": "<p>As counterfactual examples become increasingly popular for explaining\ndecisions of deep learning models, it is essential to understand what\nproperties quantitative evaluation metrics do capture and equally important\nwhat they do not capture. Currently, such understanding is lacking, potentially\nslowing down scientific progress. In this paper, we consolidate the work on\nevaluating visual counterfactual examples through an analysis and experiments.\nWe find that while most metrics behave as intended for sufficiently simple\ndatasets, some fail to tell the difference between good and bad counterfactuals\nwhen the complexity increases. We observe experimentally that metrics give good\nscores to tiny adversarial-like changes, wrongly identifying such changes as\nsuperior counterfactual examples. To mitigate this issue, we propose two new\nmetrics, the Label Variation Score and the Oracle score, which are both less\nvulnerable to such tiny changes. We conclude that a proper quantitative\nevaluation of visual counterfactual examples should combine metrics to ensure\nthat all aspects of good counterfactuals are quantified.</p>\n", "tags": [] },
{"key": "höllig2022tsinterpret", "year": "2022", "title":"TSInterpret: A unified framework for time series interpretability", "abstract": "<p>With the increasing application of deep learning algorithms to time series\nclassification, especially in high-stake scenarios, the relevance of\ninterpreting those algorithms becomes key. Although research in time series\ninterpretability has grown, accessibility for practitioners is still an\nobstacle. Interpretability approaches and their visualizations are diverse in\nuse without a unified API or framework. To close this gap, we introduce\nTSInterpret an easily extensible open-source Python library for interpreting\npredictions of time series classifiers that combines existing interpretation\napproaches into one unified framework. The library features (i)\nstate-of-the-art interpretability algorithms, (ii) exposes a unified API\nenabling users to work with explanations consistently and provides (iii)\nsuitable visualizations for each explanation.</p>\n", "tags": [] },
{"key": "ignatiev2020relating", "year": "2020", "title":"On Relating 'Why?' and 'Why Not?' Explanations", "abstract": "<p>Explanations of Machine Learning (ML) models often address a ‘Why?’ question.\nSuch explanations can be related with selecting feature-value pairs which are\nsufficient for the prediction. Recent work has investigated explanations that\naddress a ‘Why Not?’ question, i.e. finding a change of feature values that\nguarantee a change of prediction. Given their goals, these two forms of\nexplaining predictions of ML models appear to be mostly unrelated. However,\nthis paper demonstrates otherwise, and establishes a rigorous formal\nrelationship between ‘Why?’ and ‘Why Not?’ explanations. Concretely, the paper\nproves that, for any given instance, ‘Why?’ explanations are minimal hitting\nsets of ‘Why Not?’ explanations and vice-versa. Furthermore, the paper devises\nnovel algorithms for extracting and enumerating both forms of explanations.</p>\n", "tags": [] },
{"key": "ismail2020benchmarking", "year": "2020", "title":"Benchmarking Deep Learning Interpretability in Time Series Predictions", "abstract": "<p>Saliency methods are used extensively to highlight the importance of input\nfeatures in model predictions. These methods are mostly used in vision and\nlanguage tasks, and their applications to time series data is relatively\nunexplored. In this paper, we set out to extensively compare the performance of\nvarious saliency-based interpretability methods across diverse neural\narchitectures, including Recurrent Neural Network, Temporal Convolutional\nNetworks, and Transformers in a new benchmark of synthetic time series data. We\npropose and report multiple metrics to empirically evaluate the performance of\nsaliency methods for detecting feature importance over time using both\nprecision (i.e., whether identified features contain meaningful signals) and\nrecall (i.e., the number of features with signal identified as important).\nThrough several experiments, we show that (i) in general, network architectures\nand saliency methods fail to reliably and accurately identify feature\nimportance over time in time series data, (ii) this failure is mainly due to\nthe conflation of time and feature domains, and (iii) the quality of saliency\nmaps can be improved substantially by using our proposed two-step temporal\nsaliency rescaling (TSR) approach that first calculates the importance of each\ntime step before calculating the importance of each feature at a time step.</p>\n", "tags": [] },
{"key": "ivankay2022fooling", "year": "2022", "title":"Fooling Explanations in Text Classifiers", "abstract": "<p>State-of-the-art text classification models are becoming increasingly reliant\non deep neural networks (DNNs). Due to their black-box nature, faithful and\nrobust explanation methods need to accompany classifiers for deployment in\nreal-life scenarios. However, it has been shown in vision applications that\nexplanation methods are susceptible to local, imperceptible perturbations that\ncan significantly alter the explanations without changing the predicted\nclasses. We show here that the existence of such perturbations extends to text\nclassifiers as well. Specifically, we introduceTextExplanationFooler (TEF), a\nnovel explanation attack algorithm that alters text input samples imperceptibly\nso that the outcome of widely-used explanation methods changes considerably\nwhile leaving classifier predictions unchanged. We evaluate the performance of\nthe attribution robustness estimation performance in TEF on five sequence\nclassification datasets, utilizing three DNN architectures and three\ntransformer architectures for each dataset. TEF can significantly decrease the\ncorrelation between unchanged and perturbed input attributions, which shows\nthat all models and explanation methods are susceptible to TEF perturbations.\nMoreover, we evaluate how the perturbations transfer to other model\narchitectures and attribution methods, and show that TEF perturbations are also\neffective in scenarios where the target model and explanation method are\nunknown. Finally, we introduce a semi-universal attack that is able to compute\nfast, computationally light perturbations with no knowledge of the attacked\nclassifier nor explanation method. Overall, our work shows that explanations in\ntext classifiers are very fragile and users need to carefully address their\nrobustness before relying on them in critical applications.</p>\n", "tags": [] },
{"key": "jain2019attention", "year": "2019", "title":"Attention is not Explanation", "abstract": "<p>Attention mechanisms have seen wide adoption in neural NLP models. In\naddition to improving predictive performance, these are often touted as\naffording transparency: models equipped with attention provide a distribution\nover attended-to input units, and this is often presented (at least implicitly)\nas communicating the relative importance of inputs. However, it is unclear what\nrelationship exists between attention weights and model outputs. In this work,\nwe perform extensive experiments across a variety of NLP tasks that aim to\nassess the degree to which attention weights provide meaningful `explanations’\nfor predictions. We find that they largely do not. For example, learned\nattention weights are frequently uncorrelated with gradient-based measures of\nfeature importance, and one can identify very different attention distributions\nthat nonetheless yield equivalent predictions. Our findings show that standard\nattention modules do not provide meaningful explanations and should not be\ntreated as though they do. Code for all experiments is available at\nhttps://github.com/successar/AttentionExplanation.</p>\n", "tags": [] },
{"key": "jalwana2021cameras", "year": "2021", "title":"CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency", "abstract": "<p>Backpropagation image saliency aims at explaining model predictions by\nestimating model-centric importance of individual pixels in the input. However,\nclass-insensitivity of the earlier layers in a network only allows saliency\ncomputation with low resolution activation maps of the deeper layers, resulting\nin compromised image saliency. Remedifying this can lead to sanity failures. We\npropose CAMERAS, a technique to compute high-fidelity backpropagation saliency\nmaps without requiring any external priors and preserving the map sanity. Our\nmethod systematically performs multi-scale accumulation and fusion of the\nactivation maps and backpropagated gradients to compute precise saliency maps.\nFrom accurate image saliency to articulation of relative importance of input\nfeatures for different models, and precise discrimination between model\nperception of visually similar objects, our high-resolution mapping offers\nmultiple novel insights into the black-box deep visual models, which are\npresented in the paper. We also demonstrate the utility of our saliency maps in\nadversarial setup by drastically reducing the norm of attack signals by\nfocusing them on the precise regions identified by our maps. Our method also\ninspires new evaluation metrics and a sanity check for this developing research\ndirection. Code is available here https://github.com/VisMIL/CAMERAS</p>\n", "tags": [] },
{"key": "joshi2020explainable", "year": "2020", "title":"Explainable Disease Classification via weakly-supervised segmentation", "abstract": "<p>Deep learning based approaches to Computer Aided Diagnosis (CAD) typically\npose the problem as an image classification (Normal or Abnormal) problem. These\nsystems achieve high to very high accuracy in specific disease detection for\nwhich they are trained but lack in terms of an explanation for the provided\ndecision/classification result. The activation maps which correspond to\ndecisions do not correlate well with regions of interest for specific diseases.\nThis paper examines this problem and proposes an approach which mimics the\nclinical practice of looking for an evidence prior to diagnosis. A CAD model is\nlearnt using a mixed set of information: class labels for the entire training\nset of images plus a rough localisation of suspect regions as an extra input\nfor a smaller subset of training images for guiding the learning. The proposed\napproach is illustrated with detection of diabetic macular edema (DME) from OCT\nslices. Results of testing on on a large public dataset show that with just a\nthird of images with roughly segmented fluid filled regions, the classification\naccuracy is on par with state of the art methods while providing a good\nexplanation in the form of anatomically accurate heatmap /region of interest.\nThe proposed solution is then adapted to Breast Cancer detection from\nmammographic images. Good evaluation results on public datasets underscores the\ngeneralisability of the proposed solution.</p>\n", "tags": [] },
{"key": "kang2019interpreting", "year": "2019", "title":"Interpreting Undesirable Pixels for Image Classification on Black-Box Models", "abstract": "<p>In an effort to interpret black-box models, researches for developing\nexplanation methods have proceeded in recent years. Most studies have tried to\nidentify input pixels that are crucial to the prediction of a classifier. While\nthis approach is meaningful to analyse the characteristic of blackbox models,\nit is also important to investigate pixels that interfere with the prediction.\nTo tackle this issue, in this paper, we propose an explanation method that\nvisualizes undesirable regions to classify an image as a target class. To be\nspecific, we divide the concept of undesirable regions into two terms: (1)\nfactors for a target class, which hinder that black-box models identify\nintrinsic characteristics of a target class and (2) factors for non-target\nclasses that are important regions for an image to be classified as other\nclasses. We visualize such undesirable regions on heatmaps to qualitatively\nvalidate the proposed method. Furthermore, we present an evaluation metric to\nprovide quantitative results on ImageNet.</p>\n", "tags": [] },
{"key": "kang2020multivariate", "year": "2020", "title":"Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs", "abstract": "<p>Information visualization significantly enhances human perception by\ngraphically representing complex data sets. The variety of visualization\ndesigns makes it challenging to efficiently evaluate all possible designs\ncatering to users’ preferences and characteristics. Most of existing evaluation\nmethods perform user studies to obtain multivariate qualitative responses from\nusers via questionnaires and interviews. However, these methods cannot support\nonline evaluation of designs as they are often time-consuming. A statistical\nmodel is desired to predict users’ preferences on visualization designs based\non non-interference measurements (i.e., wearable sensor signals). In this work,\nwe propose a multivariate regression of mixed responses (MRMR) to facilitate\nquantitative evaluation of visualization designs. The proposed MRMR method is\nable to provide accurate model prediction with meaningful variable selection. A\nsimulation study and a user study of evaluating visualization designs with 14\neffective participants are conducted to illustrate the merits of the proposed\nmodel.</p>\n", "tags": [] },
{"key": "karimi2019model", "year": "2019", "title":"Model-Agnostic Counterfactual Explanations for Consequential Decisions", "abstract": "<p>Predictive models are being increasingly used to support consequential\ndecision making at the individual level in contexts such as pretrial bail and\nloan approval. As a result, there is increasing social and legal pressure to\nprovide explanations that help the affected individuals not only to understand\nwhy a prediction was output, but also how to act to obtain a desired outcome.\nTo this end, several works have proposed optimization-based methods to generate\nnearest counterfactual explanations. However, these methods are often\nrestricted to a particular subset of models (e.g., decision trees or linear\nmodels) and differentiable distance functions. In contrast, we build on\nstandard theory and tools from formal verification and propose a novel\nalgorithm that solves a sequence of satisfiability problems, where both the\ndistance function (objective) and predictive model (constraints) are\nrepresented as logic formulae. As shown by our experiments on real-world data,\nour algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable,\n{non-}convex); ii) data-type-agnostic (heterogeneous features); iii)\ndistance-agnostic ($\\ell_0, \\ell_1, \\ell_\\infty$, and combinations thereof);\niv) able to generate plausible and diverse counterfactuals for any sample\n(i.e., 100% coverage); and v) at provably optimal distances.</p>\n", "tags": [] },
{"key": "khakzar2022explanations", "year": "2022", "title":"Do Explanations Explain? Model Knows Best", "abstract": "<p>It is a mystery which input features contribute to a neural network’s output.\nVarious explanation (feature attribution) methods are proposed in the\nliterature to shed light on the problem. One peculiar observation is that these\nexplanations (attributions) point to different features as being important. The\nphenomenon raises the question, which explanation to trust? We propose a\nframework for evaluating the explanations using the neural network model\nitself. The framework leverages the network to generate input features that\nimpose a particular behavior on the output. Using the generated features, we\ndevise controlled experimental setups to evaluate whether an explanation method\nconforms to an axiom. Thus we propose an empirical framework for axiomatic\nevaluation of explanation methods. We evaluate well-known and promising\nexplanation solutions using the proposed framework. The framework provides a\ntoolset to reveal properties and drawbacks within existing and future\nexplanation solutions.</p>\n", "tags": [] },
{"key": "kim2019learning", "year": "2019", "title":"Learning Interpretable Models with Causal Guarantees", "abstract": "<p>Machine learning has shown much promise in helping improve the quality of\nmedical, legal, and financial decision-making. In these applications, machine\nlearning models must satisfy two important criteria: (i) they must be causal,\nsince the goal is typically to predict individual treatment effects, and (ii)\nthey must be interpretable, so that human decision makers can validate and\ntrust the model predictions. There has recently been much progress along each\ndirection independently, yet the state-of-the-art approaches are fundamentally\nincompatible. We propose a framework for learning interpretable models from\nobservational data that can be used to predict individual treatment effects\n(ITEs). In particular, our framework converts any supervised learning algorithm\ninto an algorithm for estimating ITEs. Furthermore, we prove an error bound on\nthe treatment effects predicted by our model. Finally, in an experiment on\nreal-world data, we show that the models trained using our framework\nsignificantly outperform a number of baselines.</p>\n", "tags": [] },
{"key": "kim2021sanity", "year": "2021", "title":"Sanity Simulations for Saliency Methods", "abstract": "<p>Saliency methods are a popular class of feature attribution explanation\nmethods that aim to capture a model’s predictive reasoning by identifying\n“important” pixels in an input image. However, the development and adoption of\nthese methods are hindered by the lack of access to ground-truth model\nreasoning, which prevents accurate evaluation. In this work, we design a\nsynthetic benchmarking framework, SMERF, that allows us to perform\nground-truth-based evaluation while controlling the complexity of the model’s\nreasoning. Experimentally, SMERF reveals significant limitations in existing\nsaliency methods and, as a result, represents a useful tool for the development\nof new saliency methods.</p>\n", "tags": [] },
{"key": "kindermans2017unreliability", "year": "2017", "title":"The (Un)reliability of saliency methods", "abstract": "<p>Saliency methods aim to explain the predictions of deep neural networks.\nThese methods lack reliability when the explanation is sensitive to factors\nthat do not contribute to the model prediction. We use a simple and common\npre-processing step —adding a constant shift to the input data— to show\nthat a transformation with no effect on the model can cause numerous methods to\nincorrectly attribute. In order to guarantee reliability, we posit that methods\nshould fulfill input invariance, the requirement that a saliency method mirror\nthe sensitivity of the model with respect to transformations of the input. We\nshow, through several examples, that saliency methods that do not satisfy input\ninvariance result in misleading attribution.</p>\n", "tags": [] },
{"key": "koh2017understanding", "year": "2017", "title":"Understanding Black-box Predictions via Influence Functions", "abstract": "<p>How can we explain the predictions of a black-box model? In this paper, we\nuse influence functions – a classic technique from robust statistics – to\ntrace a model’s prediction through the learning algorithm and back to its\ntraining data, thereby identifying training points most responsible for a given\nprediction. To scale up influence functions to modern machine learning\nsettings, we develop a simple, efficient implementation that requires only\noracle access to gradients and Hessian-vector products. We show that even on\nnon-convex and non-differentiable models where the theory breaks down,\napproximations to influence functions can still provide valuable information.\nOn linear models and convolutional neural networks, we demonstrate that\ninfluence functions are useful for multiple purposes: understanding model\nbehavior, debugging models, detecting dataset errors, and even creating\nvisually-indistinguishable training-set attacks.</p>\n", "tags": [] },
{"key": "kokhlikyan2021investigating", "year": "2021", "title":"Investigating sanity checks for saliency maps with image and text classification", "abstract": "<p>Saliency maps have shown to be both useful and misleading for explaining\nmodel predictions especially in the context of images. In this paper, we\nperform sanity checks for text modality and show that the conclusions made for\nimage do not directly transfer to text. We also analyze the effects of the\ninput multiplier in certain saliency maps using similarity scores,\nmax-sensitivity and infidelity evaluation metrics. Our observations reveal that\nthe input multiplier carries input’s structural patterns in explanation maps,\nthus leading to similar results regardless of the choice of model parameters.\nWe also show that the smoothness of a Neural Network (NN) function can affect\nthe quality of saliency-based explanations. Our investigations reveal that\nreplacing ReLUs with Softplus and MaxPool with smoother variants such as\nLogSumExp (LSE) can lead to explanations that are more reliable based on the\ninfidelity evaluation metric.</p>\n", "tags": [] },
{"key": "kong2021deeprare", "year": "2021", "title":"DeepRare: Generic Unsupervised Visual Attention Models", "abstract": "<p>Human visual system is modeled in engineering field providing\nfeature-engineered methods which detect contrasted/surprising/unusual data into\nimages. This data is “interesting” for humans and leads to numerous\napplications. Deep learning (DNNs) drastically improved the algorithms\nefficiency on the main benchmark datasets. However, DNN-based models are\ncounter-intuitive: surprising or unusual data is by definition difficult to\nlearn because of its low occurrence probability. In reality, DNN-based models\nmainly learn top-down features such as faces, text, people, or animals which\nusually attract human attention, but they have low efficiency in extracting\nsurprising or unusual data in the images. In this paper, we propose a new\nvisual attention model called DeepRare2021 (DR21) which uses the power of DNNs\nfeature extraction and the genericity of feature-engineered algorithms. This\nalgorithm is an evolution of a previous version called DeepRare2019 (DR19)\nbased on a common framework. DR21 1) does not need any training and uses the\ndefault ImageNet training, 2) is fast even on CPU, 3) is tested on four very\ndifferent eye-tracking datasets showing that the DR21 is generic and is always\nin the within the top models on all datasets and metrics while no other model\nexhibits such a regularity and genericity. Finally DR21 4) is tested with\nseveral network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2\n(MN2) and 5) it provides explanation and transparency on which parts of the\nimage are the most surprising at different levels despite the use of a\nDNN-based feature extractor. DeepRare2021 code can be found at\nhttps://github.com/numediart/VisualAttention-RareFamil}.</p>\n", "tags": [] },
{"key": "konstantinov2021attention", "year": "2021", "title":"Attention-like feature explanation for tabular data", "abstract": "<p>A new method for local and global explanation of the machine learning\nblack-box model predictions by tabular data is proposed. It is implemented as a\nsystem called AFEX (Attention-like Feature EXplanation) and consisting of two\nmain parts. The first part is a set of the one-feature neural subnetworks which\naim to get a specific representation for every feature in the form of a basis\nof shape functions. The subnetworks use shortcut connections with trainable\nparameters to improve the network performance. The second part of AFEX produces\nshape functions of features as the weighted sum of the basis shape functions\nwhere weights are computed by using an attention-like mechanism. AFEX\nidentifies pairwise interactions between features based on pairwise\nmultiplications of shape functions corresponding to different features. A\nmodification of AFEX with incorporating an additional surrogate model which\napproximates the black-box model is proposed. AFEX is trained end-to-end on a\nwhole dataset only once such that it does not require to train neural networks\nagain in the explanation stage. Numerical experiments with synthetic and real\ndata illustrate AFEX.</p>\n", "tags": [] },
{"key": "krause2017workflow", "year": "2017", "title":"A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations", "abstract": "<p>Human-in-the-loop data analysis applications necessitate greater transparency\nin machine learning models for experts to understand and trust their decisions.\nTo this end, we propose a visual analytics workflow to help data scientists and\ndomain experts explore, diagnose, and understand the decisions made by a binary\nclassifier. The approach leverages “instance-level explanations”, measures of\nlocal feature relevance that explain single instances, and uses them to build a\nset of visual representations that guide the users in their investigation. The\nworkflow is based on three main visual representations and steps: one based on\naggregate statistics to see how data distributes across correct / incorrect\ndecisions; one based on explanations to understand which features are used to\nmake these decisions; and one based on raw data, to derive insights on\npotential root causes for the observed patterns. The workflow is derived from a\nlong-term collaboration with a group of machine learning and healthcare\nprofessionals who used our method to make sense of machine learning models they\ndeveloped. The case study from this collaboration demonstrates that the\nproposed workflow helps experts derive useful knowledge about the model and the\nphenomena it describes, thus experts can generate useful hypotheses on how a\nmodel can be improved.</p>\n", "tags": [] },
{"key": "kumar2016understanding", "year": "2016", "title":"Understanding Anatomy Classification Through Attentive Response Maps", "abstract": "<p>One of the main challenges for broad adoption of deep learning based models\nsuch as convolutional neural networks (CNN), is the lack of understanding of\ntheir decisions. In many applications, a simpler, less capable model that can\nbe easily understood is favorable to a black-box model that has superior\nperformance. In this paper, we present an approach for designing CNNs based on\nvisualization of the internal activations of the model. We visualize the\nmodel’s response through attentive response maps obtained using a fractional\nstride convolution technique and compare the results with known imaging\nlandmarks from the medical literature. We show that sufficiently deep and\ncapable models can be successfully trained to use the same medical landmarks a\nhuman expert would use. Our approach allows for communicating the model\ndecision process well, but also offers insight towards detecting biases.</p>\n", "tags": [] },
{"key": "lage2019human", "year": "2019", "title":"Human Evaluation of Models Built for Interpretability", "abstract": "<p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>\n", "tags": ["evaluation"] },
{"key": "lai2019many", "year": "2019", "title":"Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification", "abstract": "<p>Feature importance is commonly used to explain machine predictions. While\nfeature importance can be derived from a machine learning model with a variety\nof methods, the consistency of feature importance via different methods remains\nunderstudied. In this work, we systematically compare feature importance from\nbuilt-in mechanisms in a model such as attention values and post-hoc methods\nthat approximate model behavior such as LIME. Using text classification as a\ntestbed, we find that 1) no matter which method we use, important features from\ntraditional models such as SVM and XGBoost are more similar with each other,\nthan with deep learning models; 2) post-hoc methods tend to generate more\nsimilar important features for two models than built-in methods. We further\ndemonstrate how such similarity varies across instances. Notably, important\nfeatures do not always resemble each other better when two models agree on the\npredicted label than when they disagree.</p>\n", "tags": [] },
{"key": "lakkaraju2017interpretable", "year": "2017", "title":"Interpretable & Explorable Approximations of Black Box Models", "abstract": "<p>We propose Black Box Explanations through Transparent Approximations (BETA),\na novel model agnostic framework for explaining the behavior of any black-box\nclassifier by simultaneously optimizing for fidelity to the original model and\ninterpretability of the explanation. To this end, we develop a novel objective\nfunction which allows us to learn (with optimality guarantees), a small number\nof compact decision sets each of which explains the behavior of the black box\nmodel in unambiguous, well-defined regions of feature space. Furthermore, our\nframework also is capable of accepting user input when generating these\napproximations, thus allowing users to interactively explore how the black-box\nmodel behaves in different subspaces that are of interest to the user. To the\nbest of our knowledge, this is the first approach which can produce global\nexplanations of the behavior of any given black box model through joint\noptimization of unambiguity, fidelity, and interpretability, while also\nallowing users to explore model behavior based on their preferences.\nExperimental evaluation with real-world datasets and user studies demonstrates\nthat our approach can generate highly compact, easy-to-understand, yet accurate\napproximations of various kinds of predictive models compared to\nstate-of-the-art baselines.</p>\n", "tags": [] },
{"key": "lakkaraju2019how", "year": "2019", "title":"\"How do I fool you?\": Manipulating User Trust via Misleading Black Box Explanations", "abstract": "<p>As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a human\ninterpretable manner. It has recently become apparent that a high-fidelity\nexplanation of a black box ML model may not accurately reflect the biases in\nthe black box. As a consequence, explanations have the potential to mislead\nhuman users into trusting a problematic black box. In this work, we rigorously\nexplore the notion of misleading explanations and how they influence user trust\nin black-box models. More specifically, we propose a novel theoretical\nframework for understanding and generating misleading explanations, and carry\nout a user study with domain experts to demonstrate how these explanations can\nbe used to mislead users. Our work is the first to empirically establish how\nuser trust in black box models can be manipulated via misleading explanations.</p>\n", "tags": [] },
{"key": "lampinen2021tell", "year": "2021", "title":"Tell me why! Explanations support learning relational and causal structure", "abstract": "<p>Inferring the abstract relational and causal structure of the world is a\nmajor challenge for reinforcement-learning (RL) agents. For humans,\nlanguage–particularly in the form of explanations–plays a considerable role\nin overcoming this challenge. Here, we show that language can play a similar\nrole for deep RL agents in complex environments. While agents typically\nstruggle to acquire relational and causal knowledge, augmenting their\nexperience by training them to predict language descriptions and explanations\ncan overcome these limitations. We show that language can help agents learn\nchallenging relational tasks, and examine which aspects of language contribute\nto its benefits. We then show that explanations can help agents to infer not\nonly relational but also causal structure. Language can shape the way that\nagents to generalize out-of-distribution from ambiguous, causally-confounded\ntraining, and explanations even allow agents to learn to perform experimental\ninterventions to identify causal relationships. Our results suggest that\nlanguage description and explanation may be powerful tools for improving agent\nlearning and generalization.</p>\n", "tags": [] },
{"key": "langer2021want", "year": "2021", "title":"What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research", "abstract": "<p>Previous research in Explainable Artificial Intelligence (XAI) suggests that\na main aim of explainability approaches is to satisfy specific interests,\ngoals, expectations, needs, and demands regarding artificial systems (we call\nthese stakeholders’ desiderata) in a variety of contexts. However, the\nliterature on XAI is vast, spreads out across multiple largely disconnected\ndisciplines, and it often remains unclear how explainability approaches are\nsupposed to achieve the goal of satisfying stakeholders’ desiderata. This paper\ndiscusses the main classes of stakeholders calling for explainability of\nartificial systems and reviews their desiderata. We provide a model that\nexplicitly spells out the main concepts and relations necessary to consider and\ninvestigate when evaluating, adjusting, choosing, and developing explainability\napproaches that aim to satisfy stakeholders’ desiderata. This model can serve\nresearchers from the variety of different disciplines involved in XAI as a\ncommon ground. It emphasizes where there is interdisciplinary potential in the\nevaluation and the development of explainability approaches.</p>\n", "tags": [] },
{"key": "leavitt2020towards", "year": "2020", "title":"Towards falsifiable interpretability research", "abstract": "<p>Methods for understanding the decisions of and mechanisms underlying deep\nneural networks (DNNs) typically rely on building intuition by emphasizing\nsensory or semantic features of individual examples. For instance, methods aim\nto visualize the components of an input which are “important” to a network’s\ndecision, or to measure the semantic properties of single neurons. Here, we\nargue that interpretability research suffers from an over-reliance on\nintuition-based approaches that risk-and in some cases have caused-illusory\nprogress and misleading conclusions. We identify a set of limitations that we\nargue impede meaningful progress in interpretability research, and examine two\npopular classes of interpretability methods-saliency and single-neuron-based\napproaches-that serve as case studies for how overreliance on intuition and\nlack of falsifiability can undermine interpretability research. To address\nthese concerns, we propose a strategy to address these impediments in the form\nof a framework for strongly falsifiable interpretability research. We encourage\nresearchers to use their intuitions as a starting point to develop and test\nclear, falsifiable hypotheses, and hope that our framework yields robust,\nevidence-based interpretability methods that generate meaningful advances in\nour understanding of DNNs.</p>\n", "tags": [] },
{"key": "lecue2018semantic", "year": "2018", "title":"Semantic Explanations of Predictions", "abstract": "<p>The main objective of explanations is to transmit knowledge to humans. This\nwork proposes to construct informative explanations for predictions made from\nmachine learning models. Motivated by the observations from social sciences,\nour approach selects data points from the training sample that exhibit special\ncharacteristics crucial for explanation, for instance, ones contrastive to the\nclassification prediction and ones representative of the models. Subsequently,\nsemantic concepts are derived from the selected data points through the use of\ndomain ontologies. These concepts are filtered and ranked to produce\ninformative explanations that improves human understanding. The main features\nof our approach are that (1) knowledge about explanations is captured in the\nform of ontological concepts, (2) explanations include contrastive evidences in\naddition to normal evidences, and (3) explanations are user relevant.</p>\n", "tags": [] },
{"key": "lee2016going", "year": "2016", "title":"Going Deeper with Contextual CNN for Hyperspectral Image Classification", "abstract": "<p>In this paper, we describe a novel deep convolutional neural network (CNN)\nthat is deeper and wider than other existing deep networks for hyperspectral\nimage classification. Unlike current state-of-the-art approaches in CNN-based\nhyperspectral image classification, the proposed network, called contextual\ndeep CNN, can optimally explore local contextual interactions by jointly\nexploiting local spatio-spectral relationships of neighboring individual pixel\nvectors. The joint exploitation of the spatio-spectral information is achieved\nby a multi-scale convolutional filter bank used as an initial component of the\nproposed CNN pipeline. The initial spatial and spectral feature maps obtained\nfrom the multi-scale filter bank are then combined together to form a joint\nspatio-spectral feature map. The joint feature map representing rich spectral\nand spatial properties of the hyperspectral image is then fed through a fully\nconvolutional network that eventually predicts the corresponding label of each\npixel vector. The proposed approach is tested on three benchmark datasets: the\nIndian Pines dataset, the Salinas dataset and the University of Pavia dataset.\nPerformance comparison shows enhanced classification performance of the\nproposed approach over the current state-of-the-art on the three datasets.</p>\n", "tags": [] },
{"key": "lertvittayakumjorn2019human-grounded", "year": "2019", "title":"Human-grounded Evaluations of Explanation Methods for Text Classification", "abstract": "<p>Due to the black-box nature of deep learning models, methods for explaining\nthe models’ results are crucial to gain trust from humans and support\ncollaboration between AIs and humans. In this paper, we consider several\nmodel-agnostic and model-specific explanation methods for CNNs for text\nclassification and conduct three human-grounded evaluations, focusing on\ndifferent purposes of explanations: (1) revealing model behavior, (2)\njustifying model predictions, and (3) helping humans investigate uncertain\npredictions. The results highlight dissimilar qualities of the various\nexplanation methods we consider and show the degree to which these methods\ncould serve for each purpose.</p>\n", "tags": [] },
{"key": "lertvittayakumjorn2022argumentative", "year": "2022", "title":"Argumentative Explanations for Pattern-Based Text Classifiers", "abstract": "<p>Recent works in Explainable AI mostly address the transparency issue of\nblack-box models or create explanations for any kind of models (i.e., they are\nmodel-agnostic), while leaving explanations of interpretable models largely\nunderexplored. In this paper, we fill this gap by focusing on explanations for\na specific interpretable model, namely pattern-based logistic regression (PLR)\nfor binary text classification. We do so because, albeit interpretable, PLR is\nchallenging when it comes to explanations. In particular, we found that a\nstandard way to extract explanations from this model does not consider\nrelations among the features, making the explanations hardly plausible to\nhumans. Hence, we propose AXPLR, a novel explanation method using (forms of)\ncomputational argumentation to generate explanations (for outputs computed by\nPLR) which unearth model agreements and disagreements among the features.\nSpecifically, we use computational argumentation as follows: we see features\n(patterns) in PLR as arguments in a form of quantified bipolar argumentation\nframeworks (QBAFs) and extract attacks and supports between arguments based on\nspecificity of the arguments; we understand logistic regression as a gradual\nsemantics for these QBAFs, used to determine the arguments’ dialectic strength;\nand we study standard properties of gradual semantics for QBAFs in the context\nof our argumentative re-interpretation of PLR, sanctioning its suitability for\nexplanatory purposes. We then show how to extract intuitive explanations (for\noutputs computed by PLR) from the constructed QBAFs. Finally, we conduct an\nempirical evaluation and two experiments in the context of human-AI\ncollaboration to demonstrate the advantages of our resulting AXPLR method.</p>\n", "tags": [] },
{"key": "letham2015interpretable", "year": "2015", "title":"Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model", "abstract": "<p>We aim to produce predictive models that are not only accurate, but are also\ninterpretable to human experts. Our models are decision lists, which consist of\na series of if…then… statements (e.g., if high blood pressure, then stroke)\nthat discretize a high-dimensional, multivariate feature space into a series of\nsimple, readily interpretable decision statements. We introduce a generative\nmodel called Bayesian Rule Lists that yields a posterior distribution over\npossible decision lists. It employs a novel prior structure to encourage\nsparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy\non par with the current top algorithms for prediction in machine learning. Our\nmethod is motivated by recent developments in personalized medicine, and can be\nused to produce highly accurate and interpretable medical scoring systems. We\ndemonstrate this by producing an alternative to the CHADS$_2$ score, actively\nused in clinical practice for estimating the risk of stroke in patients that\nhave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more\naccurate.</p>\n", "tags": [] },
{"key": "li2015visual", "year": "2015", "title":"Visual Saliency Based on Multiscale Deep Features", "abstract": "<p>Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this CVPR 2015 paper, we discover that\na high-quality visual saliency model can be trained with multiscale features\nextracted using a popular deep learning architecture, convolutional neural\nnetworks (CNNs), which have had many successes in visual recognition tasks. For\nlearning such saliency models, we introduce a neural network architecture,\nwhich has fully connected layers on top of CNNs responsible for extracting\nfeatures at three different scales. We then propose a refinement method to\nenhance the spatial coherence of our saliency results. Finally, aggregating\nmultiple saliency maps computed for different levels of image segmentation can\nfurther boost the performance, yielding saliency maps better than those\ngenerated from a single segmentation. To promote further research and\nevaluation of visual saliency models, we also construct a new large database of\n4447 challenging images and their pixelwise saliency annotation. Experimental\nresults demonstrate that our proposed method is capable of achieving\nstate-of-the-art performance on all public benchmarks, improving the F-Measure\nby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset\n(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively\non these two datasets.</p>\n", "tags": [] },
{"key": "li2019deepgcns", "year": "2019", "title":"DeepGCNs: Can GCNs Go as Deep as CNNs?", "abstract": "<p>Convolutional Neural Networks (CNNs) achieve impressive performance in a wide\nvariety of fields. Their success benefited from a massive boost when very deep\nCNN models were able to be reliably trained. Despite their merits, CNNs fail to\nproperly address problems with non-Euclidean data. To overcome this challenge,\nGraph Convolutional Networks (GCNs) build graphs to represent non-Euclidean\ndata, borrow concepts from CNNs, and apply them in training. GCNs show\npromising results, but they are usually limited to very shallow models due to\nthe vanishing gradient problem. As a result, most state-of-the-art GCN models\nare no deeper than 3 or 4 layers. In this work, we present new ways to\nsuccessfully train very deep GCNs. We do this by borrowing concepts from CNNs,\nspecifically residual/dense connections and dilated convolutions, and adapting\nthem to GCN architectures. Extensive experiments show the positive effect of\nthese deep GCN frameworks. Finally, we use these new concepts to build a very\ndeep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU\nover state-of-the-art) in the task of point cloud semantic segmentation. We\nbelieve that the community can greatly benefit from this work, as it opens up\nmany opportunities for advancing GCN-based research.</p>\n", "tags": [] },
{"key": "li2019interpretable", "year": "2019", "title":"Interpretable Neural Network Decoupling", "abstract": "<p>The remarkable performance of convolutional neural networks (CNNs) is\nentangled with their huge number of uninterpretable parameters, which has\nbecome the bottleneck limiting the exploitation of their full potential.\nTowards network interpretation, previous endeavors mainly resort to the single\nfilter analysis, which however ignores the relationship between filters. In\nthis paper, we propose a novel architecture decoupling method to interpret the\nnetwork from a perspective of investigating its calculation paths. More\nspecifically, we introduce a novel architecture controlling module in each\nlayer to encode the network architecture by a vector. By maximizing the mutual\ninformation between the vectors and input images, the module is trained to\nselect specific filters to distill a unique calculation path for each input.\nFurthermore, to improve the interpretability and compactness of the decoupled\nnetwork, the output of each layer is encoded to align the architecture encoding\nvector with the constraint of sparsity regularization. Unlike conventional\npixel-level or filter-level network interpretation methods, we propose a\npath-level analysis to explore the relationship between the combination of\nfilter and semantic concepts, which is more suitable to interpret the working\nrationale of the decoupled network. Extensive experiments show that the\ndecoupled network achieves several applications, i.e., network interpretation,\nnetwork acceleration, and adversarial samples detection.</p>\n", "tags": [] },
{"key": "li2021interpretable", "year": "2021", "title":"Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond", "abstract": "<p>Deep neural networks have been well-known for their superb handling of\nvarious machine learning and artificial intelligence tasks. However, due to\ntheir over-parameterized black-box nature, it is often difficult to understand\nthe prediction results of deep models. In recent years, many interpretation\ntools have been proposed to explain or reveal how deep models make decisions.\nIn this paper, we review this line of research and try to make a comprehensive\nsurvey. Specifically, we first introduce and clarify two basic concepts –\ninterpretations and interpretability – that people usually get confused about.\nTo address the research efforts in interpretations, we elaborate the designs of\na number of interpretation algorithms, from different perspectives, by\nproposing a new taxonomy. Then, to understand the interpretation results, we\nalso survey the performance metrics for evaluating interpretation algorithms.\nFurther, we summarize the current works in evaluating models’ interpretability\nusing “trustworthy” interpretation algorithms. Finally, we review and discuss\nthe connections between deep models’ interpretations and other factors, such as\nadversarial robustness and learning from interpretations, and we introduce\nseveral open-source libraries for interpretation algorithms and evaluation\napproaches.</p>\n", "tags": [] },
{"key": "liang2017interpretable", "year": "2017", "title":"Interpretable Structure-Evolving LSTM", "abstract": "<p>This paper develops a general framework for learning interpretable data\nrepresentation via Long Short-Term Memory (LSTM) recurrent neural networks over\nhierarchal graph structures. Instead of learning LSTM models over the pre-fixed\nstructures, we propose to further learn the intermediate interpretable\nmulti-level graph structures in a progressive and stochastic way from data\nduring the LSTM network optimization. We thus call this model the\nstructure-evolving LSTM. In particular, starting with an initial element-level\ngraph representation where each node is a small data element, the\nstructure-evolving LSTM gradually evolves the multi-level graph representations\nby stochastically merging the graph nodes with high compatibilities along the\nstacked LSTM layers. In each LSTM layer, we estimate the compatibility of two\nconnected nodes from their corresponding LSTM gate outputs, which is used to\ngenerate a merging probability. The candidate graph structures are accordingly\ngenerated where the nodes are grouped into cliques with their merging\nprobabilities. We then produce the new graph structure with a\nMetropolis-Hasting algorithm, which alleviates the risk of getting stuck in\nlocal optimums by stochastic sampling with an acceptance probability. Once a\ngraph structure is accepted, a higher-level graph is then constructed by taking\nthe partitioned cliques as its nodes. During the evolving process,\nrepresentation becomes more abstracted in higher-levels where redundant\ninformation is filtered out, allowing more efficient propagation of long-range\ndata dependencies. We evaluate the effectiveness of structure-evolving LSTM in\nthe application of semantic object parsing and demonstrate its advantage over\nstate-of-the-art LSTM models on standard benchmarks.</p>\n", "tags": [] },
{"key": "lin2020see", "year": "2020", "title":"What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors", "abstract": "<p>EXplainable AI (XAI) methods have been proposed to interpret how a deep\nneural network predicts inputs through model saliency explanations that\nhighlight the parts of the inputs deemed important to arrive a decision at a\nspecific target. However, it remains challenging to quantify correctness of\ntheir interpretability as current evaluation approaches either require\nsubjective input from humans or incur high computation cost with automated\nevaluation. In this paper, we propose backdoor trigger patterns–hidden\nmalicious functionalities that cause misclassification–to automate the\nevaluation of saliency explanations. Our key observation is that triggers\nprovide ground truth for inputs to evaluate whether the regions identified by\nan XAI method are truly relevant to its output. Since backdoor triggers are the\nmost important features that cause deliberate misclassification, a robust XAI\nmethod should reveal their presence at inference time. We introduce three\ncomplementary metrics for systematic evaluation of explanations that an XAI\nmethod generates and evaluate seven state-of-the-art model-free and\nmodel-specific posthoc methods through 36 models trojaned with specifically\ncrafted triggers using color, shape, texture, location, and size. We discovered\nsix methods that use local explanation and feature relevance fail to completely\nhighlight trigger regions, and only a model-free approach can uncover the\nentire trigger region.</p>\n", "tags": [] },
{"key": "lipton2017mythos", "year": "2017", "title":"The Mythos of Model Interpretability", "abstract": "<p>Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.</p>\n", "tags": ["understanding"] },
{"key": "liu2019towards", "year": "2019", "title":"Towards Visually Explaining Variational Autoencoders", "abstract": "<p>Recent advances in Convolutional Neural Network (CNN) model interpretability\nhave led to impressive progress in visualizing and understanding model\npredictions. In particular, gradient-based visual attention methods have driven\nmuch recent effort in using visual attention maps as a means for visual\nexplanations. A key problem, however, is these methods are designed for\nclassification and categorization tasks, and their extension to explaining\ngenerative models, e.g. variational autoencoders (VAE) is not trivial. In this\nwork, we take a step towards bridging this crucial gap, proposing the first\ntechnique to visually explain VAEs by means of gradient-based attention. We\npresent methods to generate visual attention from the learned latent space, and\nalso demonstrate such attention explanations serve more than just explaining\nVAE predictions. We show how these attention maps can be used to localize\nanomalies in images, demonstrating state-of-the-art performance on the MVTec-AD\ndataset. We also show how they can be infused into model training, helping\nbootstrap the VAE into learning improved latent space disentanglement,\ndemonstrated on the Dsprites dataset.</p>\n", "tags": [] },
{"key": "liu2021going", "year": "2021", "title":"Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models", "abstract": "<p>Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer’s\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.</p>\n", "tags": [] },
{"key": "liu2021synthetic", "year": "2021", "title":"Synthetic Benchmarks for Scientific Research in Explainable Machine Learning", "abstract": "<p>As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. This has spurred a flurry of research in model\nexplainability and has given rise to feature attribution methods such as LIME\nand SHAP. Despite their widespread use, evaluating and comparing different\nfeature attribution methods remains challenging: evaluations ideally require\nhuman studies, and empirical evaluation metrics are often data-intensive or\ncomputationally prohibitive on real-world datasets. In this work, we address\nthis issue by releasing XAI-Bench: a suite of synthetic datasets along with a\nlibrary for benchmarking feature attribution algorithms. Unlike real-world\ndatasets, synthetic datasets allow the efficient computation of conditional\nexpected values that are needed to evaluate ground-truth Shapley values and\nother metrics. The synthetic datasets we release offer a wide variety of\nparameters that can be configured to simulate real-world data. We demonstrate\nthe power of our library by benchmarking popular explainability techniques\nacross several evaluation metrics and across a variety of settings. The\nversatility and efficiency of our library will help researchers bring their\nexplainability methods from development to deployment. Our code is available at\nhttps://github.com/abacusai/xai-bench.</p>\n", "tags": [] },
{"key": "lundberg2017unified", "year": "2017", "title":"A Unified Approach to Interpreting Model Predictions", "abstract": "<p>Understanding why a model makes a certain prediction can be as crucial as the\nprediction’s accuracy in many applications. However, the highest accuracy for\nlarge modern datasets is often achieved by complex models that even experts\nstruggle to interpret, such as ensemble or deep learning models, creating a\ntension between accuracy and interpretability. In response, various methods\nhave recently been proposed to help users interpret the predictions of complex\nmodels, but it is often unclear how these methods are related and when one\nmethod is preferable over another. To address this problem, we present a\nunified framework for interpreting predictions, SHAP (SHapley Additive\nexPlanations). SHAP assigns each feature an importance value for a particular\nprediction. Its novel components include: (1) the identification of a new class\nof additive feature importance measures, and (2) theoretical results showing\nthere is a unique solution in this class with a set of desirable properties.\nThe new class unifies six existing methods, notable because several recent\nmethods in the class lack the proposed desirable properties. Based on insights\nfrom this unification, we present new methods that show improved computational\nperformance and/or better consistency with human intuition than previous\napproaches.</p>\n", "tags": [] },
{"key": "mahendran2014understanding", "year": "2014", "title":"Understanding Deep Image Representations by Inverting Them", "abstract": "<p>Image representations, from SIFT and Bag of Visual Words to Convolutional\nNeural Networks (CNNs), are a crucial component of almost any image\nunderstanding system. Nevertheless, our understanding of them remains limited.\nIn this paper we conduct a direct analysis of the visual information contained\nin representations by asking the following question: given an encoding of an\nimage, to which extent is it possible to reconstruct the image itself? To\nanswer this question we contribute a general framework to invert\nrepresentations. We show that this method can invert representations such as\nHOG and SIFT more accurately than recent alternatives while being applicable to\nCNNs too. We then use this technique to study the inverse of recent\nstate-of-the-art CNN image representations for the first time. Among our\nfindings, we show that several layers in CNNs retain photographically accurate\ninformation about the image, with different degrees of geometric and\nphotometric invariance.</p>\n", "tags": [] },
{"key": "mangla2020saliency", "year": "2020", "title":"On Saliency Maps and Adversarial Robustness", "abstract": "<p>A Very recent trend has emerged to couple the notion of interpretability and\nadversarial robustness, unlike earlier efforts which solely focused on good\ninterpretations or robustness against adversaries. Works have shown that\nadversarially trained models exhibit more interpretable saliency maps than\ntheir non-robust counterparts, and that this behavior can be quantified by\nconsidering the alignment between input image and saliency map. In this work,\nwe provide a different perspective to this coupling, and provide a method,\nSaliency based Adversarial training (SAT), to use saliency maps to improve\nadversarial robustness of a model. In particular, we show that using\nannotations such as bounding boxes and segmentation masks, already provided\nwith a dataset, as weak saliency maps, suffices to improve adversarial\nrobustness with no additional effort to generate the perturbations themselves.\nOur empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17\ndatasets consistently corroborate our claim, by showing improved adversarial\nrobustness using our method. saliency maps. We also show how using finer and\nstronger saliency maps leads to more robust models, and how integrating SAT\nwith existing adversarial training methods, further boosts performance of these\nexisting methods.</p>\n", "tags": [] },
{"key": "marcos2020contextual", "year": "2020", "title":"Contextual Semantic Interpretability", "abstract": "<p>Convolutional neural networks (CNN) are known to learn an image\nrepresentation that captures concepts relevant to the task, but do so in an\nimplicit way that hampers model interpretability. However, one could argue that\nsuch a representation is hidden in the neurons and can be made explicit by\nteaching the model to recognize semantically interpretable attributes that are\npresent in the scene. We call such an intermediate layer a \\emph{semantic\nbottleneck}. Once the attributes are learned, they can be re-combined to reach\nthe final decision and provide both an accurate prediction and an explicit\nreasoning behind the CNN decision. In this paper, we look into semantic\nbottlenecks that capture context: we want attributes to be in groups of a few\nmeaningful elements and participate jointly to the final decision. We use a\ntwo-layer semantic bottleneck that gathers attributes into interpretable,\nsparse groups, allowing them contribute differently to the final output\ndepending on the context. We test our contextual semantic interpretable\nbottleneck (CSIB) on the task of landscape scenicness estimation and train the\nsemantic interpretable bottleneck using an auxiliary database (SUN Attributes).\nOur model yields in predictions as accurate as a non-interpretable baseline\nwhen applied to a real-world test set of Flickr images, all while providing\nclear and interpretable explanations for each prediction.</p>\n", "tags": [] },
{"key": "marx2019disentangling", "year": "2019", "title":"Disentangling Influence: Using Disentangled Representations to Audit Model Predictions", "abstract": "<p>Motivated by the need to audit complex and black box models, there has been\nextensive research on quantifying how data features influence model\npredictions. Feature influence can be direct (a direct influence on model\noutcomes) and indirect (model outcomes are influenced via proxy features).\nFeature influence can also be expressed in aggregate over the training or test\ndata or locally with respect to a single point. Current research has typically\nfocused on one of each of these dimensions. In this paper, we develop\ndisentangled influence audits, a procedure to audit the indirect influence of\nfeatures. Specifically, we show that disentangled representations provide a\nmechanism to identify proxy features in the dataset, while allowing an explicit\ncomputation of feature influence on either individual outcomes or\naggregate-level outcomes. We show through both theory and experiments that\ndisentangled influence audits can both detect proxy features and show, for each\nindividual or in aggregate, which of these proxy features affects the\nclassifier being audited the most. In this respect, our method is more powerful\nthan existing methods for ascertaining feature influence.</p>\n", "tags": [] },
{"key": "miller2017explanation", "year": "2017", "title":"Explanation in Artificial Intelligence: Insights from the Social Sciences", "abstract": "<p>There has been a recent resurgence in the area of explainable artificial\nintelligence as researchers and practitioners seek to make their algorithms\nmore understandable. Much of this research is focused on explicitly explaining\ndecisions or actions to a human observer, and it should not be controversial to\nsay that looking at how humans explain to each other can serve as a useful\nstarting point for explanation in artificial intelligence. However, it is fair\nto say that most work in explainable artificial intelligence uses only the\nresearchers’ intuition of what constitutes a `good’ explanation. There exists\nvast and valuable bodies of research in philosophy, psychology, and cognitive\nscience of how people define, generate, select, evaluate, and present\nexplanations, which argues that people employ certain cognitive biases and\nsocial expectations towards the explanation process. This paper argues that the\nfield of explainable artificial intelligence should build on this existing\nresearch, and reviews relevant papers from philosophy, cognitive\npsychology/science, and social psychology, which study these topics. It draws\nout some important findings, and discusses ways that these can be infused with\nwork on explainable artificial intelligence.</p>\n", "tags": [] },
{"key": "miller2018contrastive", "year": "2018", "title":"Contrastive Explanation: A Structural-Model Approach", "abstract": "<p>This paper presents a model of contrastive explanation using structural\ncasual models. The topic of causal explanation in artificial intelligence has\ngathered interest in recent years as researchers and practitioners aim to\nincrease trust and understanding of intelligent decision-making. While\ndifferent sub-fields of artificial intelligence have looked into this problem\nwith a sub-field-specific view, there are few models that aim to capture\nexplanation more generally. One general model is based on structural causal\nmodels. It defines an explanation as a fact that, if found to be true, would\nconstitute an actual cause of a specific event. However, research in philosophy\nand social sciences shows that explanations are contrastive: that is, when\npeople ask for an explanation of an event – the fact – they (sometimes\nimplicitly) are asking for an explanation relative to some contrast case; that\nis, “Why P rather than Q?”. In this paper, we extend the structural causal\nmodel approach to define two complementary notions of contrastive explanation,\nand demonstrate them on two classical problems in artificial intelligence:\nclassification and planning. We believe that this model can help researchers in\nsubfields of artificial intelligence to better understand contrastive\nexplanation.</p>\n", "tags": [] },
{"key": "mittelstadt2018explaining", "year": "2018", "title":"Explaining Explanations in AI", "abstract": "<p>Recent work on interpretability in machine learning and AI has focused on the\nbuilding of simplified models that approximate the true criteria used to make\ndecisions. These models are a useful pedagogical device for teaching trained\nprofessionals how to predict what decisions will be made by the complex system,\nand most importantly how the system might break. However, when considering any\nsuch model it’s important to remember Box’s maxim that “All models are wrong\nbut some are useful.” We focus on the distinction between these models and\nexplanations in philosophy and sociology. These models can be understood as a\n“do it yourself kit” for explanations, allowing a practitioner to directly\nanswer “what if questions” or generate contrastive explanations without\nexternal assistance. Although a valuable ability, giving these models as\nexplanations appears more difficult than necessary, and other forms of\nexplanation may not have the same trade-offs. We contrast the different schools\nof thought on what makes an explanation, and suggest that machine learning\nmight benefit from viewing the problem more broadly.</p>\n", "tags": [] },
{"key": "mohseni2018human-grounded", "year": "2018", "title":"A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning", "abstract": "<p>Research in interpretable machine learning proposes different computational\nand human subject approaches to evaluate model saliency explanations. These\napproaches measure different qualities of explanations to achieve diverse goals\nin designing interpretable machine learning systems. In this paper, we propose\na human attention benchmark for image and text domains using multi-layer human\nattention masks aggregated from multiple human annotators. We then present an\nevaluation study to evaluate model saliency explanations obtained using\nGrad-cam and LIME techniques. We demonstrate our benchmark’s utility for\nquantitative evaluation of model explanations by comparing it with human\nsubjective ratings and ground-truth single-layer segmentation masks\nevaluations. Our study results show that our threshold agnostic evaluation\nmethod with the human attention baseline is more effective than single-layer\nobject segmentation masks to ground truth. Our experiments also reveal user\nbiases in the subjective rating of model saliency explanations.</p>\n", "tags": [] },
{"key": "molnar2020interpretable", "year": "2020", "title":"Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges", "abstract": "<p>We present a brief history of the field of interpretable machine learning\n(IML), give an overview of state-of-the-art interpretation methods, and discuss\nchallenges. Research in IML has boomed in recent years. As young as the field\nis, it has over 200 years old roots in regression modeling and rule-based\nmachine learning, starting in the 1960s. Recently, many new IML methods have\nbeen proposed, many of them model-agnostic, but also interpretation techniques\nspecific to deep learning and tree-based ensembles. IML methods either directly\nanalyze model components, study sensitivity to input perturbations, or analyze\nlocal or global surrogate approximations of the ML model. The field approaches\na state of readiness and stability, with many methods not only proposed in\nresearch, but also implemented in open-source software. But many important\nchallenges remain for IML, such as dealing with dependent features, causal\ninterpretation, and uncertainty estimation, which need to be resolved for its\nsuccessful application to scientific problems. A further challenge is a missing\nrigorous definition of interpretability, which is accepted by the community. To\naddress the challenges and advance the field, we urge to recall our roots of\ninterpretable, data-driven modeling in statistics and (rule-based) ML, but also\nto consider other areas such as sensitivity analysis, causal inference, and the\nsocial sciences.</p>\n", "tags": [] },
{"key": "moorman2022people", "year": "2022", "title":"Do People Trust Robots that Learn in the Home?", "abstract": "<p>It is not scalable for assistive robotics to have all functionalities\npre-programmed prior to user introduction. Instead, it is more realistic for\nagents to perform supplemental on site learning. This opportunity to learn user\nand environment particularities is especially helpful for care robots that\nassist with individualized caregiver activities in residential or nursing home\nenvironments. Many assistive robots, ranging in complexity from Roomba to\nPepper, already conduct some of their learning in the home, observable to the\nuser. We lack an understanding of how witnessing this learning impacts the\nuser. Thus, we propose to assess end-user attitudes towards the concept of\nembodied robots that conduct some learning in the home as compared to robots\nthat are delivered fully-capable. In this virtual, between-subjects study, we\nrecruit end users (care-givers and care-takers) from nursing homes, and\ninvestigate user trust in three different domains: navigation, manipulation,\nand preparation. Informed by the first study where we identify agent learning\nas a key factor in determining trust, we propose a second study to explore how\nto modulate that trust. This second, in-person study investigates the\neffectiveness of apologies, explanations of robot failure, and transparency of\nlearning at improving trust in embodied learning robots.</p>\n", "tags": [] },
{"key": "moradi2020explaining", "year": "2020", "title":"Explaining Black-box Models for Biomedical Text Classification", "abstract": "<p>In this paper, we propose a novel method named Biomedical Confident Itemsets\nExplanation (BioCIE), aiming at post-hoc explanation of black-box machine\nlearning models for biomedical text classification. Using sources of domain\nknowledge and a confident itemset mining method, BioCIE discretizes the\ndecision space of a black-box into smaller subspaces and extracts semantic\nrelationships between the input text and class labels in different subspaces.\nConfident itemsets discover how biomedical concepts are related to class labels\nin the black-box’s decision space. BioCIE uses the itemsets to approximate the\nblack-box’s behavior for individual predictions. Optimizing fidelity,\ninterpretability, and coverage measures, BioCIE produces class-wise\nexplanations that represent decision boundaries of the black-box. Results of\nevaluations on various biomedical text classification tasks and black-box\nmodels demonstrated that BioCIE can outperform perturbation-based and decision\nset methods in terms of producing concise, accurate, and interpretable\nexplanations. BioCIE improved the fidelity of instance-wise and class-wise\nexplanations by 11.6% and 7.5%, respectively. It also improved the\ninterpretability of explanations by 8%. BioCIE can be effectively used to\nexplain how a black-box biomedical text classification model semantically\nrelates input texts to class labels. The source code and supplementary material\nare available at https://github.com/mmoradi-iut/BioCIE.</p>\n", "tags": [] },
{"key": "mukhopadhyay2020decoding", "year": "2020", "title":"Decoding CNN based Object Classifier Using Visualization", "abstract": "<p>This paper investigates how working of Convolutional Neural Network (CNN) can\nbe explained through visualization in the context of machine perception of\nautonomous vehicles. We visualize what type of features are extracted in\ndifferent convolution layers of CNN that helps to understand how CNN gradually\nincreases spatial information in every layer. Thus, it concentrates on region\nof interests in every transformation. Visualizing heat map of activation helps\nus to understand how CNN classifies and localizes different objects in image.\nThis study also helps us to reason behind low accuracy of a model helps to\nincrease trust on object detection module.</p>\n", "tags": [] },
{"key": "mundhenk2019efficient", "year": "2019", "title":"Efficient Saliency Maps for Explainable AI", "abstract": "<p>We describe an explainable AI saliency map method for use with deep\nconvolutional neural networks (CNN) that is much more efficient than popular\nfine-resolution gradient methods. It is also quantitatively similar or better\nin accuracy. Our technique works by measuring information at the end of each\nnetwork scale which is then combined into a single saliency map. We describe\nhow saliency measures can be made more efficient by exploiting Saliency Map\nOrder Equivalence. We visualize individual scale/layer contributions by using a\nLayer Ordered Visualization of Information. This provides an interesting\ncomparison of scale information contributions within the network not provided\nby other saliency map methods. Using our method instead of Guided Backprop,\ncoarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem\nto yield demonstrably superior results without sacrificing speed. This will\nmake fine-resolution saliency methods feasible on resource limited platforms\nsuch as robots, cell phones, low-cost industrial devices, astronomy and\nsatellite imagery.</p>\n", "tags": [] },
{"key": "müller2022interactive", "year": "2022", "title":"An Interactive Explanatory AI System for Industrial Quality Control", "abstract": "<p>Machine learning based image classification algorithms, such as deep neural\nnetwork approaches, will be increasingly employed in critical settings such as\nquality control in industry, where transparency and comprehensibility of\ndecisions are crucial. Therefore, we aim to extend the defect detection task\ntowards an interactive human-in-the-loop approach that allows us to integrate\nrich background knowledge and the inference of complex relationships going\nbeyond traditional purely data-driven approaches. We propose an approach for an\ninteractive support system for classifications in an industrial quality control\nsetting that combines the advantages of both (explainable) knowledge-driven and\ndata-driven machine learning methods, in particular inductive logic programming\nand convolutional neural networks, with human expertise and control. The\nresulting system can assist domain experts with decisions, provide transparent\nexplanations for results, and integrate feedback from users; thus reducing\nworkload for humans while both respecting their expertise and without removing\ntheir agency or accountability.</p>\n", "tags": [] },
{"key": "naik2020explanation", "year": "2020", "title":"Explanation from Specification", "abstract": "<p>Explainable components in XAI algorithms often come from a familiar set of\nmodels, such as linear models or decision trees. We formulate an approach where\nthe type of explanation produced is guided by a specification. Specifications\nare elicited from the user, possibly using interaction with the user and\ncontributions from other areas. Areas where a specification could be obtained\ninclude forensic, medical, and scientific applications. Providing a menu of\npossible types of specifications in an area is an exploratory knowledge\nrepresentation and reasoning task for the algorithm designer, aiming at\nunderstanding the possibilities and limitations of efficiently computable modes\nof explanations. Two examples are discussed: explanations for Bayesian networks\nusing the theory of argumentation, and explanations for graph neural networks.\nThe latter case illustrates the possibility of having a representation\nformalism available to the user for specifying the type of explanation\nrequested, for example, a chemical query language for classifying molecules.\nThe approach is motivated by a theory of explanation in the philosophy of\nscience, and it is related to current questions in the philosophy of science on\nthe role of machine learning.</p>\n", "tags": [] },
{"key": "nallbani2021resvgae", "year": "2021", "title":"ResVGAE: Going Deeper with Residual Modules for Link Prediction", "abstract": "<p>Graph autoencoders are efficient at embedding graph-based data sets. Most\ngraph autoencoder architectures have shallow depths which limits their ability\nto capture meaningful relations between nodes separated by multi-hops. In this\npaper, we propose Residual Variational Graph Autoencoder, ResVGAE, a deep\nvariational graph autoencoder model with multiple residual modules. We show\nthat our multiple residual modules, a convolutional layer with residual\nconnection, improve the average precision of the graph autoencoders.\nExperimental results suggest that our proposed model with residual modules\noutperforms the models without residual modules and achieves similar results\nwhen compared with other state-of-the-art methods.</p>\n", "tags": [] },
{"key": "nauta2022anecdotal", "year": "2022", "title":"From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI", "abstract": "<p>The rising popularity of explainable artificial intelligence (XAI) to\nunderstand high-performing black boxes, also raised the question of how to\nevaluate explanations of machine learning (ML) models. While interpretability\nand explainability are often presented as a subjectively validated binary\nproperty, we consider it a multi-faceted concept. We identify 12 conceptual\nproperties, such as Compactness and Correctness, that should be evaluated for\ncomprehensively assessing the quality of an explanation. Our so-called Co-12\nproperties serve as categorization scheme for systematically reviewing the\nevaluation practice of more than 300 papers published in the last 7 years at\nmajor AI and ML conferences that introduce an XAI method. We find that 1 in 3\npapers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate\nwith users. We also contribute to the call for objective, quantifiable\nevaluation methods by presenting an extensive overview of quantitative XAI\nevaluation methods. This systematic collection of evaluation methods provides\nresearchers and practitioners with concrete tools to thoroughly validate,\nbenchmark and compare new and existing XAI methods. This also opens up\nopportunities to include quantitative metrics as optimization criteria during\nmodel training in order to optimize for accuracy and interpretability\nsimultaneously.</p>\n", "tags": [] },
{"key": "nguyen2016plug", "year": "2016", "title":"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space", "abstract": "<p>Generating high-resolution, photo-realistic images has been a long-standing\ngoal in machine learning. Recently, Nguyen et al. (2016) showed one interesting\nway to synthesize novel images by performing gradient ascent in the latent\nspace of a generator network to maximize the activations of one or multiple\nneurons in a separate classifier network. In this paper we extend this method\nby introducing an additional prior on the latent code, improving both sample\nquality and sample diversity, leading to a state-of-the-art generative model\nthat produces high quality images at higher resolutions (227x227) than previous\ngenerative models, and does so for all 1000 ImageNet categories. In addition,\nwe provide a unified probabilistic interpretation of related activation\nmaximization methods and call the general class of models “Plug and Play\nGenerative Networks”. PPGNs are composed of 1) a generator network G that is\ncapable of drawing a wide range of image types and 2) a replaceable “condition”\nnetwork C that tells the generator what to draw. We demonstrate the generation\nof images conditioned on a class (when C is an ImageNet or MIT Places\nclassification network) and also conditioned on a caption (when C is an image\ncaptioning network). Our method also improves the state of the art of\nMultifaceted Feature Visualization, which generates the set of synthetic inputs\nthat activate a neuron in order to better understand how deep neural networks\noperate. Finally, we show that our model performs reasonably well at the task\nof image inpainting. While image models are used in this paper, the approach is\nmodality-agnostic and can be applied to many types of data.</p>\n", "tags": [] },
{"key": "nguyen2021effectiveness", "year": "2021", "title":"The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "abstract": "<p>Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et\nal. 2018). In this paper, we conduct the first user study to measure\nattribution map effectiveness in assisting humans in ImageNet classification\nand Stanford Dogs fine-grained classification, and when an image is natural or\nadversarial (i.e., contains adversarial perturbations). Overall, feature\nattribution is surprisingly not more effective than showing humans nearest\ntraining-set examples. On a harder task of fine-grained dog categorization,\npresenting attribution maps to humans does not help, but instead hurts the\nperformance of human-AI teams compared to AI alone. Importantly, we found\nautomatic attribution-map evaluation measures to correlate poorly with the\nactual human-AI team performance. Our findings encourage the community to\nrigorously test their methods on the downstream human-in-the-loop applications\nand to rethink the existing evaluation metrics.</p>\n", "tags": [] },
{"key": "nguyen2022visual", "year": "2022", "title":"Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "abstract": "<p>Explaining artificial intelligence (AI) predictions is increasingly important\nand even imperative in many high-stakes applications where humans are the\nultimate decision-makers. In this work, we propose two novel architectures of\nself-interpretable image classifiers that first explain, and then predict (as\nopposed to post-hoc explanations) by harnessing the visual correspondences\nbetween a query image and exemplars. Our models consistently improve (by 1 to 4\npoints) on out-of-distribution (OOD) datasets while performing marginally worse\n(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest\nneighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,\nour correspondence-based explanations are found to be more useful to users than\nkNN explanations. Our explanations help users more accurately reject AI’s wrong\ndecisions than all other tested methods. Interestingly, for the first time, we\nshow that it is possible to achieve complementary human-AI team accuracy (i.e.,\nthat is higher than either AI-alone or human-alone), in ImageNet and CUB image\nclassification tasks.</p>\n", "tags": [] },
{"key": "norelli2022explanatory", "year": "2022", "title":"Explanatory Learning: Beyond Empiricism in Neural Networks", "abstract": "<p>We introduce Explanatory Learning (EL), a framework to let machines use\nexisting knowledge buried in symbolic sequences – e.g. explanations written in\nhieroglyphic – by autonomously learning to interpret them. In EL, the burden\nof interpreting symbols is not left to humans or rigid human-coded compilers,\nas done in Program Synthesis. Rather, EL calls for a learned interpreter, built\nupon a limited collection of symbolic sequences paired with observations of\nseveral phenomena. This interpreter can be used to make predictions on a novel\nphenomenon given its explanation, and even to find that explanation using only\na handful of observations, like human scientists do. We formulate the EL\nproblem as a simple binary classification task, so that common end-to-end\napproaches aligned with the dominant empiricist view of machine learning could,\nin principle, solve it. To these models, we oppose Critical Rationalist\nNetworks (CRNs), which instead embrace a rationalist view on the acquisition of\nknowledge. CRNs express several desired properties by construction, they are\ntruly explainable, can adjust their processing at test-time for harder\ninferences, and can offer strong confidence guarantees on their predictions. As\na final contribution, we introduce Odeen, a basic EL environment that simulates\na small flatland-style universe full of phenomena to explain. Using Odeen as a\ntestbed, we show how CRNs outperform empiricist end-to-end approaches of\nsimilar size and architecture (Transformers) in discovering explanations for\nnovel phenomena.</p>\n", "tags": [] },
{"key": "nushi2018towards", "year": "2018", "title":"Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure", "abstract": "<p>As machine learning systems move from computer-science laboratories into the\nopen world, their accountability becomes a high priority problem.\nAccountability requires deep understanding of system behavior and its failures.\nCurrent evaluation methods such as single-score error metrics and confusion\nmatrices provide aggregate views of system performance that hide important\nshortcomings. Understanding details about failures is important for identifying\npathways for refinement, communicating the reliability of systems in different\nsettings, and for specifying appropriate human oversight and engagement.\nCharacterization of failures and shortcomings is particularly complex for\nsystems composed of multiple machine learned components. For such systems,\nexisting evaluation methods have limited expressiveness in describing and\nexplaining the relationship among input content, the internal states of system\ncomponents, and final output quality. We present Pandora, a set of hybrid\nhuman-machine methods and tools for describing and explaining system failures.\nPandora leverages both human and system-generated observations to summarize\nconditions of system malfunction with respect to the input content and system\narchitecture. We share results of a case study with a machine learning pipeline\nfor image captioning that show how detailed performance views can be beneficial\nfor analysis and debugging.</p>\n", "tags": [] },
{"key": "pan2016shallow", "year": "2016", "title":"Shallow and Deep Convolutional Networks for Saliency Prediction", "abstract": "<p>The prediction of salient areas in images has been traditionally addressed\nwith hand-crafted features based on neuroscience principles. This paper,\nhowever, addresses the problem with a completely data-driven approach by\ntraining a convolutional neural network (convnet). The learning process is\nformulated as a minimization of a loss function that measures the Euclidean\ndistance of the predicted saliency map with the provided ground truth. The\nrecent publication of large datasets of saliency prediction has provided enough\ndata to train end-to-end architectures that are both fast and accurate. Two\ndesigns are proposed: a shallow convnet trained from scratch, and a another\ndeeper solution whose first three layers are adapted from another network\ntrained for classification. To the authors knowledge, these are the first\nend-to-end CNNs trained and tested for the purpose of saliency prediction.</p>\n", "tags": [] },
{"key": "pan2021definitions", "year": "2021", "title":"The Definitions of Interpretability and Learning of Interpretable Models", "abstract": "<p>As machine learning algorithms getting adopted in an ever-increasing number\nof applications, interpretation has emerged as a crucial desideratum. In this\npaper, we propose a mathematical definition for the human-interpretable model.\nIn particular, we define interpretability between two information process\nsystems. If a prediction model is interpretable by a human recognition system\nbased on the above interpretability definition, the prediction model is defined\nas a completely human-interpretable model. We further design a practical\nframework to train a completely human-interpretable model by user interactions.\nExperiments on image datasets show the advantages of our proposed model in two\naspects: 1) The completely human-interpretable model can provide an entire\ndecision-making process that is human-understandable; 2) The completely\nhuman-interpretable model is more robust against adversarial attacks.</p>\n", "tags": [] },
{"key": "park2022vision", "year": "2022", "title":"How Do Vision Transformers Work?", "abstract": "<p>The success of multi-head self-attentions (MSAs) for computer vision is now\nindisputable. However, little is known about how MSAs work. We present\nfundamental explanations to help better understand the nature of MSAs. In\nparticular, we demonstrate the following properties of MSAs and Vision\nTransformers (ViTs): (1) MSAs improve not only accuracy but also generalization\nby flattening the loss landscapes. Such improvement is primarily attributable\nto their data specificity, not long-range dependency. On the other hand, ViTs\nsuffer from non-convex losses. Large datasets and loss landscape smoothing\nmethods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.\nFor example, MSAs are low-pass filters, but Convs are high-pass filters.\nTherefore, MSAs and Convs are complementary; (3) Multi-stage neural networks\nbehave like a series connection of small individual models. In addition, MSAs\nat the end of a stage play a key role in prediction. Based on these insights,\nwe propose AlterNet, a model in which Conv blocks at the end of a stage are\nreplaced with MSA blocks. AlterNet outperforms CNNs not only in large data\nregimes but also in small data regimes. The code is available at\nhttps://github.com/xxxnell/how-do-vits-work.</p>\n", "tags": [] },
{"key": "patro2019explanation", "year": "2019", "title":"Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA", "abstract": "<p>In this paper, we aim to obtain improved attention for a visual question\nanswering (VQA) task. It is challenging to provide supervision for attention.\nAn observation we make is that visual explanations as obtained through class\nactivation mappings (specifically Grad-CAM) that are meant to explain the\nperformance of various networks could form a means of supervision. However, as\nthe distributions of attention maps and that of Grad-CAMs differ, it would not\nbe suitable to directly use these as a form of supervision. Rather, we propose\nthe use of a discriminator that aims to distinguish samples of visual\nexplanation and attention maps. The use of adversarial training of the\nattention regions as a two-player game between attention and explanation serves\nto bring the distributions of attention maps and visual explanations closer.\nSignificantly, we observe that providing such a means of supervision also\nresults in attention maps that are more closely related to human attention\nresulting in a substantial improvement over baseline stacked attention network\n(SAN) models. It also results in a good improvement in rank correlation metric\non the VQA task. This method can also be combined with recent MCB based methods\nand results in consistent improvement. We also provide comparisons with other\nmeans for learning distributions such as based on Correlation Alignment\n(Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and\nobserve that the adversarial loss outperforms the other forms of learning the\nattention maps. Visualization of the results also confirms our hypothesis that\nattention maps improve using this form of supervision.</p>\n", "tags": [] },
{"key": "phuong2021towards", "year": "2021", "title":"Towards Understanding Knowledge Distillation", "abstract": "<p>Knowledge distillation, i.e., one classifier being trained on the outputs of\nanother classifier, is an empirically very successful technique for knowledge\ntransfer between classifiers. It has even been observed that classifiers learn\nmuch faster and more reliably if trained with the outputs of another classifier\nas soft labels, instead of from ground truth data. So far, however, there is no\nsatisfactory theoretical explanation of this phenomenon. In this work, we\nprovide the first insights into the working mechanisms of distillation by\nstudying the special case of linear and deep linear classifiers. Specifically,\nwe prove a generalization bound that establishes fast convergence of the\nexpected risk of a distillation-trained linear classifier. From the bound and\nits proof we extract three key factors that determine the success of\ndistillation: * data geometry – geometric properties of the data distribution,\nin particular class separation, has a direct influence on the convergence speed\nof the risk; * optimization bias – gradient descent optimization finds a very\nfavorable minimum of the distillation objective; and * strong monotonicity –\nthe expected risk of the student classifier always decreases when the size of\nthe training set grows.</p>\n", "tags": [] },
{"key": "poerner2018evaluating", "year": "2018", "title":"Evaluating neural network explanation methods using hybrid documents and morphological agreement", "abstract": "<p>The behavior of deep neural networks (DNNs) is hard to understand. This makes\nit necessary to explore post hoc explanation methods. We conduct the first\ncomprehensive evaluation of explanation methods for NLP. To this end, we design\ntwo novel evaluation paradigms that cover two important classes of NLP\nproblems: small context and large context problems. Both paradigms require no\nmanual annotation and are therefore broadly applicable. We also introduce\nLIMSSE, an explanation method inspired by LIME that is designed for NLP. We\nshow empirically that LIMSSE, LRP and DeepLIFT are the most effective\nexplanation methods and recommend them for explaining DNNs in NLP.</p>\n", "tags": [] },
{"key": "poursabzisangdeh2021manipulating", "year": "2021", "title":"Manipulating and Measuring Model Interpretability", "abstract": "<p>With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.</p>\n", "tags": ["measuring"] },
{"key": "pruthi2020evaluating", "year": "2020", "title":"Evaluating Explanations: How much do explanations from the teacher aid students?", "abstract": "<p>While many methods purport to explain predictions by highlighting salient\nfeatures, what aims these explanations serve and how they ought to be evaluated\noften go unstated. In this work, we introduce a framework to quantify the value\nof explanations via the accuracy gains that they confer on a student model\ntrained to simulate a teacher model. Crucially, the explanations are available\nto the student during training, but are not available at test time. Compared to\nprior proposals, our approach is less easily gamed, enabling principled,\nautomatic, model-agnostic evaluation of attributions. Using our framework, we\ncompare numerous attribution methods for text classification and question\nanswering, and observe quantitative differences that are consistent (to a\nmoderate to high degree) across different student model architectures and\nlearning strategies.</p>\n", "tags": [] },
{"key": "puiutta2020explainable", "year": "2020", "title":"Explainable Reinforcement Learning: A Survey", "abstract": "<p>Explainable Artificial Intelligence (XAI), i.e., the development of more\ntransparent and interpretable AI models, has gained increased traction over the\nlast few years. This is due to the fact that, in conjunction with their growth\ninto powerful and ubiquitous tools, AI models exhibit one detrimential\ncharacteristic: a performance-transparency trade-off. This describes the fact\nthat the more complex a model’s inner workings, the less clear it is how its\npredictions or decisions were achieved. But, especially considering Machine\nLearning (ML) methods like Reinforcement Learning (RL) where the system learns\nautonomously, the necessity to understand the underlying reasoning for their\ndecisions becomes apparent. Since, to the best of our knowledge, there exists\nno single work offering an overview of Explainable Reinforcement Learning (XRL)\nmethods, this survey attempts to address this gap. We give a short summary of\nthe problem, a definition of important terms, and offer a classification and\nassessment of current XRL methods. We found that a) the majority of XRL methods\nfunction by mimicking and simplifying a complex model instead of designing an\ninherently simple one, and b) XRL (and XAI) methods often neglect to consider\nthe human side of the equation, not taking into account research from related\nfields like psychology or philosophy. Thus, an interdisciplinary effort is\nneeded to adapt the generated explanations to a (non-expert) human user in\norder to effectively progress in the field of XRL and XAI in general.</p>\n", "tags": [] },
{"key": "ralekar2021understanding", "year": "2021", "title":"Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks", "abstract": "<p>Human observers engage in selective information uptake when classifying\nvisual patterns. The same is true of deep neural networks, which currently\nconstitute the best performing artificial vision systems. Our goal is to\nexamine the congruence, or lack thereof, in the information-gathering\nstrategies of the two systems. We have operationalized our investigation as a\ncharacter recognition task. We have used eye-tracking to assay the spatial\ndistribution of information hotspots for humans via fixation maps and an\nactivation mapping technique for obtaining analogous distributions for deep\nnetworks through visualization maps. Qualitative comparison between\nvisualization maps and fixation maps reveals an interesting correlate of\ncongruence. The deep learning model considered similar regions in character,\nwhich humans have fixated in the case of correctly classified characters. On\nthe other hand, when the focused regions are different for humans and deep\nnets, the characters are typically misclassified by the latter. Hence, we\npropose to use the visual fixation maps obtained from the eye-tracking\nexperiment as a supervisory input to align the model’s focus on relevant\ncharacter regions. We find that such supervision improves the model’s\nperformance significantly and does not require any additional parameters. This\napproach has the potential to find applications in diverse domains such as\nmedical analysis and surveillance in which explainability helps to determine\nsystem fidelity.</p>\n", "tags": [] },
{"key": "ramamurthy2022analogies", "year": "2022", "title":"Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners", "abstract": "<p>Post-hoc explanations for black box models have been studied extensively in\nclassification and regression settings. However, explanations for models that\noutput similarity between two inputs have received comparatively lesser\nattention. In this paper, we provide model agnostic local explanations for\nsimilarity learners applicable to tabular and text data. We first propose a\nmethod that provides feature attributions to explain the similarity between a\npair of inputs as determined by a black box similarity learner. We then propose\nanalogies as a new form of explanation in machine learning. Here the goal is to\nidentify diverse analogous pairs of examples that share the same level of\nsimilarity as the input pair and provide insight into (latent) factors\nunderlying the model’s prediction. The selection of analogies can optionally\nleverage feature attributions, thus connecting the two forms of explanation\nwhile still maintaining complementarity. We prove that our analogy objective\nfunction is submodular, making the search for good-quality analogies efficient.\nWe apply the proposed approaches to explain similarities between sentences as\npredicted by a state-of-the-art sentence encoder, and between patients in a\nhealthcare utilization application. Efficacy is measured through quantitative\nevaluations, a careful user study, and examples of explanations.</p>\n", "tags": [] },
{"key": "rao2021first", "year": "2021", "title":"A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations", "abstract": "<p>Explainable deep learning models are advantageous in many situations. Prior\nwork mostly provide unimodal explanations through post-hoc approaches not part\nof the original system design. Explanation mechanisms also ignore useful\ntextual information present in images. In this paper, we propose MTXNet, an\nend-to-end trainable multimodal architecture to generate multimodal\nexplanations, which focuses on the text in the image. We curate a novel dataset\nTextVQA-X, containing ground truth visual and multi-reference textual\nexplanations that can be leveraged during both training and evaluation. We then\nquantitatively show that training with multimodal explanations complements\nmodel performance and surpasses unimodal baselines by up to 7% in CIDEr scores\nand 2% in IoU. More importantly, we demonstrate that the multimodal\nexplanations are consistent with human interpretations, help justify the\nmodels’ decision, and provide useful insights to help diagnose an incorrect\nprediction. Finally, we describe a real-world e-commerce application for using\nthe generated multimodal explanations.</p>\n", "tags": [] },
{"key": "rao2022towards", "year": "2022", "title":"Towards Better Understanding Attribution Methods", "abstract": "<p>Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models’ decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods. Finally, we propose a post-processing smoothing step that\nsignificantly improves the performance of some attribution methods, and discuss\nits applicability.</p>\n", "tags": [] },
{"key": "ras2020explainable", "year": "2020", "title":"Explainable Deep Learning: A Field Guide for the Uninitiated", "abstract": "<p>Deep neural networks (DNNs) have become a proven and indispensable machine\nlearning tool. As a black-box model, it remains difficult to diagnose what\naspects of the model’s input drive the decisions of a DNN. In countless\nreal-world domains, from legislation and law enforcement to healthcare, such\ndiagnosis is essential to ensure that DNN decisions are driven by aspects\nappropriate in the context of its use. The development of methods and studies\nenabling the explanation of a DNN’s decisions has thus blossomed into an\nactive, broad area of research. A practitioner wanting to study explainable\ndeep learning may be intimidated by the plethora of orthogonal directions the\nfield has taken. This complexity is further exacerbated by competing\ndefinitions of what it means <code class=\"language-plaintext highlighter-rouge\">to explain'' the actions of a DNN and to\nevaluate an approach's</code>ability to explain’’. This article offers a field\nguide to explore the space of explainable deep learning aimed at those\nuninitiated in the field. The field guide: i) Introduces three simple\ndimensions defining the space of foundational methods that contribute to\nexplainable deep learning, ii) discusses the evaluations for model\nexplanations, iii) places explainability in the context of other related deep\nlearning research areas, and iv) finally elaborates on user-oriented\nexplanation designing and potential future directions on explainable deep\nlearning. We hope the guide is used as an easy-to-digest starting point for\nthose just embarking on research in this field.</p>\n", "tags": [] },
{"key": "ravanelli2018interpretable", "year": "2018", "title":"Interpretable Convolutional Filters with SincNet", "abstract": "<p>Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal “black-box” representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs.</p>\n", "tags": [] },
{"key": "ray2019explain", "year": "2019", "title":"Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval", "abstract": "<p>While there have been many proposals on making AI algorithms explainable, few\nhave attempted to evaluate the impact of AI-generated explanations on human\nperformance in conducting human-AI collaborative tasks. To bridge the gap, we\npropose a Twenty-Questions style collaborative image retrieval game,\nExplanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy\nof explanations (visual evidence or textual justification) in the context of\nVisual Question Answering (VQA). In our proposed ExAG, a human user needs to\nguess a secret image picked by the VQA agent by asking natural language\nquestions to it. We show that overall, when AI explains its answers, users\nsucceed more often in guessing the secret image correctly. Notably, a few\ncorrect explanations can readily improve human performance when VQA answers are\nmostly incorrect as compared to no-explanation games. Furthermore, we also show\nthat while explanations rated as “helpful” significantly improve human\nperformance, “incorrect” and “unhelpful” explanations can degrade performance\nas compared to no-explanation games. Our experiments, therefore, demonstrate\nthat ExAG is an effective means to evaluate the efficacy of AI-generated\nexplanations on a human-AI collaborative task.</p>\n", "tags": [] },
{"key": "ribeiro2016why", "year": "2016", "title":"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "abstract": "<p>Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.</p>\n", "tags": [] },
{"key": "ross2017improving", "year": "2017", "title":"Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients", "abstract": "<p>Deep neural networks have proven remarkably effective at solving many\nclassification problems, but have been criticized recently for two major\nweaknesses: the reasons behind their predictions are uninterpretable, and the\npredictions themselves can often be fooled by small adversarial perturbations.\nThese problems pose major obstacles for the adoption of neural networks in\ndomains that require security or transparency. In this work, we evaluate the\neffectiveness of defenses that differentiably penalize the degree to which\nsmall changes in inputs can alter model predictions. Across multiple attacks,\narchitectures, defenses, and datasets, we find that neural networks trained\nwith this input gradient regularization exhibit robustness to transferred\nadversarial examples generated to fool all of the other models. We also find\nthat adversarial examples generated to fool gradient-regularized models fool\nall other models equally well, and actually lead to more “legitimate,”\ninterpretable misclassifications as rated by people (which we confirm in a\nhuman subject experiment). Finally, we demonstrate that regularizing input\ngradients makes them more naturally interpretable as rationales for model\npredictions. We conclude by discussing this relationship between\ninterpretability and robustness in deep neural networks.</p>\n", "tags": [] },
{"key": "ross2017right", "year": "2017", "title":"Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations", "abstract": "<p>Neural networks are among the most accurate supervised learning methods in\nuse today, but their opacity makes them difficult to trust in critical\napplications, especially when conditions in training differ from those in test.\nRecent work on explanations for black-box models has produced tools (e.g. LIME)\nto show the implicit rules behind predictions, which can help us identify when\nmodels are right for the wrong reasons. However, these methods do not scale to\nexplaining entire datasets and cannot correct the problems they reveal. We\nintroduce a method for efficiently explaining and regularizing differentiable\nmodels by examining and selectively penalizing their input gradients, which\nprovide a normal to the decision boundary. We apply these penalties both based\non expert annotation and in an unsupervised fashion that encourages diverse\nmodels with qualitatively different decision boundaries for the same\nclassification problem. On multiple datasets, we show our approach generates\nfaithful explanations and models that generalize much better when conditions\ndiffer between training and test.</p>\n", "tags": [] },
{"key": "roxlo2018opening", "year": "2018", "title":"Opening the black box of neural nets: case studies in stop/top discrimination", "abstract": "<p>We introduce techniques for exploring the functionality of a neural network\nand extracting simple, human-readable approximations to its performance. By\nperforming gradient ascent on the input space of the network, we are able to\nproduce large populations of artificial events which strongly excite a given\nclassifier. By studying the populations of these events, we then directly\nproduce what are essentially contour maps of the network’s classification\nfunction. Combined with a suite of tools for identifying the input dimensions\ndeemed most important by the network, we can utilize these maps to efficiently\ninterpret the dominant criteria by which the network makes its classification.\n  As a test case, we study networks trained to discriminate supersymmetric stop\nproduction in the dilepton channel from Standard Model backgrounds. In the case\nof a heavy stop decaying to a light neutralino, we find individual neurons with\nlarge mutual information with $m_{T2}^{\\ell\\ell}$, a human-designed variable\nfor optimizing the analysis. The network selects events with significant\nmissing $p_T$ oriented azimuthally away from both leptons, efficiently\nrejecting $t\\overline{t}$ background. In the case of a light stop with\nthree-body decays to $Wb{\\widetilde \\chi}$ and little phase space, we find\nneurons that smoothly interpolate between a similar top-rejection strategy and\nan ISR-tagging strategy allowing for more missing momentum. We also find that a\nneural network trained on a stealth stop parameter point learns novel angular\ncorrelations.</p>\n", "tags": [] },
{"key": "rudin2018stop", "year": "2018", "title":"Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead", "abstract": "<p>Black box machine learning models are currently being used for high stakes\ndecision-making throughout society, causing problems throughout healthcare,\ncriminal justice, and in other domains. People have hoped that creating methods\nfor explaining these black box models will alleviate some of these problems,\nbut trying to \\textit{explain} black box models, rather than creating models\nthat are \\textit{interpretable} in the first place, is likely to perpetuate bad\npractices and can potentially cause catastrophic harm to society. There is a\nway forward – it is to design models that are inherently interpretable. This\nmanuscript clarifies the chasm between explaining black boxes and using\ninherently interpretable models, outlines several key reasons why explainable\nblack boxes should be avoided in high-stakes decisions, identifies challenges\nto interpretable machine learning, and provides several example applications\nwhere interpretable models could potentially replace black box models in\ncriminal justice, healthcare, and computer vision.</p>\n", "tags": [] },
{"key": "rudin2021interpretable", "year": "2021", "title":"Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges", "abstract": "<p>Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the “Rashomon set” of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.</p>\n", "tags": [] },
{"key": "räuker2022toward", "year": "2022", "title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks", "abstract": "<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities, and deep neural networks (DNNs) are increasingly being deployed\nacross a wide range of domains. However, the inner workings of DNNs are\ngenerally difficult to understand, raising concerns about the safety of using\nthese systems without a rigorous understanding of how they function. In this\nsurvey, we review literature on techniques for interpreting the inner\ncomponents of DNNs, which we call “inner” interpretability methods.\nSpecifically, we review methods for interpreting weights, neurons, subnetworks,\nand latent representations with a focus on how these techniques relate to the\ngoal of designing safer, more trustworthy AI systems. We also highlight\nconnections between interpretability and work in modularity, adversarial\nrobustness, continual learning, network compression, and studying the human\nvisual system. Finally, we discuss key challenges and argue for future work in\ninterpretability for AI safety that focuses on diagnostics, benchmarking, and\nrobustness.</p>\n", "tags": [] },
{"key": "sabour2017dynamic", "year": "2017", "title":"Dynamic Routing Between Capsules", "abstract": "<p>A capsule is a group of neurons whose activity vector represents the\ninstantiation parameters of a specific type of entity such as an object or an\nobject part. We use the length of the activity vector to represent the\nprobability that the entity exists and its orientation to represent the\ninstantiation parameters. Active capsules at one level make predictions, via\ntransformation matrices, for the instantiation parameters of higher-level\ncapsules. When multiple predictions agree, a higher level capsule becomes\nactive. We show that a discrimininatively trained, multi-layer capsule system\nachieves state-of-the-art performance on MNIST and is considerably better than\na convolutional net at recognizing highly overlapping digits. To achieve these\nresults we use an iterative routing-by-agreement mechanism: A lower-level\ncapsule prefers to send its output to higher level capsules whose activity\nvectors have a big scalar product with the prediction coming from the\nlower-level capsule.</p>\n", "tags": [] },
{"key": "samek2015evaluating", "year": "2015", "title":"Evaluating the visualization of what a Deep Neural Network has learned", "abstract": "<p>Deep Neural Networks (DNNs) have demonstrated impressive performance in\ncomplex machine learning tasks such as image classification or speech\nrecognition. However, due to their multi-layer nonlinear structure, they are\nnot transparent, i.e., it is hard to grasp what makes them arrive at a\nparticular classification or recognition decision given a new unseen data\nsample. Recently, several approaches have been proposed enabling one to\nunderstand and interpret the reasoning embodied in a DNN for a single test\nimage. These methods quantify the ‘‘importance’’ of individual pixels wrt the\nclassification decision and allow a visualization in terms of a heatmap in\npixel/input space. While the usefulness of heatmaps can be judged subjectively\nby a human, an objective quality measure is missing. In this paper we present a\ngeneral methodology based on region perturbation for evaluating ordered\ncollections of pixels such as heatmaps. We compare heatmaps computed by three\ndifferent methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main\nresult is that the recently proposed Layer-wise Relevance Propagation (LRP)\nalgorithm qualitatively and quantitatively provides a better explanation of\nwhat made a DNN arrive at a particular classification decision than the\nsensitivity-based approach or the deconvolution method. We provide theoretical\narguments to explain this result and discuss its practical implications.\nFinally, we investigate the use of heatmaps for unsupervised assessment of\nneural network performance.</p>\n", "tags": [] },
{"key": "samek2019towards", "year": "2019", "title":"Towards Explainable Artificial Intelligence", "abstract": "<p>In recent years, machine learning (ML) has become a key enabling technology\nfor the sciences and industry. Especially through improvements in methodology,\nthe availability of large databases and increased computational power, today’s\nML algorithms are able to achieve excellent performance (at times even\nexceeding the human level) on an increasing number of complex tasks. Deep\nlearning models are at the forefront of this development. However, due to their\nnested non-linear structure, these powerful models have been generally\nconsidered “black boxes”, not providing any information about what exactly\nmakes them arrive at their predictions. Since in many applications, e.g., in\nthe medical domain, such lack of transparency may be not acceptable, the\ndevelopment of methods for visualizing, explaining and interpreting deep\nlearning models has recently attracted increasing attention. This introductory\npaper presents recent developments and applications in this field and makes a\nplea for a wider use of explainable learning algorithms in practice.</p>\n", "tags": [] },
{"key": "samuel2021evaluation", "year": "2021", "title":"Evaluation of Saliency-based Explainability Method", "abstract": "<p>A particular class of Explainable AI (XAI) methods provide saliency maps to\nhighlight part of the image a Convolutional Neural Network (CNN) model looks at\nto classify the image as a way to explain its working. These methods provide an\nintuitive way for users to understand predictions made by CNNs. Other than\nquantitative computational tests, the vast majority of evidence to highlight\nthat the methods are valuable is anecdotal. Given that humans would be the\nend-users of such methods, we devise three human subject experiments through\nwhich we gauge the effectiveness of these saliency-based explainability\nmethods.</p>\n", "tags": [] },
{"key": "sanakoyeu2018deep", "year": "2018", "title":"Deep Unsupervised Learning of Visual Similarities", "abstract": "<p>Exemplar learning of visual similarities in an unsupervised manner is a\nproblem of paramount importance to Computer Vision. In this context, however,\nthe recent breakthrough in deep learning could not yet unfold its full\npotential. With only a single positive sample, a great imbalance between one\npositive and many negatives, and unreliable relationships between most samples,\ntraining of Convolutional Neural networks is impaired. In this paper we use\nweak estimates of local similarities and propose a single optimization problem\nto extract batches of samples with mutually consistent relations. Conflicting\nrelations are distributed over different batches and similar samples are\ngrouped into compact groups. Learning visual similarities is then framed as a\nsequence of categorization tasks. The CNN then consolidates transitivity\nrelations within and between groups and learns a single representation for all\nsamples without the need for labels. The proposed unsupervised approach has\nshown competitive performance on detailed posture analysis and object\nclassification.</p>\n", "tags": [] },
{"key": "santos2021impact", "year": "2021", "title":"On the Impact of Interpretability Methods in Active Image Augmentation Method", "abstract": "<p>Robustness is a significant constraint in machine learning models. The\nperformance of the algorithms must not deteriorate when training and testing\nwith slightly different data. Deep neural network models achieve awe-inspiring\nresults in a wide range of applications of computer vision. Still, in the\npresence of noise or region occlusion, some models exhibit inaccurate\nperformance even with data handled in training. Besides, some experiments\nsuggest deep learning models sometimes use incorrect parts of the input\ninformation to perform inference. Activate Image Augmentation (ADA) is an\naugmentation method that uses interpretability methods to augment the training\ndata and improve its robustness to face the described problems. Although ADA\npresented interesting results, its original version only used the Vanilla\nBackpropagation interpretability to train the U-Net model. In this work, we\npropose an extensive experimental analysis of the interpretability method’s\nimpact on ADA. We use five interpretability methods: Vanilla Backpropagation,\nGuided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The\nresults show that all methods achieve similar performance at the ending of\ntraining, but when combining ADA with GradCam, the U-Net model presented an\nimpressive fast convergence.</p>\n", "tags": [] },
{"key": "scafarto2022calibrate", "year": "2022", "title":"Calibrate to Interpret", "abstract": "<p>Trustworthy machine learning is driving a large number of ML community works\nin order to improve ML acceptance and adoption. The main aspect of trustworthy\nmachine learning are the followings: fairness, uncertainty, robustness,\nexplainability and formal guaranties. Each of these individual domains gains\nthe ML community interest, visible by the number of related publications.\nHowever few works tackle the interconnection between these fields. In this\npaper we show a first link between uncertainty and explainability, by studying\nthe relation between calibration and interpretation. As the calibration of a\ngiven model changes the way it scores samples, and interpretation approaches\noften rely on these scores, it seems safe to assume that the\nconfidence-calibration of a model interacts with our ability to interpret such\nmodel. In this paper, we show, in the context of networks trained on image\nclassification tasks, to what extent interpretations are sensitive to\nconfidence-calibration. It leads us to suggest a simple practice to improve the\ninterpretation outcomes: Calibrate to Interpret.</p>\n", "tags": [] },
{"key": "schmidt2019quantifying", "year": "2019", "title":"Quantifying Interpretability and Trust in Machine Learning Systems", "abstract": "<p>Decisions by Machine Learning (ML) models have become ubiquitous. Trusting\nthese decisions requires understanding how algorithms take them. Hence\ninterpretability methods for ML are an active focus of research. A central\nproblem in this context is that both the quality of interpretability methods as\nwell as trust in ML predictions are difficult to measure. Yet evaluations,\ncomparisons and improvements of trust and interpretability require quantifiable\nmeasures. Here we propose a quantitative measure for the quality of\ninterpretability methods. Based on that we derive a quantitative measure of\ntrust in ML decisions. Building on previous work we propose to measure\nintuitive understanding of algorithmic decisions using the information transfer\nrate at which humans replicate ML model predictions. We provide empirical\nevidence from crowdsourcing experiments that the proposed metric robustly\ndifferentiates interpretability methods. The proposed metric also demonstrates\nthe value of interpretability for ML assisted human decision making: in our\nexperiments providing explanations more than doubled productivity in annotation\ntasks. However unbiased human judgement is critical for doctors, judges, policy\nmakers and others. Here we derive a trust metric that identifies when human\ndecisions are overly biased towards ML predictions. Our results complement\nexisting qualitative work on trust and interpretability by quantifiable\nmeasures that can serve as objectives for further improving methods in this\nfield of research.</p>\n", "tags": [] },
{"key": "schneider2020deceptive", "year": "2020", "title":"Deceptive AI Explanations: Creation and Detection", "abstract": "<p>Artificial intelligence (AI) comes with great opportunities but can also pose\nsignificant risks. Automatically generated explanations for decisions can\nincrease transparency and foster trust, especially for systems based on\nautomated predictions by AI models. However, given, e.g., economic incentives\nto create dishonest AI, to what extent can we trust explanations? To address\nthis issue, our work investigates how AI models (i.e., deep learning, and\nexisting instruments to increase transparency regarding AI decisions) can be\nused to create and detect deceptive explanations. As an empirical evaluation,\nwe focus on text classification and alter the explanations generated by\nGradCAM, a well-established explanation technique in neural networks. Then, we\nevaluate the effect of deceptive explanations on users in an experiment with\n200 participants. Our findings confirm that deceptive explanations can indeed\nfool humans. However, one can deploy machine learning (ML) methods to detect\nseemingly minor deception attempts with accuracy exceeding 80% given sufficient\ndomain knowledge. Without domain knowledge, one can still infer inconsistencies\nin the explanations in an unsupervised manner, given basic knowledge of the\npredictive model under scrutiny.</p>\n", "tags": [] },
{"key": "schramowski2020making", "year": "2020", "title":"Making deep neural networks right for the right scientific reasons by interacting with their explanations", "abstract": "<p>Deep neural networks have shown excellent performances in many real-world\napplications. Unfortunately, they may show “Clever Hans”-like behavior—making\nuse of confounding factors within datasets—to achieve high performance. In\nthis work, we introduce the novel learning setting of “explanatory interactive\nlearning” (XIL) and illustrate its benefits on a plant phenotyping research\ntask. XIL adds the scientist into the training loop such that she interactively\nrevises the original model via providing feedback on its explanations. Our\nexperimental results demonstrate that XIL can help avoiding Clever Hans moments\nin machine learning and encourages (or discourages, if appropriate) trust into\nthe underlying model.</p>\n", "tags": [] },
{"key": "schwalbe2021comprehensive", "year": "2021", "title":"A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts", "abstract": "<p>In the meantime, a wide variety of terminologies, motivations, approaches,\nand evaluation criteria have been developed within the research field of\nexplainable artificial intelligence (XAI). With the amount of XAI methods\nvastly growing, a taxonomy of methods is needed by researchers as well as\npractitioners: To grasp the breadth of the topic, compare methods, and to\nselect the right XAI method based on traits required by a specific use-case\ncontext. Many taxonomies for XAI methods of varying level of detail and depth\ncan be found in the literature. While they often have a different focus, they\nalso exhibit many points of overlap. This paper unifies these efforts and\nprovides a complete taxonomy of XAI methods with respect to notions present in\nthe current state of research. In a structured literature analysis and\nmeta-study, we identified and reviewed more than 50 of the most cited and\ncurrent surveys on XAI methods, metrics, and method traits. After summarizing\nthem in a survey of surveys, we merge terminologies and concepts of the\narticles into a unified structured taxonomy. Single concepts therein are\nillustrated by more than 50 diverse selected example methods in total, which we\ncategorize accordingly. The taxonomy may serve both beginners, researchers, and\npractitioners as a reference and wide-ranging overview of XAI method traits and\naspects. Hence, it provides foundations for targeted, use-case-oriented, and\ncontext-sensitive future research.</p>\n", "tags": [] },
{"key": "seo2018regional", "year": "2018", "title":"Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks", "abstract": "<p>Recently, many methods to interpret and visualize deep neural network\npredictions have been proposed and significant progress has been made. However,\na more class-discriminative and visually pleasing explanation is required.\nThus, this paper proposes a region-based approach that estimates feature\nimportance in terms of appropriately segmented regions. By fusing the saliency\nmaps generated from multi-scale segmentations, a more class-discriminative and\nvisually pleasing map is obtained. We incorporate this regional multi-scale\nconcept into a prediction difference method that is model-agnostic. An input\nimage is segmented in several scales using the super-pixel method, and\nexclusion of a region is simulated by sampling a normal distribution\nconstructed using the boundary prior. The experimental results demonstrate that\nthe regional multi-scale method produces much more class-discriminative and\nvisually pleasing saliency maps.</p>\n", "tags": [] },
{"key": "shankaranarayana2019alime", "year": "2019", "title":"ALIME: Autoencoder Based Approach for Local Interpretability", "abstract": "<p>Machine learning and especially deep learning have garneredtremendous\npopularity in recent years due to their increased performanceover other\nmethods. The availability of large amount of data has aidedin the progress of\ndeep learning. Nevertheless, deep learning models areopaque and often seen as\nblack boxes. Thus, there is an inherent need tomake the models interpretable,\nespecially so in the medical domain. Inthis work, we propose a locally\ninterpretable method, which is inspiredby one of the recent tools that has\ngained a lot of interest, called localinterpretable model-agnostic explanations\n(LIME). LIME generates singleinstance level explanation by artificially\ngenerating a dataset aroundthe instance (by randomly sampling and using\nperturbations) and thentraining a local linear interpretable model. One of the\nmajor issues inLIME is the instability in the generated explanation, which is\ncaused dueto the randomly generated dataset. Another issue in these kind of\nlocalinterpretable models is the local fidelity. We propose novel\nmodificationsto LIME by employing an autoencoder, which serves as a better\nweightingfunction for the local model. We perform extensive comparisons\nwithdifferent datasets and show that our proposed method results in\nbothimproved stability, as well as local fidelity.</p>\n", "tags": [] },
{"key": "shen2020explain", "year": "2020", "title":"To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles", "abstract": "<p>Explainable AI, in the context of autonomous systems, like self driving cars,\nhas drawn broad interests from researchers. Recent studies have found that\nproviding explanations for an autonomous vehicle actions has many benefits,\ne.g., increase trust and acceptance, but put little emphasis on when an\nexplanation is needed and how the content of explanation changes with context.\nIn this work, we investigate which scenarios people need explanations and how\nthe critical degree of explanation shifts with situations and driver types.\nThrough a user experiment, we ask participants to evaluate how necessary an\nexplanation is and measure the impact on their trust in the self driving cars\nin different contexts. We also present a self driving explanation dataset with\nfirst person explanations and associated measure of the necessity for 1103\nvideo clips, augmenting the Berkeley Deep Drive Attention dataset.\nAdditionally, we propose a learning based model that predicts how necessary an\nexplanation for a given situation in real time, using camera data inputs. Our\nresearch reveals that driver types and context dictates whether or not an\nexplanation is necessary and what is helpful for improved interaction and\nunderstanding.</p>\n", "tags": [] },
{"key": "shen2021interpretable", "year": "2021", "title":"Interpretable Compositional Convolutional Neural Networks", "abstract": "<p>The reasonable definition of semantic interpretability presents the core\nchallenge in explainable AI. This paper proposes a method to modify a\ntraditional convolutional neural network (CNN) into an interpretable\ncompositional CNN, in order to learn filters that encode meaningful visual\npatterns in intermediate convolutional layers. In a compositional CNN, each\nfilter is supposed to consistently represent a specific compositional object\npart or image region with a clear meaning. The compositional CNN learns from\nimage labels for classification without any annotations of parts or regions for\nsupervision. Our method can be broadly applied to different types of CNNs.\nExperiments have demonstrated the effectiveness of our method.</p>\n", "tags": [] },
{"key": "shvo2020towards", "year": "2020", "title":"Towards the Role of Theory of Mind in Explanation", "abstract": "<p>Theory of Mind is commonly defined as the ability to attribute mental states\n(e.g., beliefs, goals) to oneself, and to others. A large body of previous work</p>\n<ul>\n  <li>from the social sciences to artificial intelligence - has observed that\nTheory of Mind capabilities are central to providing an explanation to another\nagent or when explaining that agent’s behaviour. In this paper, we build and\nexpand upon previous work by providing an account of explanation in terms of\nthe beliefs of agents and the mechanism by which agents revise their beliefs\ngiven possible explanations. We further identify a set of desiderata for\nexplanations that utilize Theory of Mind. These desiderata inform our\nbelief-based account of explanation.</li>\n</ul>\n", "tags": [] },
{"key": "simonyan2013deep", "year": "2013", "title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "abstract": "<p>This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013].</p>\n", "tags": [] },
{"key": "sokol2020towards", "year": "2020", "title":"Towards Faithful and Meaningful Interpretable Representations", "abstract": "<p>Interpretable representations are the backbone of many black-box explainers.\nThey translate the low-level data representation necessary for good predictive\nperformance into high-level human-intelligible concepts used to convey the\nexplanation. Notably, the explanation type and its cognitive complexity are\ndirectly controlled by the interpretable representation, allowing to target a\nparticular audience and use case. However, many explainers that rely on\ninterpretable representations overlook their merit and fall back on default\nsolutions, which may introduce implicit assumptions, thereby degrading the\nexplanatory power of such techniques. To address this problem, we study\nproperties of interpretable representations that encode presence and absence of\nhuman-comprehensible concepts. We show how they are operationalised for\ntabular, image and text data, discussing their strengths and weaknesses.\nFinally, we analyse their explanatory properties in the context of tabular\ndata, where a linear model is used to quantify the importance of interpretable\nconcepts.</p>\n", "tags": [] },
{"key": "springenberg2014striving", "year": "2014", "title":"Striving for Simplicity: The All Convolutional Net", "abstract": "<p>Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding – and building on other recent work for\nfinding simple network structures – we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n“deconvolution approach” for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches.</p>\n", "tags": [] },
{"key": "stalder2022see", "year": "2022", "title":"What You See is What You Classify: Black Box Attributions", "abstract": "<p>An important step towards explaining deep image classifiers lies in the\nidentification of image regions that contribute to individual class scores in\nthe model’s output. However, doing this accurately is a difficult task due to\nthe black-box nature of such networks. Most existing approaches find such\nattributions either using activations and gradients or by repeatedly perturbing\nthe input. We instead address this challenge by training a second deep network,\nthe Explainer, to predict attributions for a pre-trained black-box classifier,\nthe Explanandum. These attributions are in the form of masks that only show the\nclassifier-relevant parts of an image, masking out the rest. Our approach\nproduces sharper and more boundary-precise masks when compared to the saliency\nmaps generated by other methods. Moreover, unlike most existing approaches,\nours is capable of directly generating very distinct class-specific masks.\nFinally, the proposed method is very efficient for inference since it only\ntakes a single forward pass through the Explainer to generate all\nclass-specific masks. We show that our attributions are superior to established\nmethods both visually and quantitatively, by evaluating them on the PASCAL\nVOC-2007 and Microsoft COCO-2014 datasets.</p>\n", "tags": [] },
{"key": "stylianou2019visualizing", "year": "2019", "title":"Visualizing Deep Similarity Networks", "abstract": "<p>For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image.</p>\n", "tags": [] },
{"key": "su2017one", "year": "2017", "title":"One pixel attack for fooling deep neural networks", "abstract": "<p>Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness.</p>\n", "tags": [] },
{"key": "su2020sanity-checking", "year": "2020", "title":"Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot", "abstract": "<p>Network pruning is a method for reducing test-time computational resource\nrequirements with minimal performance degradation. Conventional wisdom of\npruning algorithms suggests that: (1) Pruning methods exploit information from\ntraining data to find good subnetworks; (2) The architecture of the pruned\nnetwork is crucial for good performance. In this paper, we conduct sanity\nchecks for the above beliefs on several recent unstructured pruning methods and\nsurprisingly find that: (1) A set of methods which aims to find good\nsubnetworks of the randomly-initialized network (which we call “initial\ntickets”), hardly exploits any information from the training data; (2) For the\npruned networks obtained by these methods, randomly changing the preserved\nweights in each layer, while keeping the total number of preserved weights\nunchanged per layer, does not affect the final performance. These findings\ninspire us to choose a series of simple \\emph{data-independent} prune ratios\nfor each layer, and randomly prune each layer accordingly to get a subnetwork\n(which we call “random tickets”). Experimental results show that our zero-shot\nrandom tickets outperform or attain a similar performance compared to existing\n“initial tickets”. In addition, we identify one existing pruning method that\npasses our sanity checks. We hybridize the ratios in our random ticket with\nthis method and propose a new method called “hybrid tickets”, which achieves\nfurther improvement. (Our code is publicly available at\nhttps://github.com/JingtongSu/sanity-checking-pruning)</p>\n", "tags": [] },
{"key": "su2020sanity", "year": "2020", "title":"Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot", "abstract": "<p>Network pruning is a method for reducing test-time computational resource\nrequirements with minimal performance degradation. Conventional wisdom of\npruning algorithms suggests that: (1) Pruning methods exploit information from\ntraining data to find good subnetworks; (2) The architecture of the pruned\nnetwork is crucial for good performance. In this paper, we conduct sanity\nchecks for the above beliefs on several recent unstructured pruning methods and\nsurprisingly find that: (1) A set of methods which aims to find good\nsubnetworks of the randomly-initialized network (which we call “initial\ntickets”), hardly exploits any information from the training data; (2) For the\npruned networks obtained by these methods, randomly changing the preserved\nweights in each layer, while keeping the total number of preserved weights\nunchanged per layer, does not affect the final performance. These findings\ninspire us to choose a series of simple \\emph{data-independent} prune ratios\nfor each layer, and randomly prune each layer accordingly to get a subnetwork\n(which we call “random tickets”). Experimental results show that our zero-shot\nrandom tickets outperform or attain a similar performance compared to existing\n“initial tickets”. In addition, we identify one existing pruning method that\npasses our sanity checks. We hybridize the ratios in our random ticket with\nthis method and propose a new method called “hybrid tickets”, which achieves\nfurther improvement. (Our code is publicly available at\nhttps://github.com/JingtongSu/sanity-checking-pruning)</p>\n", "tags": [] },
{"key": "sudhakar2021ada", "year": "2021", "title":"Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks", "abstract": "<p>Explainable AI (XAI) is an active research area to interpret a neural\nnetwork’s decision by ensuring transparency and trust in the task-specified\nlearned models. Recently, perturbation-based model analysis has shown better\ninterpretation, but backpropagation techniques are still prevailing because of\ntheir computational efficiency. In this work, we combine both approaches as a\nhybrid visual explanation algorithm and propose an efficient interpretation\nmethod for convolutional neural networks. Our method adaptively selects the\nmost critical features that mainly contribute towards a prediction to probe the\nmodel by finding the activated features. Experimental results show that the\nproposed method can reduce the execution time up to 30% while enhancing\ncompetitive interpretability without compromising the quality of explanation\ngenerated.</p>\n", "tags": [] },
{"key": "teso2018why", "year": "2018", "title":"\"Why Should I Trust Interactive Learners?\" Explaining Interactive Queries of Classifiers to Users", "abstract": "<p>Although interactive learning puts the user into the loop, the learner\nremains mostly a black box for the user. Understanding the reasons behind\nqueries and predictions is important when assessing how the learner works and,\nin turn, trust. Consequently, we propose the novel framework of explanatory\ninteractive learning: in each step, the learner explains its interactive query\nto the user, and she queries of any active classifier for visualizing\nexplanations of the corresponding predictions. We demonstrate that this can\nboost the predictive and explanatory powers of and the trust into the learned\nmodel, using text (e.g. SVMs) and image classification (e.g. neural networks)\nexperiments as well as a user study.</p>\n", "tags": [] },
{"key": "tjoa2020quantifying", "year": "2020", "title":"Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset", "abstract": "<p>Post-hoc analysis is a popular category in eXplainable artificial\nintelligence (XAI) study. In particular, methods that generate heatmaps have\nbeen used to explain the deep neural network (DNN), a black-box model. Heatmaps\ncan be appealing due to the intuitive and visual ways to understand them but\nassessing their qualities might not be straightforward. Different ways to\nassess heatmaps’ quality have their own merits and shortcomings. This paper\nintroduces a synthetic dataset that can be generated adhoc along with the\nground-truth heatmaps for more objective quantitative assessment. Each sample\ndata is an image of a cell with easily recognized features that are\ndistinguished from localization ground-truth mask, hence facilitating a more\ntransparent assessment of different XAI methods. Comparison and recommendations\nare made, shortcomings are clarified along with suggestions for future research\ndirections to handle the finer details of select post-hoc analysis methods.</p>\n", "tags": [] },
{"key": "tomsett2019sanity", "year": "2019", "title":"Sanity Checks for Saliency Metrics", "abstract": "<p>Saliency maps are a popular approach to creating post-hoc explanations of\nimage classifier outputs. These methods produce estimates of the relevance of\neach pixel to the classification output score, which can be displayed as a\nsaliency map that highlights important pixels. Despite a proliferation of such\nmethods, little effort has been made to quantify how good these saliency maps\nare at capturing the true relevance of the pixels to the classifier output\n(i.e. their “fidelity”). We therefore investigate existing metrics for\nevaluating the fidelity of saliency methods (i.e. saliency metrics). We find\nthat there is little consistency in the literature in how such metrics are\ncalculated, and show that such inconsistencies can have a significant effect on\nthe measured fidelity. Further, we apply measures of reliability developed in\nthe psychometric testing literature to assess the consistency of saliency\nmetrics when applied to individual saliency maps. Our results show that\nsaliency metrics can be statistically unreliable and inconsistent, indicating\nthat comparative rankings between saliency methods generated using such metrics\ncan be untrustworthy.</p>\n", "tags": [] },
{"key": "tuan2021local", "year": "2021", "title":"Local Explanation of Dialogue Response Generation", "abstract": "<p>In comparison to the interpretation of classification models, the explanation\nof sequence generation models is also an important problem, however it has seen\nlittle attention. In this work, we study model-agnostic explanations of a\nrepresentative text generation task – dialogue response generation. Dialog\nresponse generation is challenging with its open-ended sentences and multiple\nacceptable responses. To gain insights into the reasoning process of a\ngeneration model, we propose a new method, local explanation of response\ngeneration (LERG) that regards the explanations as the mutual interaction of\nsegments in input and output sentences. LERG views the sequence prediction as\nuncertainty estimation of a human response and then creates explanations by\nperturbing the input and calculating the certainty change over the human\nresponse. We show that LERG adheres to desired properties of explanations for\ntext generation including unbiased approximation, consistency and cause\nidentification. Empirically, our results show that our method consistently\nimproves other widely used methods on proposed automatic- and human- evaluation\nmetrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can\nextract both explicit and implicit relations between input and output segments.</p>\n", "tags": [] },
{"key": "vaishnav2021understanding", "year": "2021", "title":"Understanding the computational demands underlying visual reasoning", "abstract": "<p>Visual understanding requires comprehending complex visual relations between\nobjects within a scene. Here, we seek to characterize the computational demands\nfor abstract visual reasoning. We do this by systematically assessing the\nability of modern deep convolutional neural networks (CNNs) to learn to solve\nthe “Synthetic Visual Reasoning Test” (SVRT) challenge, a collection of\ntwenty-three visual reasoning problems. Our analysis reveals a novel taxonomy\nof visual reasoning tasks, which can be primarily explained by both the type of\nrelations (same-different vs. spatial-relation judgments) and the number of\nrelations used to compose the underlying rules. Prior cognitive neuroscience\nwork suggests that attention plays a key role in humans’ visual reasoning\nability. To test this hypothesis, we extended the CNNs with spatial and\nfeature-based attention mechanisms. In a second series of experiments, we\nevaluated the ability of these attention networks to learn to solve the SVRT\nchallenge and found the resulting architectures to be much more efficient at\nsolving the hardest of these visual reasoning tasks. Most importantly, the\ncorresponding improvements on individual tasks partially explained our novel\ntaxonomy. Overall, this work provides an granular computational account of\nvisual reasoning and yields testable neuroscience predictions regarding the\ndifferential need for feature-based vs. spatial attention depending on the type\nof visual reasoning problem.</p>\n", "tags": [] },
{"key": "vashishth2019attention", "year": "2019", "title":"Attention Interpretability Across NLP Tasks", "abstract": "<p>The attention layer in a neural network model provides insights into the\nmodel’s reasoning behind its prediction, which are usually criticized for being\nopaque. Recently, seemingly contradictory viewpoints have emerged about the\ninterpretability of attention weights (Jain &amp; Wallace, 2019; Vig &amp; Belinkov,\n2019). Amid such confusion arises the need to understand attention mechanism\nmore systematically. In this work, we attempt to fill this gap by giving a\ncomprehensive explanation which justifies both kinds of observations (i.e.,\nwhen is attention interpretable and when it is not). Through a series of\nexperiments on diverse NLP tasks, we validate our observations and reinforce\nour claim of interpretability of attention through manual evaluation.</p>\n", "tags": [] },
{"key": "vergari2016visualizing", "year": "2016", "title":"Visualizing and Understanding Sum-Product Networks", "abstract": "<p>Sum-Product Networks (SPNs) are recently introduced deep tractable\nprobabilistic models by which several kinds of inference queries can be\nanswered exactly and in a tractable time. Up to now, they have been largely\nused as black box density estimators, assessed only by comparing their\nlikelihood scores only. In this paper we explore and exploit the inner\nrepresentations learned by SPNs. We do this with a threefold aim: first we want\nto get a better understanding of the inner workings of SPNs; secondly, we seek\nadditional ways to evaluate one SPN model and compare it against other\nprobabilistic models, providing diagnostic tools to practitioners; lastly, we\nwant to empirically evaluate how good and meaningful the extracted\nrepresentations are, as in a classic Representation Learning framework. In\norder to do so we revise their interpretation as deep neural networks and we\npropose to exploit several visualization techniques on their node activations\nand network outputs under different types of inference queries. To investigate\nthese models as feature extractors, we plug some SPNs, learned in a greedy\nunsupervised fashion on image datasets, in supervised classification learning\ntasks. We extract several embedding types from node activations by filtering\nnodes by their type, by their associated feature abstraction level and by their\nscope. In a thorough empirical comparison we prove them to be competitive\nagainst those generated from popular feature extractors as Restricted Boltzmann\nMachines. Finally, we investigate embeddings generated from random\nprobabilistic marginal queries as means to compare other tractable\nprobabilistic models on a common ground, extending our experiments to Mixtures\nof Trees.</p>\n", "tags": [] },
{"key": "verma2021pitfalls", "year": "2021", "title":"Pitfalls of Explainable ML: An Industry Perspective", "abstract": "<p>As machine learning (ML) systems take a more prominent and central role in\ncontributing to life-impacting decisions, ensuring their trustworthiness and\naccountability is of utmost importance. Explanations sit at the core of these\ndesirable attributes of a ML system. The emerging field is frequently called\n<code class=\"language-plaintext highlighter-rouge\">Explainable AI (XAI)'' or</code>Explainable ML.’’ The goal of explainable ML is\nto intuitively explain the predictions of a ML system, while adhering to the\nneeds to various stakeholders. Many explanation techniques were developed with\ncontributions from both academia and industry. However, there are several\nexisting challenges that have not garnered enough interest and serve as\nroadblocks to widespread adoption of explainable ML. In this short paper, we\nenumerate challenges in explainable ML from an industry perspective. We hope\nthese challenges will serve as promising future research directions, and would\ncontribute to democratizing explainable ML.</p>\n", "tags": [] },
{"key": "vivek2018gray-box", "year": "2018", "title":"Gray-box Adversarial Training", "abstract": "<p>Adversarial samples are perturbed inputs crafted to mislead the machine\nlearning systems. A training mechanism, called adversarial training, which\npresents adversarial samples along with clean samples has been introduced to\nlearn robust models. In order to scale adversarial training for large datasets,\nthese perturbations can only be crafted using fast and simple methods (e.g.,\ngradient ascent). However, it is shown that adversarial training converges to a\ndegenerate minimum, where the model appears to be robust by generating weaker\nadversaries. As a result, the models are vulnerable to simple black-box\nattacks. In this paper we, (i) demonstrate the shortcomings of existing\nevaluation policy, (ii) introduce novel variants of white-box and black-box\nattacks, dubbed gray-box adversarial attacks” based on which we propose novel\nevaluation method to assess the robustness of the learned models, and (iii)\npropose a novel variant of adversarial training, named Graybox Adversarial\nTraining” that uses intermediate versions of the models to seed the\nadversaries. Experimental evaluation demonstrates that the models trained using\nour method exhibit better robustness compared to both undefended and\nadversarially trained model</p>\n", "tags": [] },
{"key": "wallace2018interpreting", "year": "2018", "title":"Interpreting Neural Networks With Nearest Neighbors", "abstract": "<p>Local model interpretation methods explain individual predictions by\nassigning an importance value to each input feature. This value is often\ndetermined by measuring the change in confidence when a feature is removed.\nHowever, the confidence of neural networks is not a robust measure of model\nuncertainty. This issue makes reliably judging the importance of the input\nfeatures difficult. We address this by changing the test-time behavior of\nneural networks using Deep k-Nearest Neighbors. Without harming text\nclassification accuracy, this algorithm provides a more robust uncertainty\nmetric which we use to generate feature importance values. The resulting\ninterpretations better align with human perception than baseline methods.\nFinally, we use our interpretation method to analyze model predictions on\ndataset annotation artifacts.</p>\n", "tags": [] },
{"key": "wang2017deep", "year": "2017", "title":"Deep Visual Attention Prediction", "abstract": "<p>In this work, we aim to predict human eye fixation with view-free scenes\nbased on an end-to-end deep learning architecture. Although Convolutional\nNeural Networks (CNNs) have made substantial improvement on human attention\nprediction, it is still needed to improve CNN based attention models by\nefficiently leveraging multi-scale features. Our visual attention network is\nproposed to capture hierarchical saliency information from deep, coarse layers\nwith global saliency information to shallow, fine layers with local saliency\nresponse. Our model is based on a skip-layer network structure, which predicts\nhuman attention from multiple convolutional layers with various reception\nfields. Final saliency prediction is achieved via the cooperation of those\nglobal and local predictions. Our model is learned in a deep supervision\nmanner, where supervision is directly fed into multi-level layers, instead of\nprevious approaches of providing supervision only at the output layer and\npropagating this supervision back to earlier layers. Our model thus\nincorporates multi-level saliency predictions within a single network, which\nsignificantly decreases the redundancy of previous approaches of learning\nmultiple network streams with different input scales. Extensive experimental\nanalysis on various challenging benchmark datasets demonstrate our method\nyields state-of-the-art performance with competitive inference time.</p>\n", "tags": [] },
{"key": "wang2020interpreting", "year": "2020", "title":"Interpreting Interpretations: Organizing Attribution Methods by Criteria", "abstract": "<p>Motivated by distinct, though related, criteria, a growing number of\nattribution methods have been developed tointerprete deep learning. While each\nrelies on the interpretability of the concept of “importance” and our ability\nto visualize patterns, explanations produced by the methods often differ. As a\nresult, input attribution for vision models fail to provide any level of human\nunderstanding of model behaviour. In this work we expand the foundationsof\nhuman-understandable concepts with which attributionscan be interpreted beyond\n“importance” and its visualization; we incorporate the logical concepts of\nnecessity andsufficiency, and the concept of proportionality. We definemetrics\nto represent these concepts as quantitative aspectsof an attribution. This\nallows us to compare attributionsproduced by different methods and interpret\nthem in novelways: to what extent does this attribution (or this\nmethod)represent the necessity or sufficiency of the highlighted inputs, and to\nwhat extent is it proportional? We evaluate our measures on a collection of\nmethods explaining convolutional neural networks (CNN) for image\nclassification. We conclude that some attribution methods are more appropriate\nfor interpretation in terms of necessity while others are in terms of\nsufficiency, while no method is always the most appropriate in terms of both.</p>\n", "tags": [] },
{"key": "wang2021fast", "year": "2021", "title":"On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "abstract": "<p>Model-agnostic meta-learning (MAML) has emerged as one of the most successful\nmeta-learning techniques in few-shot learning. It enables us to learn a\nmeta-initialization} of model parameters (that we call meta-model) to rapidly\nadapt to new tasks using a small amount of labeled training data. Despite the\ngeneralization power of the meta-model, it remains elusive that how adversarial\nrobustness can be maintained by MAML in few-shot learning. In addition to\ngeneralization, robustness is also desired for a meta-model to defend\nadversarial examples (attacks). Toward promoting adversarial robustness in\nMAML, we first study WHEN a robustness-promoting regularization should be\nincorporated, given the fact that MAML adopts a bi-level (fine-tuning vs.\nmeta-update) learning procedure. We show that robustifying the meta-update\nstage is sufficient to make robustness adapted to the task-specific fine-tuning\nstage even if the latter uses a standard training protocol. We also make\nadditional justification on the acquired robustness adaptation by peering into\nthe interpretability of neurons’ activation maps. Furthermore, we investigate\nHOW robust regularization can efficiently be designed in MAML. We propose a\ngeneral but easily-optimized robustness-regularized meta-learning framework,\nwhich allows the use of unlabeled data augmentation, fast adversarial attack\ngeneration, and computationally-light fine-tuning. In particular, we for the\nfirst time show that the auxiliary contrastive learning task can enhance the\nadversarial robustness of MAML. Finally, extensive experiments are conducted to\ndemonstrate the effectiveness of our proposed methods in robust few-shot\nlearning.</p>\n", "tags": [] },
{"key": "warnecke2019evaluating", "year": "2019", "title":"Evaluating Explanation Methods for Deep Learning in Security", "abstract": "<p>Deep learning is increasingly used as a building block of security systems.\nUnfortunately, neural networks are hard to interpret and typically opaque to\nthe practitioner. The machine learning community has started to address this\nproblem by developing methods for explaining the predictions of neural\nnetworks. While several of these approaches have been successfully applied in\nthe area of computer vision, their application in security has received little\nattention so far. It is an open question which explanation methods are\nappropriate for computer security and what requirements they need to satisfy.\nIn this paper, we introduce criteria for comparing and evaluating explanation\nmethods in the context of computer security. These cover general properties,\nsuch as the accuracy of explanations, as well as security-focused aspects, such\nas the completeness, efficiency, and robustness. Based on our criteria, we\ninvestigate six popular explanation methods and assess their utility in\nsecurity systems for malware detection and vulnerability discovery. We observe\nsignificant differences between the methods and build on these to derive\ngeneral recommendations for selecting and applying explanation methods in\ncomputer security.</p>\n", "tags": [] },
{"key": "weller2019transparency", "year": "2019", "title":"Transparency: Motivations and Challenges", "abstract": "<p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>\n", "tags": ["evaluation"] },
{"key": "wiegreffe2019attention", "year": "2019", "title":"Attention is not not Explanation", "abstract": "<p>Attention mechanisms play a central role in NLP systems, especially within\nrecurrent neural network (RNN) models. Recently, there has been increasing\ninterest in whether or not the intermediate representations offered by these\nmodules may be used to explain the reasoning for a model’s prediction, and\nconsequently reach insights regarding the model’s decision-making process. A\nrecent paper claims that `Attention is not Explanation’ (Jain and Wallace,\n2019). We challenge many of the assumptions underlying this work, arguing that\nsuch a claim depends on one’s definition of explanation, and that testing it\nneeds to take into account all elements of the model, using a rigorous\nexperimental design. We propose four alternative tests to determine\nwhen/whether attention can be used as explanation: a simple uniform-weights\nbaseline; a variance calibration based on multiple random seed runs; a\ndiagnostic framework using frozen weights from pretrained models; and an\nend-to-end adversarial attention training protocol. Each allows for meaningful\ninterpretation of attention mechanisms in RNN models. We show that even when\nreliable adversarial distributions can be found, they don’t perform well on the\nsimple diagnostic, indicating that prior work does not disprove the usefulness\nof attention mechanisms for explainability.</p>\n", "tags": [] },
{"key": "wiegreffe2021teach", "year": "2021", "title":"Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing", "abstract": "<p>Explainable NLP (ExNLP) has increasingly focused on collecting\nhuman-annotated textual explanations. These explanations are used downstream in\nthree ways: as data augmentation to improve performance on a predictive task,\nas supervision to train models to produce explanations for their predictions,\nand as a ground-truth to evaluate model-generated explanations. In this review,\nwe identify 65 datasets with three predominant classes of textual explanations\n(highlights, free-text, and structured), organize the literature on annotating\neach type, identify strengths and shortcomings of existing collection\nmethodologies, and give recommendations for collecting ExNLP datasets in the\nfuture.</p>\n", "tags": [] },
{"key": "woodcock2022impact", "year": "2022", "title":"The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study", "abstract": "<p>To achieve the promoted benefits of an AI symptom checker, laypeople must\ntrust and subsequently follow its instructions. In AI, explanations are seen as\na tool to communicate the rationale behind black-box decisions to encourage\ntrust and adoption. However, the effectiveness of the types of explanations\nused in AI-driven symptom checkers has not yet been studied. Social theories\nsuggest that why-explanations are better at communicating knowledge and\ncultivating trust among laypeople. This study ascertains whether explanations\nprovided by a symptom checker affect explanatory trust among laypeople (N=750)\nand whether this trust is impacted by their existing knowledge of disease.\n  Results suggest system builders developing explanations for symptom-checking\napps should consider the recipient’s knowledge of a disease and tailor\nexplanations to each user’s specific need. Effort should be placed on\ngenerating explanations that are personalized to each user of a symptom checker\nto fully discount the diseases that they may be aware of and to close their\ninformation gap.</p>\n", "tags": [] },
{"key": "wu2017towards", "year": "2017", "title":"Towards Interpretable R-CNN by Unfolding Latent Structures", "abstract": "<p>This paper first proposes a method of formulating model interpretability in\nvisual understanding tasks based on the idea of unfolding latent structures. It\nthen presents a case study in object detection using popular two-stage\nregion-based convolutional network (i.e., R-CNN) detection systems. We focus on\nweakly-supervised extractive rationale generation, that is learning to unfold\nlatent discriminative part configurations of object instances automatically and\nsimultaneously in detection without using any supervision for part\nconfigurations. We utilize a top-down hierarchical and compositional grammar\nmodel embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold\nthe space of latent part configurations of regions of interest (RoIs). We\npropose an AOGParsing operator to substitute the RoIPooling operator widely\nused in R-CNN. In detection, a bounding box is interpreted by the best parse\ntree derived from the AOG on-the-fly, which is treated as the qualitatively\nextractive rationale generated for interpreting detection. We propose a\nfolding-unfolding method to train the AOG and convolutional networks\nend-to-end. In experiments, we build on R-FCN and test our method on the PASCAL\nVOC 2007 and 2012 datasets. We show that the method can unfold promising latent\nstructures without hurting the performance.</p>\n", "tags": [] },
{"key": "xie2022towards", "year": "2022", "title":"Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning", "abstract": "<p>Artificial intelligence, particularly through recent advancements in deep\nlearning, has achieved exceptional performances in many tasks in fields such as\nnatural language processing and computer vision. In addition to desirable\nevaluation metrics, a high level of interpretability is often required for\nthese models to be reliably utilized. Therefore, explanations that offer\ninsight into the process by which a model maps its inputs onto its outputs are\nmuch sought-after. Unfortunately, current black box nature of machine learning\nmodels is still an unresolved issue and this very nature prevents researchers\nfrom learning and providing explicative descriptions for a model’s behavior and\nfinal predictions. In this work, we propose a novel framework utilizing\nAdversarial Inverse Reinforcement Learning that can provide global explanations\nfor decisions made by a Reinforcement Learning model and capture intuitive\ntendencies that the model follows by summarizing the model’s decision-making\nprocess.</p>\n", "tags": [] },
{"key": "yadav2018sanity", "year": "2018", "title":"Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering", "abstract": "<p>While increasingly complex approaches to question answering (QA) have been\nproposed, the true gain of these systems, particularly with respect to their\nexpensive training requirements, can be inflated when they are not compared to\nadequate baselines. Here we propose an unsupervised, simple, and fast alignment\nand information retrieval baseline that incorporates two novel contributions: a\n\\textit{one-to-many alignment} between query and document terms and\n\\textit{negative alignment} as a proxy for discriminative information. Our\napproach not only outperforms all conventional baselines as well as many\nsupervised recurrent neural networks, but also approaches the state of the art\nfor supervised systems on three QA datasets. With only three hyperparameters,\nwe achieve 47\\% P@1 on an 8th grade Science QA dataset, 32.9\\% P@1 on a Yahoo!\nanswers QA dataset and 64\\% MAP on WikiQA. We also achieve 26.56\\% and 58.36\\%\non ARC challenge and easy dataset respectively. In addition to including the\nadditional ARC results in this version of the paper, for the ARC easy set only\nwe also experimented with one additional parameter – number of justifications\nretrieved.</p>\n", "tags": [] },
{"key": "yang2019benchmarking", "year": "2019", "title":"Benchmarking Attribution Methods with Relative Feature Importance", "abstract": "<p>Interpretability is an important area of research for safe deployment of\nmachine learning systems. One particular type of interpretability method\nattributes model decisions to input features. Despite active development,\nquantitative evaluation of feature attribution methods remains difficult due to\nthe lack of ground truth: we do not know which input features are in fact\nimportant to a model. In this work, we propose a framework for Benchmarking\nAttribution Methods (BAM) with a priori knowledge of relative feature\nimportance. BAM includes 1) a carefully crafted dataset and models trained with\nknown relative feature importance and 2) three complementary metrics to\nquantitatively evaluate attribution methods by comparing feature attributions\nbetween pairs of models and pairs of inputs. Our evaluation on several\nwidely-used attribution methods suggests that certain methods are more likely\nto produce false positive explanations—features that are incorrectly\nattributed as more important to model prediction. We open source our dataset,\nmodels, and metrics.</p>\n", "tags": [] },
{"key": "yang2022robust", "year": "2022", "title":"On Robust Prefix-Tuning for Text Classification", "abstract": "<p>Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.</p>\n", "tags": [] },
{"key": "yao2021explanatory", "year": "2021", "title":"Explanatory Pluralism in Explainable AI", "abstract": "<p>The increasingly widespread application of AI models motivates increased\ndemand for explanations from a variety of stakeholders. However, this demand is\nambiguous because there are many types of ‘explanation’ with different\nevaluative criteria. In the spirit of pluralism, I chart a taxonomy of types of\nexplanation and the associated XAI methods that can address them. When we look\nto expose the inner mechanisms of AI models, we develop\nDiagnostic-explanations. When we seek to render model output understandable, we\nproduce Explication-explanations. When we wish to form stable generalizations\nof our models, we produce Expectation-explanations. Finally, when we want to\njustify the usage of a model, we produce Role-explanations that situate models\nwithin their social context. The motivation for such a pluralistic view stems\nfrom a consideration of causes as manipulable relationships and the different\ntypes of explanations as identifying the relevant points in AI systems we can\nintervene upon to affect our desired changes. This paper reduces the ambiguity\nin use of the word ‘explanation’ in the field of XAI, allowing practitioners\nand stakeholders a useful template for avoiding equivocation and evaluating XAI\nmethods and putative explanations.</p>\n", "tags": [] },
{"key": "yin2021sensitivity", "year": "2021", "title":"On the Sensitivity and Stability of Model Interpretations in NLP", "abstract": "<p>Recent years have witnessed the emergence of a variety of post-hoc\ninterpretations that aim to uncover how natural language processing (NLP)\nmodels make predictions. Despite the surge of new interpretation methods, it\nremains an open problem how to define and quantitatively measure the\nfaithfulness of interpretations, i.e., to what extent interpretations reflect\nthe reasoning process by a model. We propose two new criteria, sensitivity and\nstability, that provide complementary notions of faithfulness to the existed\nremoval-based criteria. Our results show that the conclusion for how faithful\ninterpretations are could vary substantially based on different notions.\nMotivated by the desiderata of sensitivity and stability, we introduce a new\nclass of interpretation methods that adopt techniques from adversarial\nrobustness. Empirical results show that our proposed methods are effective\nunder the new criteria and overcome limitations of gradient-based methods on\nremoval-based criteria. Besides text classification, we also apply\ninterpretation methods and metrics to dependency parsing. Our results shed\nlight on understanding the diverse set of interpretations.</p>\n", "tags": [] },
{"key": "yona2021revisiting", "year": "2021", "title":"Revisiting Sanity Checks for Saliency Maps", "abstract": "<p>Saliency methods are a popular approach for model debugging and\nexplainability. However, in the absence of ground-truth data for what the\ncorrect maps should be, evaluating and comparing different approaches remains a\nlong-standing challenge. The sanity checks methodology of Adebayo et al\n[Neurips 2018] has sought to address this challenge. They argue that some\npopular saliency methods should not be used for explainability purposes since\nthe maps they produce are not sensitive to the underlying model that is to be\nexplained. Through a causal re-framing of their objective, we argue that their\nempirical evaluation does not fully establish these conclusions, due to a form\nof confounding introduced by the tasks they evaluate on. Through various\nexperiments on simple custom tasks we demonstrate that some of their\nconclusions may indeed be artifacts of the tasks more than a criticism of the\nsaliency methods themselves. More broadly, our work challenges the utility of\nthe sanity check methodology, and further highlights that saliency map\nevaluation beyond ad-hoc visual examination remains a fundamental challenge.</p>\n", "tags": [] },
{"key": "yosinski2014transferable", "year": "2014", "title":"How transferable are features in deep neural networks?", "abstract": "<p>Many deep neural networks trained on natural images exhibit a curious\nphenomenon in common: on the first layer they learn features similar to Gabor\nfilters and color blobs. Such first-layer features appear not to be specific to\na particular dataset or task, but general in that they are applicable to many\ndatasets and tasks. Features must eventually transition from general to\nspecific by the last layer of the network, but this transition has not been\nstudied extensively. In this paper we experimentally quantify the generality\nversus specificity of neurons in each layer of a deep convolutional neural\nnetwork and report a few surprising results. Transferability is negatively\naffected by two distinct issues: (1) the specialization of higher layer neurons\nto their original task at the expense of performance on the target task, which\nwas expected, and (2) optimization difficulties related to splitting networks\nbetween co-adapted neurons, which was not expected. In an example network\ntrained on ImageNet, we demonstrate that either of these two issues may\ndominate, depending on whether features are transferred from the bottom,\nmiddle, or top of the network. We also document that the transferability of\nfeatures decreases as the distance between the base task and target task\nincreases, but that transferring features even from distant tasks can be better\nthan using random features. A final surprising result is that initializing a\nnetwork with transferred features from almost any number of layers can produce\na boost to generalization that lingers even after fine-tuning to the target\ndataset.</p>\n", "tags": [] },
{"key": "yosinski2015understanding", "year": "2015", "title":"Understanding Neural Networks Through Deep Visualization", "abstract": "<p>Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.</p>\n", "tags": [] },
{"key": "you2020large-scale", "year": "2020", "title":"Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study", "abstract": "<p>The quality and accuracy of power system models is critical for\nsimulation-based studies, especially for studying actual stability issues in\nlarge-scale systems. With the deployment of wide-area monitoring systems\n(WAMSs), the high-reporting-rate frequency measurement provides a trustworthy\nground truth for validating models in frequency response studies. This paper\ndocumented an effort to check, tune, and validate the U.S. power system model\nbased on a WAMS called FNET/GridEye. Four metrics are used to quantitatively\ncompare the simulation results and the actual measurement, including frequency\nnadir, RoCoF, settling frequency and settling time. After tuning governor\ndeadband and the governor ratio, the model frequency response shows significant\nimprovement and matches well with the event measurement data. This work serves\nas an example for tuning and validating large-scale power system models.</p>\n", "tags": [] },
{"key": "yousefzadeh2019interpreting", "year": "2019", "title":"Interpreting Neural Networks Using Flip Points", "abstract": "<p>Neural networks have been criticized for their lack of easy interpretation,\nwhich undermines confidence in their use for important applications. Here, we\nintroduce a novel technique, interpreting a trained neural network by\ninvestigating its flip points. A flip point is any point that lies on the\nboundary between two output classes: e.g. for a neural network with a binary\nyes/no output, a flip point is any input that generates equal scores for “yes”\nand “no”. The flip point closest to a given input is of particular importance,\nand this point is the solution to a well-posed optimization problem. This paper\ngives an overview of the uses of flip points and how they are computed. Through\nresults on standard datasets, we demonstrate how flip points can be used to\nprovide detailed interpretation of the output produced by a neural network.\nMoreover, for a given input, flip points enable us to measure confidence in the\ncorrectness of outputs much more effectively than softmax score. They also\nidentify influential features of the inputs, identify bias, and find changes in\nthe input that change the output of the model. We show that distance between an\ninput and the closest flip point identifies the most influential points in the\ntraining data. Using principal component analysis (PCA) and rank-revealing QR\nfactorization (RR-QR), the set of directions from each training input to its\nclosest flip point provides explanations of how a trained neural network\nprocesses an entire dataset: what features are most important for\nclassification into a given class, which features are most responsible for\nparticular misclassifications, how an adversary might fool the network, etc.\nAlthough we investigate flip points for neural networks, their usefulness is\nactually model-agnostic.</p>\n", "tags": [] },
{"key": "yu2019interpreting", "year": "2019", "title":"Interpreting and Evaluating Neural Network Robustness", "abstract": "<p>Recently, adversarial deception becomes one of the most considerable threats\nto deep neural networks. However, compared to extensive research in new designs\nof various adversarial attacks and defenses, the neural networks’ intrinsic\nrobustness property is still lack of thorough investigation. This work aims to\nqualitatively interpret the adversarial attack and defense mechanism through\nloss visualization, and establish a quantitative metric to evaluate the neural\nnetwork model’s intrinsic robustness. The proposed robustness metric identifies\nthe upper bound of a model’s prediction divergence in the given domain and thus\nindicates whether the model can maintain a stable prediction. With extensive\nexperiments, our metric demonstrates several advantages over conventional\nadversarial testing accuracy based robustness estimation: (1) it provides a\nuniformed evaluation to models with different structures and parameter scales;\n(2) it over-performs conventional accuracy based robustness estimation and\nprovides a more reliable evaluation that is invariant to different test\nsettings; (3) it can be fast generated without considerable testing cost.</p>\n", "tags": [] },
{"key": "zafar2021lack", "year": "2021", "title":"On the Lack of Robust Interpretability of Neural Text Classifiers", "abstract": "<p>With the ever-increasing complexity of neural language models, practitioners\nhave turned to methods for understanding the predictions of these models. One\nof the most well-adopted approaches for model interpretability is feature-based\ninterpretability, i.e., ranking the features in terms of their impact on model\npredictions. Several prior studies have focused on assessing the fidelity of\nfeature-based interpretability methods, i.e., measuring the impact of dropping\nthe top-ranked features on the model output. However, relatively little work\nhas been conducted on quantifying the robustness of interpretations. In this\nwork, we assess the robustness of interpretations of neural text classifiers,\nspecifically, those based on pretrained Transformer encoders, using two\nrandomization tests. The first compares the interpretations of two models that\nare identical except for their initializations. The second measures whether the\ninterpretations differ between a model with trained parameters and a model with\nrandom parameters. Both tests show surprising deviations from expected\nbehavior, raising questions about the extent of insights that practitioners may\ndraw from interpretations.</p>\n", "tags": [] },
{"key": "zahavy2016graying", "year": "2016", "title":"Graying the black box: Understanding DQNs", "abstract": "<p>In recent years there is a growing interest in using deep representations for\nreinforcement learning. In this paper, we present a methodology and tools to\nanalyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a\nnew model, the Semi Aggregated Markov Decision Process (SAMDP), and an\nalgorithm that learns it automatically. The SAMDP model allows us to identify\nspatio-temporal abstractions directly from features and may be used as a\nsub-goal detector in future work. Using our tools we reveal that the features\nlearned by DQNs aggregate the state space in a hierarchical fashion, explaining\nits success. Moreover, we are able to understand and describe the policies\nlearned by DQNs for three different Atari2600 games and suggest ways to\ninterpret, debug and optimize deep neural networks in reinforcement learning.</p>\n", "tags": [] },
{"key": "zeiler2013visualizing", "year": "2013", "title":"Visualizing and Understanding Convolutional Networks", "abstract": "<p>Large Convolutional Network models have recently demonstrated impressive\nclassification performance on the ImageNet benchmark. However there is no clear\nunderstanding of why they perform so well, or how they might be improved. In\nthis paper we address both issues. We introduce a novel visualization technique\nthat gives insight into the function of intermediate feature layers and the\noperation of the classifier. We also perform an ablation study to discover the\nperformance contribution from different model layers. This enables us to find\nmodel architectures that outperform Krizhevsky \\etal on the ImageNet\nclassification benchmark. We show our ImageNet model generalizes well to other\ndatasets: when the softmax classifier is retrained, it convincingly beats the\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.</p>\n", "tags": [] },
{"key": "zha2021invertible", "year": "2021", "title":"Invertible Attention", "abstract": "<p>Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention</p>\n", "tags": [] },
{"key": "zhang2016growing", "year": "2016", "title":"Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning", "abstract": "<p>This paper proposes a learning strategy that extracts object-part concepts\nfrom a pre-trained convolutional neural network (CNN), in an attempt to 1)\nexplore explicit semantics hidden in CNN units and 2) gradually grow a\nsemantically interpretable graphical model on the pre-trained CNN for\nhierarchical object understanding. Given part annotations on very few (e.g.,\n3-12) objects, our method mines certain latent patterns from the pre-trained\nCNN and associates them with different semantic parts. We use a four-layer\nAnd-Or graph to organize the mined latent patterns, so as to clarify their\ninternal semantic hierarchy. Our method is guided by a small number of part\nannotations, and it achieves superior performance (about 13%-107% improvement)\nin part center prediction on the PASCAL VOC and ImageNet datasets.</p>\n", "tags": [] },
{"key": "zhang2017interpretable", "year": "2017", "title":"Interpretable Convolutional Neural Networks", "abstract": "<p>This paper proposes a method to modify traditional convolutional neural\nnetworks (CNNs) into interpretable CNNs, in order to clarify knowledge\nrepresentations in high conv-layers of CNNs. In an interpretable CNN, each\nfilter in a high conv-layer represents a certain object part. We do not need\nany annotations of object parts or textures to supervise the learning process.\nInstead, the interpretable CNN automatically assigns each filter in a high\nconv-layer with an object part during the learning process. Our method can be\napplied to different types of CNNs with different structures. The clear\nknowledge representation in an interpretable CNN can help people understand the\nlogics inside a CNN, i.e., based on which patterns the CNN makes the decision.\nExperiments showed that filters in an interpretable CNN were more semantically\nmeaningful than those in traditional CNNs.</p>\n", "tags": [] },
{"key": "zhang2017interpreting", "year": "2017", "title":"Interpreting CNN Knowledge via an Explanatory Graph", "abstract": "<p>This paper learns a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering\nthat each filter in a conv-layer of a pre-trained CNN usually represents a\nmixture of object parts, we propose a simple yet efficient method to\nautomatically disentangles different part patterns from each filter, and\nconstruct an explanatory graph. In the explanatory graph, each node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More importantly, we learn the explanatory\ngraph for a pre-trained CNN in an unsupervised manner, i.e., without a need of\nannotating object parts. Experiments show that each graph node consistently\nrepresents the same object part through different images. We transfer part\npatterns in the explanatory graph to the task of part localization, and our\nmethod significantly outperforms other approaches.</p>\n", "tags": [] },
{"key": "zhang2018explanatory", "year": "2018", "title":"Explanatory Graphs for CNNs", "abstract": "<p>This paper introduces a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside conv-layers of a pre-trained CNN.\nEach filter in a conv-layer of a CNN for object classification usually\nrepresents a mixture of object parts. We develop a simple yet effective method\nto disentangle object-part pattern components from each filter. We construct an\nexplanatory graph to organize the mined part patterns, where a node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More crucially, given a pre-trained CNN, the\nexplanatory graph is learned without a need of annotating object parts.\nExperiments show that each graph node consistently represented the same object\npart through different images, which boosted the transferability of CNN\nfeatures. We transferred part patterns in the explanatory graph to the task of\npart localization, and our method significantly outperformed other approaches.</p>\n", "tags": [] },
{"key": "zhang2018interpretable", "year": "2018", "title":"Interpretable Deep Learning under Fire", "abstract": "<p>Providing explanations for deep neural network (DNN) models is crucial for\ntheir use in security-sensitive domains. A plethora of interpretation models\nhave been proposed to help users understand the inner workings of DNNs: how\ndoes a DNN arrive at a specific decision for a given input? The improved\ninterpretability is believed to offer a sense of security by involving human in\nthe decision-making process. Yet, due to its data-driven nature, the\ninterpretability itself is potentially susceptible to malicious manipulations,\nabout which little is known thus far.\n  Here we bridge this gap by conducting the first systematic study on the\nsecurity of interpretable deep learning systems (IDLSes). We show that existing\n\\imlses are highly vulnerable to adversarial manipulations. Specifically, we\npresent ADV^2, a new class of attacks that generate adversarial inputs not only\nmisleading target DNNs but also deceiving their coupled interpretation models.\nThrough empirical evaluation against four major types of IDLSes on benchmark\ndatasets and in security-critical applications (e.g., skin cancer diagnosis),\nwe demonstrate that with ADV^2 the adversary is able to arbitrarily designate\nan input’s prediction and interpretation. Further, with both analytical and\nempirical evidence, we identify the prediction-interpretation gap as one root\ncause of this vulnerability – a DNN and its interpretation model are often\nmisaligned, resulting in the possibility of exploiting both models\nsimultaneously. Finally, we explore potential countermeasures against ADV^2,\nincluding leveraging its low transferability and incorporating it in an\nadversarial training framework. Our findings shed light on designing and\noperating IDLSes in a more secure and informative fashion, leading to several\npromising research directions.</p>\n", "tags": [] },
{"key": "zhang2018interpreting", "year": "2018", "title":"Interpreting CNNs via Decision Trees", "abstract": "<p>This paper aims to quantitatively explain rationales of each prediction that\nis made by a pre-trained convolutional neural network (CNN). We propose to\nlearn a decision tree, which clarifies the specific reason for each prediction\nmade by the CNN at the semantic level. I.e., the decision tree decomposes\nfeature representations in high conv-layers of the CNN into elementary concepts\nof object parts. In this way, the decision tree tells people which object parts\nactivate which filters for the prediction and how much they contribute to the\nprediction score. Such semantic and quantitative explanations for CNN\npredictions have specific values beyond the traditional pixel-level analysis of\nCNNs. More specifically, our method mines all potential decision modes of the\nCNN, where each mode represents a common case of how the CNN uses object parts\nfor prediction. The decision tree organizes all potential decision modes in a\ncoarse-to-fine manner to explain CNN predictions at different fine-grained\nlevels. Experiments have demonstrated the effectiveness of the proposed method.</p>\n", "tags": [] },
{"key": "zhang2018non-rigid", "year": "2018", "title":"Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps", "abstract": "<p>In this paper, we propose a novel effective non-rigid object tracking\nframework based on the spatial-temporal consistent saliency detection. In\ncontrast to most existing trackers that utilize a bounding box to specify the\ntracked target, the proposed framework can extract accurate regions of the\ntarget as tracking outputs. It achieves a better description of the non-rigid\nobjects and reduces the background pollution for the tracking model.\nFurthermore, our model has several unique features. First, a tailored fully\nconvolutional neural network (TFCN) is developed to model the local saliency\nprior for a given image region, which not only provides the pixel-wise outputs\nbut also integrates the semantic information. Second, a novel multi-scale\nmulti-region mechanism is proposed to generate local saliency maps that\neffectively consider visual perceptions with different spatial layouts and\nscale variations. Subsequently, local saliency maps are fused via a weighted\nentropy method, resulting in a final discriminative saliency map. Finally, we\npresent a non-rigid object tracking algorithm based on the predicted saliency\nmaps. By utilizing a spatial-temporal consistent saliency map (STCSM), we\nconduct target-background classification and use a simple fine-tuning scheme\nfor online updating. Extensive experiments demonstrate that the proposed\nalgorithm achieves competitive performance in both saliency detection and\nvisual tracking, especially outperforming other related trackers on the\nnon-rigid object tracking datasets.</p>\n", "tags": [] },
{"key": "zhang2018visual", "year": "2018", "title":"Visual Interpretability for Deep Learning: a Survey", "abstract": "<p>This paper reviews recent studies in understanding neural-network\nrepresentations and learning neural networks with interpretable/disentangled\nmiddle-layer representations. Although deep neural networks have exhibited\nsuperior performance in various tasks, the interpretability is always the\nAchilles’ heel of deep neural networks. At present, deep neural networks obtain\nhigh discrimination power at the cost of low interpretability of their\nblack-box representations. We believe that high model interpretability may help\npeople to break several bottlenecks of deep learning, e.g., learning from very\nfew annotations, learning via human-computer communications at the semantic\nlevel, and semantically debugging network representations. We focus on\nconvolutional neural networks (CNNs), and we revisit the visualization of CNN\nrepresentations, methods of diagnosing representations of pre-trained CNNs,\napproaches for disentangling pre-trained CNN representations, learning of CNNs\nwith disentangled representations, and middle-to-end learning based on model\ninterpretability. Finally, we discuss prospective trends in explainable\nartificial intelligence.</p>\n", "tags": [] },
{"key": "zhang2019towards", "year": "2019", "title":"Towards a Unified Evaluation of Explanation Methods without Ground Truth", "abstract": "<p>This paper proposes a set of criteria to evaluate the objectiveness of\nexplanation methods of neural networks, which is crucial for the development of\nexplainable AI, but it also presents significant challenges. The core challenge\nis that people usually cannot obtain ground-truth explanations of the neural\nnetwork. To this end, we design four metrics to evaluate explanation results\nwithout ground-truth explanations. Our metrics can be broadly applied to nine\nbenchmark methods of interpreting neural networks, which provides new insights\nof explanation methods.</p>\n", "tags": [] },
{"key": "zhang2020survey", "year": "2020", "title":"A Survey on Neural Network Interpretability", "abstract": "<p>Along with the great success of deep neural networks, there is also growing\nconcern about their black-box nature. The interpretability issue affects\npeople’s trust on deep learning systems. It is also related to many ethical\nproblems, e.g., algorithmic discrimination. Moreover, interpretability is a\ndesired property for deep networks to become powerful tools in other research\nfields, e.g., drug discovery and genomics. In this survey, we conduct a\ncomprehensive review of the neural network interpretability research. We first\nclarify the definition of interpretability as it has been used in many\ndifferent contexts. Then we elaborate on the importance of interpretability and\npropose a novel taxonomy organized along three dimensions: type of engagement\n(passive vs. active interpretation approaches), the type of explanation, and\nthe focus (from local to global interpretability). This taxonomy provides a\nmeaningful 3D view of distribution of papers from the relevant literature as\ntwo of the dimensions are not simply categorical but allow ordinal\nsubcategories. Finally, we summarize the existing interpretability evaluation\nmethods and suggest possible research directions inspired by our new taxonomy.</p>\n", "tags": [] },
{"key": "zhang2021evaluating", "year": "2021", "title":"Evaluating Deep Graph Neural Networks", "abstract": "<p>Graph Neural Networks (GNNs) have already been widely applied in various\ngraph mining tasks. However, they suffer from the shallow architecture issue,\nwhich is the key impediment that hinders the model performance improvement.\nAlthough several relevant approaches have been proposed, none of the existing\nstudies provides an in-depth understanding of the root causes of performance\ndegradation in deep GNNs. In this paper, we conduct the first systematic\nexperimental evaluation to present the fundamental limitations of shallow\narchitectures. Based on the experimental results, we answer the following two\nessential questions: (1) what actually leads to the compromised performance of\ndeep GNNs; (2) when we need and how to build deep GNNs. The answers to the\nabove questions provide empirical insights and guidelines for researchers to\ndesign deep and well-performed GNNs. To show the effectiveness of our proposed\nguidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful\napproach (a paradigm in its own right) that helps guide deep GNN designs.\nExperimental results demonstrate three advantages of DGMLP: 1) high accuracy –\nit achieves state-of-the-art node classification performance on various\ndatasets; 2) high flexibility – it can flexibly choose different propagation\nand transformation depths according to graph size and sparsity; 3) high\nscalability and efficiency – it supports fast training on large-scale graphs.\nOur code is available in https://github.com/zwt233/DGMLP.</p>\n", "tags": [] },
{"key": "zhang2021head", "year": "2021", "title":"From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust", "abstract": "<p>Automated vehicles (AVs) are social robots that can potentially benefit our\nsociety. According to the existing literature, AV explanations can promote\npassengers’ trust by reducing the uncertainty associated with the AV’s\nreasoning and actions. However, the literature on AV explanations and trust has\nfailed to consider how the type of trust</p>\n<ul>\n  <li>cognitive versus affective - might alter this relationship. Yet, the\nexisting literature has shown that the implications associated with trust vary\nwidely depending on whether it is cognitive or affective. To address this\nshortcoming and better understand the impacts of explanations on trust in AVs,\nwe designed a study to investigate the effectiveness of explanations on both\ncognitive and affective trust. We expect these results to be of great\nsignificance in designing AV explanations to promote AV trust.</li>\n</ul>\n", "tags": [] },
{"key": "zhao2014person", "year": "2014", "title":"Person Re-identification by Saliency Learning", "abstract": "<p>Human eyes can recognize person identities based on small salient regions,\ni.e. human saliency is distinctive and reliable in pedestrian matching across\ndisjoint camera views. However, such valuable information is often hidden when\ncomputing similarities of pedestrian images with existing approaches. Inspired\nby our user study result of human perception on human saliency, we propose a\nnovel perspective for person re-identification based on learning human saliency\nand matching saliency distribution. The proposed saliency learning and matching\nframework consists of four steps: (1) To handle misalignment caused by drastic\nviewpoint change and pose variations, we apply adjacency constrained patch\nmatching to build dense correspondence between image pairs. (2) We propose two\nalternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a\nsaliency score for each image patch, through which distinctive features stand\nout without using identity labels in the training procedure. (3) saliency\nmatching is proposed based on patch matching. Matching patches with\ninconsistent saliency brings penalty, and images of the same identity are\nrecognized by minimizing the saliency matching cost. (4) Furthermore, saliency\nmatching is tightly integrated with patch matching in a unified structural\nRankSVM learning framework. The effectiveness of our approach is validated on\nthe VIPeR dataset and the CUHK01 dataset. Our approach outperforms the\nstate-of-the-art person re-identification methods on both datasets.</p>\n", "tags": [] },
{"key": "zhao2019confounder-aware", "year": "2019", "title":"Confounder-Aware Visualization of ConvNets", "abstract": "<p>With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.</p>\n", "tags": [] },
{"key": "zhao2020baylime", "year": "2020", "title":"BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations", "abstract": "<p>Given the pressing need for assuring algorithmic transparency, Explainable AI\n(XAI) has emerged as one of the key areas of AI research. In this paper, we\ndevelop a novel Bayesian extension to the LIME framework, one of the most\nwidely used approaches in XAI – which we call BayLIME. Compared to LIME,\nBayLIME exploits prior knowledge and Bayesian reasoning to improve both the\nconsistency in repeated explanations of a single prediction and the robustness\nto kernel settings. BayLIME also exhibits better explanation fidelity than the\nstate-of-the-art (LIME, SHAP and GradCAM) by its ability to integrate prior\nknowledge from, e.g., a variety of other XAI techniques, as well as\nverification and validation (V&amp;V) methods. We demonstrate the desirable\nproperties of BayLIME through both theoretical analysis and extensive\nexperiments.</p>\n", "tags": [] },
{"key": "zhou2014object", "year": "2014", "title":"Object Detectors Emerge in Deep Scene CNNs", "abstract": "<p>With the success of new computational architectures for visual processing,\nsuch as convolutional neural networks (CNN) and access to image databases with\nmillions of labeled examples (e.g., ImageNet, Places), the state of the art in\ncomputer vision is advancing rapidly. One important factor for continued\nprogress is to understand the representations that are learned by the inner\nlayers of these deep architectures. Here we show that object detectors emerge\nfrom training CNNs to perform scene classification. As scenes are composed of\nobjects, the CNN for scene classification automatically discovers meaningful\nobjects detectors, representative of the learned scene categories. With object\ndetectors emerging as a result of learning to recognize scenes, our work\ndemonstrates that the same network can perform both scene recognition and\nobject localization in a single forward-pass, without ever having been\nexplicitly taught the notion of objects.</p>\n", "tags": [] },
{"key": "zhou2015learning", "year": "2015", "title":"Learning Deep Features for Discriminative Localization", "abstract": "<p>In this work, we revisit the global average pooling layer proposed in [13],\nand shed light on how it explicitly enables the convolutional neural network to\nhave remarkable localization ability despite being trained on image-level\nlabels. While this technique was previously proposed as a means for\nregularizing training, we find that it actually builds a generic localizable\ndeep representation that can be applied to a variety of tasks. Despite the\napparent simplicity of global average pooling, we are able to achieve 37.1%\ntop-5 error for object localization on ILSVRC 2014, which is remarkably close\nto the 34.2% top-5 error achieved by a fully supervised CNN approach. We\ndemonstrate that our network is able to localize the discriminative image\nregions on a variety of tasks despite not being trained for them</p>\n", "tags": [] },
{"key": "zhou2017interpreting", "year": "2017", "title":"Interpreting Deep Visual Representations via Network Dissection", "abstract": "<p>The success of recent deep convolutional neural networks (CNNs) depends on\nlearning hidden representations that can summarize the important factors of\nvariation behind the data. However, CNNs often criticized as being black boxes\nthat lack interpretability, since they have millions of unexplained model\nparameters. In this work, we describe Network Dissection, a method that\ninterprets networks by providing labels for the units of their deep visual\nrepresentations. The proposed method quantifies the interpretability of CNN\nrepresentations by evaluating the alignment between individual hidden units and\na set of visual semantic concepts. By identifying the best alignments, units\nare given human interpretable labels across a range of objects, parts, scenes,\ntextures, materials, and colors. The method reveals that deep representations\nare more transparent and interpretable than expected: we find that\nrepresentations are significantly more interpretable than they would be under a\nrandom equivalently powerful basis. We apply the method to interpret and\ncompare the latent representations of various network architectures trained to\nsolve different supervised and self-supervised training tasks. We then examine\nfactors affecting the network interpretability such as the number of the\ntraining iterations, regularizations, different initializations, and the\nnetwork depth and width. Finally we show that the interpreted units can be used\nto provide explicit explanations of a prediction given by a CNN for an image.\nOur results highlight that interpretability is an important property of deep\nneural networks that provides new insights into their hierarchical structure.</p>\n", "tags": [] },
{"key": "zhou2022solvability", "year": "2022", "title":"The Solvability of Interpretability Evaluation Metrics", "abstract": "<p>Feature attribution methods are popular for explaining neural network\npredictions, and they are often evaluated on metrics such as comprehensiveness\nand sufficiency, which are motivated by the principle that more important\nfeatures – as judged by the explanation – should have larger impacts on model\nprediction. In this paper, we highlight an intriguing property of these\nmetrics: their solvability. Concretely, we can define the problem of optimizing\nan explanation for a metric and solve it using beam search. This brings up the\nobvious question: given such solvability, why do we still develop other\nexplainers and then evaluate them on the metric? We present a series of\ninvestigations showing that this beam search explainer is generally comparable\nor favorable to current choices such as LIME and SHAP, suggest rethinking the\ngoals of model interpretability, and identify several directions towards better\nevaluations of new method proposals.</p>\n", "tags": [] },
{"key": "zhu2018exploiting", "year": "2018", "title":"Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection", "abstract": "<p>Saliency detection aims to detect the most attractive objects in images and\nis widely used as a foundation for various applications. In this paper, we\npropose a novel salient object detection algorithm for RGB-D images using\ncenter-dark channel priors. First, we generate an initial saliency map based on\na color saliency map and a depth saliency map of a given RGB-D image. Then, we\ngenerate a center-dark channel map based on center saliency and dark channel\npriors. Finally, we fuse the initial saliency map with the center dark channel\nmap to generate the final saliency map. Extensive evaluations over four\nbenchmark datasets demonstrate that our proposed method performs favorably\nagainst most of the state-of-the-art approaches. Besides, we further discuss\nthe application of the proposed algorithm in small target detection and\ndemonstrate the universal value of center-dark channel priors in the field of\nobject detection.</p>\n", "tags": [] },
{"key": "zhu2021going", "year": "2021", "title":"Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective", "abstract": "<p>Convolutional neural network (CNN) is one of the most widely-used successful\narchitectures in the era of deep learning. However, the high-computational cost\nof CNN still hampers more universal uses to light devices. Fortunately, the\nFourier transform on convolution gives an elegant and promising solution to\ndramatically reduce the computation cost. Recently, some studies devote to such\na challenging problem and pursue the complete frequency computation without any\nswitching between spatial domain and frequent domain. In this work, we revisit\nthe Fourier transform theory to derive feed-forward and back-propagation\nfrequency operations of typical network modules such as convolution, activation\nand pooling. Due to the calculation limitation of complex numbers on most\ncomputation tools, we especially extend the Fourier transform to the Laplace\ntransform for CNN, which can run in the real domain with more relaxed\nconstraints. This work more focus on a theoretical extension and discussion\nabout frequency CNN, and lay some theoretical ground for real application.</p>\n", "tags": [] },
{"key": "zunino2020explainable", "year": "2020", "title":"Explainable Deep Classification Models for Domain Generalization", "abstract": "<p>Conventionally, AI models are thought to trade off explainability for lower\naccuracy. We develop a training strategy that not only leads to a more\nexplainable AI system for object classification, but as a consequence, suffers\nno perceptible accuracy degradation. Explanations are defined as regions of\nvisual evidence upon which a deep classification network makes a decision. This\nis represented in the form of a saliency map conveying how much each pixel\ncontributed to the network’s decision. Our training strategy enforces a\nperiodic saliency-based feedback to encourage the model to focus on the image\nregions that directly correspond to the ground-truth object. We quantify\nexplainability using an automated metric, and using human judgement. We propose\nexplainability as a means for bridging the visual-semantic gap between\ndifferent domains where model explanations are used as a means of disentagling\ndomain specific information from otherwise relevant features. We demonstrate\nthat this leads to improved generalization to new domains without hindering\nperformance on the original domain.</p>\n", "tags": [] }

]

