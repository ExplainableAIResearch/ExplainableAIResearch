<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Global Site Tag (gtag.js) - Google Analytics -->
  <!-- TODO - Enable this when we have the Google Analytics page set up -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-107339008-1"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments)};
    gtag('js', new Date());
    gtag('config', 'UA-107339008-1');
  </script> -->

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <!-- TODO - Add more keywords for SEO -->
  <meta name="keywords" content="xai, explainable ai, explainability, interpretablity">

  <title>
    
      Search all Publications on Machine Learning for Source Code &middot; Explainable AI
    
  </title>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <!-- TODO - Write a better description -->
  <!-- <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="ML4Code" /> -->

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.12.1/css/dataTables.bootstrap5.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.12.1/js/jquery.dataTables.min.js"></script>
</head>


  <body class="theme-base-0f layout">

    <!-- <a href='/contributing.html' class='ribbon'>Contribute!</a> -->
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Explainable AI
        </a>
      </h1>
      <p class="lead">Current research on explainability and interpretability of machine learning algorithms</p>      
    </div>

  <nav class="sidebar-nav">
   <div class="sidebar-item">
    <p style="font-size: 12px">
      <input type='text' id='searchTarget' placeholder="Search Repository"/> 
      <button class="button-23 draw" onClick="search();"><i class="fa fa-search"></i></button>
    </p>
  </div>
   <a class="sidebar-nav-item active" href="/papers.html">List of Papers</a>
   <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
   <!-- <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a> -->
   <!-- <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a> -->
  <a class="sidebar-nav-item" href="/resources.html">Resources</a>
  <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
  </nav>

  <div class="sidebar-item">
    <p style="font-size: 12px">This site is a community effort by the <a href="explainableaiworld.slack.com">Explainable AI</a> members. Please join the group and reach out to the administrators if you have any questions.</p>
    <p style="font-size: 12px"><span style="font-size: 12px">
      Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>. Idea and base code for the website adapted from <a href="https://ml4code.github.io/">ml4code</a>.
    </span></p>
  </div>
</div></div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      Search across all paper titles, abstracts, authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work.

<table id="allPapers">
<thead><th>Year</th><th>Title</th><th>Authors</th><th>Venue</th><th>Abstract</th></thead><tbody>



<tr>
	<td>2022</td>
	<td><a href="/publications/stalder2022see/">What You See is What You Classify: Black Box Attributions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What You See is What You Classify: Black Box Attributions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What You See is What You Classify: Black Box Attributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Steven Stalder, Nathanaël Perraudin, Radhakrishna Achanta, Fernando Perez-Cruz, Michele Volpi</td>
	<td></td>
	<td><p>An important step towards explaining deep image classifiers lies in the
identification of image regions that contribute to individual class scores in
the model’s output. However, doing this accurately is a difficult task due to
the black-box nature of such networks. Most existing approaches find such
attributions either using activations and gradients or by repeatedly perturbing
the input. We instead address this challenge by training a second deep network,
the Explainer, to predict attributions for a pre-trained black-box classifier,
the Explanandum. These attributions are in the form of masks that only show the
classifier-relevant parts of an image, masking out the rest. Our approach
produces sharper and more boundary-precise masks when compared to the saliency
maps generated by other methods. Moreover, unlike most existing approaches,
ours is capable of directly generating very distinct class-specific masks.
Finally, the proposed method is very efficient for inference since it only
takes a single forward pass through the Explainer to generate all
class-specific masks. We show that our attributions are superior to established
methods both visually and quantitatively, by evaluating them on the PASCAL
VOC-2007 and Microsoft COCO-2014 datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/r%C3%A4uker2022toward/">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell</td>
	<td></td>
	<td><p>The last decade of machine learning has seen drastic increases in scale and
capabilities, and deep neural networks (DNNs) are increasingly being deployed
across a wide range of domains. However, the inner workings of DNNs are
generally difficult to understand, raising concerns about the safety of using
these systems without a rigorous understanding of how they function. In this
survey, we review literature on techniques for interpreting the inner
components of DNNs, which we call “inner” interpretability methods.
Specifically, we review methods for interpreting weights, neurons, subnetworks,
and latent representations with a focus on how these techniques relate to the
goal of designing safer, more trustworthy AI systems. We also highlight
connections between interpretability and work in modularity, adversarial
robustness, continual learning, network compression, and studying the human
visual system. Finally, we discuss key challenges and argue for future work in
interpretability for AI safety that focuses on diagnostics, benchmarking, and
robustness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/benshmuel2022meet/">Meet You Halfway: Explaining Deep Learning Mysteries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Meet You Halfway: Explaining Deep Learning Mysteries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Meet You Halfway: Explaining Deep Learning Mysteries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Oriel BenShmuel</td>
	<td></td>
	<td><p>Deep neural networks perform exceptionally well on various learning tasks
with state-of-the-art results. While these models are highly expressive and
achieve impressively accurate solutions with excellent generalization
abilities, they are susceptible to minor perturbations. Samples that suffer
such perturbations are known as “adversarial examples”. Even though deep
learning is an extensively researched field, many questions about the nature of
deep learning models remain unanswered. In this paper, we introduce a new
conceptual framework attached with a formal description that aims to shed light
on the network’s behavior and interpret the behind-the-scenes of the learning
process. Our framework provides an explanation for inherent questions
concerning deep learning. Particularly, we clarify: (1) Why do neural networks
acquire generalization abilities? (2) Why do adversarial examples transfer
between different models?. We provide a comprehensive set of experiments that
support this new framework, as well as its underlying theory.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/rao2022towards/">Towards Better Understanding Attribution Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Better Understanding Attribution Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Better Understanding Attribution Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sukrut Rao, Moritz Böhle, Bernt Schiele</td>
	<td></td>
	<td><p>Deep neural networks are very successful on many vision tasks, but hard to
interpret due to their black box nature. To overcome this, various post-hoc
attribution methods have been proposed to identify image regions most
influential to the models’ decisions. Evaluating such methods is challenging
since no ground truth attributions exist. We thus propose three novel
evaluation schemes to more reliably measure the faithfulness of those methods,
to make comparisons between them more fair, and to make visual inspection more
systematic. To address faithfulness, we propose a novel evaluation setting
(DiFull) in which we carefully control which parts of the input can influence
the output in order to distinguish possible from impossible attributions. To
address fairness, we note that different methods are applied at different
layers, which skews any comparison, and so evaluate all methods on the same
layers (ML-Att) and discuss how this impacts their performance on quantitative
metrics. For more systematic visualizations, we propose a scheme (AggAtt) to
qualitatively evaluate the methods on complete datasets. We use these
evaluation schemes to study strengths and shortcomings of some widely used
attribution methods. Finally, we propose a post-processing smoothing step that
significantly improves the performance of some attribution methods, and discuss
its applicability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ramamurthy2022analogies/">Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Karthikeyan Natesan Ramamurthy, Amit Dhurandhar, Dennis Wei, Zaid Bin Tariq</td>
	<td></td>
	<td><p>Post-hoc explanations for black box models have been studied extensively in
classification and regression settings. However, explanations for models that
output similarity between two inputs have received comparatively lesser
attention. In this paper, we provide model agnostic local explanations for
similarity learners applicable to tabular and text data. We first propose a
method that provides feature attributions to explain the similarity between a
pair of inputs as determined by a black box similarity learner. We then propose
analogies as a new form of explanation in machine learning. Here the goal is to
identify diverse analogous pairs of examples that share the same level of
similarity as the input pair and provide insight into (latent) factors
underlying the model’s prediction. The selection of analogies can optionally
leverage feature attributions, thus connecting the two forms of explanation
while still maintaining complementarity. We prove that our analogy objective
function is submodular, making the search for good-quality analogies efficient.
We apply the proposed approaches to explain similarities between sentences as
predicted by a state-of-the-art sentence encoder, and between patients in a
healthcare utilization application. Efficacy is measured through quantitative
evaluations, a careful user study, and examples of explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/zhou2022solvability/">The Solvability of Interpretability Evaluation Metrics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Solvability of Interpretability Evaluation Metrics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Solvability of Interpretability Evaluation Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yilun Zhou, Julie Shah</td>
	<td></td>
	<td><p>Feature attribution methods are popular for explaining neural network
predictions, and they are often evaluated on metrics such as comprehensiveness
and sufficiency, which are motivated by the principle that more important
features – as judged by the explanation – should have larger impacts on model
prediction. In this paper, we highlight an intriguing property of these
metrics: their solvability. Concretely, we can define the problem of optimizing
an explanation for a metric and solve it using beam search. This brings up the
obvious question: given such solvability, why do we still develop other
explainers and then evaluate them on the metric? We present a series of
investigations showing that this beam search explainer is generally comparable
or favorable to current choices such as LIME and SHAP, suggest rethinking the
goals of model interpretability, and identify several directions towards better
evaluations of new method proposals.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/park2022vision/">How Do Vision Transformers Work?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Do Vision Transformers Work?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Do Vision Transformers Work?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Namuk Park, Songkuk Kim</td>
	<td></td>
	<td><p>The success of multi-head self-attentions (MSAs) for computer vision is now
indisputable. However, little is known about how MSAs work. We present
fundamental explanations to help better understand the nature of MSAs. In
particular, we demonstrate the following properties of MSAs and Vision
Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization
by flattening the loss landscapes. Such improvement is primarily attributable
to their data specificity, not long-range dependency. On the other hand, ViTs
suffer from non-convex losses. Large datasets and loss landscape smoothing
methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.
For example, MSAs are low-pass filters, but Convs are high-pass filters.
Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks
behave like a series connection of small individual models. In addition, MSAs
at the end of a stage play a key role in prediction. Based on these insights,
we propose AlterNet, a model in which Conv blocks at the end of a stage are
replaced with MSA blocks. AlterNet outperforms CNNs not only in large data
regimes but also in small data regimes. The code is available at
https://github.com/xxxnell/how-do-vits-work.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/norelli2022explanatory/">Explanatory Learning: Beyond Empiricism in Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Learning: Beyond Empiricism in Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Learning: Beyond Empiricism in Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Antonio Norelli, Giorgio Mariani, Luca Moschella, Andrea Santilli, Giambattista Parascandolo, Simone Melzi, Emanuele Rodolà</td>
	<td></td>
	<td><p>We introduce Explanatory Learning (EL), a framework to let machines use
existing knowledge buried in symbolic sequences – e.g. explanations written in
hieroglyphic – by autonomously learning to interpret them. In EL, the burden
of interpreting symbols is not left to humans or rigid human-coded compilers,
as done in Program Synthesis. Rather, EL calls for a learned interpreter, built
upon a limited collection of symbolic sequences paired with observations of
several phenomena. This interpreter can be used to make predictions on a novel
phenomenon given its explanation, and even to find that explanation using only
a handful of observations, like human scientists do. We formulate the EL
problem as a simple binary classification task, so that common end-to-end
approaches aligned with the dominant empiricist view of machine learning could,
in principle, solve it. To these models, we oppose Critical Rationalist
Networks (CRNs), which instead embrace a rationalist view on the acquisition of
knowledge. CRNs express several desired properties by construction, they are
truly explainable, can adjust their processing at test-time for harder
inferences, and can offer strong confidence guarantees on their predictions. As
a final contribution, we introduce Odeen, a basic EL environment that simulates
a small flatland-style universe full of phenomena to explain. Using Odeen as a
testbed, we show how CRNs outperform empiricist end-to-end approaches of
similar size and architecture (Transformers) in discovering explanations for
novel phenomena.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/nguyen2022visual/">Visual correspondence-based explanations improve AI robustness and human-AI team accuracy</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual correspondence-based explanations improve AI robustness and human-AI team accuracy' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual correspondence-based explanations improve AI robustness and human-AI team accuracy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Giang Nguyen, Mohammad Reza Taesiri, Anh Nguyen</td>
	<td></td>
	<td><p>Explaining artificial intelligence (AI) predictions is increasingly important
and even imperative in many high-stakes applications where humans are the
ultimate decision-makers. In this work, we propose two novel architectures of
self-interpretable image classifiers that first explain, and then predict (as
opposed to post-hoc explanations) by harnessing the visual correspondences
between a query image and exemplars. Our models consistently improve (by 1 to 4
points) on out-of-distribution (OOD) datasets while performing marginally worse
(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest
neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,
our correspondence-based explanations are found to be more useful to users than
kNN explanations. Our explanations help users more accurately reject AI’s wrong
decisions than all other tested methods. Interestingly, for the first time, we
show that it is possible to achieve complementary human-AI team accuracy (i.e.,
that is higher than either AI-alone or human-alone), in ImageNet and CUB image
classification tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/nauta2022anecdotal/">From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice van Keulen, Christin Seifert</td>
	<td></td>
	<td><p>The rising popularity of explainable artificial intelligence (XAI) to
understand high-performing black boxes, also raised the question of how to
evaluate explanations of machine learning (ML) models. While interpretability
and explainability are often presented as a subjectively validated binary
property, we consider it a multi-faceted concept. We identify 12 conceptual
properties, such as Compactness and Correctness, that should be evaluated for
comprehensively assessing the quality of an explanation. Our so-called Co-12
properties serve as categorization scheme for systematically reviewing the
evaluation practice of more than 300 papers published in the last 7 years at
major AI and ML conferences that introduce an XAI method. We find that 1 in 3
papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate
with users. We also contribute to the call for objective, quantifiable
evaluation methods by presenting an extensive overview of quantitative XAI
evaluation methods. This systematic collection of evaluation methods provides
researchers and practitioners with concrete tools to thoroughly validate,
benchmark and compare new and existing XAI methods. This also opens up
opportunities to include quantitative metrics as optimization criteria during
model training in order to optimize for accuracy and interpretability
simultaneously.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/m%C3%BCller2022interactive/">An Interactive Explanatory AI System for Industrial Quality Control</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Interactive Explanatory AI System for Industrial Quality Control' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Interactive Explanatory AI System for Industrial Quality Control' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dennis Müller, Michael März, Stephan Scheele, Ute Schmid</td>
	<td></td>
	<td><p>Machine learning based image classification algorithms, such as deep neural
network approaches, will be increasingly employed in critical settings such as
quality control in industry, where transparency and comprehensibility of
decisions are crucial. Therefore, we aim to extend the defect detection task
towards an interactive human-in-the-loop approach that allows us to integrate
rich background knowledge and the inference of complex relationships going
beyond traditional purely data-driven approaches. We propose an approach for an
interactive support system for classifications in an industrial quality control
setting that combines the advantages of both (explainable) knowledge-driven and
data-driven machine learning methods, in particular inductive logic programming
and convolutional neural networks, with human expertise and control. The
resulting system can assist domain experts with decisions, provide transparent
explanations for results, and integrate feedback from users; thus reducing
workload for humans while both respecting their expertise and without removing
their agency or accountability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/moorman2022people/">Do People Trust Robots that Learn in the Home?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Do People Trust Robots that Learn in the Home?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Do People Trust Robots that Learn in the Home?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nina Moorman, Matthew Gombolay</td>
	<td></td>
	<td><p>It is not scalable for assistive robotics to have all functionalities
pre-programmed prior to user introduction. Instead, it is more realistic for
agents to perform supplemental on site learning. This opportunity to learn user
and environment particularities is especially helpful for care robots that
assist with individualized caregiver activities in residential or nursing home
environments. Many assistive robots, ranging in complexity from Roomba to
Pepper, already conduct some of their learning in the home, observable to the
user. We lack an understanding of how witnessing this learning impacts the
user. Thus, we propose to assess end-user attitudes towards the concept of
embodied robots that conduct some learning in the home as compared to robots
that are delivered fully-capable. In this virtual, between-subjects study, we
recruit end users (care-givers and care-takers) from nursing homes, and
investigate user trust in three different domains: navigation, manipulation,
and preparation. Informed by the first study where we identify agent learning
as a key factor in determining trust, we propose a second study to explore how
to modulate that trust. This second, in-person study investigates the
effectiveness of apologies, explanations of robot failure, and transparency of
learning at improving trust in embodied learning robots.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/lertvittayakumjorn2022argumentative/">Argumentative Explanations for Pattern-Based Text Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Argumentative Explanations for Pattern-Based Text Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Argumentative Explanations for Pattern-Based Text Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Piyawat Lertvittayakumjorn, Francesca Toni</td>
	<td></td>
	<td><p>Recent works in Explainable AI mostly address the transparency issue of
black-box models or create explanations for any kind of models (i.e., they are
model-agnostic), while leaving explanations of interpretable models largely
underexplored. In this paper, we fill this gap by focusing on explanations for
a specific interpretable model, namely pattern-based logistic regression (PLR)
for binary text classification. We do so because, albeit interpretable, PLR is
challenging when it comes to explanations. In particular, we found that a
standard way to extract explanations from this model does not consider
relations among the features, making the explanations hardly plausible to
humans. Hence, we propose AXPLR, a novel explanation method using (forms of)
computational argumentation to generate explanations (for outputs computed by
PLR) which unearth model agreements and disagreements among the features.
Specifically, we use computational argumentation as follows: we see features
(patterns) in PLR as arguments in a form of quantified bipolar argumentation
frameworks (QBAFs) and extract attacks and supports between arguments based on
specificity of the arguments; we understand logistic regression as a gradual
semantics for these QBAFs, used to determine the arguments’ dialectic strength;
and we study standard properties of gradual semantics for QBAFs in the context
of our argumentative re-interpretation of PLR, sanctioning its suitability for
explanatory purposes. We then show how to extract intuitive explanations (for
outputs computed by PLR) from the constructed QBAFs. Finally, we conduct an
empirical evaluation and two experiments in the context of human-AI
collaboration to demonstrate the advantages of our resulting AXPLR method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/khakzar2022explanations/">Do Explanations Explain? Model Knows Best</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Do Explanations Explain? Model Knows Best' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Do Explanations Explain? Model Knows Best' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ashkan Khakzar, Pedram Khorsandi, Rozhin Nobahari, Nassir Navab</td>
	<td></td>
	<td><p>It is a mystery which input features contribute to a neural network’s output.
Various explanation (feature attribution) methods are proposed in the
literature to shed light on the problem. One peculiar observation is that these
explanations (attributions) point to different features as being important. The
phenomenon raises the question, which explanation to trust? We propose a
framework for evaluating the explanations using the neural network model
itself. The framework leverages the network to generate input features that
impose a particular behavior on the output. Using the generated features, we
devise controlled experimental setups to evaluate whether an explanation method
conforms to an axiom. Thus we propose an empirical framework for axiomatic
evaluation of explanation methods. We evaluate well-known and promising
explanation solutions using the proposed framework. The framework provides a
toolset to reveal properties and drawbacks within existing and future
explanation solutions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ivankay2022fooling/">Fooling Explanations in Text Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fooling Explanations in Text Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fooling Explanations in Text Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard</td>
	<td>International Conference on Learning Representations, 2022</td>
	<td><p>State-of-the-art text classification models are becoming increasingly reliant
on deep neural networks (DNNs). Due to their black-box nature, faithful and
robust explanation methods need to accompany classifiers for deployment in
real-life scenarios. However, it has been shown in vision applications that
explanation methods are susceptible to local, imperceptible perturbations that
can significantly alter the explanations without changing the predicted
classes. We show here that the existence of such perturbations extends to text
classifiers as well. Specifically, we introduceTextExplanationFooler (TEF), a
novel explanation attack algorithm that alters text input samples imperceptibly
so that the outcome of widely-used explanation methods changes considerably
while leaving classifier predictions unchanged. We evaluate the performance of
the attribution robustness estimation performance in TEF on five sequence
classification datasets, utilizing three DNN architectures and three
transformer architectures for each dataset. TEF can significantly decrease the
correlation between unchanged and perturbed input attributions, which shows
that all models and explanation methods are susceptible to TEF perturbations.
Moreover, we evaluate how the perturbations transfer to other model
architectures and attribution methods, and show that TEF perturbations are also
effective in scenarios where the target model and explanation method are
unknown. Finally, we introduce a semi-universal attack that is able to compute
fast, computationally light perturbations with no knowledge of the attacked
classifier nor explanation method. Overall, our work shows that explanations in
text classifiers are very fragile and users need to carefully address their
robustness before relying on them in critical applications.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/h%C3%B6llig2022tsinterpret/">TSInterpret: A unified framework for time series interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TSInterpret: A unified framework for time series interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TSInterpret: A unified framework for time series interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jacqueline Höllig, Cedric Kulbach, Steffen Thoma</td>
	<td></td>
	<td><p>With the increasing application of deep learning algorithms to time series
classification, especially in high-stake scenarios, the relevance of
interpreting those algorithms becomes key. Although research in time series
interpretability has grown, accessibility for practitioners is still an
obstacle. Interpretability approaches and their visualizations are diverse in
use without a unified API or framework. To close this gap, we introduce
TSInterpret an easily extensible open-source Python library for interpreting
predictions of time series classifiers that combines existing interpretation
approaches into one unified framework. The library features (i)
state-of-the-art interpretability algorithms, (ii) exposes a unified API
enabling users to work with explanations consistently and provides (iii)
suitable visualizations for each explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/alregib2022explanatory/">Explanatory Paradigms in Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Paradigms in Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Paradigms in Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ghassan AlRegib, Mohit Prabhushankar</td>
	<td></td>
	<td><p>In this article, we present a leap-forward expansion to the study of
explainability in neural networks by considering explanations as answers to
abstract reasoning-based questions. With $P$ as the prediction from a neural
network, these questions are <code class="language-plaintext highlighter-rouge">Why P?', </code>What if not P?’, and `Why P, rather
than Q?’ for a given contrast prediction $Q$. The answers to these questions
are observed correlations, observed counterfactuals, and observed contrastive
explanations respectively. Together, these explanations constitute the
abductive reasoning scheme. We term the three explanatory schemes as observed
explanatory paradigms. The term observed refers to the specific case of
post-hoc explainability, when an explanatory technique explains the decision
$P$ after a trained neural network has made the decision $P$. The primary
advantage of viewing explanations through the lens of abductive reasoning-based
questions is that explanations can be used as reasons while making decisions.
The post-hoc field of explainability, that previously only justified decisions,
becomes active by being involved in the decision making process and providing
limited, but relevant and contextual interventions. The contributions of this
article are: ($i$) realizing explanations as reasoning paradigms, ($ii$)
providing a probabilistic definition of observed explanations and their
completeness, ($iii$) creating a taxonomy for evaluation of explanations, and
($iv$) positioning gradient-based complete explanainability’s replicability and
reproducibility across multiple applications and data modalities, ($v$) code
repositories, publicly available at
https://github.com/olivesgatech/Explanatory-Paradigms.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ai2022explanatory/">Explanatory machine learning for sequential human teaching</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory machine learning for sequential human teaching' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory machine learning for sequential human teaching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lun Ai, Johannes Langer, Stephen H. Muggleton, Ute Schmid</td>
	<td></td>
	<td><p>The topic of comprehensibility of machine-learned theories has recently drawn
increasing attention. Inductive Logic Programming (ILP) uses logic programming
to derive logic theories from small data based on abduction and induction
techniques. Learned theories are represented in the form of rules as
declarative descriptions of obtained knowledge. In earlier work, the authors
provided the first evidence of a measurable increase in human comprehension
based on machine-learned logic rules for simple classification tasks. In a
later study, it was found that the presentation of machine-learned explanations
to humans can produce both beneficial and harmful effects in the context of
game learning. We continue our investigation of comprehensibility by examining
the effects of the ordering of concept presentations on human comprehension. In
this work, we examine the explanatory effects of curriculum order and the
presence of machine-learned explanations for sequential problem-solving. We
show that 1) there exist tasks A and B such that learning A before B has a
better human comprehension with respect to learning B before A and 2) there
exist tasks A and B such that the presence of explanations when learning A
contributes to improved human comprehension when subsequently learning B. We
propose a framework for the effects of sequential teaching on comprehension
based on an existing definition of comprehensibility and provide evidence for
support from data collected in human trials. Empirical results show that
sequential teaching of concepts with increasing complexity a) has a beneficial
effect on human comprehension and b) leads to human re-discovery of
divide-and-conquer problem-solving strategies, and c) studying machine-learned
explanations allows adaptations of human problem-solving strategy with better
performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/herm2022stop/">Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lukas-Valentin Herm, Kai Heinrich, Jonas Wanner, Christian Janiesch</td>
	<td>International Journal of Information Management, 2002, 102538</td>
	<td><p>Machine learning algorithms enable advanced decision making in contemporary
intelligent systems. Research indicates that there is a tradeoff between their
model performance and explainability. Machine learning models with higher
performance are often based on more complex algorithms and therefore lack
explainability and vice versa. However, there is little to no empirical
evidence of this tradeoff from an end user perspective. We aim to provide
empirical evidence by conducting two user experiments. Using two distinct
datasets, we first measure the tradeoff for five common classes of machine
learning algorithms. Second, we address the problem of end user perceptions of
explainable artificial intelligence augmentations aimed at increasing the
understanding of the decision logic of high-performing complex models. Our
results diverge from the widespread assumption of a tradeoff curve and indicate
that the tradeoff between model performance and explainability is much less
gradual in the end user’s perception. This is a stark contrast to assumed
inherent model interpretability. Further, we found the tradeoff to be
situational for example due to data complexity. Results of our second
experiment show that while explainable artificial intelligence augmentations
can be used to increase explainability, the type of explanation plays an
essential role in end user perception.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ge2022explainable/">Explainable Fairness in Recommendation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Fairness in Recommendation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Fairness in Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yingqiang Ge, Juntao Tan, Yan Zhu, Yinglong Xia, Jiebo Luo, Shuchang Liu, Zuohui Fu, Shijie Geng, Zelong Li, Yongfeng Zhang</td>
	<td></td>
	<td><p>Existing research on fairness-aware recommendation has mainly focused on the
quantification of fairness and the development of fair recommendation models,
neither of which studies a more substantial problem–identifying the underlying
reason of model disparity in recommendation. This information is critical for
recommender system designers to understand the intrinsic recommendation
mechanism and provides insights on how to improve model fairness to decision
makers. Fortunately, with the rapid development of Explainable AI, we can use
model explainability to gain insights into model (un)fairness. In this paper,
we study the problem of explainable fairness, which helps to gain insights
about why a system is fair or unfair, and guides the design of fair recommender
systems with a more informed and unified methodology. Particularly, we focus on
a common setting with feature-aware recommendation and exposure unfairness, but
the proposed explainable fairness framework is general and can be applied to
other recommendation settings and fairness definitions. We propose a
Counterfactual Explainable Fairness framework, called CEF, which generates
explanations about model fairness that can improve the fairness without
significantly hurting the performance.The CEF framework formulates an
optimization problem to learn the “minimal” change of the input features that
changes the recommendation results to a certain level of fairness. Based on the
counterfactual recommendation result of each feature, we calculate an
explainability score in terms of the fairness-utility trade-off to rank all the
feature-based explanations, and select the top ones as fairness explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/dalvi2022towards/">Towards Teachable Reasoning Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Teachable Reasoning Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Teachable Reasoning Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bhavana Dalvi, Oyvind Tafjord, Peter Clark</td>
	<td></td>
	<td><p>Our goal is a teachable reasoning system for question-answering (QA), where a
user can interact with faithful answer explanations, and correct errors so that
the system improves over time. Our approach is three-fold: First, generated
chains of reasoning show how answers are implied by the system’s own internal
beliefs. Second, users can interact with the explanations to identify erroneous
model beliefs and provide corrections. Third, we augment the model with a
dynamic memory of such corrections. Retrievals from memory are used as
additional context for QA, to help avoid previous mistakes in similar new
situations - a novel type of memory-based continuous learning. To our
knowledge, this is the first system to generate chains that are both faithful
(the answer follows from the reasoning) and truthful (the chain reflects the
system’s own beliefs, as ascertained by self-querying). In evaluation, users
judge that a majority (65%+) of generated chains clearly show how an answer
follows from a set of facts - substantially better than a high-performance
baseline. We also find that using simulated feedback, our system (called
EntailmentWriter) continually improves with time, requiring feedback on only
25% of training examples to reach within 1% of the upper-bound (feedback on all
examples). We observe a similar trend with real users. This suggests new
opportunities for using language models in an interactive setting where users
can inspect, debug, correct, and improve a system’s performance over time.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yang2022robust/">On Robust Prefix-Tuning for Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Robust Prefix-Tuning for Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Robust Prefix-Tuning for Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zonghan Yang, Yang Liu</td>
	<td></td>
	<td><p>Recently, prefix-tuning has gained increasing attention as a
parameter-efficient finetuning method for large-scale pretrained language
models. The method keeps the pretrained models fixed and only updates the
prefix token parameters for each downstream task. Despite being lightweight and
modular, prefix-tuning still lacks robustness to textual adversarial attacks.
However, most currently developed defense techniques necessitate auxiliary
model update and storage, which inevitably hamper the modularity and low
storage of prefix-tuning. In this work, we propose a robust prefix-tuning
framework that preserves the efficiency and modularity of prefix-tuning. The
core idea of our framework is leveraging the layerwise activations of the
language model by correctly-classified training data as the standard for
additional prefix finetuning. During the test phase, an extra batch-level
prefix is tuned for each batch and added to the original prefix for robustness
enhancement. Extensive experiments on three text classification benchmarks show
that our framework substantially improves robustness over several strong
baselines against five textual attacks of different types while maintaining
comparable accuracy on clean texts. We also interpret our robust prefix-tuning
framework from the optimal control perspective and pose several directions for
future research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/buchholz2022means-end/">A Means-End Account of Explainable Artificial Intelligence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Means-End Account of Explainable Artificial Intelligence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Means-End Account of Explainable Artificial Intelligence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Oliver Buchholz</td>
	<td></td>
	<td><p>Explainable artificial intelligence (XAI) seeks to produce explanations for
those machine learning methods which are deemed opaque. However, there is
considerable disagreement about what this means and how to achieve it. Authors
disagree on what should be explained (topic), to whom something should be
explained (stakeholder), how something should be explained (instrument), and
why something should be explained (goal). In this paper, I employ insights from
means-end epistemology to structure the field. According to means-end
epistemology, different means ought to be rationally adopted to achieve
different epistemic ends. Applied to XAI, different topics, stakeholders, and
goals thus require different instruments. I call this the means-end account of
XAI. The means-end account has a descriptive and a normative component: on the
one hand, I show how the specific means-end relations give rise to a taxonomy
of existing contributions to the field of XAI; on the other hand, I argue that
the suitability of XAI methods can be assessed by analyzing whether they are
prescribed by a given topic, stakeholder, and goal.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/bra%C5%9Foveanu2022visualizing/">Visualizing and Explaining Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Explaining Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Explaining Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adrian M. P. Braşoveanu, Răzvan Andonie</td>
	<td></td>
	<td><p>During the last decade, Natural Language Processing has become, after
Computer Vision, the second field of Artificial Intelligence that was massively
changed by the advent of Deep Learning. Regardless of the architecture, the
language models of the day need to be able to process or generate text, as well
as predict missing words, sentences or relations depending on the task. Due to
their black-box nature, such models are difficult to interpret and explain to
third parties. Visualization is often the bridge that language model designers
use to explain their work, as the coloring of the salient words and phrases,
clustering or neuron activations can be used to quickly understand the
underlying models. This paper showcases the techniques used in some of the most
popular Deep Learning for NLP visualizations, with a special focus on
interpretability and explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/achtibat2022where/">From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Reduan Achtibat, Maximilian Dreyer, Ilona Eisenbraun, Sebastian Bosse, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</td>
	<td></td>
	<td><p>The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring
transparency to today’s powerful but opaque deep learning models. While local
XAI methods explain individual predictions in form of attribution maps, thereby
identifying where important features occur (but not providing information about
what they represent), global explanation techniques visualize what concepts a
model has generally learned to encode. Both types of methods thus only provide
partial insights and leave the burden of interpreting the model’s reasoning to
the user. Only few contemporary techniques aim at combining the principles
behind both local and global XAI for obtaining more informative explanations.
Those methods, however, are often limited to specific model architectures or
impose additional requirements on training regimes or data and label
availability, which renders the post-hoc application to arbitrarily pre-trained
models practically impossible. In this work we introduce the Concept Relevance
Propagation (CRP) approach, which combines the local and global perspectives of
XAI and thus allows answering both the “where” and “what” questions for
individual predictions, without additional constraints imposed. We further
introduce the principle of Relevance Maximization for finding representative
examples of encoded concepts based on their usefulness to the model. We thereby
lift the dependency on the common practice of Activation Maximization and its
limitations. We demonstrate the capabilities of our methods in various
settings, showcasing that Concept Relevance Propagation and Relevance
Maximization lead to more human interpretable explanations and provide deep
insights into the model’s representations and reasoning through concept
atlases, concept composition analyses, and quantitative investigations of
concept subspaces and their role in fine-grained decision making.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/xie2022towards/">Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yuansheng Xie, Soroush Vosoughi, Saeed Hassanpour</td>
	<td></td>
	<td><p>Artificial intelligence, particularly through recent advancements in deep
learning, has achieved exceptional performances in many tasks in fields such as
natural language processing and computer vision. In addition to desirable
evaluation metrics, a high level of interpretability is often required for
these models to be reliably utilized. Therefore, explanations that offer
insight into the process by which a model maps its inputs onto its outputs are
much sought-after. Unfortunately, current black box nature of machine learning
models is still an unresolved issue and this very nature prevents researchers
from learning and providing explicative descriptions for a model’s behavior and
final predictions. In this work, we propose a novel framework utilizing
Adversarial Inverse Reinforcement Learning that can provide global explanations
for decisions made by a Reinforcement Learning model and capture intuitive
tendencies that the model follows by summarizing the model’s decision-making
process.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/woodcock2022impact/">The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Claire Woodcock, Brent Mittelstadt, Dan Busbridge, Grant Blank</td>
	<td>J Med Internet Res 2021;23(11):e29386</td>
	<td><p>To achieve the promoted benefits of an AI symptom checker, laypeople must
trust and subsequently follow its instructions. In AI, explanations are seen as
a tool to communicate the rationale behind black-box decisions to encourage
trust and adoption. However, the effectiveness of the types of explanations
used in AI-driven symptom checkers has not yet been studied. Social theories
suggest that why-explanations are better at communicating knowledge and
cultivating trust among laypeople. This study ascertains whether explanations
provided by a symptom checker affect explanatory trust among laypeople (N=750)
and whether this trust is impacted by their existing knowledge of disease.
  Results suggest system builders developing explanations for symptom-checking
apps should consider the recipient’s knowledge of a disease and tailor
explanations to each user’s specific need. Effort should be placed on
generating explanations that are personalized to each user of a symptom checker
to fully discount the diseases that they may be aware of and to close their
information gap.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/scafarto2022calibrate/">Calibrate to Interpret</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Calibrate to Interpret' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Calibrate to Interpret' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gregory Scafarto, Nicolas Posocco, Antoine Bonnefoy</td>
	<td></td>
	<td><p>Trustworthy machine learning is driving a large number of ML community works
in order to improve ML acceptance and adoption. The main aspect of trustworthy
machine learning are the followings: fairness, uncertainty, robustness,
explainability and formal guaranties. Each of these individual domains gains
the ML community interest, visible by the number of related publications.
However few works tackle the interconnection between these fields. In this
paper we show a first link between uncertainty and explainability, by studying
the relation between calibration and interpretation. As the calibration of a
given model changes the way it scores samples, and interpretation approaches
often rely on these scores, it seems safe to assume that the
confidence-calibration of a model interacts with our ability to interpret such
model. In this paper, we show, in the context of networks trained on image
classification tasks, to what extent interpretations are sensitive to
confidence-calibration. It leads us to suggest a simple practice to improve the
interpretation outcomes: Calibrate to Interpret.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2021</td>
	<td><a href="/publications/yona2021revisiting/">Revisiting Sanity Checks for Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting Sanity Checks for Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Revisiting Sanity Checks for Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gal Yona, Daniel Greenfeld</td>
	<td></td>
	<td><p>Saliency methods are a popular approach for model debugging and
explainability. However, in the absence of ground-truth data for what the
correct maps should be, evaluating and comparing different approaches remains a
long-standing challenge. The sanity checks methodology of Adebayo et al
[Neurips 2018] has sought to address this challenge. They argue that some
popular saliency methods should not be used for explainability purposes since
the maps they produce are not sensitive to the underlying model that is to be
explained. Through a causal re-framing of their objective, we argue that their
empirical evaluation does not fully establish these conclusions, due to a form
of confounding introduced by the tasks they evaluate on. Through various
experiments on simple custom tasks we demonstrate that some of their
conclusions may indeed be artifacts of the tasks more than a criticism of the
saliency methods themselves. More broadly, our work challenges the utility of
the sanity check methodology, and further highlights that saliency map
evaluation beyond ad-hoc visual examination remains a fundamental challenge.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/akhtar2021attack/">Attack to Fool and Explain Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attack to Fool and Explain Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attack to Fool and Explain Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Naveed Akhtar, Muhammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal Mian</td>
	<td></td>
	<td><p>Deep visual models are susceptible to adversarial perturbations to inputs.
Although these signals are carefully crafted, they still appear noise-like
patterns to humans. This observation has led to the argument that deep visual
representation is misaligned with human perception. We counter-argue by
providing evidence of human-meaningful patterns in adversarial perturbations.
We first propose an attack that fools a network to confuse a whole category of
objects (source class) with a target label. Our attack also limits the
unintended fooling by samples from non-sources classes, thereby circumscribing
human-defined semantic notions for network fooling. We show that the proposed
attack not only leads to the emergence of regular geometric patterns in the
perturbations, but also reveals insightful information about the decision
boundaries of deep models. Exploring this phenomenon further, we alter the
<code class="language-plaintext highlighter-rouge">adversarial' objective of our attack to use it as a tool to </code>explain’ deep
visual representation. We show that by careful channeling and projection of the
perturbations computed by our method, we can visualize a model’s understanding
of human-defined semantic notions. Finally, we exploit the explanability
properties of our perturbations to perform image generation, inpainting and
interactive image manipulation by attacking adversarialy robust
`classifiers’.In all, our major contribution is a novel pragmatic adversarial
attack that is subsequently transformed into a tool to interpret the visual
models. The article also makes secondary contributions in terms of establishing
the utility of our attack beyond the adversarial objective with multiple
interesting applications.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/artelt2021evaluating/">Evaluating Robustness of Counterfactual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Robustness of Counterfactual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Robustness of Counterfactual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>André Artelt, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, Barbara Hammer</td>
	<td></td>
	<td><p>Transparency is a fundamental requirement for decision making systems when
these should be deployed in the real world. It is usually achieved by providing
explanations of the system’s behavior. A prominent and intuitive type of
explanations are counterfactual explanations. Counterfactual explanations
explain a behavior to the user by proposing actions – as changes to the input
– that would cause a different (specified) behavior of the system. However,
such explanation methods can be unstable with respect to small changes to the
input – i.e. even a small change in the input can lead to huge or arbitrary
changes in the output and of the explanation. This could be problematic for
counterfactual explanations, as two similar individuals might get very
different explanations. Even worse, if the recommended actions differ
considerably in their complexity, one would consider such unstable
(counterfactual) explanations as individually unfair.
  In this work, we formally and empirically study the robustness of
counterfactual explanations in general, as well as under different models and
different kinds of perturbations. Furthermore, we propose that plausible
counterfactual explanations can be used instead of closest counterfactual
explanations to improve the robustness and consequently the individual fairness
of counterfactual explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/augenstein2021towards/">Towards Explainable Fact Checking</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explainable Fact Checking' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explainable Fact Checking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Isabelle Augenstein</td>
	<td></td>
	<td><p>The past decade has seen a substantial rise in the amount of mis- and
disinformation online, from targeted disinformation campaigns to influence
politics, to the unintentional spreading of misinformation about public health.
This development has spurred research in the area of automatic fact checking,
from approaches to detect check-worthy claims and determining the stance of
tweets towards claims, to methods to determine the veracity of claims given
evidence documents. These automatic methods are often content-based, using
natural language processing methods, which in turn utilise deep neural networks
to learn higher-order features from text in order to make predictions. As deep
neural networks are black-box models, their inner workings cannot be easily
explained. At the same time, it is desirable to explain how they arrive at
certain decisions, especially if they are to be used for decision making. While
this has been known for some time, the issues this raises have been exacerbated
by models increasing in size, and by EU legislation requiring models to be used
for decision making to provide explanations, and, very recently, by legislation
requiring online platforms operating in the EU to provide transparent reporting
on their services. Despite this, current solutions for explainability are still
lacking in the area of fact checking. This thesis presents my research on
automatic fact checking, including claim check-worthiness detection, stance
detection and veracity prediction. Its contributions go beyond fact checking,
with the thesis proposing more general machine learning solutions for natural
language processing in the area of learning with limited labelled data.
Finally, the thesis presents some first solutions for explainable fact
checking.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/bodria2021benchmarking/">Benchmarking and Survey of Explanation Methods for Black Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking and Survey of Explanation Methods for Black Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Benchmarking and Survey of Explanation Methods for Black Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo</td>
	<td></td>
	<td><p>The widespread adoption of black-box models in Artificial Intelligence has
enhanced the need for explanation methods to reveal how these obscure models
reach specific decisions. Retrieving explanations is fundamental to unveil
possible biases and to resolve practical or ethical issues. Nowadays, the
literature is full of methods with different explanations. We provide a
categorization of explanation methods based on the type of explanation
returned. We present the most recent and widely used explainers, and we show a
visual comparison among explanations and a quantitative benchmarking.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/b%C3%B6hle2021convolutional/">Convolutional Dynamic Alignment Networks for Interpretable Classifications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Dynamic Alignment Networks for Interpretable Classifications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Convolutional Dynamic Alignment Networks for Interpretable Classifications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Moritz Böhle, Mario Fritz, Bernt Schiele</td>
	<td></td>
	<td><p>We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which linearly transform their input with
weight vectors that dynamically align with task-relevant patterns. As a result,
CoDA-Nets model the classification prediction through a series of
input-dependent linear transformations, allowing for linear decomposition of
the output into individual input contributions. Given the alignment of the
DAUs, the resulting contribution maps align with discriminative input patterns.
These model-inherent decompositions are of high visual quality and outperform
existing attribution methods under quantitative metrics. Further, CoDA-Nets
constitute performant classifiers, achieving on par results to ResNet and VGG
models on e.g. CIFAR-10 and TinyImagenet.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/b%C3%B6hle2021optimising/">Optimising for Interpretability: Convolutional Dynamic Alignment Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimising for Interpretability: Convolutional Dynamic Alignment Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimising for Interpretability: Convolutional Dynamic Alignment Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Moritz Böhle, Mario Fritz, Bernt Schiele</td>
	<td></td>
	<td><p>We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which are optimised to transform their inputs
with dynamically computed weight vectors that align with task-relevant
patterns. As a result, CoDA Nets model the classification prediction through a
series of input-dependent linear transformations, allowing for linear
decomposition of the output into individual input contributions. Given the
alignment of the DAUs, the resulting contribution maps align with
discriminative input patterns. These model-inherent decompositions are of high
visual quality and outperform existing attribution methods under quantitative
metrics. Further, CoDA Nets constitute performant classifiers, achieving on par
results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,
CoDA Nets can be combined with conventional neural network models to yield
powerful classifiers that more easily scale to complex datasets such as
Imagenet whilst exhibiting an increased interpretable depth, i.e., the output
can be explained well in terms of contributions from intermediate layers within
the network.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/cheng2021tsgb/">TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lin Cheng, Pengfei Fang, Yanjie Liang, Liao Zhang, Chunhua Shen, Hanzi Wang</td>
	<td></td>
	<td><p>The explanation for deep neural networks has drawn extensive attention in the
deep learning community over the past few years. In this work, we study the
visual saliency, a.k.a. visual explanation, to interpret convolutional neural
networks. Compared to iteration based saliency methods, single backward pass
based saliency methods benefit from faster speed, and they are widely used in
downstream visual tasks. Thus, we focus on single backward pass based methods.
However, existing methods in this category struggle to uccessfully produce
fine-grained saliency maps concentrating on specific target classes. That said,
producing faithful saliency maps satisfying both target-selectiveness and
fine-grainedness using a single backward pass is a challenging problem in the
field. To mitigate this problem, we revisit the gradient flow inside the
network, and find that the entangled semantics and original weights may disturb
the propagation of target-relevant saliency. Inspired by those observations, we
propose a novel visual saliency method, termed Target-Selective Gradient
Backprop (TSGB), which leverages rectification operations to effectively
emphasize target classes and further efficiently propagate the saliency to the
image space, thereby generating target-selective and fine-grained saliency
maps. The proposed TSGB consists of two components, namely, TSGB-Conv and
TSGB-FC, which rectify the gradients for convolutional layers and
fully-connected layers, respectively. Extensive qualitative and quantitative
experiments on the ImageNet and Pascal VOC datasets show that the proposed
method achieves more accurate and reliable results than the other competitive
methods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/chrysostomou2021enjoy/">Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>George Chrysostomou, Nikolaos Aletras</td>
	<td></td>
	<td><p>Pretrained transformer-based models such as BERT have demonstrated
state-of-the-art predictive performance when adapted into a range of natural
language processing tasks. An open problem is how to improve the faithfulness
of explanations (rationales) for the predictions of these models. In this
paper, we hypothesize that salient information extracted a priori from the
training data can complement the task-specific information learned by the model
during fine-tuning on a downstream task. In this way, we aim to help BERT not
to forget assigning importance to informative input tokens when making
predictions by proposing SaLoss; an auxiliary loss function for guiding the
multi-head attention mechanism during training to be close to salient
information extracted a priori using TextRank. Experiments for explanation
faithfulness across five datasets, show that models trained with SaLoss
consistently provide more faithful explanations across four different feature
attribution methods compared to vanilla BERT. Using the rationales extracted
from vanilla BERT and SaLoss models to train inherently faithful classifiers,
we further show that the latter result in higher predictive performance in
downstream tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ding2021evaluating/">Evaluating Saliency Methods for Neural Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Saliency Methods for Neural Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Saliency Methods for Neural Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shuoyang Ding, Philipp Koehn</td>
	<td></td>
	<td><p>Saliency methods are widely used to interpret neural network predictions, but
different variants of saliency methods often disagree even on the
interpretations of the same prediction made by the same model. In these cases,
how do we identify when are these interpretations trustworthy enough to be used
in analyses? To address this question, we conduct a comprehensive and
quantitative evaluation of saliency methods on a fundamental category of NLP
models: neural language models. We evaluate the quality of prediction
interpretations from two perspectives that each represents a desirable property
of these interpretations: plausibility and faithfulness. Our evaluation is
conducted on four different datasets constructed from the existing human
annotation of syntactic and semantic agreements, on both sentence-level and
document-level. Through our evaluation, we identified various ways saliency
methods could yield interpretations of low quality. We recommend that future
work deploying such methods to neural language models should carefully validate
their interpretations before drawing insights.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/gerlings2021explainable/">Explainable AI, but explainable to whom?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable AI, but explainable to whom?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable AI, but explainable to whom?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Julie Gerlings, Millie Søndergaard Jensen, Arisa Shollo</td>
	<td></td>
	<td><p>Advances in AI technologies have resulted in superior levels of AI-based
model performance. However, this has also led to a greater degree of model
complexity, resulting in ‘black box’ models. In response to the AI black box
problem, the field of explainable AI (xAI) has emerged with the aim of
providing explanations catered to human understanding, trust, and transparency.
Yet, we still have a limited understanding of how xAI addresses the need for
explainable AI in the context of healthcare. Our research explores the
differing explanation needs amongst stakeholders during the development of an
AI-system for classifying COVID-19 patients for the ICU. We demonstrate that
there is a constellation of stakeholders who have different explanation needs,
not just the ‘user’. Further, the findings demonstrate how the need for xAI
emerges through concerns associated with specific stakeholder groups i.e., the
development team, subject matter experts, decision makers, and the audience.
Our findings contribute to the expansion of xAI by highlighting that different
stakeholders have different explanation needs. From a practical perspective,
the study provides insights on how AI systems can be adjusted to support
different stakeholders needs, ensuring better implementation and operation in a
healthcare context.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hase2021models/">When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Peter Hase, Mohit Bansal</td>
	<td></td>
	<td><p>Many methods now exist for conditioning model outputs on task instructions,
retrieved documents, and user-provided explanations and feedback. Rather than
relying solely on examples of task inputs and outputs, these approaches use
valuable additional data for improving model correctness and aligning learned
models with human priors. Meanwhile, a growing body of evidence suggests that
some language models can (1) store a large amount of knowledge in their
parameters, and (2) perform inference over tasks in textual inputs at test
time. These results raise the possibility that, for some tasks, humans cannot
explain to a model any more about the task than it already knows or could infer
on its own. In this paper, we study the circumstances under which explanations
of individual data points can (or cannot) improve modeling performance. In
order to carefully control important properties of the data and explanations,
we introduce a synthetic dataset for experiments, and we also make use of three
existing datasets with explanations: e-SNLI, TACRED, and SemEval. We first give
a formal framework for the available modeling approaches, in which explanation
data can be used as model inputs, as targets, or as a prior. After arguing that
the most promising role for explanation data is as model inputs, we propose to
use a retrieval-based method and show that it solves our synthetic task with
accuracies upwards of 95%, while baselines without explanation data achieve
below 65% accuracy. We then identify properties of datasets for which
retrieval-based modeling fails. With the three existing datasets, we find no
improvements from explanation retrieval. Drawing on findings from our synthetic
task, we suggest that at least one of six preconditions for successful modeling
fails to hold with these datasets. Our code is publicly available at
https://github.com/peterbhase/ExplanationRoles</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/huang2021physically/">Physically Explainable CNN for SAR Image Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Physically Explainable CNN for SAR Image Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Physically Explainable CNN for SAR Image Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhongling Huang, Xiwen Yao, Ying Liu, Corneliu Octavian Dumitru, Mihai Datcu, Junwei Han</td>
	<td></td>
	<td><p>Integrating the special electromagnetic characteristics of Synthetic Aperture
Radar (SAR) in deep neural networks is essential in order to enhance the
explainability and physics awareness of deep learning. In this paper, we first
propose a novel physically explainable convolutional neural network for SAR
image classification, namely physics guided and injected learning (PGIL). It
comprises three parts: (1) explainable models (XM) to provide prior physics
knowledge, (2) physics guided network (PGN) to encode the knowledge into
physics-aware features, and (3) physics injected network (PIN) to adaptively
introduce the physics-aware features into classification pipeline for label
prediction. A hybrid Image-Physics SAR dataset format is proposed for
evaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The
results show that the proposed PGIL substantially improve the classification
performance in case of limited labeled data compared with the counterpart
data-driven CNN and other pre-training methods. Additionally, the physics
explanations are discussed to indicate the interpretability and the physical
consistency preserved in the predictions. We deem the proposed method would
promote the development of physically explainable deep learning in SAR image
interpretation field.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hvilsh%C3%B8j2021quantitative/">On Quantitative Evaluations of Counterfactuals</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Quantitative Evaluations of Counterfactuals' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Quantitative Evaluations of Counterfactuals' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Frederik Hvilshøj, Alexandros Iosifidis, Ira Assent</td>
	<td></td>
	<td><p>As counterfactual examples become increasingly popular for explaining
decisions of deep learning models, it is essential to understand what
properties quantitative evaluation metrics do capture and equally important
what they do not capture. Currently, such understanding is lacking, potentially
slowing down scientific progress. In this paper, we consolidate the work on
evaluating visual counterfactual examples through an analysis and experiments.
We find that while most metrics behave as intended for sufficiently simple
datasets, some fail to tell the difference between good and bad counterfactuals
when the complexity increases. We observe experimentally that metrics give good
scores to tiny adversarial-like changes, wrongly identifying such changes as
superior counterfactual examples. To mitigate this issue, we propose two new
metrics, the Label Variation Score and the Oracle score, which are both less
vulnerable to such tiny changes. We conclude that a proper quantitative
evaluation of visual counterfactual examples should combine metrics to ensure
that all aspects of good counterfactuals are quantified.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/jalwana2021cameras/">CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal Mian</td>
	<td></td>
	<td><p>Backpropagation image saliency aims at explaining model predictions by
estimating model-centric importance of individual pixels in the input. However,
class-insensitivity of the earlier layers in a network only allows saliency
computation with low resolution activation maps of the deeper layers, resulting
in compromised image saliency. Remedifying this can lead to sanity failures. We
propose CAMERAS, a technique to compute high-fidelity backpropagation saliency
maps without requiring any external priors and preserving the map sanity. Our
method systematically performs multi-scale accumulation and fusion of the
activation maps and backpropagated gradients to compute precise saliency maps.
From accurate image saliency to articulation of relative importance of input
features for different models, and precise discrimination between model
perception of visually similar objects, our high-resolution mapping offers
multiple novel insights into the black-box deep visual models, which are
presented in the paper. We also demonstrate the utility of our saliency maps in
adversarial setup by drastically reducing the norm of attack signals by
focusing them on the precise regions identified by our maps. Our method also
inspires new evaluation metrics and a sanity check for this developing research
direction. Code is available here https://github.com/VisMIL/CAMERAS</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kim2021sanity/">Sanity Simulations for Saliency Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Simulations for Saliency Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Simulations for Saliency Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Joon Sik Kim, Gregory Plumb, Ameet Talwalkar</td>
	<td></td>
	<td><p>Saliency methods are a popular class of feature attribution explanation
methods that aim to capture a model’s predictive reasoning by identifying
“important” pixels in an input image. However, the development and adoption of
these methods are hindered by the lack of access to ground-truth model
reasoning, which prevents accurate evaluation. In this work, we design a
synthetic benchmarking framework, SMERF, that allows us to perform
ground-truth-based evaluation while controlling the complexity of the model’s
reasoning. Experimentally, SMERF reveals significant limitations in existing
saliency methods and, as a result, represents a useful tool for the development
of new saliency methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kokhlikyan2021investigating/">Investigating sanity checks for saliency maps with image and text classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Investigating sanity checks for saliency maps with image and text classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Investigating sanity checks for saliency maps with image and text classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Narine Kokhlikyan, Vivek Miglani, Bilal Alsallakh, Miguel Martin, Orion Reblitz-Richardson</td>
	<td></td>
	<td><p>Saliency maps have shown to be both useful and misleading for explaining
model predictions especially in the context of images. In this paper, we
perform sanity checks for text modality and show that the conclusions made for
image do not directly transfer to text. We also analyze the effects of the
input multiplier in certain saliency maps using similarity scores,
max-sensitivity and infidelity evaluation metrics. Our observations reveal that
the input multiplier carries input’s structural patterns in explanation maps,
thus leading to similar results regardless of the choice of model parameters.
We also show that the smoothness of a Neural Network (NN) function can affect
the quality of saliency-based explanations. Our investigations reveal that
replacing ReLUs with Softplus and MaxPool with smoother variants such as
LogSumExp (LSE) can lead to explanations that are more reliable based on the
infidelity evaluation metric.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kong2021deeprare/">DeepRare: Generic Unsupervised Visual Attention Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepRare: Generic Unsupervised Visual Attention Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepRare: Generic Unsupervised Visual Attention Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Phutphalla Kong, Matei Mancas, Bernard Gosselin, Kimtho Po</td>
	<td></td>
	<td><p>Human visual system is modeled in engineering field providing
feature-engineered methods which detect contrasted/surprising/unusual data into
images. This data is “interesting” for humans and leads to numerous
applications. Deep learning (DNNs) drastically improved the algorithms
efficiency on the main benchmark datasets. However, DNN-based models are
counter-intuitive: surprising or unusual data is by definition difficult to
learn because of its low occurrence probability. In reality, DNN-based models
mainly learn top-down features such as faces, text, people, or animals which
usually attract human attention, but they have low efficiency in extracting
surprising or unusual data in the images. In this paper, we propose a new
visual attention model called DeepRare2021 (DR21) which uses the power of DNNs
feature extraction and the genericity of feature-engineered algorithms. This
algorithm is an evolution of a previous version called DeepRare2019 (DR19)
based on a common framework. DR21 1) does not need any training and uses the
default ImageNet training, 2) is fast even on CPU, 3) is tested on four very
different eye-tracking datasets showing that the DR21 is generic and is always
in the within the top models on all datasets and metrics while no other model
exhibits such a regularity and genericity. Finally DR21 4) is tested with
several network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2
(MN2) and 5) it provides explanation and transparency on which parts of the
image are the most surprising at different levels despite the use of a
DNN-based feature extractor. DeepRare2021 code can be found at
https://github.com/numediart/VisualAttention-RareFamil}.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/konstantinov2021attention/">Attention-like feature explanation for tabular data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention-like feature explanation for tabular data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention-like feature explanation for tabular data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrei V. Konstantinov, Lev V. Utkin</td>
	<td></td>
	<td><p>A new method for local and global explanation of the machine learning
black-box model predictions by tabular data is proposed. It is implemented as a
system called AFEX (Attention-like Feature EXplanation) and consisting of two
main parts. The first part is a set of the one-feature neural subnetworks which
aim to get a specific representation for every feature in the form of a basis
of shape functions. The subnetworks use shortcut connections with trainable
parameters to improve the network performance. The second part of AFEX produces
shape functions of features as the weighted sum of the basis shape functions
where weights are computed by using an attention-like mechanism. AFEX
identifies pairwise interactions between features based on pairwise
multiplications of shape functions corresponding to different features. A
modification of AFEX with incorporating an additional surrogate model which
approximates the black-box model is proposed. AFEX is trained end-to-end on a
whole dataset only once such that it does not require to train neural networks
again in the explanation stage. Numerical experiments with synthetic and real
data illustrate AFEX.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/lampinen2021tell/">Tell me why! Explanations support learning relational and causal structure</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Tell me why! Explanations support learning relational and causal structure' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Tell me why! Explanations support learning relational and causal structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrew K. Lampinen, Nicholas A. Roy, Ishita Dasgupta, Stephanie C. Y. Chan, Allison C. Tam, James L. McClelland, Chen Yan, Adam Santoro, Neil C. Rabinowitz, Jane X. Wang, Felix Hill</td>
	<td></td>
	<td><p>Inferring the abstract relational and causal structure of the world is a
major challenge for reinforcement-learning (RL) agents. For humans,
language–particularly in the form of explanations–plays a considerable role
in overcoming this challenge. Here, we show that language can play a similar
role for deep RL agents in complex environments. While agents typically
struggle to acquire relational and causal knowledge, augmenting their
experience by training them to predict language descriptions and explanations
can overcome these limitations. We show that language can help agents learn
challenging relational tasks, and examine which aspects of language contribute
to its benefits. We then show that explanations can help agents to infer not
only relational but also causal structure. Language can shape the way that
agents to generalize out-of-distribution from ambiguous, causally-confounded
training, and explanations even allow agents to learn to perform experimental
interventions to identify causal relationships. Our results suggest that
language description and explanation may be powerful tools for improving agent
learning and generalization.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/langer2021want/">What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, Kevin Baum</td>
	<td></td>
	<td><p>Previous research in Explainable Artificial Intelligence (XAI) suggests that
a main aim of explainability approaches is to satisfy specific interests,
goals, expectations, needs, and demands regarding artificial systems (we call
these stakeholders’ desiderata) in a variety of contexts. However, the
literature on XAI is vast, spreads out across multiple largely disconnected
disciplines, and it often remains unclear how explainability approaches are
supposed to achieve the goal of satisfying stakeholders’ desiderata. This paper
discusses the main classes of stakeholders calling for explainability of
artificial systems and reviews their desiderata. We provide a model that
explicitly spells out the main concepts and relations necessary to consider and
investigate when evaluating, adjusting, choosing, and developing explainability
approaches that aim to satisfy stakeholders’ desiderata. This model can serve
researchers from the variety of different disciplines involved in XAI as a
common ground. It emphasizes where there is interdisciplinary potential in the
evaluation and the development of explainability approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/li2021interpretable/">Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, Dejing Dou</td>
	<td></td>
	<td><p>Deep neural networks have been well-known for their superb handling of
various machine learning and artificial intelligence tasks. However, due to
their over-parameterized black-box nature, it is often difficult to understand
the prediction results of deep models. In recent years, many interpretation
tools have been proposed to explain or reveal how deep models make decisions.
In this paper, we review this line of research and try to make a comprehensive
survey. Specifically, we first introduce and clarify two basic concepts –
interpretations and interpretability – that people usually get confused about.
To address the research efforts in interpretations, we elaborate the designs of
a number of interpretation algorithms, from different perspectives, by
proposing a new taxonomy. Then, to understand the interpretation results, we
also survey the performance metrics for evaluating interpretation algorithms.
Further, we summarize the current works in evaluating models’ interpretability
using “trustworthy” interpretation algorithms. Finally, we review and discuss
the connections between deep models’ interpretations and other factors, such as
adversarial robustness and learning from interpretations, and we introduce
several open-source libraries for interpretation algorithms and evaluation
approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/liu2021going/">Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zixuan Liu, Ehsan Adeli, Kilian M. Pohl, Qingyu Zhao</td>
	<td></td>
	<td><p>Interpretability is a critical factor in applying complex deep learning
models to advance the understanding of brain disorders in neuroimaging studies.
To interpret the decision process of a trained classifier, existing techniques
typically rely on saliency maps to quantify the voxel-wise or feature-level
importance for classification through partial derivatives. Despite providing
some level of localization, these maps are not human-understandable from the
neuroscience perspective as they do not inform the specific meaning of the
alteration linked to the brain disorder. Inspired by the image-to-image
translation scheme, we propose to train simulator networks that can warp a
given image to inject or remove patterns of the disease. These networks are
trained such that the classifier produces consistently increased or decreased
prediction logits for the simulated images. Moreover, we propose to couple all
the simulators into a unified model based on conditional convolution. We
applied our approach to interpreting classifiers trained on a synthetic dataset
and two neuroimaging datasets to visualize the effect of the Alzheimer’s
disease and alcohol use disorder. Compared to the saliency maps generated by
baseline approaches, our simulations and visualizations based on the Jacobian
determinants of the warping field reveal meaningful and understandable patterns
related to the diseases.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/liu2021synthetic/">Synthetic Benchmarks for Scientific Research in Explainable Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Synthetic Benchmarks for Scientific Research in Explainable Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Synthetic Benchmarks for Scientific Research in Explainable Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yang Liu, Sujay Khandagale, Colin White, Willie Neiswanger</td>
	<td></td>
	<td><p>As machine learning models grow more complex and their applications become
more high-stakes, tools for explaining model predictions have become
increasingly important. This has spurred a flurry of research in model
explainability and has given rise to feature attribution methods such as LIME
and SHAP. Despite their widespread use, evaluating and comparing different
feature attribution methods remains challenging: evaluations ideally require
human studies, and empirical evaluation metrics are often data-intensive or
computationally prohibitive on real-world datasets. In this work, we address
this issue by releasing XAI-Bench: a suite of synthetic datasets along with a
library for benchmarking feature attribution algorithms. Unlike real-world
datasets, synthetic datasets allow the efficient computation of conditional
expected values that are needed to evaluate ground-truth Shapley values and
other metrics. The synthetic datasets we release offer a wide variety of
parameters that can be configured to simulate real-world data. We demonstrate
the power of our library by benchmarking popular explainability techniques
across several evaluation metrics and across a variety of settings. The
versatility and efficiency of our library will help researchers bring their
explainability methods from development to deployment. Our code is available at
https://github.com/abacusai/xai-bench.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/nallbani2021resvgae/">ResVGAE: Going Deeper with Residual Modules for Link Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=ResVGAE: Going Deeper with Residual Modules for Link Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=ResVGAE: Going Deeper with Residual Modules for Link Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Indrit Nallbani, Reyhan Kevser Keser, Aydin Ayanzadeh, Nurullah Çalık, Behçet Uğur Töreyin</td>
	<td></td>
	<td><p>Graph autoencoders are efficient at embedding graph-based data sets. Most
graph autoencoder architectures have shallow depths which limits their ability
to capture meaningful relations between nodes separated by multi-hops. In this
paper, we propose Residual Variational Graph Autoencoder, ResVGAE, a deep
variational graph autoencoder model with multiple residual modules. We show
that our multiple residual modules, a convolutional layer with residual
connection, improve the average precision of the graph autoencoders.
Experimental results suggest that our proposed model with residual modules
outperforms the models without residual modules and achieves similar results
when compared with other state-of-the-art methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/nguyen2021effectiveness/">The effectiveness of feature attribution methods and its correlation with automatic evaluation scores</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The effectiveness of feature attribution methods and its correlation with automatic evaluation scores' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The effectiveness of feature attribution methods and its correlation with automatic evaluation scores' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Giang Nguyen, Daeyoung Kim, Anh Nguyen</td>
	<td></td>
	<td><p>Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et
al. 2018). In this paper, we conduct the first user study to measure
attribution map effectiveness in assisting humans in ImageNet classification
and Stanford Dogs fine-grained classification, and when an image is natural or
adversarial (i.e., contains adversarial perturbations). Overall, feature
attribution is surprisingly not more effective than showing humans nearest
training-set examples. On a harder task of fine-grained dog categorization,
presenting attribution maps to humans does not help, but instead hurts the
performance of human-AI teams compared to AI alone. Importantly, we found
automatic attribution-map evaluation measures to correlate poorly with the
actual human-AI team performance. Our findings encourage the community to
rigorously test their methods on the downstream human-in-the-loop applications
and to rethink the existing evaluation metrics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/pan2021definitions/">The Definitions of Interpretability and Learning of Interpretable Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Definitions of Interpretability and Learning of Interpretable Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Definitions of Interpretability and Learning of Interpretable Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Weishen Pan, Changshui Zhang</td>
	<td></td>
	<td><p>As machine learning algorithms getting adopted in an ever-increasing number
of applications, interpretation has emerged as a crucial desideratum. In this
paper, we propose a mathematical definition for the human-interpretable model.
In particular, we define interpretability between two information process
systems. If a prediction model is interpretable by a human recognition system
based on the above interpretability definition, the prediction model is defined
as a completely human-interpretable model. We further design a practical
framework to train a completely human-interpretable model by user interactions.
Experiments on image datasets show the advantages of our proposed model in two
aspects: 1) The completely human-interpretable model can provide an entire
decision-making process that is human-understandable; 2) The completely
human-interpretable model is more robust against adversarial attacks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/phuong2021towards/">Towards Understanding Knowledge Distillation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Understanding Knowledge Distillation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Understanding Knowledge Distillation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mary Phuong, Christoph H. Lampert</td>
	<td></td>
	<td><p>Knowledge distillation, i.e., one classifier being trained on the outputs of
another classifier, is an empirically very successful technique for knowledge
transfer between classifiers. It has even been observed that classifiers learn
much faster and more reliably if trained with the outputs of another classifier
as soft labels, instead of from ground truth data. So far, however, there is no
satisfactory theoretical explanation of this phenomenon. In this work, we
provide the first insights into the working mechanisms of distillation by
studying the special case of linear and deep linear classifiers. Specifically,
we prove a generalization bound that establishes fast convergence of the
expected risk of a distillation-trained linear classifier. From the bound and
its proof we extract three key factors that determine the success of
distillation: * data geometry – geometric properties of the data distribution,
in particular class separation, has a direct influence on the convergence speed
of the risk; * optimization bias – gradient descent optimization finds a very
favorable minimum of the distillation objective; and * strong monotonicity –
the expected risk of the student classifier always decreases when the size of
the training set grows.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/poursabzisangdeh2021manipulating/">Manipulating and Measuring Model Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Manipulating and Measuring Model Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Manipulating and Measuring Model Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, Hanna Wallach</td>
	<td>CHI</td>
	<td><p>With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.</p>
</td>
	<td>measuring </td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ralekar2021understanding/">Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chetan Ralekar, Shubham Choudhary, Tapan Kumar Gandhi, Santanu Chaudhury</td>
	<td></td>
	<td><p>Human observers engage in selective information uptake when classifying
visual patterns. The same is true of deep neural networks, which currently
constitute the best performing artificial vision systems. Our goal is to
examine the congruence, or lack thereof, in the information-gathering
strategies of the two systems. We have operationalized our investigation as a
character recognition task. We have used eye-tracking to assay the spatial
distribution of information hotspots for humans via fixation maps and an
activation mapping technique for obtaining analogous distributions for deep
networks through visualization maps. Qualitative comparison between
visualization maps and fixation maps reveals an interesting correlate of
congruence. The deep learning model considered similar regions in character,
which humans have fixated in the case of correctly classified characters. On
the other hand, when the focused regions are different for humans and deep
nets, the characters are typically misclassified by the latter. Hence, we
propose to use the visual fixation maps obtained from the eye-tracking
experiment as a supervisory input to align the model’s focus on relevant
character regions. We find that such supervision improves the model’s
performance significantly and does not require any additional parameters. This
approach has the potential to find applications in diverse domains such as
medical analysis and surveillance in which explainability helps to determine
system fidelity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/rao2021first/">A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Varun Nagaraj Rao, Xingjian Zhen, Karen Hovsepian, Mingwei Shen</td>
	<td></td>
	<td><p>Explainable deep learning models are advantageous in many situations. Prior
work mostly provide unimodal explanations through post-hoc approaches not part
of the original system design. Explanation mechanisms also ignore useful
textual information present in images. In this paper, we propose MTXNet, an
end-to-end trainable multimodal architecture to generate multimodal
explanations, which focuses on the text in the image. We curate a novel dataset
TextVQA-X, containing ground truth visual and multi-reference textual
explanations that can be leveraged during both training and evaluation. We then
quantitatively show that training with multimodal explanations complements
model performance and surpasses unimodal baselines by up to 7% in CIDEr scores
and 2% in IoU. More importantly, we demonstrate that the multimodal
explanations are consistent with human interpretations, help justify the
models’ decision, and provide useful insights to help diagnose an incorrect
prediction. Finally, we describe a real-world e-commerce application for using
the generated multimodal explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/rudin2021interpretable/">Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, Chudi Zhong</td>
	<td>Statistics Surveys, 2021</td>
	<td><p>Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the “Rashomon set” of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/samuel2021evaluation/">Evaluation of Saliency-based Explainability Method</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluation of Saliency-based Explainability Method' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluation of Saliency-based Explainability Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sam Zabdiel Sunder Samuel, Vidhya Kamakshi, Namrata Lodhi, Narayanan C Krishnan</td>
	<td></td>
	<td><p>A particular class of Explainable AI (XAI) methods provide saliency maps to
highlight part of the image a Convolutional Neural Network (CNN) model looks at
to classify the image as a way to explain its working. These methods provide an
intuitive way for users to understand predictions made by CNNs. Other than
quantitative computational tests, the vast majority of evidence to highlight
that the methods are valuable is anecdotal. Given that humans would be the
end-users of such methods, we devise three human subject experiments through
which we gauge the effectiveness of these saliency-based explainability
methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/santos2021impact/">On the Impact of Interpretability Methods in Active Image Augmentation Method</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Impact of Interpretability Methods in Active Image Augmentation Method' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Impact of Interpretability Methods in Active Image Augmentation Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Flavio Santos, Cleber Zanchettin, Leonardo Matos, Paulo Novais</td>
	<td>Logic Journal of the IGPL, 2021, jzab006</td>
	<td><p>Robustness is a significant constraint in machine learning models. The
performance of the algorithms must not deteriorate when training and testing
with slightly different data. Deep neural network models achieve awe-inspiring
results in a wide range of applications of computer vision. Still, in the
presence of noise or region occlusion, some models exhibit inaccurate
performance even with data handled in training. Besides, some experiments
suggest deep learning models sometimes use incorrect parts of the input
information to perform inference. Activate Image Augmentation (ADA) is an
augmentation method that uses interpretability methods to augment the training
data and improve its robustness to face the described problems. Although ADA
presented interesting results, its original version only used the Vanilla
Backpropagation interpretability to train the U-Net model. In this work, we
propose an extensive experimental analysis of the interpretability method’s
impact on ADA. We use five interpretability methods: Vanilla Backpropagation,
Guided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The
results show that all methods achieve similar performance at the ending of
training, but when combining ADA with GradCam, the U-Net model presented an
impressive fast convergence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/schwalbe2021comprehensive/">A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gesina Schwalbe, Bettina Finzel</td>
	<td></td>
	<td><p>In the meantime, a wide variety of terminologies, motivations, approaches,
and evaluation criteria have been developed within the research field of
explainable artificial intelligence (XAI). With the amount of XAI methods
vastly growing, a taxonomy of methods is needed by researchers as well as
practitioners: To grasp the breadth of the topic, compare methods, and to
select the right XAI method based on traits required by a specific use-case
context. Many taxonomies for XAI methods of varying level of detail and depth
can be found in the literature. While they often have a different focus, they
also exhibit many points of overlap. This paper unifies these efforts and
provides a complete taxonomy of XAI methods with respect to notions present in
the current state of research. In a structured literature analysis and
meta-study, we identified and reviewed more than 50 of the most cited and
current surveys on XAI methods, metrics, and method traits. After summarizing
them in a survey of surveys, we merge terminologies and concepts of the
articles into a unified structured taxonomy. Single concepts therein are
illustrated by more than 50 diverse selected example methods in total, which we
categorize accordingly. The taxonomy may serve both beginners, researchers, and
practitioners as a reference and wide-ranging overview of XAI method traits and
aspects. Hence, it provides foundations for targeted, use-case-oriented, and
context-sensitive future research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/shen2021interpretable/">Interpretable Compositional Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Compositional Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Compositional Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Jiaqi Fan, Ping Zhao, Quanshi Zhang</td>
	<td></td>
	<td><p>The reasonable definition of semantic interpretability presents the core
challenge in explainable AI. This paper proposes a method to modify a
traditional convolutional neural network (CNN) into an interpretable
compositional CNN, in order to learn filters that encode meaningful visual
patterns in intermediate convolutional layers. In a compositional CNN, each
filter is supposed to consistently represent a specific compositional object
part or image region with a clear meaning. The compositional CNN learns from
image labels for classification without any annotations of parts or regions for
supervision. Our method can be broadly applied to different types of CNNs.
Experiments have demonstrated the effectiveness of our method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/sudhakar2021ada/">Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mahesh Sudhakar, Sam Sattarzadeh, Konstantinos N. Plataniotis, Jongseong Jang, Yeonjeong Jeong, Hyunwoo Kim</td>
	<td></td>
	<td><p>Explainable AI (XAI) is an active research area to interpret a neural
network’s decision by ensuring transparency and trust in the task-specified
learned models. Recently, perturbation-based model analysis has shown better
interpretation, but backpropagation techniques are still prevailing because of
their computational efficiency. In this work, we combine both approaches as a
hybrid visual explanation algorithm and propose an efficient interpretation
method for convolutional neural networks. Our method adaptively selects the
most critical features that mainly contribute towards a prediction to probe the
model by finding the activated features. Experimental results show that the
proposed method can reduce the execution time up to 30% while enhancing
competitive interpretability without compromising the quality of explanation
generated.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/tuan2021local/">Local Explanation of Dialogue Response Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Local Explanation of Dialogue Response Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Local Explanation of Dialogue Response Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang Wang</td>
	<td></td>
	<td><p>In comparison to the interpretation of classification models, the explanation
of sequence generation models is also an important problem, however it has seen
little attention. In this work, we study model-agnostic explanations of a
representative text generation task – dialogue response generation. Dialog
response generation is challenging with its open-ended sentences and multiple
acceptable responses. To gain insights into the reasoning process of a
generation model, we propose a new method, local explanation of response
generation (LERG) that regards the explanations as the mutual interaction of
segments in input and output sentences. LERG views the sequence prediction as
uncertainty estimation of a human response and then creates explanations by
perturbing the input and calculating the certainty change over the human
response. We show that LERG adheres to desired properties of explanations for
text generation including unbiased approximation, consistency and cause
identification. Empirically, our results show that our method consistently
improves other widely used methods on proposed automatic- and human- evaluation
metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can
extract both explicit and implicit relations between input and output segments.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/vaishnav2021understanding/">Understanding the computational demands underlying visual reasoning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding the computational demands underlying visual reasoning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding the computational demands underlying visual reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mohit Vaishnav, Remi Cadene, Andrea Alamia, Drew Linsley, Rufin VanRullen, Thomas Serre</td>
	<td>Neural Computation, 2022</td>
	<td><p>Visual understanding requires comprehending complex visual relations between
objects within a scene. Here, we seek to characterize the computational demands
for abstract visual reasoning. We do this by systematically assessing the
ability of modern deep convolutional neural networks (CNNs) to learn to solve
the “Synthetic Visual Reasoning Test” (SVRT) challenge, a collection of
twenty-three visual reasoning problems. Our analysis reveals a novel taxonomy
of visual reasoning tasks, which can be primarily explained by both the type of
relations (same-different vs. spatial-relation judgments) and the number of
relations used to compose the underlying rules. Prior cognitive neuroscience
work suggests that attention plays a key role in humans’ visual reasoning
ability. To test this hypothesis, we extended the CNNs with spatial and
feature-based attention mechanisms. In a second series of experiments, we
evaluated the ability of these attention networks to learn to solve the SVRT
challenge and found the resulting architectures to be much more efficient at
solving the hardest of these visual reasoning tasks. Most importantly, the
corresponding improvements on individual tasks partially explained our novel
taxonomy. Overall, this work provides an granular computational account of
visual reasoning and yields testable neuroscience predictions regarding the
differential need for feature-based vs. spatial attention depending on the type
of visual reasoning problem.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/verma2021pitfalls/">Pitfalls of Explainable ML: An Industry Perspective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pitfalls of Explainable ML: An Industry Perspective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pitfalls of Explainable ML: An Industry Perspective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sahil Verma, Aditya Lahiri, John P. Dickerson, Su-In Lee</td>
	<td></td>
	<td><p>As machine learning (ML) systems take a more prominent and central role in
contributing to life-impacting decisions, ensuring their trustworthiness and
accountability is of utmost importance. Explanations sit at the core of these
desirable attributes of a ML system. The emerging field is frequently called
<code class="language-plaintext highlighter-rouge">Explainable AI (XAI)'' or</code>Explainable ML.’’ The goal of explainable ML is
to intuitively explain the predictions of a ML system, while adhering to the
needs to various stakeholders. Many explanation techniques were developed with
contributions from both academia and industry. However, there are several
existing challenges that have not garnered enough interest and serve as
roadblocks to widespread adoption of explainable ML. In this short paper, we
enumerate challenges in explainable ML from an industry perspective. We hope
these challenges will serve as promising future research directions, and would
contribute to democratizing explainable ML.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wang2021fast/">On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, Meng Wang</td>
	<td></td>
	<td><p>Model-agnostic meta-learning (MAML) has emerged as one of the most successful
meta-learning techniques in few-shot learning. It enables us to learn a
meta-initialization} of model parameters (that we call meta-model) to rapidly
adapt to new tasks using a small amount of labeled training data. Despite the
generalization power of the meta-model, it remains elusive that how adversarial
robustness can be maintained by MAML in few-shot learning. In addition to
generalization, robustness is also desired for a meta-model to defend
adversarial examples (attacks). Toward promoting adversarial robustness in
MAML, we first study WHEN a robustness-promoting regularization should be
incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs.
meta-update) learning procedure. We show that robustifying the meta-update
stage is sufficient to make robustness adapted to the task-specific fine-tuning
stage even if the latter uses a standard training protocol. We also make
additional justification on the acquired robustness adaptation by peering into
the interpretability of neurons’ activation maps. Furthermore, we investigate
HOW robust regularization can efficiently be designed in MAML. We propose a
general but easily-optimized robustness-regularized meta-learning framework,
which allows the use of unlabeled data augmentation, fast adversarial attack
generation, and computationally-light fine-tuning. In particular, we for the
first time show that the auxiliary contrastive learning task can enhance the
adversarial robustness of MAML. Finally, extensive experiments are conducted to
demonstrate the effectiveness of our proposed methods in robust few-shot
learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wiegreffe2021teach/">Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarah Wiegreffe, Ana Marasović</td>
	<td></td>
	<td><p>Explainable NLP (ExNLP) has increasingly focused on collecting
human-annotated textual explanations. These explanations are used downstream in
three ways: as data augmentation to improve performance on a predictive task,
as supervision to train models to produce explanations for their predictions,
and as a ground-truth to evaluate model-generated explanations. In this review,
we identify 65 datasets with three predominant classes of textual explanations
(highlights, free-text, and structured), organize the literature on annotating
each type, identify strengths and shortcomings of existing collection
methodologies, and give recommendations for collecting ExNLP datasets in the
future.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yao2021explanatory/">Explanatory Pluralism in Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Pluralism in Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Pluralism in Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yiheng Yao</td>
	<td></td>
	<td><p>The increasingly widespread application of AI models motivates increased
demand for explanations from a variety of stakeholders. However, this demand is
ambiguous because there are many types of ‘explanation’ with different
evaluative criteria. In the spirit of pluralism, I chart a taxonomy of types of
explanation and the associated XAI methods that can address them. When we look
to expose the inner mechanisms of AI models, we develop
Diagnostic-explanations. When we seek to render model output understandable, we
produce Explication-explanations. When we wish to form stable generalizations
of our models, we produce Expectation-explanations. Finally, when we want to
justify the usage of a model, we produce Role-explanations that situate models
within their social context. The motivation for such a pluralistic view stems
from a consideration of causes as manipulable relationships and the different
types of explanations as identifying the relevant points in AI systems we can
intervene upon to affect our desired changes. This paper reduces the ambiguity
in use of the word ‘explanation’ in the field of XAI, allowing practitioners
and stakeholders a useful template for avoiding equivocation and evaluating XAI
methods and putative explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yin2021sensitivity/">On the Sensitivity and Stability of Model Interpretations in NLP</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Sensitivity and Stability of Model Interpretations in NLP' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Sensitivity and Stability of Model Interpretations in NLP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang</td>
	<td></td>
	<td><p>Recent years have witnessed the emergence of a variety of post-hoc
interpretations that aim to uncover how natural language processing (NLP)
models make predictions. Despite the surge of new interpretation methods, it
remains an open problem how to define and quantitatively measure the
faithfulness of interpretations, i.e., to what extent interpretations reflect
the reasoning process by a model. We propose two new criteria, sensitivity and
stability, that provide complementary notions of faithfulness to the existed
removal-based criteria. Our results show that the conclusion for how faithful
interpretations are could vary substantially based on different notions.
Motivated by the desiderata of sensitivity and stability, we introduce a new
class of interpretation methods that adopt techniques from adversarial
robustness. Empirical results show that our proposed methods are effective
under the new criteria and overcome limitations of gradient-based methods on
removal-based criteria. Besides text classification, we also apply
interpretation methods and metrics to dependency parsing. Our results shed
light on understanding the diverse set of interpretations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zafar2021lack/">On the Lack of Robust Interpretability of Neural Text Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Lack of Robust Interpretability of Neural Text Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Lack of Robust Interpretability of Neural Text Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Muhammad Bilal Zafar, Michele Donini, Dylan Slack, Cédric Archambeau, Sanjiv Das, Krishnaram Kenthapadi</td>
	<td></td>
	<td><p>With the ever-increasing complexity of neural language models, practitioners
have turned to methods for understanding the predictions of these models. One
of the most well-adopted approaches for model interpretability is feature-based
interpretability, i.e., ranking the features in terms of their impact on model
predictions. Several prior studies have focused on assessing the fidelity of
feature-based interpretability methods, i.e., measuring the impact of dropping
the top-ranked features on the model output. However, relatively little work
has been conducted on quantifying the robustness of interpretations. In this
work, we assess the robustness of interpretations of neural text classifiers,
specifically, those based on pretrained Transformer encoders, using two
randomization tests. The first compares the interpretations of two models that
are identical except for their initializations. The second measures whether the
interpretations differ between a model with trained parameters and a model with
random parameters. Both tests show surprising deviations from expected
behavior, raising questions about the extent of insights that practitioners may
draw from interpretations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zha2021invertible/">Invertible Attention</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Invertible Attention' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Invertible Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiajun Zha, Yiran Zhong, Jing Zhang, Richard Hartley, Liang Zheng</td>
	<td></td>
	<td><p>Attention has been proved to be an efficient mechanism to capture long-range
dependencies. However, so far it has not been deployed in invertible networks.
This is due to the fact that in order to make a network invertible, every
component within the network needs to be a bijective transformation, but a
normal attention block is not. In this paper, we propose invertible attention
that can be plugged into existing invertible models. We mathematically and
experimentally prove that the invertibility of an attention model can be
achieved by carefully constraining its Lipschitz constant. We validate the
invertibility of our invertible attention on image reconstruction task with 3
popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible
attention achieves similar performance in comparison with normal non-invertible
attention on dense prediction tasks. The code is available at
https://github.com/Schwartz-Zha/InvertibleAttention</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhang2021evaluating/">Evaluating Deep Graph Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Deep Graph Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Deep Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wentao Zhang, Zeang Sheng, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, Bin Cui</td>
	<td></td>
	<td><p>Graph Neural Networks (GNNs) have already been widely applied in various
graph mining tasks. However, they suffer from the shallow architecture issue,
which is the key impediment that hinders the model performance improvement.
Although several relevant approaches have been proposed, none of the existing
studies provides an in-depth understanding of the root causes of performance
degradation in deep GNNs. In this paper, we conduct the first systematic
experimental evaluation to present the fundamental limitations of shallow
architectures. Based on the experimental results, we answer the following two
essential questions: (1) what actually leads to the compromised performance of
deep GNNs; (2) when we need and how to build deep GNNs. The answers to the
above questions provide empirical insights and guidelines for researchers to
design deep and well-performed GNNs. To show the effectiveness of our proposed
guidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful
approach (a paradigm in its own right) that helps guide deep GNN designs.
Experimental results demonstrate three advantages of DGMLP: 1) high accuracy –
it achieves state-of-the-art node classification performance on various
datasets; 2) high flexibility – it can flexibly choose different propagation
and transformation depths according to graph size and sparsity; 3) high
scalability and efficiency – it supports fast training on large-scale graphs.
Our code is available in https://github.com/zwt233/DGMLP.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhang2021head/">From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Qiaoning Zhang, X. Jessie Yang, Lionel P. Robert Jr</td>
	<td></td>
	<td><p>Automated vehicles (AVs) are social robots that can potentially benefit our
society. According to the existing literature, AV explanations can promote
passengers’ trust by reducing the uncertainty associated with the AV’s
reasoning and actions. However, the literature on AV explanations and trust has
failed to consider how the type of trust</p>
<ul>
  <li>cognitive versus affective - might alter this relationship. Yet, the
existing literature has shown that the implications associated with trust vary
widely depending on whether it is cognitive or affective. To address this
shortcoming and better understand the impacts of explanations on trust in AVs,
we designed a study to investigate the effectiveness of explanations on both
cognitive and affective trust. We expect these results to be of great
significance in designing AV explanations to promote AV trust.</li>
</ul>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhu2021going/">Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaohan Zhu, Zhen Cui, Tong Zhang, Yong Li, Jian Yang</td>
	<td></td>
	<td><p>Convolutional neural network (CNN) is one of the most widely-used successful
architectures in the era of deep learning. However, the high-computational cost
of CNN still hampers more universal uses to light devices. Fortunately, the
Fourier transform on convolution gives an elegant and promising solution to
dramatically reduce the computation cost. Recently, some studies devote to such
a challenging problem and pursue the complete frequency computation without any
switching between spatial domain and frequent domain. In this work, we revisit
the Fourier transform theory to derive feed-forward and back-propagation
frequency operations of typical network modules such as convolution, activation
and pooling. Due to the calculation limitation of complex numbers on most
computation tools, we especially extend the Fourier transform to the Laplace
transform for CNN, which can run in the real domain with more relaxed
constraints. This work more focus on a theoretical extension and discussion
about frequency CNN, and lay some theoretical ground for real application.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2020</td>
	<td><a href="/publications/joshi2020explainable/">Explainable Disease Classification via weakly-supervised segmentation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Disease Classification via weakly-supervised segmentation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Disease Classification via weakly-supervised segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aniket Joshi, Gaurav Mishra, Jayanthi Sivaswamy</td>
	<td>Interpretable and Annotation-Efficient Learning for Medical Image Computing. IMIMIC 2020, MIL3ID 2020, LABELS 2020. Lecture Notes in Computer Science, vol 12446. Springer, Cham</td>
	<td><p>Deep learning based approaches to Computer Aided Diagnosis (CAD) typically
pose the problem as an image classification (Normal or Abnormal) problem. These
systems achieve high to very high accuracy in specific disease detection for
which they are trained but lack in terms of an explanation for the provided
decision/classification result. The activation maps which correspond to
decisions do not correlate well with regions of interest for specific diseases.
This paper examines this problem and proposes an approach which mimics the
clinical practice of looking for an evidence prior to diagnosis. A CAD model is
learnt using a mixed set of information: class labels for the entire training
set of images plus a rough localisation of suspect regions as an extra input
for a smaller subset of training images for guiding the learning. The proposed
approach is illustrated with detection of diabetic macular edema (DME) from OCT
slices. Results of testing on on a large public dataset show that with just a
third of images with roughly segmented fluid filled regions, the classification
accuracy is on par with state of the art methods while providing a good
explanation in the form of anatomically accurate heatmap /region of interest.
The proposed solution is then adapted to Breast Cancer detection from
mammographic images. Good evaluation results on public datasets underscores the
generalisability of the proposed solution.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/schneider2020deceptive/">Deceptive AI Explanations: Creation and Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deceptive AI Explanations: Creation and Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deceptive AI Explanations: Creation and Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Johannes Schneider, Christian Meske, Michalis Vlachos</td>
	<td>International Conference on Agents and Artificial Intelligence (2022)</td>
	<td><p>Artificial intelligence (AI) comes with great opportunities but can also pose
significant risks. Automatically generated explanations for decisions can
increase transparency and foster trust, especially for systems based on
automated predictions by AI models. However, given, e.g., economic incentives
to create dishonest AI, to what extent can we trust explanations? To address
this issue, our work investigates how AI models (i.e., deep learning, and
existing instruments to increase transparency regarding AI decisions) can be
used to create and detect deceptive explanations. As an empirical evaluation,
we focus on text classification and alter the explanations generated by
GradCAM, a well-established explanation technique in neural networks. Then, we
evaluate the effect of deceptive explanations on users in an experiment with
200 participants. Our findings confirm that deceptive explanations can indeed
fool humans. However, one can deploy machine learning (ML) methods to detect
seemingly minor deception attempts with accuracy exceeding 80% given sufficient
domain knowledge. Without domain knowledge, one can still infer inconsistencies
in the explanations in an unsupervised manner, given basic knowledge of the
predictive model under scrutiny.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/schramowski2020making/">Making deep neural networks right for the right scientific reasons by interacting with their explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Making deep neural networks right for the right scientific reasons by interacting with their explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Making deep neural networks right for the right scientific reasons by interacting with their explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, Kristian Kersting</td>
	<td></td>
	<td><p>Deep neural networks have shown excellent performances in many real-world
applications. Unfortunately, they may show “Clever Hans”-like behavior—making
use of confounding factors within datasets—to achieve high performance. In
this work, we introduce the novel learning setting of “explanatory interactive
learning” (XIL) and illustrate its benefits on a plant phenotyping research
task. XIL adds the scientist into the training loop such that she interactively
revises the original model via providing feedback on its explanations. Our
experimental results demonstrate that XIL can help avoiding Clever Hans moments
in machine learning and encourages (or discourages, if appropriate) trust into
the underlying model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shen2020explain/">To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yuan Shen, Shanduojiao Jiang, Yanlin Chen, Eileen Yang, Xilun Jin, Yuliang Fan, Katie Driggs Campbell</td>
	<td></td>
	<td><p>Explainable AI, in the context of autonomous systems, like self driving cars,
has drawn broad interests from researchers. Recent studies have found that
providing explanations for an autonomous vehicle actions has many benefits,
e.g., increase trust and acceptance, but put little emphasis on when an
explanation is needed and how the content of explanation changes with context.
In this work, we investigate which scenarios people need explanations and how
the critical degree of explanation shifts with situations and driver types.
Through a user experiment, we ask participants to evaluate how necessary an
explanation is and measure the impact on their trust in the self driving cars
in different contexts. We also present a self driving explanation dataset with
first person explanations and associated measure of the necessity for 1103
video clips, augmenting the Berkeley Deep Drive Attention dataset.
Additionally, we propose a learning based model that predicts how necessary an
explanation for a given situation in real time, using camera data inputs. Our
research reveals that driver types and context dictates whether or not an
explanation is necessary and what is helpful for improved interaction and
understanding.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shvo2020towards/">Towards the Role of Theory of Mind in Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards the Role of Theory of Mind in Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards the Role of Theory of Mind in Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Maayan Shvo, Toryn Q. Klassen, Sheila A. McIlraith</td>
	<td></td>
	<td><p>Theory of Mind is commonly defined as the ability to attribute mental states
(e.g., beliefs, goals) to oneself, and to others. A large body of previous work</p>
<ul>
  <li>from the social sciences to artificial intelligence - has observed that
Theory of Mind capabilities are central to providing an explanation to another
agent or when explaining that agent’s behaviour. In this paper, we build and
expand upon previous work by providing an account of explanation in terms of
the beliefs of agents and the mechanism by which agents revise their beliefs
given possible explanations. We further identify a set of desiderata for
explanations that utilize Theory of Mind. These desiderata inform our
belief-based account of explanation.</li>
</ul>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/sokol2020towards/">Towards Faithful and Meaningful Interpretable Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Faithful and Meaningful Interpretable Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Faithful and Meaningful Interpretable Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kacper Sokol, Peter Flach</td>
	<td></td>
	<td><p>Interpretable representations are the backbone of many black-box explainers.
They translate the low-level data representation necessary for good predictive
performance into high-level human-intelligible concepts used to convey the
explanation. Notably, the explanation type and its cognitive complexity are
directly controlled by the interpretable representation, allowing to target a
particular audience and use case. However, many explainers that rely on
interpretable representations overlook their merit and fall back on default
solutions, which may introduce implicit assumptions, thereby degrading the
explanatory power of such techniques. To address this problem, we study
properties of interpretable representations that encode presence and absence of
human-comprehensible concepts. We show how they are operationalised for
tabular, image and text data, discussing their strengths and weaknesses.
Finally, we analyse their explanatory properties in the context of tabular
data, where a linear model is used to quantify the importance of interpretable
concepts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/you2020large-scale/">Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shutang You, Yilu Liu</td>
	<td></td>
	<td><p>The quality and accuracy of power system models is critical for
simulation-based studies, especially for studying actual stability issues in
large-scale systems. With the deployment of wide-area monitoring systems
(WAMSs), the high-reporting-rate frequency measurement provides a trustworthy
ground truth for validating models in frequency response studies. This paper
documented an effort to check, tune, and validate the U.S. power system model
based on a WAMS called FNET/GridEye. Four metrics are used to quantitatively
compare the simulation results and the actual measurement, including frequency
nadir, RoCoF, settling frequency and settling time. After tuning governor
deadband and the governor ratio, the model frequency response shows significant
improvement and matches well with the event measurement data. This work serves
as an example for tuning and validating large-scale power system models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/su2020sanity-checking/">Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, Jason D. Lee</td>
	<td></td>
	<td><p>Network pruning is a method for reducing test-time computational resource
requirements with minimal performance degradation. Conventional wisdom of
pruning algorithms suggests that: (1) Pruning methods exploit information from
training data to find good subnetworks; (2) The architecture of the pruned
network is crucial for good performance. In this paper, we conduct sanity
checks for the above beliefs on several recent unstructured pruning methods and
surprisingly find that: (1) A set of methods which aims to find good
subnetworks of the randomly-initialized network (which we call “initial
tickets”), hardly exploits any information from the training data; (2) For the
pruned networks obtained by these methods, randomly changing the preserved
weights in each layer, while keeping the total number of preserved weights
unchanged per layer, does not affect the final performance. These findings
inspire us to choose a series of simple \emph{data-independent} prune ratios
for each layer, and randomly prune each layer accordingly to get a subnetwork
(which we call “random tickets”). Experimental results show that our zero-shot
random tickets outperform or attain a similar performance compared to existing
“initial tickets”. In addition, we identify one existing pruning method that
passes our sanity checks. We hybridize the ratios in our random ticket with
this method and propose a new method called “hybrid tickets”, which achieves
further improvement. (Our code is publicly available at
https://github.com/JingtongSu/sanity-checking-pruning)</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/su2020sanity/">Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, Jason D. Lee</td>
	<td></td>
	<td><p>Network pruning is a method for reducing test-time computational resource
requirements with minimal performance degradation. Conventional wisdom of
pruning algorithms suggests that: (1) Pruning methods exploit information from
training data to find good subnetworks; (2) The architecture of the pruned
network is crucial for good performance. In this paper, we conduct sanity
checks for the above beliefs on several recent unstructured pruning methods and
surprisingly find that: (1) A set of methods which aims to find good
subnetworks of the randomly-initialized network (which we call “initial
tickets”), hardly exploits any information from the training data; (2) For the
pruned networks obtained by these methods, randomly changing the preserved
weights in each layer, while keeping the total number of preserved weights
unchanged per layer, does not affect the final performance. These findings
inspire us to choose a series of simple \emph{data-independent} prune ratios
for each layer, and randomly prune each layer accordingly to get a subnetwork
(which we call “random tickets”). Experimental results show that our zero-shot
random tickets outperform or attain a similar performance compared to existing
“initial tickets”. In addition, we identify one existing pruning method that
passes our sanity checks. We hybridize the ratios in our random ticket with
this method and propose a new method called “hybrid tickets”, which achieves
further improvement. (Our code is publicly available at
https://github.com/JingtongSu/sanity-checking-pruning)</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/chen2020generating/">Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hanjie Chen, Guangtao Zheng, Yangfeng Ji</td>
	<td></td>
	<td><p>Generating explanations for neural networks has become crucial for their
applications in real-world with respect to reliability and trustworthiness. In
natural language processing, existing methods usually provide important
features which are words or phrases selected from an input text as an
explanation, but ignore the interactions between them. It poses challenges for
humans to interpret an explanation and connect it to model prediction. In this
work, we build hierarchical explanations by detecting feature interactions.
Such explanations visualize how words and phrases are combined at different
levels of the hierarchy, which can help users understand the decision-making of
black-box models. The proposed method is evaluated with three neural text
classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic
and human evaluations. Experiments show the effectiveness of the proposed
method in providing explanations that are both faithful to models and
interpretable to humans.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/tjoa2020quantifying/">Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Erico Tjoa, Cuntai Guan</td>
	<td></td>
	<td><p>Post-hoc analysis is a popular category in eXplainable artificial
intelligence (XAI) study. In particular, methods that generate heatmaps have
been used to explain the deep neural network (DNN), a black-box model. Heatmaps
can be appealing due to the intuitive and visual ways to understand them but
assessing their qualities might not be straightforward. Different ways to
assess heatmaps’ quality have their own merits and shortcomings. This paper
introduces a synthetic dataset that can be generated adhoc along with the
ground-truth heatmaps for more objective quantitative assessment. Each sample
data is an image of a cell with easily recognized features that are
distinguished from localization ground-truth mask, hence facilitating a more
transparent assessment of different XAI methods. Comparison and recommendations
are made, shortcomings are clarified along with suggestions for future research
directions to handle the finer details of select post-hoc analysis methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/arras2020ground/">Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leila Arras, Ahmed Osman, Wojciech Samek</td>
	<td></td>
	<td><p>The rise of deep learning in today’s applications entailed an increasing need
in explaining the model’s decisions beyond prediction performances in order to
foster trust and accountability. Recently, the field of explainable AI (XAI)
has developed methods that provide such explanations for already trained neural
networks. In computer vision tasks such explanations, termed heatmaps,
visualize the contributions of individual pixels to the prediction. So far XAI
methods along with their heatmaps were mainly validated qualitatively via
human-based assessment, or evaluated through auxiliary proxy tasks such as
pixel perturbation, weak object localization or randomization tests. Due to the
lack of an objective and commonly accepted quality measure for heatmaps, it was
debatable which XAI method performs best and whether explanations can be
trusted at all. In the present work, we tackle the problem by proposing a
ground truth based evaluation framework for XAI methods based on the CLEVR
visual question answering task. Our framework provides a (1) selective, (2)
controlled and (3) realistic testbed for the evaluation of neural network
explanations. We compare ten different explanation methods, resulting in new
insights about the quality and properties of XAI methods, sometimes
contradicting with conclusions from previous comparative studies. The CLEVR-XAI
dataset and the benchmarking code can be found at
https://github.com/ahmedmagdiosman/clevr-xai.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/wang2020interpreting/">Interpreting Interpretations: Organizing Attribution Methods by Criteria</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Interpretations: Organizing Attribution Methods by Criteria' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Interpretations: Organizing Attribution Methods by Criteria' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zifan Wang, Piotr Mardziel, Anupam Datta, Matt Fredrikson</td>
	<td></td>
	<td><p>Motivated by distinct, though related, criteria, a growing number of
attribution methods have been developed tointerprete deep learning. While each
relies on the interpretability of the concept of “importance” and our ability
to visualize patterns, explanations produced by the methods often differ. As a
result, input attribution for vision models fail to provide any level of human
understanding of model behaviour. In this work we expand the foundationsof
human-understandable concepts with which attributionscan be interpreted beyond
“importance” and its visualization; we incorporate the logical concepts of
necessity andsufficiency, and the concept of proportionality. We definemetrics
to represent these concepts as quantitative aspectsof an attribution. This
allows us to compare attributionsproduced by different methods and interpret
them in novelways: to what extent does this attribution (or this
method)represent the necessity or sufficiency of the highlighted inputs, and to
what extent is it proportional? We evaluate our measures on a collection of
methods explaining convolutional neural networks (CNN) for image
classification. We conclude that some attribution methods are more appropriate
for interpretation in terms of necessity while others are in terms of
sufficiency, while no method is always the most appropriate in terms of both.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhao2020baylime/">BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xingyu Zhao, Wei Huang, Xiaowei Huang, Valentin Robu, David Flynn</td>
	<td></td>
	<td><p>Given the pressing need for assuring algorithmic transparency, Explainable AI
(XAI) has emerged as one of the key areas of AI research. In this paper, we
develop a novel Bayesian extension to the LIME framework, one of the most
widely used approaches in XAI – which we call BayLIME. Compared to LIME,
BayLIME exploits prior knowledge and Bayesian reasoning to improve both the
consistency in repeated explanations of a single prediction and the robustness
to kernel settings. BayLIME also exhibits better explanation fidelity than the
state-of-the-art (LIME, SHAP and GradCAM) by its ability to integrate prior
knowledge from, e.g., a variety of other XAI techniques, as well as
verification and validation (V&amp;V) methods. We demonstrate the desirable
properties of BayLIME through both theoretical analysis and extensive
experiments.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/danilevsky2020survey/">A Survey of the State of Explainable AI for Natural Language Processing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey of the State of Explainable AI for Natural Language Processing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey of the State of Explainable AI for Natural Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen</td>
	<td>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing 2020</td>
	<td><p>Recent years have seen important advances in the quality of state-of-the-art
models, but this has come at the expense of models becoming less interpretable.
This survey presents an overview of the current state of Explainable AI (XAI),
considered within the domain of Natural Language Processing (NLP). We discuss
the main categorization of explanations, as well as the various ways
explanations can be arrived at and visualized. We detail the operations and
explainability techniques currently available for generating explanations for
NLP model predictions, to serve as a resource for model developers in the
community. Finally, we point out the current gaps and encourage directions for
future work in this important research area.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/das2020leveraging/">Leveraging Rationales to Improve Human Task Performance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Rationales to Improve Human Task Performance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Leveraging Rationales to Improve Human Task Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Devleena Das, Sonia Chernova</td>
	<td></td>
	<td><p>Machine learning (ML) systems across many application areas are increasingly
demonstrating performance that is beyond that of humans. In response to the
proliferation of such models, the field of Explainable AI (XAI) has sought to
develop techniques that enhance the transparency and interpretability of
machine learning methods. In this work, we consider a question not previously
explored within the XAI and ML communities: Given a computational system whose
performance exceeds that of its human user, can explainable AI capabilities be
leveraged to improve the performance of the human? We study this question in
the context of the game of Chess, for which computational game engines that
surpass the performance of the average player are widely available. We
introduce the Rationale-Generating Algorithm, an automated technique for
generating rationales for utility-based computational methods, which we
evaluate with a multi-day user study against two baselines. The results show
that our approach produces rationales that lead to statistically significant
improvement in human task performance, demonstrating that rationales
automatically generated from an AI’s internal task model can be used not only
to explain what the system is doing, but also to instruct the user and
ultimately improve their task performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/dieber2020model/">Why model why? Assessing the strengths and limitations of LIME</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Why model why? Assessing the strengths and limitations of LIME' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Why model why? Assessing the strengths and limitations of LIME' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jürgen Dieber, Sabrina Kirrane</td>
	<td></td>
	<td><p>When it comes to complex machine learning models, commonly referred to as
black boxes, understanding the underlying decision making process is crucial
for domains such as healthcare and financial services, and also when it is used
in connection with safety critical systems such as autonomous vehicles. As such
interest in explainable artificial intelligence (xAI) tools and techniques has
increased in recent years. However, the effectiveness of existing xAI
frameworks, especially concerning algorithms that work with data as opposed to
images, is still an open research question. In order to address this gap, in
this paper we examine the effectiveness of the Local Interpretable
Model-Agnostic Explanations (LIME) xAI framework, one of the most popular model
agnostic frameworks found in the literature, with a specific focus on its
performance in terms of making tabular models more interpretable. In
particular, we apply several state of the art machine learning algorithms on a
tabular dataset, and demonstrate how LIME can be used to supplement
conventional performance assessment methods. In addition, we evaluate the
understandability of the output produced by LIME both via a usability study,
involving participants who are not familiar with LIME, and its overall
usability via an assessment framework, which is derived from the International
Organisation for Standardisation 9241-11:1998 standard.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/etheredge2020decontextualized/">Decontextualized learning for interpretable hierarchical representations of visual patterns</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Decontextualized learning for interpretable hierarchical representations of visual patterns' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Decontextualized learning for interpretable hierarchical representations of visual patterns' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>R. Ian Etheredge, Manfred Schartl, Alex Jordan</td>
	<td></td>
	<td><p>Apart from discriminative models for classification and object detection
tasks, the application of deep convolutional neural networks to basic research
utilizing natural imaging data has been somewhat limited; particularly in cases
where a set of interpretable features for downstream analysis is needed, a key
requirement for many scientific investigations. We present an algorithm and
training paradigm designed specifically to address this: decontextualized
hierarchical representation learning (DHRL). By combining a generative model
chaining procedure with a ladder network architecture and latent space
regularization for inference, DHRL address the limitations of small datasets
and encourages a disentangled set of hierarchically organized features. In
addition to providing a tractable path for analyzing complex hierarchal
patterns using variation inference, this approach is generative and can be
directly combined with empirical and theoretical approaches. To highlight the
extensibility and usefulness of DHRL, we demonstrate this method in application
to a question from evolutionary biology.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fan2020trust/">Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ming Fan, Wenying Wei, Xiaofei Xie, Yang Liu, Xiaohong Guan, Ting Liu</td>
	<td></td>
	<td><p>With the rapid growth of Android malware, many machine learning-based malware
analysis approaches are proposed to mitigate the severe phenomenon. However,
such classifiers are opaque, non-intuitive, and difficult for analysts to
understand the inner decision reason. For this reason, a variety of explanation
approaches are proposed to interpret predictions by providing important
features. Unfortunately, the explanation results obtained in the malware
analysis domain cannot achieve a consensus in general, which makes the analysts
confused about whether they can trust such results. In this work, we propose
principled guidelines to assess the quality of five explanation approaches by
designing three critical quantitative metrics to measure their stability,
robustness, and effectiveness. Furthermore, we collect five widely-used malware
datasets and apply the explanation approaches on them in two tasks, including
malware detection and familial identification. Based on the generated
explanation results, we conduct a sanity check of such explanation approaches
in terms of the three metrics. The results demonstrate that our metrics can
assess the explanation approaches and help us obtain the knowledge of most
typical malicious behaviors for malware analysis.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fr%C3%A4mling2020explainable/">Explainable AI without Interpretable Model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable AI without Interpretable Model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable AI without Interpretable Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kary Främling</td>
	<td></td>
	<td><p>Explainability has been a challenge in AI for as long as AI has existed. With
the recently increased use of AI in society, it has become more important than
ever that AI systems would be able to explain the reasoning behind their
results also to end-users in situations such as being eliminated from a
recruitment process or having a bank loan application refused by an AI system.
Especially if the AI system has been trained using Machine Learning, it tends
to contain too many parameters for them to be analysed and understood, which
has caused them to be called `black-box’ systems. Most Explainable AI (XAI)
methods are based on extracting an interpretable model that can be used for
producing explanations. However, the interpretable model does not necessarily
map accurately to the original black-box model. Furthermore, the
understandability of interpretable models for an end-user remains questionable.
The notions of Contextual Importance and Utility (CIU) presented in this paper
make it possible to produce human-like explanations of black-box outcomes
directly, without creating an interpretable model. Therefore, CIU explanations
map accurately to the black-box model itself. CIU is completely model-agnostic
and can be used with any black-box system. In addition to feature importance,
the utility concept that is well-known in Decision Theory provides a new
dimension to explanations compared to most existing XAI methods. Finally, CIU
can produce explanations at any level of abstraction and using different
vocabularies and other means of interaction, which makes it possible to adjust
explanations and interaction according to the context and to the target users.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fu2020interactive/">Interactive Knowledge Distillation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interactive Knowledge Distillation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interactive Knowledge Distillation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shipeng Fu, Zhen Li, Jun Xu, Ming-Ming Cheng, Zitao Liu, Xiaomin Yang</td>
	<td></td>
	<td><p>Knowledge distillation is a standard teacher-student learning framework to
train a light-weight student network under the guidance of a well-trained large
teacher network. As an effective teaching strategy, interactive teaching has
been widely employed at school to motivate students, in which teachers not only
provide knowledge but also give constructive feedback to students upon their
responses, to improve their learning performance. In this work, we propose an
InterActive Knowledge Distillation (IAKD) scheme to leverage the interactive
teaching strategy for efficient knowledge distillation. In the distillation
process, the interaction between teacher and student networks is implemented by
a swapping-in operation: randomly replacing the blocks in the student network
with the corresponding blocks in the teacher network. In the way, we directly
involve the teacher’s powerful feature transformation ability to largely boost
the student’s performance. Experiments with typical settings of teacher-student
networks demonstrate that the student networks trained by our IAKD achieve
better performance than those trained by conventional knowledge distillation
methods on diverse image classification datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/arun2020assessing/">Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nishanth Arun, Nathan Gaw, Praveer Singh, Ken Chang, Mehak Aggarwal, Bryan Chen, Katharina Hoebel, Sharut Gupta, Jay Patel, Mishka Gidwani, Julius Adebayo, Matthew D. Li, Jayashree Kalpathy-Cramer</td>
	<td></td>
	<td><p>Saliency maps have become a widely used method to make deep learning models
more interpretable by providing post-hoc explanations of classifiers through
identification of the most pertinent areas of the input medical image. They are
increasingly being used in medical imaging to provide clinically plausible
explanations for the decisions the neural network makes. However, the utility
and robustness of these visualization maps has not yet been rigorously examined
in the context of medical imaging. We posit that trustworthiness in this
context requires 1) localization utility, 2) sensitivity to model weight
randomization, 3) repeatability, and 4) reproducibility. Using the localization
information available in two large public radiology datasets, we quantify the
performance of eight commonly used saliency map approaches for the above
criteria using area under the precision-recall curves (AUPRC) and structural
similarity index (SSIM), comparing their performance to various baseline
measures. Using our framework to quantify the trustworthiness of saliency maps,
we show that all eight saliency map techniques fail at least one of the
criteria and are, in most cases, less trustworthy when compared to the
baselines. We suggest that their usage in the high-risk domain of medical
imaging warrants additional scrutiny and recommend that detection or
segmentation models be used if localization is the desired output of the
network. Additionally, to promote reproducibility of our findings, we provide
the code we used for all tests performed in this work at this link:
https://github.com/QTIM-Lab/Assessing-Saliency-Maps.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/gu2020interpretable/">Interpretable Graph Capsule Networks for Object Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Graph Capsule Networks for Object Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Graph Capsule Networks for Object Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jindong Gu, Volker Tresp</td>
	<td>The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI), 2021</td>
	<td><p>Capsule Networks, as alternatives to Convolutional Neural Networks, have been
proposed to recognize objects from images. The current literature demonstrates
many advantages of CapsNets over CNNs. However, how to create explanations for
individual classifications of CapsNets has not been well explored. The widely
used saliency methods are mainly proposed for explaining CNN-based
classifications; they create saliency map explanations by combining activation
values and the corresponding gradients, e.g., Grad-CAM. These saliency methods
require a specific architecture of the underlying classifiers and cannot be
trivially applied to CapsNets due to the iterative routing mechanism therein.
To overcome the lack of interpretability, we can either propose new post-hoc
interpretation methods for CapsNets or modifying the model to have build-in
explanations. In this work, we explore the latter. Specifically, we propose
interpretable Graph Capsule Networks (GraCapsNets), where we replace the
routing part with a multi-head attention-based Graph Pooling approach. In the
proposed model, individual classification explanations can be created
effectively and efficiently. Our model also demonstrates some unexpected
benefits, even though it replaces the fundamental part of CapsNets. Our
GraCapsNets achieve better classification performance with fewer parameters and
better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets
also keep other advantages of CapsNets, namely, disentangled representations
and affine transformation robustness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hase2020evaluating/">Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Peter Hase, Mohit Bansal</td>
	<td></td>
	<td><p>Algorithmic approaches to interpreting machine learning models have
proliferated in recent years. We carry out human subject tests that are the
first of their kind to isolate the effect of algorithmic explanations on a key
aspect of model interpretability, simulatability, while avoiding important
confounding experimental factors. A model is simulatable when a person can
predict its behavior on new inputs. Through two kinds of simulation tests
involving text and tabular data, we evaluate five explanations methods: (1)
LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a
Composite approach that combines explanations from each method. Clear evidence
of method effectiveness is found in very few cases: LIME improves
simulatability in tabular classification, and our Prototype method is effective
in counterfactual simulation tests. We also collect subjective ratings of
explanations, but we do not find that ratings are predictive of how helpful
explanations are. Our results provide the first reliable and comprehensive
estimates of how explanations influence simulatability across a variety of
explanation methods and data domains. We show that (1) we need to be careful
about the metrics we use to evaluate explanation methods, and (2) there is
significant room for improvement in current methods. All our supporting code,
data, and models are publicly available at:
https://github.com/peterbhase/InterpretableNLP-ACL2020</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/asher2020adequate/">Adequate and fair explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adequate and fair explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adequate and fair explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nicholas Asher, Soumya Paul, Chris Russell</td>
	<td>Machine Learning and Knowledge Extraction, eds. Andreas Holzinger, Peter Kieseberg, A Min Tjoa, Edgar Weippl, Lecture Notes in Computer Science 12844, Springer, pp. 79-99, 2021</td>
	<td><p>Explaining sophisticated machine-learning based systems is an important issue
at the foundations of AI. Recent efforts have shown various methods for
providing explanations. These approaches can be broadly divided into two
schools: those that provide a local and human interpreatable approximation of a
machine learning algorithm, and logical approaches that exactly characterise
one aspect of the decision. In this paper we focus upon the second school of
exact explanations with a rigorous logical foundation. There is an
epistemological problem with these exact methods. While they can furnish
complete explanations, such explanations may be too complex for humans to
understand or even to write down in human readable form. Interpretability
requires epistemically accessible explanations, explanations humans can grasp.
Yet what is a sufficiently complete epistemically accessible explanation still
needs clarification. We do this here in terms of counterfactuals, following
[Wachter et al., 2017]. With counterfactual explanations, many of the
assumptions needed to provide a complete explanation are left implicit. To do
so, counterfactual explanations exploit the properties of a particular data
point or sample, and as such are also local as well as partial explanations. We
explore how to move from local partial explanations to what we call complete
local explanations and then to global ones. But to preserve accessibility we
argue for the need for partiality. This partiality makes it possible to hide
explicit biases present in the algorithm that may be injurious or unfair.We
investigate how easy it is to uncover these biases in providing complete and
fair explanations by exploiting the structure of the set of counterfactuals
providing a complete local explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/honeycutt2020soliciting/">Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Donald R. Honeycutt, Mahsan Nourani, Eric D. Ragan</td>
	<td></td>
	<td><p>Mixed-initiative systems allow users to interactively provide feedback to
potentially improve system performance. Human feedback can correct model errors
and update model parameters to dynamically adapt to changing data.
Additionally, many users desire the ability to have a greater level of control
and fix perceived flaws in systems they rely on. However, how the ability to
provide feedback to autonomous systems influences user trust is a largely
unexplored area of research. Our research investigates how the act of providing
feedback can affect user understanding of an intelligent system and its
accuracy. We present a controlled experiment using a simulated object detection
system with image data to study the effects of interactive feedback collection
on user impressions. The results show that providing human-in-the-loop feedback
lowered both participants’ trust in the system and their perception of system
accuracy, regardless of whether the system accuracy improved in response to
their feedback. These results highlight the importance of considering the
effects of allowing end-user feedback on user trust when designing intelligent
systems.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hsieh2020evaluations/">Evaluations and Methods for Explanation through Robustness Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluations and Methods for Explanation through Robustness Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluations and Methods for Explanation through Robustness Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh</td>
	<td></td>
	<td><p>Feature based explanations, that provide importance of each feature towards
the model prediction, is arguably one of the most intuitive ways to explain a
model. In this paper, we establish a novel set of evaluation criteria for such
feature based explanations by robustness analysis. In contrast to existing
evaluations which require us to specify some way to “remove” features that
could inevitably introduces biases and artifacts, we make use of the subtler
notion of smaller adversarial perturbations. By optimizing towards our proposed
evaluation criteria, we obtain new explanations that are loosely necessary and
sufficient for a prediction. We further extend the explanation to extract the
set of features that would move the current prediction to a target class by
adopting targeted adversarial attack for the robustness analysis. Through
experiments across multiple domains and a user study, we validate the
usefulness of our evaluation criteria and our derived explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/atanasova2020generating/">Generating Fact Checking Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Fact Checking Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Fact Checking Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein</td>
	<td></td>
	<td><p>Most existing work on automated fact checking is concerned with predicting
the veracity of claims based on metadata, social network spread, language used
in claims, and, more recently, evidence supporting or denying claims. A crucial
piece of the puzzle that is still missing is to understand how to automate the
most elaborate part of the process – generating justifications for verdicts on
claims. This paper provides the first study of how these explanations can be
generated automatically based on available claim context, and how this task can
be modelled jointly with veracity prediction. Our results indicate that
optimising both objectives at the same time, rather than training them
separately, improves the performance of a fact checking system. The results of
a manual evaluation further suggest that the informativeness, coverage and
overall quality of the generated explanations are also improved in the
multi-task model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhang2020survey/">A Survey on Neural Network Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Neural Network Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey on Neural Network Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yu Zhang, Peter Tiňo, Aleš Leonardis, Ke Tang</td>
	<td>IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 5, no.5, pp. 726-742, Oct. 2021</td>
	<td><p>Along with the great success of deep neural networks, there is also growing
concern about their black-box nature. The interpretability issue affects
people’s trust on deep learning systems. It is also related to many ethical
problems, e.g., algorithmic discrimination. Moreover, interpretability is a
desired property for deep networks to become powerful tools in other research
fields, e.g., drug discovery and genomics. In this survey, we conduct a
comprehensive review of the neural network interpretability research. We first
clarify the definition of interpretability as it has been used in many
different contexts. Then we elaborate on the importance of interpretability and
propose a novel taxonomy organized along three dimensions: type of engagement
(passive vs. active interpretation approaches), the type of explanation, and
the focus (from local to global interpretability). This taxonomy provides a
meaningful 3D view of distribution of papers from the relevant literature as
two of the dimensions are not simply categorical but allow ordinal
subcategories. Finally, we summarize the existing interpretability evaluation
methods and suggest possible research directions inspired by our new taxonomy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ignatiev2020relating/">On Relating 'Why?' and 'Why Not?' Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Relating 'Why?' and 'Why Not?' Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Relating 'Why?' and 'Why Not?' Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alexey Ignatiev, Nina Narodytska, Nicholas Asher, Joao Marques-Silva</td>
	<td></td>
	<td><p>Explanations of Machine Learning (ML) models often address a ‘Why?’ question.
Such explanations can be related with selecting feature-value pairs which are
sufficient for the prediction. Recent work has investigated explanations that
address a ‘Why Not?’ question, i.e. finding a change of feature values that
guarantee a change of prediction. Given their goals, these two forms of
explaining predictions of ML models appear to be mostly unrelated. However,
this paper demonstrates otherwise, and establishes a rigorous formal
relationship between ‘Why?’ and ‘Why Not?’ explanations. Concretely, the paper
proves that, for any given instance, ‘Why?’ explanations are minimal hitting
sets of ‘Why Not?’ explanations and vice-versa. Furthermore, the paper devises
novel algorithms for extracting and enumerating both forms of explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ismail2020benchmarking/">Benchmarking Deep Learning Interpretability in Time Series Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking Deep Learning Interpretability in Time Series Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Benchmarking Deep Learning Interpretability in Time Series Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aya Abdelsalam Ismail, Mohamed Gunady, Héctor Corrada Bravo, Soheil Feizi</td>
	<td>NeurIPS 2020</td>
	<td><p>Saliency methods are used extensively to highlight the importance of input
features in model predictions. These methods are mostly used in vision and
language tasks, and their applications to time series data is relatively
unexplored. In this paper, we set out to extensively compare the performance of
various saliency-based interpretability methods across diverse neural
architectures, including Recurrent Neural Network, Temporal Convolutional
Networks, and Transformers in a new benchmark of synthetic time series data. We
propose and report multiple metrics to empirically evaluate the performance of
saliency methods for detecting feature importance over time using both
precision (i.e., whether identified features contain meaningful signals) and
recall (i.e., the number of features with signal identified as important).
Through several experiments, we show that (i) in general, network architectures
and saliency methods fail to reliably and accurately identify feature
importance over time in time series data, (ii) this failure is mainly due to
the conflation of time and feature domains, and (iii) the quality of saliency
maps can be improved substantially by using our proposed two-step temporal
saliency rescaling (TSR) approach that first calculates the importance of each
time step before calculating the importance of each feature at a time step.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zunino2020explainable/">Explainable Deep Classification Models for Domain Generalization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Deep Classification Models for Domain Generalization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Deep Classification Models for Domain Generalization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrea Zunino, Sarah Adel Bargal, Riccardo Volpi, Mehrnoosh Sameki, Jianming Zhang, Stan Sclaroff, Vittorio Murino, Kate Saenko</td>
	<td></td>
	<td><p>Conventionally, AI models are thought to trade off explainability for lower
accuracy. We develop a training strategy that not only leads to a more
explainable AI system for object classification, but as a consequence, suffers
no perceptible accuracy degradation. Explanations are defined as regions of
visual evidence upon which a deep classification network makes a decision. This
is represented in the form of a saliency map conveying how much each pixel
contributed to the network’s decision. Our training strategy enforces a
periodic saliency-based feedback to encourage the model to focus on the image
regions that directly correspond to the ground-truth object. We quantify
explainability using an automated metric, and using human judgement. We propose
explainability as a means for bridging the visual-semantic gap between
different domains where model explanations are used as a means of disentagling
domain specific information from otherwise relevant features. We demonstrate
that this leads to improved generalization to new domains without hindering
performance on the original domain.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/camburu2020explaining/">Explaining Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Oana-Maria Camburu</td>
	<td></td>
	<td><p>Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas, such as computer vision, natural
language processing, and speech recognition. However, the decision-making
processes of these models are generally not interpretable to users. In various
domains, such as healthcare, finance, or law, it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore,
several directions for explaining neural models have recently been explored. In
this thesis, I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods, that is, methods that aim to explain an already trained and fixed
model (post-hoc), and that provide explanations in terms of input features,
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations, that is, models that have a built-in module that
generates explanations for the predictions of the model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/kang2020multivariate/">Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaoning Kang, Xiaoyu Chen, Ran Jin, Hao Wu, Xinwei Deng</td>
	<td></td>
	<td><p>Information visualization significantly enhances human perception by
graphically representing complex data sets. The variety of visualization
designs makes it challenging to efficiently evaluate all possible designs
catering to users’ preferences and characteristics. Most of existing evaluation
methods perform user studies to obtain multivariate qualitative responses from
users via questionnaires and interviews. However, these methods cannot support
online evaluation of designs as they are often time-consuming. A statistical
model is desired to predict users’ preferences on visualization designs based
on non-interference measurements (i.e., wearable sensor signals). In this work,
we propose a multivariate regression of mixed responses (MRMR) to facilitate
quantitative evaluation of visualization designs. The proposed MRMR method is
able to provide accurate model prediction with meaningful variable selection. A
simulation study and a user study of evaluating visualization designs with 14
effective participants are conducted to illustrate the merits of the proposed
model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bai2020attentions/">Why Attentions May Not Be Interpretable?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Why Attentions May Not Be Interpretable?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Why Attentions May Not Be Interpretable?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bing Bai, Jian Liang, Guanhua Zhang, Hao Li, Kun Bai, Fei Wang</td>
	<td></td>
	<td><p>Attention-based methods have played important roles in model interpretations,
where the calculated attention weights are expected to highlight the critical
parts of inputs~(e.g., keywords in sentences). However, recent research found
that attention-as-importance interpretations often do not work as we expected.
For example, learned attention weights sometimes highlight less meaningful
tokens like “[SEP]”, “,”, and “.”, and are frequently uncorrelated with other
feature importance indicators like gradient-based measures. A recent debate
over whether attention is an explanation or not has drawn considerable
interest. In this paper, we demonstrate that one root cause of this phenomenon
is the combinatorial shortcuts, which means that, in addition to the
highlighted parts, the attention weights themselves may carry extra information
that could be utilized by downstream models after attention layers. As a
result, the attention weights are no longer pure importance indicators. We
theoretically analyze combinatorial shortcuts, design one intuitive experiment
to show their existence, and propose two methods to mitigate this issue. We
conduct empirical studies on attention-based interpretation models. The results
show that the proposed methods can effectively improve the interpretability of
attention mechanisms.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/barr2020towards/">Towards Ground Truth Explainability on Tabular Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Ground Truth Explainability on Tabular Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Ground Truth Explainability on Tabular Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Brian Barr, Ke Xu, Claudio Silva, Enrico Bertini, Robert Reilly, C. Bayan Bruss, Jason D. Wittenbach</td>
	<td></td>
	<td><p>In data science, there is a long history of using synthetic data for method
development, feature selection and feature engineering. Our current interest in
synthetic data comes from recent work in explainability. Today’s datasets are
typically larger and more complex - requiring less interpretable models. In the
setting of \textit{post hoc} explainability, there is no ground truth for
explanations. Inspired by recent work in explaining image classifiers that does
provide ground truth, we propose a similar solution for tabular data. Using
copulas, a concise specification of the desired statistical properties of a
dataset, users can build intuition around explainability using controlled data
sets and experimentation. The current capabilities are demonstrated on three
use cases: one dimensional logistic regression, impact of correlation from
informative features, impact of correlation from redundant variables.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bastings2020elephant/">The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jasmijn Bastings, Katja Filippova</td>
	<td>Proceedings of the 2020 EMNLP Workshop BlackboxNLP</td>
	<td><p>There is a recent surge of interest in using attention as explanation of
model predictions, with mixed evidence on whether attention can be used as
such. While attention conveniently gives us one weight per input token and is
easily extracted, it is often unclear toward what goal it is used as
explanation. We find that often that goal, whether explicitly stated or not, is
to find out what input tokens are the most relevant to a prediction, and that
the implied user for the explanation is a model developer. For this goal and
user, we argue that input saliency methods are better suited, and that there
are no compelling reasons to use attention, despite the coincidence that it
provides a weight for each input. With this position paper, we hope to shift
some of the recent focus on attention to saliency methods, and for authors to
clearly state the goal and user for their explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/benchekroun2020need/">The Need for Standardized Explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Need for Standardized Explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Need for Standardized Explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Othman Benchekroun, Adel Rahimi, Qini Zhang, Tetiana Kodliuk</td>
	<td></td>
	<td><p>Explainable AI (XAI) is paramount in industry-grade AI; however existing
methods fail to address this necessity, in part due to a lack of
standardisation of explainability methods. The purpose of this paper is to
offer a perspective on the current state of the area of explainability, and to
provide novel definitions for Explainability and Interpretability to begin
standardising this area of research. To do so, we provide an overview of the
literature on explainability, and of the existing methods that are already
implemented. Finally, we offer a tentative taxonomy of the different
explainability methods, opening the door to future research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/leavitt2020towards/">Towards falsifiable interpretability research</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards falsifiable interpretability research' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards falsifiable interpretability research' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matthew L. Leavitt, Ari Morcos</td>
	<td></td>
	<td><p>Methods for understanding the decisions of and mechanisms underlying deep
neural networks (DNNs) typically rely on building intuition by emphasizing
sensory or semantic features of individual examples. For instance, methods aim
to visualize the components of an input which are “important” to a network’s
decision, or to measure the semantic properties of single neurons. Here, we
argue that interpretability research suffers from an over-reliance on
intuition-based approaches that risk-and in some cases have caused-illusory
progress and misleading conclusions. We identify a set of limitations that we
argue impede meaningful progress in interpretability research, and examine two
popular classes of interpretability methods-saliency and single-neuron-based
approaches-that serve as case studies for how overreliance on intuition and
lack of falsifiability can undermine interpretability research. To address
these concerns, we propose a strategy to address these impediments in the form
of a framework for strongly falsifiable interpretability research. We encourage
researchers to use their intuitions as a starting point to develop and test
clear, falsifiable hypotheses, and hope that our framework yields robust,
evidence-based interpretability methods that generate meaningful advances in
our understanding of DNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/lin2020see/">What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik</td>
	<td></td>
	<td><p>EXplainable AI (XAI) methods have been proposed to interpret how a deep
neural network predicts inputs through model saliency explanations that
highlight the parts of the inputs deemed important to arrive a decision at a
specific target. However, it remains challenging to quantify correctness of
their interpretability as current evaluation approaches either require
subjective input from humans or incur high computation cost with automated
evaluation. In this paper, we propose backdoor trigger patterns–hidden
malicious functionalities that cause misclassification–to automate the
evaluation of saliency explanations. Our key observation is that triggers
provide ground truth for inputs to evaluate whether the regions identified by
an XAI method are truly relevant to its output. Since backdoor triggers are the
most important features that cause deliberate misclassification, a robust XAI
method should reveal their presence at inference time. We introduce three
complementary metrics for systematic evaluation of explanations that an XAI
method generates and evaluate seven state-of-the-art model-free and
model-specific posthoc methods through 36 models trojaned with specifically
crafted triggers using color, shape, texture, location, and size. We discovered
six methods that use local explanation and feature relevance fail to completely
highlight trigger regions, and only a model-free approach can uncover the
entire trigger region.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/berger2020visually/">Visually Analyzing Contextualized Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visually Analyzing Contextualized Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visually Analyzing Contextualized Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matthew Berger</td>
	<td></td>
	<td><p>In this paper we introduce a method for visually analyzing contextualized
embeddings produced by deep neural network-based language models. Our approach
is inspired by linguistic probes for natural language processing, where tasks
are designed to probe language models for linguistic structure, such as
parts-of-speech and named entities. These approaches are largely confirmatory,
however, only enabling a user to test for information known a priori. In this
work, we eschew supervised probing tasks, and advocate for unsupervised probes,
coupled with visual exploration techniques, to assess what is learned by
language models. Specifically, we cluster contextualized embeddings produced
from a large text corpus, and introduce a visualization design based on this
clustering and textual structure - cluster co-occurrences, cluster spans, and
cluster-word membership - to help elicit the functionality of, and relationship
between, individual clusters. User feedback highlights the benefits of our
design in discovering different types of linguistic structures.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mangla2020saliency/">On Saliency Maps and Adversarial Robustness</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Saliency Maps and Adversarial Robustness' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Saliency Maps and Adversarial Robustness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Puneet Mangla, Vedant Singh, Vineeth N Balasubramanian</td>
	<td></td>
	<td><p>A Very recent trend has emerged to couple the notion of interpretability and
adversarial robustness, unlike earlier efforts which solely focused on good
interpretations or robustness against adversaries. Works have shown that
adversarially trained models exhibit more interpretable saliency maps than
their non-robust counterparts, and that this behavior can be quantified by
considering the alignment between input image and saliency map. In this work,
we provide a different perspective to this coupling, and provide a method,
Saliency based Adversarial training (SAT), to use saliency maps to improve
adversarial robustness of a model. In particular, we show that using
annotations such as bounding boxes and segmentation masks, already provided
with a dataset, as weak saliency maps, suffices to improve adversarial
robustness with no additional effort to generate the perturbations themselves.
Our empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17
datasets consistently corroborate our claim, by showing improved adversarial
robustness using our method. saliency maps. We also show how using finer and
stronger saliency maps leads to more robust models, and how integrating SAT
with existing adversarial training methods, further boosts performance of these
existing methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/marcos2020contextual/">Contextual Semantic Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contextual Semantic Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contextual Semantic Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Diego Marcos, Ruth Fong, Sylvain Lobry, Remi Flamary, Nicolas Courty, Devis Tuia</td>
	<td>ACCV 2020</td>
	<td><p>Convolutional neural networks (CNN) are known to learn an image
representation that captures concepts relevant to the task, but do so in an
implicit way that hampers model interpretability. However, one could argue that
such a representation is hidden in the neurons and can be made explicit by
teaching the model to recognize semantically interpretable attributes that are
present in the scene. We call such an intermediate layer a \emph{semantic
bottleneck}. Once the attributes are learned, they can be re-combined to reach
the final decision and provide both an accurate prediction and an explicit
reasoning behind the CNN decision. In this paper, we look into semantic
bottlenecks that capture context: we want attributes to be in groups of a few
meaningful elements and participate jointly to the final decision. We use a
two-layer semantic bottleneck that gathers attributes into interpretable,
sparse groups, allowing them contribute differently to the final output
depending on the context. We test our contextual semantic interpretable
bottleneck (CSIB) on the task of landscape scenicness estimation and train the
semantic interpretable bottleneck using an auxiliary database (SUN Attributes).
Our model yields in predictions as accurate as a non-interpretable baseline
when applied to a real-world test set of Flickr images, all while providing
clear and interpretable explanations for each prediction.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/molnar2020interpretable/">Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Christoph Molnar, Giuseppe Casalicchio, Bernd Bischl</td>
	<td>Koprinska I. et al. (eds) ECML PKDD 2020 Workshops. ECML PKDD 2020. Communications in Computer and Information Science, vol 1323. Springer, Cham</td>
	<td><p>We present a brief history of the field of interpretable machine learning
(IML), give an overview of state-of-the-art interpretation methods, and discuss
challenges. Research in IML has boomed in recent years. As young as the field
is, it has over 200 years old roots in regression modeling and rule-based
machine learning, starting in the 1960s. Recently, many new IML methods have
been proposed, many of them model-agnostic, but also interpretation techniques
specific to deep learning and tree-based ensembles. IML methods either directly
analyze model components, study sensitivity to input perturbations, or analyze
local or global surrogate approximations of the ML model. The field approaches
a state of readiness and stability, with many methods not only proposed in
research, but also implemented in open-source software. But many important
challenges remain for IML, such as dealing with dependent features, causal
interpretation, and uncertainty estimation, which need to be resolved for its
successful application to scientific problems. A further challenge is a missing
rigorous definition of interpretability, which is accepted by the community. To
address the challenges and advance the field, we urge to recall our roots of
interpretable, data-driven modeling in statistics and (rule-based) ML, but also
to consider other areas such as sensitivity analysis, causal inference, and the
social sciences.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/moradi2020explaining/">Explaining Black-box Models for Biomedical Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Black-box Models for Biomedical Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Black-box Models for Biomedical Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Milad Moradi, Matthias Samwald</td>
	<td></td>
	<td><p>In this paper, we propose a novel method named Biomedical Confident Itemsets
Explanation (BioCIE), aiming at post-hoc explanation of black-box machine
learning models for biomedical text classification. Using sources of domain
knowledge and a confident itemset mining method, BioCIE discretizes the
decision space of a black-box into smaller subspaces and extracts semantic
relationships between the input text and class labels in different subspaces.
Confident itemsets discover how biomedical concepts are related to class labels
in the black-box’s decision space. BioCIE uses the itemsets to approximate the
black-box’s behavior for individual predictions. Optimizing fidelity,
interpretability, and coverage measures, BioCIE produces class-wise
explanations that represent decision boundaries of the black-box. Results of
evaluations on various biomedical text classification tasks and black-box
models demonstrated that BioCIE can outperform perturbation-based and decision
set methods in terms of producing concise, accurate, and interpretable
explanations. BioCIE improved the fidelity of instance-wise and class-wise
explanations by 11.6% and 7.5%, respectively. It also improved the
interpretability of explanations by 8%. BioCIE can be effectively used to
explain how a black-box biomedical text classification model semantically
relates input texts to class labels. The source code and supplementary material
are available at https://github.com/mmoradi-iut/BioCIE.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mukhopadhyay2020decoding/">Decoding CNN based Object Classifier Using Visualization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Decoding CNN based Object Classifier Using Visualization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Decoding CNN based Object Classifier Using Visualization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abhishek Mukhopadhyay, Imon Mukherjee, Pradipta Biswas</td>
	<td></td>
	<td><p>This paper investigates how working of Convolutional Neural Network (CNN) can
be explained through visualization in the context of machine perception of
autonomous vehicles. We visualize what type of features are extracted in
different convolution layers of CNN that helps to understand how CNN gradually
increases spatial information in every layer. Thus, it concentrates on region
of interests in every transformation. Visualizing heat map of activation helps
us to understand how CNN classifies and localizes different objects in image.
This study also helps us to reason behind low accuracy of a model helps to
increase trust on object detection module.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/naik2020explanation/">Explanation from Specification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation from Specification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation from Specification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Harish Naik, György Turán</td>
	<td></td>
	<td><p>Explainable components in XAI algorithms often come from a familiar set of
models, such as linear models or decision trees. We formulate an approach where
the type of explanation produced is guided by a specification. Specifications
are elicited from the user, possibly using interaction with the user and
contributions from other areas. Areas where a specification could be obtained
include forensic, medical, and scientific applications. Providing a menu of
possible types of specifications in an area is an exploratory knowledge
representation and reasoning task for the algorithm designer, aiming at
understanding the possibilities and limitations of efficiently computable modes
of explanations. Two examples are discussed: explanations for Bayesian networks
using the theory of argumentation, and explanations for graph neural networks.
The latter case illustrates the possibility of having a representation
formalism available to the user for specifying the type of explanation
requested, for example, a chemical query language for classifying molecules.
The approach is motivated by a theory of explanation in the philosophy of
science, and it is related to current questions in the philosophy of science on
the role of machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bhatt2020evaluating/">Evaluating and Aggregating Feature-based Model Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating and Aggregating Feature-based Model Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating and Aggregating Feature-based Model Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Umang Bhatt, Adrian Weller, José M. F. Moura</td>
	<td></td>
	<td><p>A feature-based model explanation denotes how much each input feature
contributes to a model’s output for a given data point. As the number of
proposed explanation functions grows, we lack quantitative evaluation criteria
to help practitioners know when to use which explanation function. This paper
proposes quantitative evaluation criteria for feature-based explanations: low
sensitivity, high faithfulness, and low complexity. We devise a framework for
aggregating explanation functions. We develop a procedure for learning an
aggregate explanation function with lower complexity and then derive a new
aggregate Shapley value explanation function that minimizes sensitivity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/pruthi2020evaluating/">Evaluating Explanations: How much do explanations from the teacher aid students?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Explanations: How much do explanations from the teacher aid students?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Explanations: How much do explanations from the teacher aid students?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen</td>
	<td></td>
	<td><p>While many methods purport to explain predictions by highlighting salient
features, what aims these explanations serve and how they ought to be evaluated
often go unstated. In this work, we introduce a framework to quantify the value
of explanations via the accuracy gains that they confer on a student model
trained to simulate a teacher model. Crucially, the explanations are available
to the student during training, but are not available at test time. Compared to
prior proposals, our approach is less easily gamed, enabling principled,
automatic, model-agnostic evaluation of attributions. Using our framework, we
compare numerous attribution methods for text classification and question
answering, and observe quantitative differences that are consistent (to a
moderate to high degree) across different student model architectures and
learning strategies.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/puiutta2020explainable/">Explainable Reinforcement Learning: A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Reinforcement Learning: A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Reinforcement Learning: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Erika Puiutta, Eric MSP Veith</td>
	<td></td>
	<td><p>Explainable Artificial Intelligence (XAI), i.e., the development of more
transparent and interpretable AI models, has gained increased traction over the
last few years. This is due to the fact that, in conjunction with their growth
into powerful and ubiquitous tools, AI models exhibit one detrimential
characteristic: a performance-transparency trade-off. This describes the fact
that the more complex a model’s inner workings, the less clear it is how its
predictions or decisions were achieved. But, especially considering Machine
Learning (ML) methods like Reinforcement Learning (RL) where the system learns
autonomously, the necessity to understand the underlying reasoning for their
decisions becomes apparent. Since, to the best of our knowledge, there exists
no single work offering an overview of Explainable Reinforcement Learning (XRL)
methods, this survey attempts to address this gap. We give a short summary of
the problem, a definition of important terms, and offer a classification and
assessment of current XRL methods. We found that a) the majority of XRL methods
function by mimicking and simplifying a complex model instead of designing an
inherently simple one, and b) XRL (and XAI) methods often neglect to consider
the human side of the equation, not taking into account research from related
fields like psychology or philosophy. Thus, an interdisciplinary effort is
needed to adapt the generated explanations to a (non-expert) human user in
order to effectively progress in the field of XRL and XAI in general.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/b%C3%A4uerle2020explornn/">exploRNN: Understanding Recurrent Neural Networks through Visual Exploration</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=exploRNN: Understanding Recurrent Neural Networks through Visual Exploration' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=exploRNN: Understanding Recurrent Neural Networks through Visual Exploration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alex Bäuerle, Patrick Albus, Raphael Störk, Tina Seufert, Timo Ropinski</td>
	<td></td>
	<td><p>Due to the success of deep learning (DL) and its growing job market, students
and researchers from many areas are interested in learning about DL
technologies. Visualization has proven to be of great help during this learning
process. While most current educational visualizations are targeted towards one
specific architecture or use case, recurrent neural networks (RNNs), which are
capable of processing sequential data, are not covered yet. This is despite the
fact that tasks on sequential data, such as text and function analysis, are at
the forefront of DL research. Therefore, we propose exploRNN, the first
interactively explorable educational visualization for RNNs. On the basis of
making learning easier and more fun, we define educational objectives targeted
towards understanding RNNs. We use these objectives to form guidelines for the
visual design process. By means of exploRNN, which is accessible online, we
provide an overview of the training process of RNNs at a coarse level, while
also allowing a detailed inspection of the data flow within LSTM cells. In an
empirical study, we assessed 37 subjects in a between-subjects design to
investigate the learning outcomes and cognitive load of exploRNN compared to a
classic text-based learning environment. While learners in the text group are
ahead in superficial knowledge acquisition, exploRNN is particularly helpful
for deeper understanding of the learning content. In addition, the complex
content in exploRNN is perceived as significantly easier and causes less
extraneous load than in the text group. The study shows that for difficult
learning material such as recurrent networks, where deep understanding is
important, interactive visualizations such as exploRNN can be helpful.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ras2020explainable/">Explainable Deep Learning: A Field Guide for the Uninitiated</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Deep Learning: A Field Guide for the Uninitiated' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Deep Learning: A Field Guide for the Uninitiated' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gabrielle Ras, Ning Xie, Marcel van Gerven, Derek Doran</td>
	<td></td>
	<td><p>Deep neural networks (DNNs) have become a proven and indispensable machine
learning tool. As a black-box model, it remains difficult to diagnose what
aspects of the model’s input drive the decisions of a DNN. In countless
real-world domains, from legislation and law enforcement to healthcare, such
diagnosis is essential to ensure that DNN decisions are driven by aspects
appropriate in the context of its use. The development of methods and studies
enabling the explanation of a DNN’s decisions has thus blossomed into an
active, broad area of research. A practitioner wanting to study explainable
deep learning may be intimidated by the plethora of orthogonal directions the
field has taken. This complexity is further exacerbated by competing
definitions of what it means <code class="language-plaintext highlighter-rouge">to explain'' the actions of a DNN and to
evaluate an approach's</code>ability to explain’’. This article offers a field
guide to explore the space of explainable deep learning aimed at those
uninitiated in the field. The field guide: i) Introduces three simple
dimensions defining the space of foundational methods that contribute to
explainable deep learning, ii) discusses the evaluations for model
explanations, iii) places explainability in the context of other related deep
learning research areas, and iv) finally elaborates on user-oriented
explanation designing and potential future directions on explainable deep
learning. We hope the guide is used as an easy-to-digest starting point for
those just embarking on research in this field.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2019</td>
	<td><a href="/publications/li2019deepgcns/">DeepGCNs: Can GCNs Go as Deep as CNNs?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepGCNs: Can GCNs Go as Deep as CNNs?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepGCNs: Can GCNs Go as Deep as CNNs?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Guohao Li, Matthias Müller, Ali Thabet, Bernard Ghanem</td>
	<td></td>
	<td><p>Convolutional Neural Networks (CNNs) achieve impressive performance in a wide
variety of fields. Their success benefited from a massive boost when very deep
CNN models were able to be reliably trained. Despite their merits, CNNs fail to
properly address problems with non-Euclidean data. To overcome this challenge,
Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean
data, borrow concepts from CNNs, and apply them in training. GCNs show
promising results, but they are usually limited to very shallow models due to
the vanishing gradient problem. As a result, most state-of-the-art GCN models
are no deeper than 3 or 4 layers. In this work, we present new ways to
successfully train very deep GCNs. We do this by borrowing concepts from CNNs,
specifically residual/dense connections and dilated convolutions, and adapting
them to GCN architectures. Extensive experiments show the positive effect of
these deep GCN frameworks. Finally, we use these new concepts to build a very
deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU
over state-of-the-art) in the task of point cloud semantic segmentation. We
believe that the community can greatly benefit from this work, as it opens up
many opportunities for advancing GCN-based research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019towards/">Towards a Unified Evaluation of Explanation Methods without Ground Truth</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards a Unified Evaluation of Explanation Methods without Ground Truth' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards a Unified Evaluation of Explanation Methods without Ground Truth' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hao Zhang, Jiayi Chen, Haotian Xue, Quanshi Zhang</td>
	<td></td>
	<td><p>This paper proposes a set of criteria to evaluate the objectiveness of
explanation methods of neural networks, which is crucial for the development of
explainable AI, but it also presents significant challenges. The core challenge
is that people usually cannot obtain ground-truth explanations of the neural
network. To this end, we design four metrics to evaluate explanation results
without ground-truth explanations. Our metrics can be broadly applied to nine
benchmark methods of interpreting neural networks, which provides new insights
of explanation methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/jain2019attention/">Attention is not Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention is not Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention is not Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarthak Jain, Byron C. Wallace</td>
	<td></td>
	<td><p>Attention mechanisms have seen wide adoption in neural NLP models. In
addition to improving predictive performance, these are often touted as
affording transparency: models equipped with attention provide a distribution
over attended-to input units, and this is often presented (at least implicitly)
as communicating the relative importance of inputs. However, it is unclear what
relationship exists between attention weights and model outputs. In this work,
we perform extensive experiments across a variety of NLP tasks that aim to
assess the degree to which attention weights provide meaningful `explanations’
for predictions. We find that they largely do not. For example, learned
attention weights are frequently uncorrelated with gradient-based measures of
feature importance, and one can identify very different attention distributions
that nonetheless yield equivalent predictions. Our findings show that standard
attention modules do not provide meaningful explanations and should not be
treated as though they do. Code for all experiments is available at
https://github.com/successar/AttentionExplanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/li2019interpretable/">Interpretable Neural Network Decoupling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Neural Network Decoupling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Neural Network Decoupling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang, Chenqian Yan, Yongjian Wu, Feiyue Huang, Ling Shao</td>
	<td></td>
	<td><p>The remarkable performance of convolutional neural networks (CNNs) is
entangled with their huge number of uninterpretable parameters, which has
become the bottleneck limiting the exploitation of their full potential.
Towards network interpretation, previous endeavors mainly resort to the single
filter analysis, which however ignores the relationship between filters. In
this paper, we propose a novel architecture decoupling method to interpret the
network from a perspective of investigating its calculation paths. More
specifically, we introduce a novel architecture controlling module in each
layer to encode the network architecture by a vector. By maximizing the mutual
information between the vectors and input images, the module is trained to
select specific filters to distill a unique calculation path for each input.
Furthermore, to improve the interpretability and compactness of the decoupled
network, the output of each layer is encoded to align the architecture encoding
vector with the constraint of sparsity regularization. Unlike conventional
pixel-level or filter-level network interpretation methods, we propose a
path-level analysis to explore the relationship between the combination of
filter and semantic concepts, which is more suitable to interpret the working
rationale of the decoupled network. Extensive experiments show that the
decoupled network achieves several applications, i.e., network interpretation,
network acceleration, and adversarial samples detection.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/mundhenk2019efficient/">Efficient Saliency Maps for Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Saliency Maps for Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Saliency Maps for Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>T. Nathan Mundhenk, Barry Y. Chen, Gerald Friedland</td>
	<td></td>
	<td><p>We describe an explainable AI saliency map method for use with deep
convolutional neural networks (CNN) that is much more efficient than popular
fine-resolution gradient methods. It is also quantitatively similar or better
in accuracy. Our technique works by measuring information at the end of each
network scale which is then combined into a single saliency map. We describe
how saliency measures can be made more efficient by exploiting Saliency Map
Order Equivalence. We visualize individual scale/layer contributions by using a
Layer Ordered Visualization of Information. This provides an interesting
comparison of scale information contributions within the network not provided
by other saliency map methods. Using our method instead of Guided Backprop,
coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem
to yield demonstrably superior results without sacrificing speed. This will
make fine-resolution saliency methods feasible on resource limited platforms
such as robots, cell phones, low-cost industrial devices, astronomy and
satellite imagery.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kang2019interpreting/">Interpreting Undesirable Pixels for Image Classification on Black-Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Undesirable Pixels for Image Classification on Black-Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Undesirable Pixels for Image Classification on Black-Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sin-Han Kang, Hong-Gyu Jung, Seong-Whan Lee</td>
	<td></td>
	<td><p>In an effort to interpret black-box models, researches for developing
explanation methods have proceeded in recent years. Most studies have tried to
identify input pixels that are crucial to the prediction of a classifier. While
this approach is meaningful to analyse the characteristic of blackbox models,
it is also important to investigate pixels that interfere with the prediction.
To tackle this issue, in this paper, we propose an explanation method that
visualizes undesirable regions to classify an image as a target class. To be
specific, we divide the concept of undesirable regions into two terms: (1)
factors for a target class, which hinder that black-box models identify
intrinsic characteristics of a target class and (2) factors for non-target
classes that are important regions for an image to be classified as other
classes. We visualize such undesirable regions on heatmaps to qualitatively
validate the proposed method. Furthermore, we present an evaluation metric to
provide quantitative results on ImageNet.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/warnecke2019evaluating/">Evaluating Explanation Methods for Deep Learning in Security</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Explanation Methods for Deep Learning in Security' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Explanation Methods for Deep Learning in Security' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alexander Warnecke, Daniel Arp, Christian Wressnegger, Konrad Rieck</td>
	<td></td>
	<td><p>Deep learning is increasingly used as a building block of security systems.
Unfortunately, neural networks are hard to interpret and typically opaque to
the practitioner. The machine learning community has started to address this
problem by developing methods for explaining the predictions of neural
networks. While several of these approaches have been successfully applied in
the area of computer vision, their application in security has received little
attention so far. It is an open question which explanation methods are
appropriate for computer security and what requirements they need to satisfy.
In this paper, we introduce criteria for comparing and evaluating explanation
methods in the context of computer security. These cover general properties,
such as the accuracy of explanations, as well as security-focused aspects, such
as the completeness, efficiency, and robustness. Based on our criteria, we
investigate six popular explanation methods and assess their utility in
security systems for malware detection and vulnerability discovery. We observe
significant differences between the methods and build on these to derive
general recommendations for selecting and applying explanation methods in
computer security.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/karimi2019model/">Model-Agnostic Counterfactual Explanations for Consequential Decisions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Model-Agnostic Counterfactual Explanations for Consequential Decisions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Model-Agnostic Counterfactual Explanations for Consequential Decisions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Amir-Hossein Karimi, Gilles Barthe, Borja Balle, Isabel Valera</td>
	<td></td>
	<td><p>Predictive models are being increasingly used to support consequential
decision making at the individual level in contexts such as pretrial bail and
loan approval. As a result, there is increasing social and legal pressure to
provide explanations that help the affected individuals not only to understand
why a prediction was output, but also how to act to obtain a desired outcome.
To this end, several works have proposed optimization-based methods to generate
nearest counterfactual explanations. However, these methods are often
restricted to a particular subset of models (e.g., decision trees or linear
models) and differentiable distance functions. In contrast, we build on
standard theory and tools from formal verification and propose a novel
algorithm that solves a sequence of satisfiability problems, where both the
distance function (objective) and predictive model (constraints) are
represented as logic formulae. As shown by our experiments on real-world data,
our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable,
{non-}convex); ii) data-type-agnostic (heterogeneous features); iii)
distance-agnostic ($\ell_0, \ell_1, \ell_\infty$, and combinations thereof);
iv) able to generate plausible and diverse counterfactuals for any sample
(i.e., 100% coverage); and v) at provably optimal distances.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/stylianou2019visualizing/">Visualizing Deep Similarity Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing Deep Similarity Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing Deep Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abby Stylianou, Richard Souvenir, Robert Pless</td>
	<td></td>
	<td><p>For convolutional neural network models that optimize an image embedding, we
propose a method to highlight the regions of images that contribute most to
pairwise similarity. This work is a corollary to the visualization tools
developed for classification networks, but applicable to the problem domains
better suited to similarity learning. The visualization shows how similarity
networks that are fine-tuned learn to focus on different features. We also
generalize our approach to embedding networks that use different pooling
strategies and provide a simple mechanism to support image similarity searches
on objects or sub-regions in the query image.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kim2019learning/">Learning Interpretable Models with Causal Guarantees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Interpretable Models with Causal Guarantees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Interpretable Models with Causal Guarantees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Carolyn Kim, Osbert Bastani</td>
	<td></td>
	<td><p>Machine learning has shown much promise in helping improve the quality of
medical, legal, and financial decision-making. In these applications, machine
learning models must satisfy two important criteria: (i) they must be causal,
since the goal is typically to predict individual treatment effects, and (ii)
they must be interpretable, so that human decision makers can validate and
trust the model predictions. There has recently been much progress along each
direction independently, yet the state-of-the-art approaches are fundamentally
incompatible. We propose a framework for learning interpretable models from
observational data that can be used to predict individual treatment effects
(ITEs). In particular, our framework converts any supervised learning algorithm
into an algorithm for estimating ITEs. Furthermore, we prove an error bound on
the treatment effects predicted by our model. Finally, in an experiment on
real-world data, we show that the models trained using our framework
significantly outperform a number of baselines.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/weller2019transparency/">Transparency: Motivations and Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transparency: Motivations and Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transparency: Motivations and Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adrian Weller</td>
	<td></td>
	<td><p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>
</td>
	<td>evaluation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gilpin2019explaining/">Explaining Explanations to Society</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Explanations to Society' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Explanations to Society' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leilani H. Gilpin, Cecilia Testart, Nathaniel Fruchter, Julius Adebayo</td>
	<td></td>
	<td><p>There is a disconnect between explanatory artificial intelligence (XAI)
methods and the types of explanations that are useful for and demanded by
society (policy makers, government officials, etc.) Questions that experts in
artificial intelligence (AI) ask opaque systems provide inside explanations,
focused on debugging, reliability, and validation. These are different from
those that society will ask of these systems to build trust and confidence in
their decisions. Although explanatory AI systems can answer many questions that
experts desire, they often don’t explain why they made decisions in a way that
is precise (true to the model) and understandable to humans. These outside
explanations can be used to build trust, comply with regulatory and policy
changes, and act as external validation. In this paper, we focus on XAI methods
for deep neural networks (DNNs) because of DNNs’ use in decision-making and
inherent opacity. We explore the types of questions that explanatory DNN
systems can answer and discuss challenges in building explanatory systems that
provide outside explanations for societal requirements and benefit.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/schmidt2019quantifying/">Quantifying Interpretability and Trust in Machine Learning Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Quantifying Interpretability and Trust in Machine Learning Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Quantifying Interpretability and Trust in Machine Learning Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Philipp Schmidt, Felix Biessmann</td>
	<td></td>
	<td><p>Decisions by Machine Learning (ML) models have become ubiquitous. Trusting
these decisions requires understanding how algorithms take them. Hence
interpretability methods for ML are an active focus of research. A central
problem in this context is that both the quality of interpretability methods as
well as trust in ML predictions are difficult to measure. Yet evaluations,
comparisons and improvements of trust and interpretability require quantifiable
measures. Here we propose a quantitative measure for the quality of
interpretability methods. Based on that we derive a quantitative measure of
trust in ML decisions. Building on previous work we propose to measure
intuitive understanding of algorithmic decisions using the information transfer
rate at which humans replicate ML model predictions. We provide empirical
evidence from crowdsourcing experiments that the proposed metric robustly
differentiates interpretability methods. The proposed metric also demonstrates
the value of interpretability for ML assisted human decision making: in our
experiments providing explanations more than doubled productivity in annotation
tasks. However unbiased human judgement is critical for doctors, judges, policy
makers and others. Here we derive a trust metric that identifies when human
decisions are overly biased towards ML predictions. Our results complement
existing qualitative work on trust and interpretability by quantifiable
measures that can serve as objectives for further improving methods in this
field of research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bansal2019case/">A Case for Backward Compatibility for Human-AI Teams</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Case for Backward Compatibility for Human-AI Teams' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Case for Backward Compatibility for Human-AI Teams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gagan Bansal, Besmira Nushi, Ece Kamar, Dan Weld, Walter Lasecki, Eric Horvitz</td>
	<td></td>
	<td><p>AI systems are being deployed to support human decision making in high-stakes
domains. In many cases, the human and AI form a team, in which the human makes
decisions after reviewing the AI’s inferences. A successful partnership
requires that the human develops insights into the performance of the AI
system, including its failures. We study the influence of updates to an AI
system in this setting. While updates can increase the AI’s predictive
performance, they may also lead to changes that are at odds with the user’s
prior experiences and confidence in the AI’s inferences, hurting therefore the
overall team performance. We introduce the notion of the compatibility of an AI
update with prior user experience and present methods for studying the role of
compatibility in human-AI teams. Empirical results on three high-stakes domains
show that current machine learning algorithms do not produce compatible
updates. We propose a re-training objective to improve the compatibility of an
update by penalizing new errors. The objective offers full leverage of the
performance/compatibility tradeoff, enabling more compatible yet accurate
updates.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wiegreffe2019attention/">Attention is not not Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention is not not Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention is not not Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarah Wiegreffe, Yuval Pinter</td>
	<td></td>
	<td><p>Attention mechanisms play a central role in NLP systems, especially within
recurrent neural network (RNN) models. Recently, there has been increasing
interest in whether or not the intermediate representations offered by these
modules may be used to explain the reasoning for a model’s prediction, and
consequently reach insights regarding the model’s decision-making process. A
recent paper claims that `Attention is not Explanation’ (Jain and Wallace,
2019). We challenge many of the assumptions underlying this work, arguing that
such a claim depends on one’s definition of explanation, and that testing it
needs to take into account all elements of the model, using a rigorous
experimental design. We propose four alternative tests to determine
when/whether attention can be used as explanation: a simple uniform-weights
baseline; a variance calibration based on multiple random seed runs; a
diagnostic framework using frozen weights from pretrained models; and an
end-to-end adversarial attention training protocol. Each allows for meaningful
interpretation of attention mechanisms in RNN models. We show that even when
reliable adversarial distributions can be found, they don’t perform well on the
simple diagnostic, indicating that prior work does not disprove the usefulness
of attention mechanisms for explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/marx2019disentangling/">Disentangling Influence: Using Disentangled Representations to Audit Model Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Disentangling Influence: Using Disentangled Representations to Audit Model Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Disentangling Influence: Using Disentangled Representations to Audit Model Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Charles T. Marx, Richard Lanas Phillips, Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian</td>
	<td></td>
	<td><p>Motivated by the need to audit complex and black box models, there has been
extensive research on quantifying how data features influence model
predictions. Feature influence can be direct (a direct influence on model
outcomes) and indirect (model outcomes are influenced via proxy features).
Feature influence can also be expressed in aggregate over the training or test
data or locally with respect to a single point. Current research has typically
focused on one of each of these dimensions. In this paper, we develop
disentangled influence audits, a procedure to audit the indirect influence of
features. Specifically, we show that disentangled representations provide a
mechanism to identify proxy features in the dataset, while allowing an explicit
computation of feature influence on either individual outcomes or
aggregate-level outcomes. We show through both theory and experiments that
disentangled influence audits can both detect proxy features and show, for each
individual or in aggregate, which of these proxy features affects the
classifier being audited the most. In this respect, our method is more powerful
than existing methods for ascertaining feature influence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhao2019confounder-aware/">Confounder-Aware Visualization of ConvNets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Confounder-Aware Visualization of ConvNets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Confounder-Aware Visualization of ConvNets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Qingyu Zhao, Ehsan Adeli, Adolf Pfefferbaum, Edith V. Sullivan, Kilian M. Pohl</td>
	<td></td>
	<td><p>With recent advances in deep learning, neuroimaging studies increasingly rely
on convolutional networks (ConvNets) to predict diagnosis based on MR images.
To gain a better understanding of how a disease impacts the brain, the studies
visualize the salience maps of the ConvNet highlighting voxels within the brain
majorly contributing to the prediction. However, these salience maps are
generally confounded, i.e., some salient regions are more predictive of
confounding variables (such as age) than the diagnosis. To avoid such
misinterpretation, we propose in this paper an approach that aims to visualize
confounder-free saliency maps that only highlight voxels predictive of the
diagnosis. The approach incorporates univariate statistical tests to identify
confounding effects within the intermediate features learned by ConvNet. The
influence from the subset of confounded features is then removed by a novel
partial back-propagation procedure. We use this two-step approach to visualize
confounder-free saliency maps extracted from synthetic and two real datasets.
These experiments reveal the potential of our visualization in producing
unbiased model-interpretation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yousefzadeh2019interpreting/">Interpreting Neural Networks Using Flip Points</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Neural Networks Using Flip Points' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Neural Networks Using Flip Points' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Roozbeh Yousefzadeh, Dianne P. O'Leary</td>
	<td></td>
	<td><p>Neural networks have been criticized for their lack of easy interpretation,
which undermines confidence in their use for important applications. Here, we
introduce a novel technique, interpreting a trained neural network by
investigating its flip points. A flip point is any point that lies on the
boundary between two output classes: e.g. for a neural network with a binary
yes/no output, a flip point is any input that generates equal scores for “yes”
and “no”. The flip point closest to a given input is of particular importance,
and this point is the solution to a well-posed optimization problem. This paper
gives an overview of the uses of flip points and how they are computed. Through
results on standard datasets, we demonstrate how flip points can be used to
provide detailed interpretation of the output produced by a neural network.
Moreover, for a given input, flip points enable us to measure confidence in the
correctness of outputs much more effectively than softmax score. They also
identify influential features of the inputs, identify bias, and find changes in
the input that change the output of the model. We show that distance between an
input and the closest flip point identifies the most influential points in the
training data. Using principal component analysis (PCA) and rank-revealing QR
factorization (RR-QR), the set of directions from each training input to its
closest flip point provides explanations of how a trained neural network
processes an entire dataset: what features are most important for
classification into a given class, which features are most responsible for
particular misclassifications, how an adversary might fool the network, etc.
Although we investigate flip points for neural networks, their usefulness is
actually model-agnostic.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lage2019human/">Human Evaluation of Models Built for Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human Evaluation of Models Built for Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human Evaluation of Models Built for Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, Finale Doshi-Velez</td>
	<td>AAAI Conference on Human Computation and Crowdsourcing</td>
	<td><p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>
</td>
	<td>evaluation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lai2019many/">Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vivian Lai, Jon Z. Cai, Chenhao Tan</td>
	<td></td>
	<td><p>Feature importance is commonly used to explain machine predictions. While
feature importance can be derived from a machine learning model with a variety
of methods, the consistency of feature importance via different methods remains
understudied. In this work, we systematically compare feature importance from
built-in mechanisms in a model such as attention values and post-hoc methods
that approximate model behavior such as LIME. Using text classification as a
testbed, we find that 1) no matter which method we use, important features from
traditional models such as SVM and XGBoost are more similar with each other,
than with deep learning models; 2) post-hoc methods tend to generate more
similar important features for two models than built-in methods. We further
demonstrate how such similarity varies across instances. Notably, important
features do not always resemble each other better when two models agree on the
predicted label than when they disagree.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/shankaranarayana2019alime/">ALIME: Autoencoder Based Approach for Local Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=ALIME: Autoencoder Based Approach for Local Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=ALIME: Autoencoder Based Approach for Local Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sharath M. Shankaranarayana, Davor Runje</td>
	<td></td>
	<td><p>Machine learning and especially deep learning have garneredtremendous
popularity in recent years due to their increased performanceover other
methods. The availability of large amount of data has aidedin the progress of
deep learning. Nevertheless, deep learning models areopaque and often seen as
black boxes. Thus, there is an inherent need tomake the models interpretable,
especially so in the medical domain. Inthis work, we propose a locally
interpretable method, which is inspiredby one of the recent tools that has
gained a lot of interest, called localinterpretable model-agnostic explanations
(LIME). LIME generates singleinstance level explanation by artificially
generating a dataset aroundthe instance (by randomly sampling and using
perturbations) and thentraining a local linear interpretable model. One of the
major issues inLIME is the instability in the generated explanation, which is
caused dueto the randomly generated dataset. Another issue in these kind of
localinterpretable models is the local fidelity. We propose novel
modificationsto LIME by employing an autoencoder, which serves as a better
weightingfunction for the local model. We perform extensive comparisons
withdifferent datasets and show that our proposed method results in
bothimproved stability, as well as local fidelity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lakkaraju2019how/">"How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Himabindu Lakkaraju, Osbert Bastani</td>
	<td></td>
	<td><p>As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a human
interpretable manner. It has recently become apparent that a high-fidelity
explanation of a black box ML model may not accurately reflect the biases in
the black box. As a consequence, explanations have the potential to mislead
human users into trusting a problematic black box. In this work, we rigorously
explore the notion of misleading explanations and how they influence user trust
in black-box models. More specifically, we propose a novel theoretical
framework for understanding and generating misleading explanations, and carry
out a user study with domain experts to demonstrate how these explanations can
be used to mislead users. Our work is the first to empirically establish how
user trust in black box models can be manipulated via misleading explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/samek2019towards/">Towards Explainable Artificial Intelligence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explainable Artificial Intelligence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explainable Artificial Intelligence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wojciech Samek, Klaus-Robert Müller</td>
	<td></td>
	<td><p>In recent years, machine learning (ML) has become a key enabling technology
for the sciences and industry. Especially through improvements in methodology,
the availability of large databases and increased computational power, today’s
ML algorithms are able to achieve excellent performance (at times even
exceeding the human level) on an increasing number of complex tasks. Deep
learning models are at the forefront of this development. However, due to their
nested non-linear structure, these powerful models have been generally
considered “black boxes”, not providing any information about what exactly
makes them arrive at their predictions. Since in many applications, e.g., in
the medical domain, such lack of transparency may be not acceptable, the
development of methods for visualizing, explaining and interpreting deep
learning models has recently attracted increasing attention. This introductory
paper presents recent developments and applications in this field and makes a
plea for a wider use of explainable learning algorithms in practice.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yu2019interpreting/">Interpreting and Evaluating Neural Network Robustness</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting and Evaluating Neural Network Robustness' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting and Evaluating Neural Network Robustness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen</td>
	<td></td>
	<td><p>Recently, adversarial deception becomes one of the most considerable threats
to deep neural networks. However, compared to extensive research in new designs
of various adversarial attacks and defenses, the neural networks’ intrinsic
robustness property is still lack of thorough investigation. This work aims to
qualitatively interpret the adversarial attack and defense mechanism through
loss visualization, and establish a quantitative metric to evaluate the neural
network model’s intrinsic robustness. The proposed robustness metric identifies
the upper bound of a model’s prediction divergence in the given domain and thus
indicates whether the model can maintain a stable prediction. With extensive
experiments, our metric demonstrates several advantages over conventional
adversarial testing accuracy based robustness estimation: (1) it provides a
uniformed evaluation to models with different structures and parameter scales;
(2) it over-performs conventional accuracy based robustness estimation and
provides a more reliable evaluation that is invariant to different test
settings; (3) it can be fast generated without considerable testing cost.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ray2019explain/">Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Arijit Ray, Yi Yao, Rakesh Kumar, Ajay Divakaran, Giedrius Burachas</td>
	<td>2019 AAAI Conference on Human Computation and Crowdsourcing</td>
	<td><p>While there have been many proposals on making AI algorithms explainable, few
have attempted to evaluate the impact of AI-generated explanations on human
performance in conducting human-AI collaborative tasks. To bridge the gap, we
propose a Twenty-Questions style collaborative image retrieval game,
Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy
of explanations (visual evidence or textual justification) in the context of
Visual Question Answering (VQA). In our proposed ExAG, a human user needs to
guess a secret image picked by the VQA agent by asking natural language
questions to it. We show that overall, when AI explains its answers, users
succeed more often in guessing the secret image correctly. Notably, a few
correct explanations can readily improve human performance when VQA answers are
mostly incorrect as compared to no-explanation games. Furthermore, we also show
that while explanations rated as “helpful” significantly improve human
performance, “incorrect” and “unhelpful” explanations can degrade performance
as compared to no-explanation games. Our experiments, therefore, demonstrate
that ExAG is an effective means to evaluate the efficacy of AI-generated
explanations on a human-AI collaborative task.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yang2019benchmarking/">Benchmarking Attribution Methods with Relative Feature Importance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking Attribution Methods with Relative Feature Importance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Benchmarking Attribution Methods with Relative Feature Importance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mengjiao Yang, Been Kim</td>
	<td></td>
	<td><p>Interpretability is an important area of research for safe deployment of
machine learning systems. One particular type of interpretability method
attributes model decisions to input features. Despite active development,
quantitative evaluation of feature attribution methods remains difficult due to
the lack of ground truth: we do not know which input features are in fact
important to a model. In this work, we propose a framework for Benchmarking
Attribution Methods (BAM) with a priori knowledge of relative feature
importance. BAM includes 1) a carefully crafted dataset and models trained with
known relative feature importance and 2) three complementary metrics to
quantitatively evaluate attribution methods by comparing feature attributions
between pairs of models and pairs of inputs. Our evaluation on several
widely-used attribution methods suggests that certain methods are more likely
to produce false positive explanations—features that are incorrectly
attributed as more important to model prediction. We open source our dataset,
models, and metrics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/patro2019explanation/">Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Badri N. Patro, Anupriy, Vinay P. Namboodiri</td>
	<td></td>
	<td><p>In this paper, we aim to obtain improved attention for a visual question
answering (VQA) task. It is challenging to provide supervision for attention.
An observation we make is that visual explanations as obtained through class
activation mappings (specifically Grad-CAM) that are meant to explain the
performance of various networks could form a means of supervision. However, as
the distributions of attention maps and that of Grad-CAMs differ, it would not
be suitable to directly use these as a form of supervision. Rather, we propose
the use of a discriminator that aims to distinguish samples of visual
explanation and attention maps. The use of adversarial training of the
attention regions as a two-player game between attention and explanation serves
to bring the distributions of attention maps and visual explanations closer.
Significantly, we observe that providing such a means of supervision also
results in attention maps that are more closely related to human attention
resulting in a substantial improvement over baseline stacked attention network
(SAN) models. It also results in a good improvement in rank correlation metric
on the VQA task. This method can also be combined with recent MCB based methods
and results in consistent improvement. We also provide comparisons with other
means for learning distributions such as based on Correlation Alignment
(Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and
observe that the adversarial loss outperforms the other forms of learning the
attention maps. Visualization of the results also confirms our hypothesis that
attention maps improve using this form of supervision.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lertvittayakumjorn2019human-grounded/">Human-grounded Evaluations of Explanation Methods for Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human-grounded Evaluations of Explanation Methods for Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human-grounded Evaluations of Explanation Methods for Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Piyawat Lertvittayakumjorn, Francesca Toni</td>
	<td></td>
	<td><p>Due to the black-box nature of deep learning models, methods for explaining
the models’ results are crucial to gain trust from humans and support
collaboration between AIs and humans. In this paper, we consider several
model-agnostic and model-specific explanation methods for CNNs for text
classification and conduct three human-grounded evaluations, focusing on
different purposes of explanations: (1) revealing model behavior, (2)
justifying model predictions, and (3) helping humans investigate uncertain
predictions. The results highlight dissimilar qualities of the various
explanation methods we consider and show the degree to which these methods
could serve for each purpose.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019towards/">Towards Visually Explaining Variational Autoencoders</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Visually Explaining Variational Autoencoders' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Visually Explaining Variational Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J. Radke, Octavia Camps</td>
	<td></td>
	<td><p>Recent advances in Convolutional Neural Network (CNN) model interpretability
have led to impressive progress in visualizing and understanding model
predictions. In particular, gradient-based visual attention methods have driven
much recent effort in using visual attention maps as a means for visual
explanations. A key problem, however, is these methods are designed for
classification and categorization tasks, and their extension to explaining
generative models, e.g. variational autoencoders (VAE) is not trivial. In this
work, we take a step towards bridging this crucial gap, proposing the first
technique to visually explain VAEs by means of gradient-based attention. We
present methods to generate visual attention from the learned latent space, and
also demonstrate such attention explanations serve more than just explaining
VAE predictions. We show how these attention maps can be used to localize
anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD
dataset. We also show how they can be infused into model training, helping
bootstrap the VAE into learning improved latent space disentanglement,
demonstrated on the Dsprites dataset.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/he2019understanding/">Understanding and Visualizing Deep Visual Saliency Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding and Visualizing Deep Visual Saliency Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding and Visualizing Deep Visual Saliency Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sen He, Hamed R. Tavakoli, Ali Borji, Yang Mi, Nicolas Pugeault</td>
	<td></td>
	<td><p>Recently, data-driven deep saliency models have achieved high performance and
have outperformed classical saliency models, as demonstrated by results on
datasets such as the MIT300 and SALICON. Yet, there remains a large gap between
the performance of these models and the inter-human baseline. Some outstanding
questions include what have these models learned, how and where they fail, and
how they can be improved. This article attempts to answer these questions by
analyzing the representations learned by individual neurons located at the
intermediate layers of deep saliency models. To this end, we follow the steps
of existing deep saliency models, that is borrowing a pre-trained model of
object recognition to encode the visual features and learning a decoder to
infer the saliency. We consider two cases when the encoder is used as a fixed
feature extractor and when it is fine-tuned, and compare the inner
representations of the network. To study how the learned representations depend
on the task, we fine-tune the same network using the same image set but for two
different tasks: saliency prediction versus scene classification. Our analyses
reveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are
already encoded within various layers of the network pre-trained for object
recognition, 2) using modern datasets, we find that fine-tuning pre-trained
models for saliency prediction makes them favor some categories (e.g. head)
over some others (e.g. text), 3) although deep models of saliency outperform
classical models on natural images, the converse is true for synthetic stimuli
(e.g. pop-out search arrays), an evidence of significant difference between
human and data-driven saliency models, and 4) we confirm that, after-fine
tuning, the change in inner-representations is mostly due to the task and not
the domain shift in the data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bodell2019interpretable/">Interpretable Word Embeddings via Informative Priors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Word Embeddings via Informative Priors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Word Embeddings via Informative Priors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Miriam Hurtado Bodell, Martin Arvidsson, Måns Magnusson</td>
	<td></td>
	<td><p>Word embeddings have demonstrated strong performance on NLP tasks. However,
lack of interpretability and the unsupervised nature of word embeddings have
limited their use within computational social science and digital humanities.
We propose the use of informative priors to create interpretable and
domain-informed dimensions for probabilistic word embeddings. Experimental
results show that sensible priors can capture latent semantic concepts better
than or on-par with the current state of the art, while retaining the
simplicity and generalizability of using priors.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/vashishth2019attention/">Attention Interpretability Across NLP Tasks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention Interpretability Across NLP Tasks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention Interpretability Across NLP Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, Manaal Faruqui</td>
	<td></td>
	<td><p>The attention layer in a neural network model provides insights into the
model’s reasoning behind its prediction, which are usually criticized for being
opaque. Recently, seemingly contradictory viewpoints have emerged about the
interpretability of attention weights (Jain &amp; Wallace, 2019; Vig &amp; Belinkov,
2019). Amid such confusion arises the need to understand attention mechanism
more systematically. In this work, we attempt to fill this gap by giving a
comprehensive explanation which justifies both kinds of observations (i.e.,
when is attention interpretable and when it is not). Through a series of
experiments on diverse NLP tasks, we validate our observations and reinforce
our claim of interpretability of attention through manual evaluation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/arras2019evaluating/">Evaluating Recurrent Neural Network Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Recurrent Neural Network Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Recurrent Neural Network Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leila Arras, Ahmed Osman, Klaus-Robert Müller, Wojciech Samek</td>
	<td></td>
	<td><p>Recently, several methods have been proposed to explain the predictions of
recurrent neural networks (RNNs), in particular of LSTMs. The goal of these
methods is to understand the network’s decisions by assigning to each input
variable, e.g., a word, a relevance indicating to which extent it contributed
to a particular prediction. In previous works, some of these methods were not
yet compared to one another, or were evaluated only qualitatively. We close
this gap by systematically and quantitatively comparing these methods in
different settings, namely (1) a toy arithmetic task which we use as a sanity
check, (2) a five-class sentiment prediction of movie reviews, and besides (3)
we explore the usefulness of word relevances to build sentence-level
representations. Lastly, using the method that performed best in our
experiments, we show how specific linguistic phenomena such as the negation in
sentiment analysis reflect in terms of relevance patterns, and how the
relevance visualization can help to understand the misclassification of
individual samples.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gupta2019simple/">A Simple Saliency Method That Passes the Sanity Checks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Simple Saliency Method That Passes the Sanity Checks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Simple Saliency Method That Passes the Sanity Checks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Arushi Gupta, Sanjeev Arora</td>
	<td></td>
	<td><p>There is great interest in “saliency methods” (also called “attribution
methods”), which give “explanations” for a deep net’s decision, by assigning a
“score” to each feature/pixel in the input. Their design usually involves
credit-assignment via the gradient of the output with respect to input.
Recently Adebayo et al. [arXiv:1810.03292] questioned the validity of many of
these methods since they do not pass simple <em>sanity checks</em> which test whether
the scores shift/vanish when layers of the trained net are randomized, or when
the net is retrained using random labels for inputs.
  We propose a simple fix to existing saliency methods that helps them pass
sanity checks, which we call “competition for pixels”. This involves computing
saliency maps for all possible labels in the classification task, and using a
simple competition among them to identify and remove less relevant pixels from
the map. The simplest variant of this is “Competitive Gradient $\odot$ Input
(CGI)”: it is efficient, requires no additional training, and uses only the
input and gradient. Some theoretical justification is provided for it
(especially for ReLU networks) and its performance is empirically demonstrated.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/huang2019understanding/">Understanding Generalization through Visualizations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Generalization through Visualizations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Generalization through Visualizations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>W. Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, J. K. Terry, Furong Huang, Tom Goldstein</td>
	<td></td>
	<td><p>The power of neural networks lies in their ability to generalize to unseen
data, yet the underlying reasons for this phenomenon remain elusive. Numerous
rigorous attempts have been made to explain generalization, but available
bounds are still quite loose, and analysis does not always lead to true
understanding. The goal of this work is to make generalization more intuitive.
Using visualization methods, we discuss the mystery of generalization, the
geometry of loss landscapes, and how the curse (or, rather, the blessing) of
dimensionality causes optimizers to settle into minima that generalize well.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chyung2019extracting/">Extracting Interpretable Concept-Based Decision Trees from CNNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Extracting Interpretable Concept-Based Decision Trees from CNNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Extracting Interpretable Concept-Based Decision Trees from CNNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Conner Chyung, Michael Tsang, Yan Liu</td>
	<td></td>
	<td><p>In an attempt to gather a deeper understanding of how convolutional neural
networks (CNNs) reason about human-understandable concepts, we present a method
to infer labeled concept data from hidden layer activations and interpret the
concepts through a shallow decision tree. The decision tree can provide
information about which concepts a model deems important, as well as provide an
understanding of how the concepts interact with each other. Experiments
demonstrate that the extracted decision tree is capable of accurately
representing the original CNN’s classifications at low tree depths, thus
encouraging human-in-the-loop understanding of discriminative concepts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tomsett2019sanity/">Sanity Checks for Saliency Metrics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Checks for Saliency Metrics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Checks for Saliency Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, Alun Preece</td>
	<td></td>
	<td><p>Saliency maps are a popular approach to creating post-hoc explanations of
image classifier outputs. These methods produce estimates of the relevance of
each pixel to the classification output score, which can be displayed as a
saliency map that highlights important pixels. Despite a proliferation of such
methods, little effort has been made to quantify how good these saliency maps
are at capturing the true relevance of the pixels to the classifier output
(i.e. their “fidelity”). We therefore investigate existing metrics for
evaluating the fidelity of saliency methods (i.e. saliency metrics). We find
that there is little consistency in the literature in how such metrics are
calculated, and show that such inconsistencies can have a significant effect on
the measured fidelity. Further, we apply measures of reliability developed in
the psychometric testing literature to assess the consistency of saliency
metrics when applied to individual saliency maps. Our results show that
saliency metrics can be statistically unreliable and inconsistent, indicating
that comparative rankings between saliency methods generated using such metrics
can be untrustworthy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bhatt2019explainable/">Explainable Machine Learning in Deployment</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Machine Learning in Deployment' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Machine Learning in Deployment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, Peter Eckersley</td>
	<td></td>
	<td><p>Explainable machine learning offers the potential to provide stakeholders
with insights into model behavior by using various methods such as feature
importance scores, counterfactual explanations, or influential training data.
Yet there is little understanding of how organizations use these methods in
practice. This study explores how organizations view and use explainability for
stakeholder consumption. We find that, currently, the majority of deployments
are not for end users affected by the model but rather for machine learning
engineers, who use explainability to debug the model itself. There is thus a
gap between explainability in practice and the goal of transparency, since
explanations primarily serve internal stakeholders rather than external ones.
Our study synthesizes the limitations of current explainability techniques that
hamper their use for end users. To facilitate end user interaction, we develop
a framework for establishing clear goals for explainability. We end by
discussing concerns raised regarding explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gosiewska2019trust/">Do Not Trust Additive Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Do Not Trust Additive Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Do Not Trust Additive Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alicja Gosiewska, Przemyslaw Biecek</td>
	<td></td>
	<td><p>Explainable Artificial Intelligence (XAI)has received a great deal of
attention recently. Explainability is being presented as a remedy for the
distrust of complex and opaque models. Model agnostic methods such as LIME,
SHAP, or Break Down promise instance-level interpretability for any complex
machine learning model. But how faithful are these additive explanations? Can
we rely on additive explanations for non-additive models?
  In this paper, we (1) examine the behavior of the most popular instance-level
explanations under the presence of interactions, (2) introduce a new method
that detects interactions for instance-level explanations, (3) perform a large
scale benchmark to see how frequently additive explanations may be misleading.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2018</td>
	<td><a href="/publications/yadav2018sanity/">Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vikas Yadav, Rebecca Sharp, Mihai Surdeanu</td>
	<td></td>
	<td><p>While increasingly complex approaches to question answering (QA) have been
proposed, the true gain of these systems, particularly with respect to their
expensive training requirements, can be inflated when they are not compared to
adequate baselines. Here we propose an unsupervised, simple, and fast alignment
and information retrieval baseline that incorporates two novel contributions: a
\textit{one-to-many alignment} between query and document terms and
\textit{negative alignment} as a proxy for discriminative information. Our
approach not only outperforms all conventional baselines as well as many
supervised recurrent neural networks, but also approaches the state of the art
for supervised systems on three QA datasets. With only three hyperparameters,
we achieve 47\% P@1 on an 8th grade Science QA dataset, 32.9\% P@1 on a Yahoo!
answers QA dataset and 64\% MAP on WikiQA. We also achieve 26.56\% and 58.36\%
on ARC challenge and easy dataset respectively. In addition to including the
additional ARC results in this version of the paper, for the ARC easy set only
we also experimented with one additional parameter – number of justifications
retrieved.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/miller2018contrastive/">Contrastive Explanation: A Structural-Model Approach</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Explanation: A Structural-Model Approach' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contrastive Explanation: A Structural-Model Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tim Miller</td>
	<td></td>
	<td><p>This paper presents a model of contrastive explanation using structural
casual models. The topic of causal explanation in artificial intelligence has
gathered interest in recent years as researchers and practitioners aim to
increase trust and understanding of intelligent decision-making. While
different sub-fields of artificial intelligence have looked into this problem
with a sub-field-specific view, there are few models that aim to capture
explanation more generally. One general model is based on structural causal
models. It defines an explanation as a fact that, if found to be true, would
constitute an actual cause of a specific event. However, research in philosophy
and social sciences shows that explanations are contrastive: that is, when
people ask for an explanation of an event – the fact – they (sometimes
implicitly) are asking for an explanation relative to some contrast case; that
is, “Why P rather than Q?”. In this paper, we extend the structural causal
model approach to define two complementary notions of contrastive explanation,
and demonstrate them on two classical problems in artificial intelligence:
classification and planning. We believe that this model can help researchers in
subfields of artificial intelligence to better understand contrastive
explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/mittelstadt2018explaining/">Explaining Explanations in AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Explanations in AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Explanations in AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Brent Mittelstadt, Chris Russell, Sandra Wachter</td>
	<td></td>
	<td><p>Recent work on interpretability in machine learning and AI has focused on the
building of simplified models that approximate the true criteria used to make
decisions. These models are a useful pedagogical device for teaching trained
professionals how to predict what decisions will be made by the complex system,
and most importantly how the system might break. However, when considering any
such model it’s important to remember Box’s maxim that “All models are wrong
but some are useful.” We focus on the distinction between these models and
explanations in philosophy and sociology. These models can be understood as a
“do it yourself kit” for explanations, allowing a practitioner to directly
answer “what if questions” or generate contrastive explanations without
external assistance. Although a valuable ability, giving these models as
explanations appears more difficult than necessary, and other forms of
explanation may not have the same trade-offs. We contrast the different schools
of thought on what makes an explanation, and suggest that machine learning
might benefit from viewing the problem more broadly.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/mohseni2018human-grounded/">A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sina Mohseni, Jeremy E. Block, Eric D. Ragan</td>
	<td></td>
	<td><p>Research in interpretable machine learning proposes different computational
and human subject approaches to evaluate model saliency explanations. These
approaches measure different qualities of explanations to achieve diverse goals
in designing interpretable machine learning systems. In this paper, we propose
a human attention benchmark for image and text domains using multi-layer human
attention masks aggregated from multiple human annotators. We then present an
evaluation study to evaluate model saliency explanations obtained using
Grad-cam and LIME techniques. We demonstrate our benchmark’s utility for
quantitative evaluation of model explanations by comparing it with human
subjective ratings and ground-truth single-layer segmentation masks
evaluations. Our study results show that our threshold agnostic evaluation
method with the human attention baseline is more effective than single-layer
object segmentation masks to ground truth. Our experiments also reveal user
biases in the subjective rating of model saliency explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/seo2018regional/">Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dasom Seo, Kanghan Oh, Il-Seok Oh</td>
	<td></td>
	<td><p>Recently, many methods to interpret and visualize deep neural network
predictions have been proposed and significant progress has been made. However,
a more class-discriminative and visually pleasing explanation is required.
Thus, this paper proposes a region-based approach that estimates feature
importance in terms of appropriately segmented regions. By fusing the saliency
maps generated from multi-scale segmentations, a more class-discriminative and
visually pleasing map is obtained. We incorporate this regional multi-scale
concept into a prediction difference method that is model-agnostic. An input
image is segmented in several scales using the super-pixel method, and
exclusion of a region is simulated by sampling a normal distribution
constructed using the boundary prior. The experimental results demonstrate that
the regional multi-scale method produces much more class-discriminative and
visually pleasing saliency maps.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018interpreting/">Interpreting CNNs via Decision Trees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting CNNs via Decision Trees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting CNNs via Decision Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu</td>
	<td></td>
	<td><p>This paper aims to quantitatively explain rationales of each prediction that
is made by a pre-trained convolutional neural network (CNN). We propose to
learn a decision tree, which clarifies the specific reason for each prediction
made by the CNN at the semantic level. I.e., the decision tree decomposes
feature representations in high conv-layers of the CNN into elementary concepts
of object parts. In this way, the decision tree tells people which object parts
activate which filters for the prediction and how much they contribute to the
prediction score. Such semantic and quantitative explanations for CNN
predictions have specific values beyond the traditional pixel-level analysis of
CNNs. More specifically, our method mines all potential decision modes of the
CNN, where each mode represents a common case of how the CNN uses object parts
for prediction. The decision tree organizes all potential decision modes in a
coarse-to-fine manner to explain CNN predictions at different fine-grained
levels. Experiments have demonstrated the effectiveness of the proposed method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018interpretable/">Interpretable Deep Learning under Fire</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Deep Learning under Fire' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Deep Learning under Fire' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, Ting Wang</td>
	<td></td>
	<td><p>Providing explanations for deep neural network (DNN) models is crucial for
their use in security-sensitive domains. A plethora of interpretation models
have been proposed to help users understand the inner workings of DNNs: how
does a DNN arrive at a specific decision for a given input? The improved
interpretability is believed to offer a sense of security by involving human in
the decision-making process. Yet, due to its data-driven nature, the
interpretability itself is potentially susceptible to malicious manipulations,
about which little is known thus far.
  Here we bridge this gap by conducting the first systematic study on the
security of interpretable deep learning systems (IDLSes). We show that existing
\imlses are highly vulnerable to adversarial manipulations. Specifically, we
present ADV^2, a new class of attacks that generate adversarial inputs not only
misleading target DNNs but also deceiving their coupled interpretation models.
Through empirical evaluation against four major types of IDLSes on benchmark
datasets and in security-critical applications (e.g., skin cancer diagnosis),
we demonstrate that with ADV^2 the adversary is able to arbitrarily designate
an input’s prediction and interpretation. Further, with both analytical and
empirical evidence, we identify the prediction-interpretation gap as one root
cause of this vulnerability – a DNN and its interpretation model are often
misaligned, resulting in the possibility of exploiting both models
simultaneously. Finally, we explore potential countermeasures against ADV^2,
including leveraging its low transferability and incorporating it in an
adversarial training framework. Our findings shed light on designing and
operating IDLSes in a more secure and informative fashion, leading to several
promising research directions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018explanatory/">Explanatory Graphs for CNNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Graphs for CNNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Graphs for CNNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Xin Wang, Ruiming Cao, Ying Nian Wu, Feng Shi, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper introduces a graphical model, namely an explanatory graph, which
reveals the knowledge hierarchy hidden inside conv-layers of a pre-trained CNN.
Each filter in a conv-layer of a CNN for object classification usually
represents a mixture of object parts. We develop a simple yet effective method
to disentangle object-part pattern components from each filter. We construct an
explanatory graph to organize the mined part patterns, where a node represents
a part pattern, and each edge encodes co-activation relationships and spatial
relationships between patterns. More crucially, given a pre-trained CNN, the
explanatory graph is learned without a need of annotating object parts.
Experiments show that each graph node consistently represented the same object
part through different images, which boosted the transferability of CNN
features. We transferred part patterns in the explanatory graph to the task of
part localization, and our method significantly outperformed other approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/nushi2018towards/">Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Besmira Nushi, Ece Kamar, Eric Horvitz</td>
	<td>AAAI Conference on Human Computation and Crowdsourcing 2018</td>
	<td><p>As machine learning systems move from computer-science laboratories into the
open world, their accountability becomes a high priority problem.
Accountability requires deep understanding of system behavior and its failures.
Current evaluation methods such as single-score error metrics and confusion
matrices provide aggregate views of system performance that hide important
shortcomings. Understanding details about failures is important for identifying
pathways for refinement, communicating the reliability of systems in different
settings, and for specifying appropriate human oversight and engagement.
Characterization of failures and shortcomings is particularly complex for
systems composed of multiple machine learned components. For such systems,
existing evaluation methods have limited expressiveness in describing and
explaining the relationship among input content, the internal states of system
components, and final output quality. We present Pandora, a set of hybrid
human-machine methods and tools for describing and explaining system failures.
Pandora leverages both human and system-generated observations to summarize
conditions of system malfunction with respect to the input content and system
architecture. We share results of a case study with a machine learning pipeline
for image captioning that show how detailed performance views can be beneficial
for analysis and debugging.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/adebayo2018sanity/">Sanity Checks for Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Checks for Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Checks for Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim</td>
	<td></td>
	<td><p>Saliency methods have emerged as a popular tool to highlight features in an
input deemed relevant for the prediction of a learned model. Several saliency
methods have been proposed, often guided by visual appeal on image data. In
this work, we propose an actionable methodology to evaluate what kinds of
explanations a given method can and cannot provide. We find that reliance,
solely, on visual assessment can be misleading. Through extensive experiments
we show that some existing saliency methods are independent both of the model
and of the data generating process. Consequently, methods that fail the
proposed tests are inadequate for tasks that are sensitive to either data or
model, such as, finding outliers in the data, explaining the relationship
between inputs and outputs that the model learned, and debugging the model. We
interpret our findings through an analogy with edge detection in images, a
technique that requires neither training data nor model. Theory in the case of
a linear model and a single-layer convolutional neural network supports our
experimental findings.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/poerner2018evaluating/">Evaluating neural network explanation methods using hybrid documents and morphological agreement</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating neural network explanation methods using hybrid documents and morphological agreement' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating neural network explanation methods using hybrid documents and morphological agreement' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nina Poerner, Benjamin Roth, Hinrich Schütze</td>
	<td></td>
	<td><p>The behavior of deep neural networks (DNNs) is hard to understand. This makes
it necessary to explore post hoc explanation methods. We conduct the first
comprehensive evaluation of explanation methods for NLP. To this end, we design
two novel evaluation paradigms that cover two important classes of NLP
problems: small context and large context problems. Both paradigms require no
manual annotation and are therefore broadly applicable. We also introduce
LIMSSE, an explanation method inspired by LIME that is designed for NLP. We
show empirically that LIMSSE, LRP and DeepLIFT are the most effective
explanation methods and recommend them for explaining DNNs in NLP.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/du2018towards/">Towards Explanation of DNN-based Prediction with Guided Feature Inversion</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explanation of DNN-based Prediction with Guided Feature Inversion' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explanation of DNN-based Prediction with Guided Feature Inversion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mengnan Du, Ninghao Liu, Qingquan Song, Xia Hu</td>
	<td></td>
	<td><p>While deep neural networks (DNN) have become an effective computational tool,
the prediction results are often criticized by the lack of interpretability,
which is essential in many real-world applications such as health informatics.
Existing attempts based on local interpretations aim to identify relevant
features contributing the most to the prediction of DNN by monitoring the
neighborhood of a given input. They usually simply ignore the intermediate
layers of the DNN that might contain rich information for interpretation. To
bridge the gap, in this paper, we propose to investigate a guided feature
inversion framework for taking advantage of the deep architectures towards
effective interpretation. The proposed framework not only determines the
contribution of each feature in the input but also provides insights into the
decision-making process of DNN models. By further interacting with the neuron
of the target category at the output layer of the DNN, we enforce the
interpretation result to be class-discriminative. We apply the proposed
interpretation model to different CNN architectures to provide explanations for
image data and conduct extensive experiments on ImageNet and PASCAL VOC07
datasets. The interpretation results demonstrate the effectiveness of our
proposed framework in providing class-discriminative interpretation for
DNN-based prediction.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/ravanelli2018interpretable/">Interpretable Convolutional Filters with SincNet</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Convolutional Filters with SincNet' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Convolutional Filters with SincNet' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mirco Ravanelli, Yoshua Bengio</td>
	<td></td>
	<td><p>Deep learning is currently playing a crucial role toward higher levels of
artificial intelligence. This paradigm allows neural networks to learn complex
and abstract representations, that are progressively obtained by combining
simpler ones. Nevertheless, the internal “black-box” representations
automatically discovered by current neural architectures often suffer from a
lack of interpretability, making of primary interest the study of explainable
machine learning techniques. This paper summarizes our recent efforts to
develop a more interpretable neural model for directly processing speech from
the raw waveform. In particular, we propose SincNet, a novel Convolutional
Neural Network (CNN) that encourages the first layer to discover more
meaningful filters by exploiting parametrized sinc functions. In contrast to
standard CNNs, which learn all the elements of each filter, only low and high
cutoff frequencies of band-pass filters are directly learned from data. This
inductive bias offers a very compact way to derive a customized filter-bank
front-end, that only depends on some parameters with a clear physical meaning.
Our experiments, conducted on both speaker and speech recognition, show that
the proposed architecture converges faster, performs better, and is more
interpretable than standard CNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/roxlo2018opening/">Opening the black box of neural nets: case studies in stop/top discrimination</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Opening the black box of neural nets: case studies in stop/top discrimination' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Opening the black box of neural nets: case studies in stop/top discrimination' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Thomas Roxlo, Matthew Reece</td>
	<td></td>
	<td><p>We introduce techniques for exploring the functionality of a neural network
and extracting simple, human-readable approximations to its performance. By
performing gradient ascent on the input space of the network, we are able to
produce large populations of artificial events which strongly excite a given
classifier. By studying the populations of these events, we then directly
produce what are essentially contour maps of the network’s classification
function. Combined with a suite of tools for identifying the input dimensions
deemed most important by the network, we can utilize these maps to efficiently
interpret the dominant criteria by which the network makes its classification.
  As a test case, we study networks trained to discriminate supersymmetric stop
production in the dilepton channel from Standard Model backgrounds. In the case
of a heavy stop decaying to a light neutralino, we find individual neurons with
large mutual information with $m_{T2}^{\ell\ell}$, a human-designed variable
for optimizing the analysis. The network selects events with significant
missing $p_T$ oriented azimuthally away from both leptons, efficiently
rejecting $t\overline{t}$ background. In the case of a light stop with
three-body decays to $Wb{\widetilde \chi}$ and little phase space, we find
neurons that smoothly interpolate between a similar top-rejection strategy and
an ISR-tagging strategy allowing for more missing momentum. We also find that a
neural network trained on a stealth stop parameter point learns novel angular
correlations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/rudin2018stop/">Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Cynthia Rudin</td>
	<td>Nature Machine Intelligence, Vol 1, May 2019, 206-215</td>
	<td><p>Black box machine learning models are currently being used for high stakes
decision-making throughout society, causing problems throughout healthcare,
criminal justice, and in other domains. People have hoped that creating methods
for explaining these black box models will alleviate some of these problems,
but trying to \textit{explain} black box models, rather than creating models
that are \textit{interpretable} in the first place, is likely to perpetuate bad
practices and can potentially cause catastrophic harm to society. There is a
way forward – it is to design models that are inherently interpretable. This
manuscript clarifies the chasm between explaining black boxes and using
inherently interpretable models, outlines several key reasons why explainable
black boxes should be avoided in high-stakes decisions, identifies challenges
to interpretable machine learning, and provides several example applications
where interpretable models could potentially replace black box models in
criminal justice, healthcare, and computer vision.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhu2018exploiting/">Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chunbiao Zhu, Wenhao Zhang, Thomas H. Li, Ge Li</td>
	<td></td>
	<td><p>Saliency detection aims to detect the most attractive objects in images and
is widely used as a foundation for various applications. In this paper, we
propose a novel salient object detection algorithm for RGB-D images using
center-dark channel priors. First, we generate an initial saliency map based on
a color saliency map and a depth saliency map of a given RGB-D image. Then, we
generate a center-dark channel map based on center saliency and dark channel
priors. Finally, we fuse the initial saliency map with the center dark channel
map to generate the final saliency map. Extensive evaluations over four
benchmark datasets demonstrate that our proposed method performs favorably
against most of the state-of-the-art approaches. Besides, we further discuss
the application of the proposed algorithm in small target detection and
demonstrate the universal value of center-dark channel priors in the field of
object detection.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/alvarez-melis2018robustness/">On the Robustness of Interpretability Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Robustness of Interpretability Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Robustness of Interpretability Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>David Alvarez-Melis, Tommi S. Jaakkola</td>
	<td></td>
	<td><p>We argue that robustness of explanations—i.e., that similar inputs should
give rise to similar explanations—is a key desideratum for interpretability.
We introduce metrics to quantify robustness and demonstrate that current
methods do not perform well according to these metrics. Finally, we propose
ways that robustness can be enforced on existing interpretability approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/sanakoyeu2018deep/">Deep Unsupervised Learning of Visual Similarities</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Learning of Visual Similarities' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Learning of Visual Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Artsiom Sanakoyeu, Miguel A. Bautista, Björn Ommer</td>
	<td>Pattern Recognition Volume 78, June 2018, Pages 331-343</td>
	<td><p>Exemplar learning of visual similarities in an unsupervised manner is a
problem of paramount importance to Computer Vision. In this context, however,
the recent breakthrough in deep learning could not yet unfold its full
potential. With only a single positive sample, a great imbalance between one
positive and many negatives, and unreliable relationships between most samples,
training of Convolutional Neural networks is impaired. In this paper we use
weak estimates of local similarities and propose a single optimization problem
to extract batches of samples with mutually consistent relations. Conflicting
relations are distributed over different batches and similar samples are
grouped into compact groups. Learning visual similarities is then framed as a
sequence of categorization tasks. The CNN then consolidates transitivity
relations within and between groups and learns a single representation for all
samples without the need for labels. The proposed unsupervised approach has
shown competitive performance on detailed posture analysis and object
classification.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/chen2018interpretable/">An Interpretable Model with Globally Consistent Explanations for Credit Risk</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Interpretable Model with Globally Consistent Explanations for Credit Risk' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Interpretable Model with Globally Consistent Explanations for Credit Risk' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chaofan Chen, Kangcheng Lin, Cynthia Rudin, Yaron Shaposhnik, Sijia Wang, Tong Wang</td>
	<td></td>
	<td><p>We propose a possible solution to a public challenge posed by the Fair Isaac
Corporation (FICO), which is to provide an explainable model for credit risk
assessment. Rather than present a black box model and explain it afterwards, we
provide a globally interpretable model that is as accurate as other neural
networks. Our “two-layer additive risk model” is decomposable into subscales,
where each node in the second layer represents a meaningful subscale, and all
of the nonlinearities are transparent. We provide three types of explanations
that are simpler than, but consistent with, the global model. One of these
explanation methods involves solving a minimum set cover problem to find
high-support globally-consistent explanations. We present a new online
visualization tool to allow users to explore the global model and its
explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/teso2018why/">"Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Stefano Teso, Kristian Kersting</td>
	<td></td>
	<td><p>Although interactive learning puts the user into the loop, the learner
remains mostly a black box for the user. Understanding the reasons behind
queries and predictions is important when assessing how the learner works and,
in turn, trust. Consequently, we propose the novel framework of explanatory
interactive learning: in each step, the learner explains its interactive query
to the user, and she queries of any active classifier for visualizing
explanations of the corresponding predictions. We demonstrate that this can
boost the predictive and explanatory powers of and the trust into the learned
model, using text (e.g. SVMs) and image classification (e.g. neural networks)
experiments as well as a user study.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/guo2018visualizing/">Visualizing and Understanding Deep Neural Networks in CTR Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Deep Neural Networks in CTR Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Deep Neural Networks in CTR Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lin Guo, Hui Ye, Wenbo Su, Henhuan Liu, Kai Sun, Hang Xiang</td>
	<td></td>
	<td><p>Although deep learning techniques have been successfully applied to many
tasks, interpreting deep neural network models is still a big challenge to us.
Recently, many works have been done on visualizing and analyzing the mechanism
of deep neural networks in the areas of image processing and natural language
processing. In this paper, we present our approaches to visualize and
understand deep neural networks for a very important commercial task–CTR
(Click-through rate) prediction. We conduct experiments on the productive data
from our online advertising system with daily varying distribution. To
understand the mechanism and the performance of the model, we inspect the
model’s inner status at neuron level. Also, a probe approach is implemented to
measure the layer-wise performance of the model. Moreover, to measure the
influence from the input features, we calculate saliency scores based on the
back-propagated gradients. Practical applications are also discussed, for
example, in understanding, monitoring, diagnosing and refining models and
algorithms.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/hohman2018visual/">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau</td>
	<td></td>
	<td><p>Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/vivek2018gray-box/">Gray-box Adversarial Training</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Gray-box Adversarial Training' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Gray-box Adversarial Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vivek B. S., Konda Reddy Mopuri, R. Venkatesh Babu</td>
	<td></td>
	<td><p>Adversarial samples are perturbed inputs crafted to mislead the machine
learning systems. A training mechanism, called adversarial training, which
presents adversarial samples along with clean samples has been introduced to
learn robust models. In order to scale adversarial training for large datasets,
these perturbations can only be crafted using fast and simple methods (e.g.,
gradient ascent). However, it is shown that adversarial training converges to a
degenerate minimum, where the model appears to be robust by generating weaker
adversaries. As a result, the models are vulnerable to simple black-box
attacks. In this paper we, (i) demonstrate the shortcomings of existing
evaluation policy, (ii) introduce novel variants of white-box and black-box
attacks, dubbed gray-box adversarial attacks” based on which we propose novel
evaluation method to assess the robustness of the learned models, and (iii)
propose a novel variant of adversarial training, named Graybox Adversarial
Training” that uses intermediate versions of the models to seed the
adversaries. Experimental evaluation demonstrates that the models trained using
our method exhibit better robustness compared to both undefended and
adversarially trained model</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/wallace2018interpreting/">Interpreting Neural Networks With Nearest Neighbors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Neural Networks With Nearest Neighbors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Neural Networks With Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Eric Wallace, Shi Feng, Jordan Boyd-Graber</td>
	<td></td>
	<td><p>Local model interpretation methods explain individual predictions by
assigning an importance value to each input feature. This value is often
determined by measuring the change in confidence when a feature is removed.
However, the confidence of neural networks is not a robust measure of model
uncertainty. This issue makes reliably judging the importance of the input
features difficult. We address this by changing the test-time behavior of
neural networks using Deep k-Nearest Neighbors. Without harming text
classification accuracy, this algorithm provides a more robust uncertainty
metric which we use to generate feature importance values. The resulting
interpretations better align with human perception than baseline methods.
Finally, we use our interpretation method to analyze model predictions on
dataset annotation artifacts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/chen2018classifier/">Why Is My Classifier Discriminatory?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Why Is My Classifier Discriminatory?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Why Is My Classifier Discriminatory?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Irene Chen, Fredrik D. Johansson, David Sontag</td>
	<td></td>
	<td><p>Recent attempts to achieve fairness in predictive models focus on the balance
between fairness and accuracy. In sensitive applications such as healthcare or
criminal justice, this trade-off is often undesirable as any increase in
prediction error could have devastating consequences. In this work, we argue
that the fairness of predictions should be evaluated in context of the data,
and that unfairness induced by inadequate samples sizes or unmeasured
predictive variables should be addressed through data collection, rather than
by constraining the model. We decompose cost-based metrics of discrimination
into bias, variance, and noise, and propose actions aimed at estimating and
reducing each term. Finally, we perform case-studies on prediction of income,
mortality, and review ratings, confirming the value of this analysis. We find
that data collection is often a means to reduce discrimination without
sacrificing accuracy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/ciocan2018interpretable/">Interpretable Optimal Stopping</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Optimal Stopping' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Optimal Stopping' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dragos Florin Ciocan, Velibor V. Mišić</td>
	<td></td>
	<td><p>Optimal stopping is the problem of deciding when to stop a stochastic system
to obtain the greatest reward, arising in numerous application areas such as
finance, healthcare and marketing. State-of-the-art methods for
high-dimensional optimal stopping involve approximating the value function or
the continuation value, and then using that approximation within a greedy
policy. Although such policies can perform very well, they are generally not
guaranteed to be interpretable; that is, a decision maker may not be able to
easily see the link between the current system state and the policy’s action.
In this paper, we propose a new approach to optimal stopping, wherein the
policy is represented as a binary tree, in the spirit of naturally
interpretable tree models commonly used in machine learning. We show that the
class of tree policies is rich enough to approximate the optimal policy. We
formulate the problem of learning such policies from observed trajectories of
the stochastic system as a sample average approximation (SAA) problem. We prove
that the SAA problem converges under mild conditions as the sample size
increases, but that computationally even immediate simplifications of the SAA
problem are theoretically intractable. We thus propose a tractable heuristic
for approximately solving the SAA problem, by greedily constructing the tree
from the top down. We demonstrate the value of our approach by applying it to
the canonical problem of option pricing, using both synthetic instances and
instances using real S&amp;P-500 data. Our method obtains policies that (1)
outperform state-of-the-art non-interpretable methods, based on
simulation-regression and martingale duality, and (2) possess a remarkably
simple and intuitive structure.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018visual/">Visual Interpretability for Deep Learning: a Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual Interpretability for Deep Learning: a Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual Interpretability for Deep Learning: a Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles’ heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/craye2018exploring/">Exploring to learn visual saliency: The RL-IAC approach</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring to learn visual saliency: The RL-IAC approach' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring to learn visual saliency: The RL-IAC approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Celine Craye, Timothee Lesort, David Filliat, Jean-Francois Goudou</td>
	<td></td>
	<td><p>The problem of object localization and recognition on autonomous mobile
robots is still an active topic. In this context, we tackle the problem of
learning a model of visual saliency directly on a robot. This model, learned
and improved on-the-fly during the robot’s exploration provides an efficient
tool for localizing relevant objects within their environment. The proposed
approach includes two intertwined components. On the one hand, we describe a
method for learning and incrementally updating a model of visual saliency from
a depth-based object detector. This model of saliency can also be exploited to
produce bounding box proposals around objects of interest. On the other hand,
we investigate an autonomous exploration technique to efficiently learn such a
saliency model. The proposed exploration, called Reinforcement
Learning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot’s
exploration so that samples selected by the robot are likely to improve the
current model of saliency. We then demonstrate that such a saliency model
learned directly on a robot outperforms several state-of-the-art saliency
techniques, and that RL-IAC can drastically decrease the required time for
learning a reliable saliency model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lecue2018semantic/">Semantic Explanations of Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Explanations of Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Explanations of Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Freddy Lecue, Jiewen Wu</td>
	<td></td>
	<td><p>The main objective of explanations is to transmit knowledge to humans. This
work proposes to construct informative explanations for predictions made from
machine learning models. Motivated by the observations from social sciences,
our approach selects data points from the training sample that exhibit special
characteristics crucial for explanation, for instance, ones contrastive to the
classification prediction and ones representative of the models. Subsequently,
semantic concepts are derived from the selected data points through the use of
domain ontologies. These concepts are filtered and ranked to produce
informative explanations that improves human understanding. The main features
of our approach are that (1) knowledge about explanations is captured in the
form of ontological concepts, (2) explanations include contrastive evidences in
addition to normal evidences, and (3) explanations are user relevant.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018non-rigid/">Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pingping Zhang, Wei Liu, Dong Wang, Yinjie Lei, Hongyu Wang, Chunhua Shen, Huchuan Lu</td>
	<td></td>
	<td><p>In this paper, we propose a novel effective non-rigid object tracking
framework based on the spatial-temporal consistent saliency detection. In
contrast to most existing trackers that utilize a bounding box to specify the
tracked target, the proposed framework can extract accurate regions of the
target as tracking outputs. It achieves a better description of the non-rigid
objects and reduces the background pollution for the tracking model.
Furthermore, our model has several unique features. First, a tailored fully
convolutional neural network (TFCN) is developed to model the local saliency
prior for a given image region, which not only provides the pixel-wise outputs
but also integrates the semantic information. Second, a novel multi-scale
multi-region mechanism is proposed to generate local saliency maps that
effectively consider visual perceptions with different spatial layouts and
scale variations. Subsequently, local saliency maps are fused via a weighted
entropy method, resulting in a final discriminative saliency map. Finally, we
present a non-rigid object tracking algorithm based on the predicted saliency
maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we
conduct target-background classification and use a simple fine-tuning scheme
for online updating. Extensive experiments demonstrate that the proposed
algorithm achieves competitive performance in both saliency detection and
visual tracking, especially outperforming other related trackers on the
non-rigid object tracking datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/besold2018what/">The What, the Why, and the How of Artificial Explanations in Automated Decision-Making</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The What, the Why, and the How of Artificial Explanations in Automated Decision-Making' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The What, the Why, and the How of Artificial Explanations in Automated Decision-Making' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tarek R. Besold, Sara L. Uckelman</td>
	<td></td>
	<td><p>The increasing incorporation of Artificial Intelligence in the form of
automated systems into decision-making procedures highlights not only the
importance of decision theory for automated systems but also the need for these
decision procedures to be explainable to the people involved in them.
Traditional realist accounts of explanation, wherein explanation is a relation
that holds (or does not hold) eternally between an explanans and an
explanandum, are not adequate to account for the notion of explanation required
for artificial decision procedures. We offer an alternative account of
explanation as used in the context of automated decision-making that makes
explanation an epistemic phenomenon, and one that is dependent on context. This
account of explanation better accounts for the way that we talk about, and use,
explanations and derived concepts, such as `explanatory power’, and also allows
us to differentiate between reasons or causes on the one hand, which do not
need to have an epistemic aspect, and explanations on the other, which do have
such an aspect. Against this theoretical backdrop we then review existing
approaches to explanation in Artificial Intelligence and Machine Learning, and
suggest desiderata which truly explainable decision systems should fulfill.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2017</td>
	<td><a href="/publications/lipton2017mythos/">The Mythos of Model Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Mythos of Model Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Mythos of Model Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zachary C. Lipton</td>
	<td>ICML Workshop on Human Interpretability in Machine Learning</td>
	<td><p>Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.</p>
</td>
	<td>understanding </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/kindermans2017unreliability/">The (Un)reliability of saliency methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The (Un)reliability of saliency methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The (Un)reliability of saliency methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, Been Kim</td>
	<td></td>
	<td><p>Saliency methods aim to explain the predictions of deep neural networks.
These methods lack reliability when the explanation is sensitive to factors
that do not contribute to the model prediction. We use a simple and common
pre-processing step —adding a constant shift to the input data— to show
that a transformation with no effect on the model can cause numerous methods to
incorrectly attribute. In order to guarantee reliability, we posit that methods
should fulfill input invariance, the requirement that a saliency method mirror
the sensitivity of the model with respect to transformations of the input. We
show, through several examples, that saliency methods that do not satisfy input
invariance result in misleading attribution.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/dong2017towards/">Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao</td>
	<td></td>
	<td><p>Deep neural networks (DNNs) have demonstrated impressive performance on a
wide array of tasks, but they are usually considered opaque since internal
structure and learned parameters are not interpretable. In this paper, we
re-examine the internal representations of DNNs using adversarial images, which
are generated by an ensemble-optimization algorithm. We find that: (1) the
neurons in DNNs do not truly detect semantic objects/parts, but respond to
objects/parts only as recurrent discriminative patches; (2) deep visual
representations are not robust distributed codes of visual concepts because the
representations of adversarial images are largely not consistent with those of
real images, although they have similar visual appearance, both of which are
different from previous findings. To further improve the interpretability of
DNNs, we propose an adversarial training scheme with a consistent loss such
that the neurons are endowed with human-interpretable concepts. The induced
interpretable representations enable us to trace eventual outcomes back to
influential neurons. Therefore, human users can know how the models make
predictions, as well as when and why they make errors.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/doshi-velez2017towards/">Towards A Rigorous Science of Interpretable Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards A Rigorous Science of Interpretable Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards A Rigorous Science of Interpretable Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Finale Doshi-Velez, Been Kim</td>
	<td></td>
	<td><p>As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ghosh2017towards/">Towards a New Interpretation of Separable Convolutions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards a New Interpretation of Separable Convolutions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards a New Interpretation of Separable Convolutions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tapabrata Ghosh</td>
	<td></td>
	<td><p>In recent times, the use of separable convolutions in deep convolutional
neural network architectures has been explored. Several researchers, most
notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in
their deep architectures and have demonstrated state of the art or close to
state of the art performance. However, the underlying mechanism of action of
separable convolutions are still not fully understood. Although their
mathematical definition is well understood as a depthwise convolution followed
by a pointwise convolution, deeper interpretations such as the extreme
Inception hypothesis (Chollet, 2016) have failed to provide a thorough
explanation of their efficacy. In this paper, we propose a hybrid
interpretation that we believe is a better model for explaining the efficacy of
separable convolutions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/koh2017understanding/">Understanding Black-box Predictions via Influence Functions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Black-box Predictions via Influence Functions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Black-box Predictions via Influence Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pang Wei Koh, Percy Liang</td>
	<td></td>
	<td><p>How can we explain the predictions of a black-box model? In this paper, we
use influence functions – a classic technique from robust statistics – to
trace a model’s prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/krause2017workflow/">A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon Aphinyanaphongs, Enrico Bertini</td>
	<td></td>
	<td><p>Human-in-the-loop data analysis applications necessitate greater transparency
in machine learning models for experts to understand and trust their decisions.
To this end, we propose a visual analytics workflow to help data scientists and
domain experts explore, diagnose, and understand the decisions made by a binary
classifier. The approach leverages “instance-level explanations”, measures of
local feature relevance that explain single instances, and uses them to build a
set of visual representations that guide the users in their investigation. The
workflow is based on three main visual representations and steps: one based on
aggregate statistics to see how data distributes across correct / incorrect
decisions; one based on explanations to understand which features are used to
make these decisions; and one based on raw data, to derive insights on
potential root causes for the observed patterns. The workflow is derived from a
long-term collaboration with a group of machine learning and healthcare
professionals who used our method to make sense of machine learning models they
developed. The case study from this collaboration demonstrates that the
proposed workflow helps experts derive useful knowledge about the model and the
phenomena it describes, thus experts can generate useful hypotheses on how a
model can be improved.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lakkaraju2017interpretable/">Interpretable & Explorable Approximations of Black Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable & Explorable Approximations of Black Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable & Explorable Approximations of Black Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Jure Leskovec</td>
	<td></td>
	<td><p>We propose Black Box Explanations through Transparent Approximations (BETA),
a novel model agnostic framework for explaining the behavior of any black-box
classifier by simultaneously optimizing for fidelity to the original model and
interpretability of the explanation. To this end, we develop a novel objective
function which allows us to learn (with optimality guarantees), a small number
of compact decision sets each of which explains the behavior of the black box
model in unambiguous, well-defined regions of feature space. Furthermore, our
framework also is capable of accepting user input when generating these
approximations, thus allowing users to interactively explore how the black-box
model behaves in different subspaces that are of interest to the user. To the
best of our knowledge, this is the first approach which can produce global
explanations of the behavior of any given black box model through joint
optimization of unambiguity, fidelity, and interpretability, while also
allowing users to explore model behavior based on their preferences.
Experimental evaluation with real-world datasets and user studies demonstrates
that our approach can generate highly compact, easy-to-understand, yet accurate
approximations of various kinds of predictive models compared to
state-of-the-art baselines.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lundberg2017unified/">A Unified Approach to Interpreting Model Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Unified Approach to Interpreting Model Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Unified Approach to Interpreting Model Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Scott Lundberg, Su-In Lee</td>
	<td></td>
	<td><p>Understanding why a model makes a certain prediction can be as crucial as the
prediction’s accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/bau2017network/">Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Network Dissection: Quantifying Interpretability of Deep Visual Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Network Dissection: Quantifying Interpretability of Deep Visual Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>We propose a general framework called Network Dissection for quantifying the
interpretability of latent representations of CNNs by evaluating the alignment
between individual hidden units and a set of semantic concepts. Given any CNN
model, the proposed method draws on a broad data set of visual concepts to
score the semantics of hidden units at each intermediate convolutional layer.
The units with semantics are given labels across a range of objects, parts,
scenes, textures, materials, and colors. We use the proposed method to test the
hypothesis that interpretability of units is equivalent to random linear
combinations of units, then we apply our method to compare the latent
representations of various networks when trained to solve different supervised
and self-supervised training tasks. We further analyze the effect of training
iterations, compare networks trained with different initializations, examine
the impact of network depth and width, and measure the effect of dropout and
batch normalization on the interpretability of deep visual representations. We
demonstrate that the proposed method can shed light on characteristics of CNN
models and training methods that go beyond measurements of their discriminative
power.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wu2017towards/">Towards Interpretable R-CNN by Unfolding Latent Structures</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable R-CNN by Unfolding Latent Structures' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable R-CNN by Unfolding Latent Structures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tianfu Wu, Wei Sun, Xilai Li, Xi Song, Bo Li</td>
	<td></td>
	<td><p>This paper first proposes a method of formulating model interpretability in
visual understanding tasks based on the idea of unfolding latent structures. It
then presents a case study in object detection using popular two-stage
region-based convolutional network (i.e., R-CNN) detection systems. We focus on
weakly-supervised extractive rationale generation, that is learning to unfold
latent discriminative part configurations of object instances automatically and
simultaneously in detection without using any supervision for part
configurations. We utilize a top-down hierarchical and compositional grammar
model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold
the space of latent part configurations of regions of interest (RoIs). We
propose an AOGParsing operator to substitute the RoIPooling operator widely
used in R-CNN. In detection, a bounding box is interpreted by the best parse
tree derived from the AOG on-the-fly, which is treated as the qualitatively
extractive rationale generated for interpreting detection. We propose a
folding-unfolding method to train the AOG and convolutional networks
end-to-end. In experiments, we build on R-FCN and test our method on the PASCAL
VOC 2007 and 2012 datasets. We show that the method can unfold promising latent
structures without hurting the performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/su2017one/">One pixel attack for fooling deep neural networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One pixel attack for fooling deep neural networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One pixel attack for fooling deep neural networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi</td>
	<td>IEEE Transactions on Evolutionary Computation</td>
	<td><p>Recent research has revealed that the output of Deep Neural Networks (DNN)
can be easily altered by adding relatively small perturbations to the input
vector. In this paper, we analyze an attack in an extremely limited scenario
where only one pixel can be modified. For that we propose a novel method for
generating one-pixel adversarial perturbations based on differential evolution
(DE). It requires less adversarial information (a black-box attack) and can
fool more types of networks due to the inherent features of DE. The results
show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and
16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least
one target class by modifying just one pixel with 74.03% and 22.91% confidence
on average. We also show the same vulnerability on the original CIFAR-10
dataset. Thus, the proposed attack explores a different take on adversarial
machine learning in an extreme limited scenario, showing that current DNNs are
also vulnerable to such low dimension attacks. Besides, we also illustrate an
important application of DE (or broadly speaking, evolutionary computation) in
the domain of adversarial machine learning: creating tools that can effectively
generate low-cost adversarial attacks against neural networks for evaluating
robustness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ross2017improving/">Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrew Slavin Ross, Finale Doshi-Velez</td>
	<td></td>
	<td><p>Deep neural networks have proven remarkably effective at solving many
classification problems, but have been criticized recently for two major
weaknesses: the reasons behind their predictions are uninterpretable, and the
predictions themselves can often be fooled by small adversarial perturbations.
These problems pose major obstacles for the adoption of neural networks in
domains that require security or transparency. In this work, we evaluate the
effectiveness of defenses that differentiably penalize the degree to which
small changes in inputs can alter model predictions. Across multiple attacks,
architectures, defenses, and datasets, we find that neural networks trained
with this input gradient regularization exhibit robustness to transferred
adversarial examples generated to fool all of the other models. We also find
that adversarial examples generated to fool gradient-regularized models fool
all other models equally well, and actually lead to more “legitimate,”
interpretable misclassifications as rated by people (which we confirm in a
human subject experiment). Finally, we demonstrate that regularizing input
gradients makes them more naturally interpretable as rationales for model
predictions. We conclude by discussing this relationship between
interpretability and robustness in deep neural networks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ross2017right/">Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez</td>
	<td></td>
	<td><p>Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/liang2017interpretable/">Interpretable Structure-Evolving LSTM</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Structure-Evolving LSTM' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Structure-Evolving LSTM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing</td>
	<td></td>
	<td><p>This paper develops a general framework for learning interpretable data
representation via Long Short-Term Memory (LSTM) recurrent neural networks over
hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
structures, we propose to further learn the intermediate interpretable
multi-level graph structures in a progressive and stochastic way from data
during the LSTM network optimization. We thus call this model the
structure-evolving LSTM. In particular, starting with an initial element-level
graph representation where each node is a small data element, the
structure-evolving LSTM gradually evolves the multi-level graph representations
by stochastically merging the graph nodes with high compatibilities along the
stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
connected nodes from their corresponding LSTM gate outputs, which is used to
generate a merging probability. The candidate graph structures are accordingly
generated where the nodes are grouped into cliques with their merging
probabilities. We then produce the new graph structure with a
Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
local optimums by stochastic sampling with an acceptance probability. Once a
graph structure is accepted, a higher-level graph is then constructed by taking
the partitioned cliques as its nodes. During the evolving process,
representation becomes more abstracted in higher-levels where redundant
information is filtered out, allowing more efficient propagation of long-range
data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
the application of semantic object parsing and demonstrate its advantage over
state-of-the-art LSTM models on standard benchmarks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhou2017interpreting/">Interpreting Deep Visual Representations via Network Dissection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Deep Visual Representations via Network Dissection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Deep Visual Representations via Network Dissection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bolei Zhou, David Bau, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>The success of recent deep convolutional neural networks (CNNs) depends on
learning hidden representations that can summarize the important factors of
variation behind the data. However, CNNs often criticized as being black boxes
that lack interpretability, since they have millions of unexplained model
parameters. In this work, we describe Network Dissection, a method that
interprets networks by providing labels for the units of their deep visual
representations. The proposed method quantifies the interpretability of CNN
representations by evaluating the alignment between individual hidden units and
a set of visual semantic concepts. By identifying the best alignments, units
are given human interpretable labels across a range of objects, parts, scenes,
textures, materials, and colors. The method reveals that deep representations
are more transparent and interpretable than expected: we find that
representations are significantly more interpretable than they would be under a
random equivalently powerful basis. We apply the method to interpret and
compare the latent representations of various network architectures trained to
solve different supervised and self-supervised training tasks. We then examine
factors affecting the network interpretability such as the number of the
training iterations, regularizations, different initializations, and the
network depth and width. Finally we show that the interpreted units can be used
to provide explicit explanations of a prediction given by a CNN for an image.
Our results highlight that interpretability is an important property of deep
neural networks that provides new insights into their hierarchical structure.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhang2017interpretable/">Interpretable Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper proposes a method to modify traditional convolutional neural
networks (CNNs) into interpretable CNNs, in order to clarify knowledge
representations in high conv-layers of CNNs. In an interpretable CNN, each
filter in a high conv-layer represents a certain object part. We do not need
any annotations of object parts or textures to supervise the learning process.
Instead, the interpretable CNN automatically assigns each filter in a high
conv-layer with an object part during the learning process. Our method can be
applied to different types of CNNs with different structures. The clear
knowledge representation in an interpretable CNN can help people understand the
logics inside a CNN, i.e., based on which patterns the CNN makes the decision.
Experiments showed that filters in an interpretable CNN were more semantically
meaningful than those in traditional CNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/miller2017explanation/">Explanation in Artificial Intelligence: Insights from the Social Sciences</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation in Artificial Intelligence: Insights from the Social Sciences' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation in Artificial Intelligence: Insights from the Social Sciences' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tim Miller</td>
	<td></td>
	<td><p>There has been a recent resurgence in the area of explainable artificial
intelligence as researchers and practitioners seek to make their algorithms
more understandable. Much of this research is focused on explicitly explaining
decisions or actions to a human observer, and it should not be controversial to
say that looking at how humans explain to each other can serve as a useful
starting point for explanation in artificial intelligence. However, it is fair
to say that most work in explainable artificial intelligence uses only the
researchers’ intuition of what constitutes a `good’ explanation. There exists
vast and valuable bodies of research in philosophy, psychology, and cognitive
science of how people define, generate, select, evaluate, and present
explanations, which argues that people employ certain cognitive biases and
social expectations towards the explanation process. This paper argues that the
field of explainable artificial intelligence should build on this existing
research, and reviews relevant papers from philosophy, cognitive
psychology/science, and social psychology, which study these topics. It draws
out some important findings, and discusses ways that these can be infused with
work on explainable artificial intelligence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhang2017interpreting/">Interpreting CNN Knowledge via an Explanatory Graph</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting CNN Knowledge via an Explanatory Graph' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting CNN Knowledge via an Explanatory Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper learns a graphical model, namely an explanatory graph, which
reveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering
that each filter in a conv-layer of a pre-trained CNN usually represents a
mixture of object parts, we propose a simple yet efficient method to
automatically disentangles different part patterns from each filter, and
construct an explanatory graph. In the explanatory graph, each node represents
a part pattern, and each edge encodes co-activation relationships and spatial
relationships between patterns. More importantly, we learn the explanatory
graph for a pre-trained CNN in an unsupervised manner, i.e., without a need of
annotating object parts. Experiments show that each graph node consistently
represents the same object part through different images. We transfer part
patterns in the explanatory graph to the task of part localization, and our
method significantly outperforms other approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/fong2017interpretable/">Interpretable Explanations of Black Boxes by Meaningful Perturbation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Explanations of Black Boxes by Meaningful Perturbation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Explanations of Black Boxes by Meaningful Perturbation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ruth Fong, Andrea Vedaldi</td>
	<td>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</td>
	<td><p>As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks “look” in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/sabour2017dynamic/">Dynamic Routing Between Capsules</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Routing Between Capsules' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Dynamic Routing Between Capsules' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sara Sabour, Nicholas Frosst, Geoffrey E Hinton</td>
	<td></td>
	<td><p>A capsule is a group of neurons whose activity vector represents the
instantiation parameters of a specific type of entity such as an object or an
object part. We use the length of the activity vector to represent the
probability that the entity exists and its orientation to represent the
instantiation parameters. Active capsules at one level make predictions, via
transformation matrices, for the instantiation parameters of higher-level
capsules. When multiple predictions agree, a higher level capsule becomes
active. We show that a discrimininatively trained, multi-layer capsule system
achieves state-of-the-art performance on MNIST and is considerably better than
a convolutional net at recognizing highly overlapping digits. To achieve these
results we use an iterative routing-by-agreement mechanism: A lower-level
capsule prefers to send its output to higher level capsules whose activity
vectors have a big scalar product with the prediction coming from the
lower-level capsule.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wang2017deep/">Deep Visual Attention Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Visual Attention Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Visual Attention Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wenguan Wang, Jianbing Shen</td>
	<td>IEEE Transactions on Image Processing, Vol. 27, No. 5, pp 2368-2378, 2018</td>
	<td><p>In this work, we aim to predict human eye fixation with view-free scenes
based on an end-to-end deep learning architecture. Although Convolutional
Neural Networks (CNNs) have made substantial improvement on human attention
prediction, it is still needed to improve CNN based attention models by
efficiently leveraging multi-scale features. Our visual attention network is
proposed to capture hierarchical saliency information from deep, coarse layers
with global saliency information to shallow, fine layers with local saliency
response. Our model is based on a skip-layer network structure, which predicts
human attention from multiple convolutional layers with various reception
fields. Final saliency prediction is achieved via the cooperation of those
global and local predictions. Our model is learned in a deep supervision
manner, where supervision is directly fed into multi-level layers, instead of
previous approaches of providing supervision only at the output layer and
propagating this supervision back to earlier layers. Our model thus
incorporates multi-level saliency predictions within a single network, which
significantly decreases the redundancy of previous approaches of learning
multiple network streams with different input scales. Extensive experimental
analysis on various challenging benchmark datasets demonstrate our method
yields state-of-the-art performance with competitive inference time.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2016</td>
	<td><a href="/publications/nguyen2016plug/">Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski</td>
	<td></td>
	<td><p>Generating high-resolution, photo-realistic images has been a long-standing
goal in machine learning. Recently, Nguyen et al. (2016) showed one interesting
way to synthesize novel images by performing gradient ascent in the latent
space of a generator network to maximize the activations of one or multiple
neurons in a separate classifier network. In this paper we extend this method
by introducing an additional prior on the latent code, improving both sample
quality and sample diversity, leading to a state-of-the-art generative model
that produces high quality images at higher resolutions (227x227) than previous
generative models, and does so for all 1000 ImageNet categories. In addition,
we provide a unified probabilistic interpretation of related activation
maximization methods and call the general class of models “Plug and Play
Generative Networks”. PPGNs are composed of 1) a generator network G that is
capable of drawing a wide range of image types and 2) a replaceable “condition”
network C that tells the generator what to draw. We demonstrate the generation
of images conditioned on a class (when C is an ImageNet or MIT Places
classification network) and also conditioned on a caption (when C is an image
captioning network). Our method also improves the state of the art of
Multifaceted Feature Visualization, which generates the set of synthetic inputs
that activate a neuron in order to better understand how deep neural networks
operate. Finally, we show that our model performs reasonably well at the task
of image inpainting. While image models are used in this paper, the approach is
modality-agnostic and can be applied to many types of data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/kumar2016understanding/">Understanding Anatomy Classification Through Attentive Response Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Anatomy Classification Through Attentive Response Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Anatomy Classification Through Attentive Response Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Devinder Kumar, Vlado Menkovski, Graham W. Taylor, Alexander Wong</td>
	<td></td>
	<td><p>One of the main challenges for broad adoption of deep learning based models
such as convolutional neural networks (CNN), is the lack of understanding of
their decisions. In many applications, a simpler, less capable model that can
be easily understood is favorable to a black-box model that has superior
performance. In this paper, we present an approach for designing CNNs based on
visualization of the internal activations of the model. We visualize the
model’s response through attentive response maps obtained using a fractional
stride convolution technique and compare the results with known imaging
landmarks from the medical literature. We show that sufficiently deep and
capable models can be successfully trained to use the same medical landmarks a
human expert would use. Our approach allows for communicating the model
decision process well, but also offers insight towards detecting biases.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/adler2016auditing/">Auditing Black-box Models for Indirect Influence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Auditing Black-box Models for Indirect Influence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Auditing Black-box Models for Indirect Influence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, Suresh Venkatasubramanian</td>
	<td></td>
	<td><p>Data-trained predictive models see widespread use, but for the most part they
are used as black boxes which output a prediction or score. It is therefore
hard to acquire a deeper understanding of model behavior, and in particular how
different features influence the model prediction. This is important when
interpreting the behavior of complex models, or asserting that certain
problematic attributes (like race or gender) are not unduly influencing
decisions.
  In this paper, we present a technique for auditing black-box models, which
lets us study the extent to which existing models take advantage of particular
features in the dataset, without knowing how the models work. Our work focuses
on the problem of indirect influence: how some features might indirectly
influence outcomes via other, related features. As a result, we can find
attribute influences even in cases where, upon further direct examination of
the model, the attribute is not referred to by the model at all.
  Our approach does not require the black-box model to be retrained. This is
important if (for example) the model is only accessible via an API, and
contrasts our work with other methods that investigate feature influence like
feature selection. We present experimental evidence for the effectiveness of
our procedure using a variety of publicly available datasets and models. We
also validate our procedure using techniques from interpretable learning and
feature selection, as well as against other black-box auditing procedures.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/hu2016harnessing/">Harnessing Deep Neural Networks with Logic Rules</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Harnessing Deep Neural Networks with Logic Rules' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Harnessing Deep Neural Networks with Logic Rules' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing</td>
	<td></td>
	<td><p>Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/vergari2016visualizing/">Visualizing and Understanding Sum-Product Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Sum-Product Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Sum-Product Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Antonio Vergari, Nicola Di Mauro, Floriana Esposito</td>
	<td></td>
	<td><p>Sum-Product Networks (SPNs) are recently introduced deep tractable
probabilistic models by which several kinds of inference queries can be
answered exactly and in a tractable time. Up to now, they have been largely
used as black box density estimators, assessed only by comparing their
likelihood scores only. In this paper we explore and exploit the inner
representations learned by SPNs. We do this with a threefold aim: first we want
to get a better understanding of the inner workings of SPNs; secondly, we seek
additional ways to evaluate one SPN model and compare it against other
probabilistic models, providing diagnostic tools to practitioners; lastly, we
want to empirically evaluate how good and meaningful the extracted
representations are, as in a classic Representation Learning framework. In
order to do so we revise their interpretation as deep neural networks and we
propose to exploit several visualization techniques on their node activations
and network outputs under different types of inference queries. To investigate
these models as feature extractors, we plug some SPNs, learned in a greedy
unsupervised fashion on image datasets, in supervised classification learning
tasks. We extract several embedding types from node activations by filtering
nodes by their type, by their associated feature abstraction level and by their
scope. In a thorough empirical comparison we prove them to be competitive
against those generated from popular feature extractors as Restricted Boltzmann
Machines. Finally, we investigate embeddings generated from random
probabilistic marginal queries as means to compare other tractable
probabilistic models on a common ground, extending our experiments to Mixtures
of Trees.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/pan2016shallow/">Shallow and Deep Convolutional Networks for Saliency Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Shallow and Deep Convolutional Networks for Saliency Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Shallow and Deep Convolutional Networks for Saliency Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor, Xavier Giro-i-Nieto</td>
	<td></td>
	<td><p>The prediction of salient areas in images has been traditionally addressed
with hand-crafted features based on neuroscience principles. This paper,
however, addresses the problem with a completely data-driven approach by
training a convolutional neural network (convnet). The learning process is
formulated as a minimization of a loss function that measures the Euclidean
distance of the predicted saliency map with the provided ground truth. The
recent publication of large datasets of saliency prediction has provided enough
data to train end-to-end architectures that are both fast and accurate. Two
designs are proposed: a shallow convnet trained from scratch, and a another
deeper solution whose first three layers are adapted from another network
trained for classification. To the authors knowledge, these are the first
end-to-end CNNs trained and tested for the purpose of saliency prediction.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zhang2016growing/">Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper proposes a learning strategy that extracts object-part concepts
from a pre-trained convolutional neural network (CNN), in an attempt to 1)
explore explicit semantics hidden in CNN units and 2) gradually grow a
semantically interpretable graphical model on the pre-trained CNN for
hierarchical object understanding. Given part annotations on very few (e.g.,
3-12) objects, our method mines certain latent patterns from the pre-trained
CNN and associates them with different semantic parts. We use a four-layer
And-Or graph to organize the mined latent patterns, so as to clarify their
internal semantic hierarchy. Our method is guided by a small number of part
annotations, and it achieves superior performance (about 13%-107% improvement)
in part center prediction on the PASCAL VOC and ImageNet datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/lee2016going/">Going Deeper with Contextual CNN for Hyperspectral Image Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Going Deeper with Contextual CNN for Hyperspectral Image Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Going Deeper with Contextual CNN for Hyperspectral Image Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hyungtae Lee, Heesung Kwon</td>
	<td></td>
	<td><p>In this paper, we describe a novel deep convolutional neural network (CNN)
that is deeper and wider than other existing deep networks for hyperspectral
image classification. Unlike current state-of-the-art approaches in CNN-based
hyperspectral image classification, the proposed network, called contextual
deep CNN, can optimally explore local contextual interactions by jointly
exploiting local spatio-spectral relationships of neighboring individual pixel
vectors. The joint exploitation of the spatio-spectral information is achieved
by a multi-scale convolutional filter bank used as an initial component of the
proposed CNN pipeline. The initial spatial and spectral feature maps obtained
from the multi-scale filter bank are then combined together to form a joint
spatio-spectral feature map. The joint feature map representing rich spectral
and spatial properties of the hyperspectral image is then fed through a fully
convolutional network that eventually predicts the corresponding label of each
pixel vector. The proposed approach is tested on three benchmark datasets: the
Indian Pines dataset, the Salinas dataset and the University of Pavia dataset.
Performance comparison shows enhanced classification performance of the
proposed approach over the current state-of-the-art on the three datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zahavy2016graying/">Graying the black box: Understanding DQNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graying the black box: Understanding DQNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graying the black box: Understanding DQNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tom Zahavy, Nir Ben Zrihem, Shie Mannor</td>
	<td></td>
	<td><p>In recent years there is a growing interest in using deep representations for
reinforcement learning. In this paper, we present a methodology and tools to
analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a
new model, the Semi Aggregated Markov Decision Process (SAMDP), and an
algorithm that learns it automatically. The SAMDP model allows us to identify
spatio-temporal abstractions directly from features and may be used as a
sub-goal detector in future work. Using our tools we reveal that the features
learned by DQNs aggregate the state space in a hierarchical fashion, explaining
its success. Moreover, we are able to understand and describe the policies
learned by DQNs for three different Atari2600 games and suggest ways to
interpret, debug and optimize deep neural networks in reinforcement learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/das2016human/">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra</td>
	<td></td>
	<td><p>We conduct large-scale studies on `human attention’ in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/ribeiro2016why/">"Why Should I Trust You?": Explaining the Predictions of Any Classifier</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="Why Should I Trust You?": Explaining the Predictions of Any Classifier' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="Why Should I Trust You?": Explaining the Predictions of Any Classifier' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin</td>
	<td></td>
	<td><p>Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2015</td>
	<td><a href="/publications/yosinski2015understanding/">Understanding Neural Networks Through Deep Visualization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Neural Networks Through Deep Visualization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Neural Networks Through Deep Visualization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson</td>
	<td></td>
	<td><p>Recent years have produced great advances in training large, deep neural
networks (DNNs), including notable successes in training convolutional neural
networks (convnets) to recognize natural images. However, our understanding of
how these models work, especially what computations they perform at
intermediate layers, has lagged behind. Progress in the field will be further
accelerated by the development of better tools for visualizing and interpreting
neural nets. We introduce two such tools here. The first is a tool that
visualizes the activations produced on each layer of a trained convnet as it
processes an image or video (e.g. a live webcam stream). We have found that
looking at live activations that change in response to user input helps build
valuable intuitions about how convnets work. The second tool enables
visualizing features at each layer of a DNN via regularized optimization in
image space. Because previous versions of this idea produced less recognizable
images, here we introduce several new regularization methods that combine to
produce qualitatively clearer, more interpretable visualizations. Both tools
are open source and work on a pre-trained convnet with minimal setup.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/dosovitskiy2015inverting/">Inverting Visual Representations with Convolutional Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Inverting Visual Representations with Convolutional Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Inverting Visual Representations with Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alexey Dosovitskiy, Thomas Brox</td>
	<td></td>
	<td><p>Feature representations, both hand-designed and learned ones, are often hard
to analyze and interpret, even when they are extracted from visual data. We
propose a new approach to study image representations by inverting them with an
up-convolutional neural network. We apply the method to shallow representations
(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our
approach provides significantly better reconstructions than existing methods,
revealing that there is surprisingly rich information contained in these
features. Inverting a deep network trained on ImageNet provides several
insights into the properties of the feature representation learned by the
network. Most strikingly, the colors and the rough contours of an image can be
reconstructed from activations in higher network layers and even from the
predicted class probabilities.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/letham2015interpretable/">Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan</td>
	<td>Annals of Applied Statistics 2015, Vol. 9, No. 3, 1350-1371</td>
	<td><p>We aim to produce predictive models that are not only accurate, but are also
interpretable to human experts. Our models are decision lists, which consist of
a series of if…then… statements (e.g., if high blood pressure, then stroke)
that discretize a high-dimensional, multivariate feature space into a series of
simple, readily interpretable decision statements. We introduce a generative
model called Bayesian Rule Lists that yields a posterior distribution over
possible decision lists. It employs a novel prior structure to encourage
sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy
on par with the current top algorithms for prediction in machine learning. Our
method is motivated by recent developments in personalized medicine, and can be
used to produce highly accurate and interpretable medical scoring systems. We
demonstrate this by producing an alternative to the CHADS$_2$ score, actively
used in clinical practice for estimating the risk of stroke in patients that
have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more
accurate.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/graziotin2015feel/">How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Daniel Graziotin, Xiaofeng Wang, Pekka Abrahamsson</td>
	<td>PeerJ Computer Science 1:e18</td>
	<td><p>Affects—emotions and moods—have an impact on cognitive activities and the
working performance of individuals. Development tasks are undertaken through
cognitive processes, yet software engineering research lacks theory on affects
and their impact on software development activities. In this paper, we report
on an interpretive study aimed at broadening our understanding of the
psychology of programming in terms of the experience of affects while
programming, and the impact of affects on programming performance. We conducted
a qualitative interpretive study based on: face-to-face open-ended interviews,
in-field observations, and e-mail exchanges. This enabled us to construct a
novel explanatory theory of the impact of affects on development performance.
The theory is explicated using an established taxonomy framework. The proposed
theory builds upon the concepts of events, affects, attractors, focus, goals,
and performance. Theoretical and practical implications are given.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/aubry2015understanding/">Understanding deep features with computer-generated imagery</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding deep features with computer-generated imagery' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding deep features with computer-generated imagery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mathieu Aubry, Bryan Russell</td>
	<td></td>
	<td><p>We introduce an approach for analyzing the variation of features generated by
convolutional neural networks (CNNs) with respect to scene factors that occur
in natural images. Such factors may include object style, 3D viewpoint, color,
and scene lighting configuration. Our approach analyzes CNN feature responses
corresponding to different scene factors by controlling for them via rendering
using a large database of 3D CAD models. The rendered images are presented to a
trained CNN and responses for different layers are studied with respect to the
input scene factors. We perform a decomposition of the responses based on
knowledge of the input scene factors and analyze the resulting components. In
particular, we quantify their relative importance in the CNN responses and
visualize them using principal component analysis. We show qualitative and
quantitative results of our study on three CNNs trained on large image
datasets: AlexNet, Places, and Oxford VGG. We observe important differences
across the networks and CNN layers for different scene factors and object
categories. Finally, we demonstrate that our analysis based on
computer-generated imagery translates to the network representation of natural
images.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/li2015visual/">Visual Saliency Based on Multiscale Deep Features</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual Saliency Based on Multiscale Deep Features' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual Saliency Based on Multiscale Deep Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Guanbin Li, Yizhou Yu</td>
	<td></td>
	<td><p>Visual saliency is a fundamental problem in both cognitive and computational
sciences, including computer vision. In this CVPR 2015 paper, we discover that
a high-quality visual saliency model can be trained with multiscale features
extracted using a popular deep learning architecture, convolutional neural
networks (CNNs), which have had many successes in visual recognition tasks. For
learning such saliency models, we introduce a neural network architecture,
which has fully connected layers on top of CNNs responsible for extracting
features at three different scales. We then propose a refinement method to
enhance the spatial coherence of our saliency results. Finally, aggregating
multiple saliency maps computed for different levels of image segmentation can
further boost the performance, yielding saliency maps better than those
generated from a single segmentation. To promote further research and
evaluation of visual saliency models, we also construct a new large database of
4447 challenging images and their pixelwise saliency annotation. Experimental
results demonstrate that our proposed method is capable of achieving
state-of-the-art performance on all public benchmarks, improving the F-Measure
by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset
(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively
on these two datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/samek2015evaluating/">Evaluating the visualization of what a Deep Neural Network has learned</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating the visualization of what a Deep Neural Network has learned' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating the visualization of what a Deep Neural Network has learned' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller</td>
	<td></td>
	<td><p>Deep Neural Networks (DNNs) have demonstrated impressive performance in
complex machine learning tasks such as image classification or speech
recognition. However, due to their multi-layer nonlinear structure, they are
not transparent, i.e., it is hard to grasp what makes them arrive at a
particular classification or recognition decision given a new unseen data
sample. Recently, several approaches have been proposed enabling one to
understand and interpret the reasoning embodied in a DNN for a single test
image. These methods quantify the ‘‘importance’’ of individual pixels wrt the
classification decision and allow a visualization in terms of a heatmap in
pixel/input space. While the usefulness of heatmaps can be judged subjectively
by a human, an objective quality measure is missing. In this paper we present a
general methodology based on region perturbation for evaluating ordered
collections of pixels such as heatmaps. We compare heatmaps computed by three
different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main
result is that the recently proposed Layer-wise Relevance Propagation (LRP)
algorithm qualitatively and quantitatively provides a better explanation of
what made a DNN arrive at a particular classification decision than the
sensitivity-based approach or the deconvolution method. We provide theoretical
arguments to explain this result and discuss its practical implications.
Finally, we investigate the use of heatmaps for unsupervised assessment of
neural network performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/zhou2015learning/">Learning Deep Features for Discriminative Localization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Features for Discriminative Localization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Deep Features for Discriminative Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>In this work, we revisit the global average pooling layer proposed in [13],
and shed light on how it explicitly enables the convolutional neural network to
have remarkable localization ability despite being trained on image-level
labels. While this technique was previously proposed as a means for
regularizing training, we find that it actually builds a generic localizable
deep representation that can be applied to a variety of tasks. Despite the
apparent simplicity of global average pooling, we are able to achieve 37.1%
top-5 error for object localization on ILSVRC 2014, which is remarkably close
to the 34.2% top-5 error achieved by a fully supervised CNN approach. We
demonstrate that our network is able to localize the discriminative image
regions on a variety of tasks despite not being trained for them</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2014</td>
	<td><a href="/publications/zhou2014object/">Object Detectors Emerge in Deep Scene CNNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Object Detectors Emerge in Deep Scene CNNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Object Detectors Emerge in Deep Scene CNNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>With the success of new computational architectures for visual processing,
such as convolutional neural networks (CNN) and access to image databases with
millions of labeled examples (e.g., ImageNet, Places), the state of the art in
computer vision is advancing rapidly. One important factor for continued
progress is to understand the representations that are learned by the inner
layers of these deep architectures. Here we show that object detectors emerge
from training CNNs to perform scene classification. As scenes are composed of
objects, the CNN for scene classification automatically discovers meaningful
objects detectors, representative of the learned scene categories. With object
detectors emerging as a result of learning to recognize scenes, our work
demonstrates that the same network can perform both scene recognition and
object localization in a single forward-pass, without ever having been
explicitly taught the notion of objects.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/zhao2014person/">Person Re-identification by Saliency Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Person Re-identification by Saliency Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Person Re-identification by Saliency Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Rui Zhao, Wanli Ouyang, Xiaogang Wang</td>
	<td></td>
	<td><p>Human eyes can recognize person identities based on small salient regions,
i.e. human saliency is distinctive and reliable in pedestrian matching across
disjoint camera views. However, such valuable information is often hidden when
computing similarities of pedestrian images with existing approaches. Inspired
by our user study result of human perception on human saliency, we propose a
novel perspective for person re-identification based on learning human saliency
and matching saliency distribution. The proposed saliency learning and matching
framework consists of four steps: (1) To handle misalignment caused by drastic
viewpoint change and pose variations, we apply adjacency constrained patch
matching to build dense correspondence between image pairs. (2) We propose two
alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a
saliency score for each image patch, through which distinctive features stand
out without using identity labels in the training procedure. (3) saliency
matching is proposed based on patch matching. Matching patches with
inconsistent saliency brings penalty, and images of the same identity are
recognized by minimizing the saliency matching cost. (4) Furthermore, saliency
matching is tightly integrated with patch matching in a unified structural
RankSVM learning framework. The effectiveness of our approach is validated on
the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the
state-of-the-art person re-identification methods on both datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/mahendran2014understanding/">Understanding Deep Image Representations by Inverting Them</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Deep Image Representations by Inverting Them' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Deep Image Representations by Inverting Them' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aravindh Mahendran, Andrea Vedaldi</td>
	<td></td>
	<td><p>Image representations, from SIFT and Bag of Visual Words to Convolutional
Neural Networks (CNNs), are a crucial component of almost any image
understanding system. Nevertheless, our understanding of them remains limited.
In this paper we conduct a direct analysis of the visual information contained
in representations by asking the following question: given an encoding of an
image, to which extent is it possible to reconstruct the image itself? To
answer this question we contribute a general framework to invert
representations. We show that this method can invert representations such as
HOG and SIFT more accurately than recent alternatives while being applicable to
CNNs too. We then use this technique to study the inverse of recent
state-of-the-art CNN image representations for the first time. Among our
findings, we show that several layers in CNNs retain photographically accurate
information about the image, with different degrees of geometric and
photometric invariance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/yosinski2014transferable/">How transferable are features in deep neural networks?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How transferable are features in deep neural networks?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How transferable are features in deep neural networks?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson</td>
	<td>Advances in Neural Information Processing Systems 27, pages 3320-3328. Dec. 2014</td>
	<td><p>Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/springenberg2014striving/">Striving for Simplicity: The All Convolutional Net</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Striving for Simplicity: The All Convolutional Net' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Striving for Simplicity: The All Convolutional Net' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller</td>
	<td></td>
	<td><p>Most modern convolutional neural networks (CNNs) used for object recognition
are built using the same principles: Alternating convolution and max-pooling
layers followed by a small number of fully connected layers. We re-evaluate the
state of the art for object recognition from small images with convolutional
networks, questioning the necessity of different components in the pipeline. We
find that max-pooling can simply be replaced by a convolutional layer with
increased stride without loss in accuracy on several image recognition
benchmarks. Following this finding – and building on other recent work for
finding simple network structures – we propose a new architecture that
consists solely of convolutional layers and yields competitive or state of the
art performance on several object recognition datasets (CIFAR-10, CIFAR-100,
ImageNet). To analyze the network we introduce a new variant of the
“deconvolution approach” for visualizing features learned by CNNs, which can be
applied to a broader range of network structures than existing approaches.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2013</td>
	<td><a href="/publications/zeiler2013visualizing/">Visualizing and Understanding Convolutional Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Convolutional Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matthew D Zeiler, Rob Fergus</td>
	<td></td>
	<td><p>Large Convolutional Network models have recently demonstrated impressive
classification performance on the ImageNet benchmark. However there is no clear
understanding of why they perform so well, or how they might be improved. In
this paper we address both issues. We introduce a novel visualization technique
that gives insight into the function of intermediate feature layers and the
operation of the classifier. We also perform an ablation study to discover the
performance contribution from different model layers. This enables us to find
model architectures that outperform Krizhevsky \etal on the ImageNet
classification benchmark. We show our ImageNet model generalizes well to other
datasets: when the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and Caltech-256 datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/simonyan2013deep/">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Karen Simonyan, Andrea Vedaldi, Andrew Zisserman</td>
	<td></td>
	<td><p>This paper addresses the visualisation of image classification models, learnt
using deep Convolutional Networks (ConvNets). We consider two visualisation
techniques, based on computing the gradient of the class score with respect to
the input image. The first one generates an image, which maximises the class
score [Erhan et al., 2009], thus visualising the notion of the class, captured
by a ConvNet. The second technique computes a class saliency map, specific to a
given image and class. We show that such maps can be employed for weakly
supervised object segmentation using classification ConvNets. Finally, we
establish the connection between the gradient-based ConvNet visualisation
methods and deconvolutional networks [Zeiler et al., 2013].</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2011</td>
	<td><a href="/publications/fabbri2011explanation-based/">Explanation-Based Auditing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation-Based Auditing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation-Based Auditing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Daniel Fabbri, Kristen LeFevre</td>
	<td>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp. 1-12 (2011)</td>
	<td><p>To comply with emerging privacy laws and regulations, it has become common
for applications like electronic health records systems (EHRs) to collect
access logs, which record each time a user (e.g., a hospital employee) accesses
a piece of sensitive data (e.g., a patient record). Using the access log, it is
easy to answer simple queries (e.g., Who accessed Alice’s medical record?), but
this often does not provide enough information. In addition to learning who
accessed their medical records, patients will likely want to understand why
each access occurred. In this paper, we introduce the problem of generating
explanations for individual records in an access log. The problem is motivated
by user-centric auditing applications, and it also provides a novel approach to
misuse detection. We develop a framework for modeling explanations which is
based on a fundamental observation: For certain classes of databases, including
EHRs, the reason for most data accesses can be inferred from data stored
elsewhere in the database. For example, if Alice has an appointment with Dr.
Dave, this information is stored in the database, and it explains why Dr. Dave
looked at Alice’s record. Large numbers of data accesses can be explained using
general forms called explanation templates. Rather than requiring an
administrator to manually specify explanation templates, we propose a set of
algorithms for automatically discovering frequent templates from the database
(i.e., those that explain a large number of accesses). We also propose
techniques for inferring collaborative user groups, which can be used to
enhance the quality of the discovered explanations. Finally, we have evaluated
our proposed techniques using an access log and data from the University of
Michigan Health System. Our results demonstrate that in practice we can provide
explanations for over 94% of data accesses in the log.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2004</td>
	<td><a href="/publications/ginsparg2004information/">Information, please... ?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Information, please... ?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Information, please... ?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Paul Ginsparg</td>
	<td>appeared as "The Truth Is Still Out There", New York Times Op-Ed, 3 Aug 2004, http://www.nytimes.com/2004/08/03/opinion/03ginsparg.html</td>
	<td><p>Stephen Hawking’s recent concession that black holes do not irretrievably
eradicate information after all has garnered much attention. It is refreshing
to see the public focused, if just for a moment, on an important conundrum that
has fascinated theoretical physicists for three decades, and prompted much
conceptual progress. The scientific issues, however, remain much less settled
than Dr. Hawking’s celebrated wager on the question.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2002</td>
	<td><a href="/publications/halpern2002causes/">Causes and Explanations: A Structural-Model Approach. Part II: Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Causes and Explanations: A Structural-Model Approach. Part II: Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Causes and Explanations: A Structural-Model Approach. Part II: Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Joseph Y. Halpern, Judea Pearl</td>
	<td></td>
	<td><p>We propose new definitions of (causal) explanation, using structural
equations to model counterfactuals. The definition is based on the notion of
actual cause, as defined and motivated in a companion paper. Essentially, an
explanation is a fact that is not known for certain but, if found to be true,
would constitute an actual cause of the fact to be explained, regardless of the
agent’s initial uncertainty. We show that the definition handles well a number
of problematic examples from the literature.</p>
</td>
	<td></td>
</tr>


</tbody></table>

<script>
var datatable;

function searchTable() {
    var hash = decodeURIComponent(window.location.hash.substr(1));
    datatable.search(hash).draw();
}


$(document).ready( function () {
    datatable = $('#allPapers').DataTable({
		paging: false,
		"order": [[ 0, 'desc' ], [ 1, 'asc' ]],
		columnDefs: [
			{
				targets: [3, 4, 5],
				visible: false,
				searchable: true
			}]
		});
    searchTable();
});

$(window).on('hashchange', function() {
  searchTable();
});
</script>


    </div>

  </body>
</html>
