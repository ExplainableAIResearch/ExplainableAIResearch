---
layout: publication
title: "AI Explainability 360: Impact and Design"
authors: Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilovic, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, Yunfeng Zhang
conference: IAAI 2022
year: 2021
additional_links: 
    - {name: "ArXiv", url: "http://arxiv.org/abs/2109.12151v1"}
tags: []
---
As artificial intelligence and machine learning algorithms become
increasingly prevalent in society, multiple stakeholders are calling for these
algorithms to provide explanations. At the same time, these stakeholders,
whether they be affected citizens, government regulators, domain experts, or
system developers, have different explanation needs. To address these needs, in
2019, we created AI Explainability 360 (Arya et al. 2020), an open source
software toolkit featuring ten diverse and state-of-the-art explainability
methods and two evaluation metrics. This paper examines the impact of the
toolkit with several case studies, statistics, and community feedback. The
different ways in which users have experienced AI Explainability 360 have
resulted in multiple types of impact and improvements in multiple metrics,
highlighted by the adoption of the toolkit by the independent LF AI & Data
Foundation. The paper also describes the flexible design of the toolkit,
examples of its use, and the significant educational material and documentation
available to its users.