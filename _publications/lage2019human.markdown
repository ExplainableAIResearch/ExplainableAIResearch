---
layout: publication
title: "Human Evaluation of Models Built for Interpretability"
authors: Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, Finale Doshi-Velez
conference: AAAI Conference on Human Computation and Crowdsourcing
year: 2019
additional_links:
   - {name: "AAAI", url: "https://ojs.aaai.org//index.php/HCOMP/article/view/5280"}
   - {name: "ArXiv", url: "https://arxiv.org/abs/1902.00006"}
   - {name: "Semantic Scholar", url: "https://www.semanticscholar.org/paper/Human-Evaluation-of-Models-Built-for-Lage-Chen/f9da80f9cf820b678bd891eedc5aa1f1a89d2103"}
tags: ["evaluation"]
---
Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than othersâ€“trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.