---
layout: publication
title: "Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead"
authors: Cynthia Rudin
conference: Nature Machine Intelligence, Vol 1, May 2019, 206-215
year: 2018
additional_links: 
    - {name: "ArXiv", url: "http://arxiv.org/abs/1811.10154v3"}
tags: []
---
Black box machine learning models are currently being used for high stakes
decision-making throughout society, causing problems throughout healthcare,
criminal justice, and in other domains. People have hoped that creating methods
for explaining these black box models will alleviate some of these problems,
but trying to \textit{explain} black box models, rather than creating models
that are \textit{interpretable} in the first place, is likely to perpetuate bad
practices and can potentially cause catastrophic harm to society. There is a
way forward -- it is to design models that are inherently interpretable. This
manuscript clarifies the chasm between explaining black boxes and using
inherently interpretable models, outlines several key reasons why explainable
black boxes should be avoided in high-stakes decisions, identifies challenges
to interpretable machine learning, and provides several example applications
where interpretable models could potentially replace black box models in
criminal justice, healthcare, and computer vision.