[
{"key": "abid2021meaningfully", "year": "2021", "title":"Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations", "abstract": "<p>Understanding and explaining the mistakes made by trained models is critical\nto many machine learning objectives, such as improving robustness, addressing\nconcept drift, and mitigating biases. However, this is often an ad hoc process\nthat involves manually looking at the model’s mistakes on many test samples and\nguessing at the underlying reasons for those incorrect predictions. In this\npaper, we propose a systematic approach, conceptual counterfactual explanations\n(CCE), that explains why a classifier makes a mistake on a particular test\nsample(s) in terms of human-understandable concepts (e.g. this zebra is\nmisclassified as a dog because of faint stripes). We base CCE on two prior\nideas: counterfactual explanations and concept activation vectors, and validate\nour approach on well-known pretrained models, showing that it explains the\nmodels’ mistakes meaningfully. In addition, for new models trained on data with\nspurious correlations, CCE accurately identifies the spurious correlation as\nthe cause of model mistakes from a single misclassified test sample. On two\nchallenging medical applications, CCE generated useful insights, confirmed by\nclinicians, into biases and mistakes the model makes in real-world settings.</p>\n", "tags": [] },
{"key": "abnar2020quantifying", "year": "2020", "title":"Quantifying Attention Flow in Transformers", "abstract": "<p>In the Transformer model, “self-attention” combines information from attended\nembeddings into the representation of the focal embedding in the next layer.\nThus, across layers of the Transformer, information originating from different\ntokens gets increasingly mixed. This makes attention weights unreliable as\nexplanations probes. In this paper, we consider the problem of quantifying this\nflow of information through self-attention. We propose two methods for\napproximating the attention to input tokens given attention weights, attention\nrollout and attention flow, as post hoc methods when we use attention weights\nas the relative relevance of the input tokens. We show that these methods give\ncomplementary views on the flow of information, and compared to raw attention,\nboth yield higher correlations with importance scores of input tokens obtained\nusing an ablation method and input gradients.</p>\n", "tags": [] },
{"key": "achtibat2022where", "year": "2022", "title":"From \"Where\" to \"What\": Towards Human-Understandable Explanations through Concept Relevance Propagation", "abstract": "<p>The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring\ntransparency to today’s powerful but opaque deep learning models. While local\nXAI methods explain individual predictions in form of attribution maps, thereby\nidentifying where important features occur (but not providing information about\nwhat they represent), global explanation techniques visualize what concepts a\nmodel has generally learned to encode. Both types of methods thus only provide\npartial insights and leave the burden of interpreting the model’s reasoning to\nthe user. Only few contemporary techniques aim at combining the principles\nbehind both local and global XAI for obtaining more informative explanations.\nThose methods, however, are often limited to specific model architectures or\nimpose additional requirements on training regimes or data and label\navailability, which renders the post-hoc application to arbitrarily pre-trained\nmodels practically impossible. In this work we introduce the Concept Relevance\nPropagation (CRP) approach, which combines the local and global perspectives of\nXAI and thus allows answering both the “where” and “what” questions for\nindividual predictions, without additional constraints imposed. We further\nintroduce the principle of Relevance Maximization for finding representative\nexamples of encoded concepts based on their usefulness to the model. We thereby\nlift the dependency on the common practice of Activation Maximization and its\nlimitations. We demonstrate the capabilities of our methods in various\nsettings, showcasing that Concept Relevance Propagation and Relevance\nMaximization lead to more human interpretable explanations and provide deep\ninsights into the model’s representations and reasoning through concept\natlases, concept composition analyses, and quantitative investigations of\nconcept subspaces and their role in fine-grained decision making.</p>\n", "tags": [] },
{"key": "adebayo2018sanity", "year": "2018", "title":"Sanity Checks for Saliency Maps", "abstract": "<p>Saliency methods have emerged as a popular tool to highlight features in an\ninput deemed relevant for the prediction of a learned model. Several saliency\nmethods have been proposed, often guided by visual appeal on image data. In\nthis work, we propose an actionable methodology to evaluate what kinds of\nexplanations a given method can and cannot provide. We find that reliance,\nsolely, on visual assessment can be misleading. Through extensive experiments\nwe show that some existing saliency methods are independent both of the model\nand of the data generating process. Consequently, methods that fail the\nproposed tests are inadequate for tasks that are sensitive to either data or\nmodel, such as, finding outliers in the data, explaining the relationship\nbetween inputs and outputs that the model learned, and debugging the model. We\ninterpret our findings through an analogy with edge detection in images, a\ntechnique that requires neither training data nor model. Theory in the case of\na linear model and a single-layer convolutional neural network supports our\nexperimental findings.</p>\n", "tags": [] },
{"key": "adebayo2020debugging", "year": "2020", "title":"Debugging Tests for Model Explanations", "abstract": "<p>We investigate whether post-hoc model explanations are effective for\ndiagnosing model errors–model debugging. In response to the challenge of\nexplaining a model’s prediction, a vast array of explanation methods have been\nproposed. Despite increasing use, it is unclear if they are effective. To\nstart, we categorize \\textit{bugs}, based on their source, into:~\\textit{data,\nmodel, and test-time} contamination bugs. For several explanation methods, we\nassess their ability to: detect spurious correlation artifacts (data\ncontamination), diagnose mislabeled training examples (data contamination),\ndifferentiate between a (partially) re-initialized model and a trained one\n(model contamination), and detect out-of-distribution inputs (test-time\ncontamination). We find that the methods tested are able to diagnose a spurious\nbackground bug, but not conclusively identify mislabeled training examples. In\naddition, a class of methods, that modify the back-propagation algorithm are\ninvariant to the higher layer parameters of a deep network; hence, ineffective\nfor diagnosing model contamination. We complement our analysis with a human\nsubject study, and find that subjects fail to identify defective models using\nattributions, but instead rely, primarily, on model predictions. Taken\ntogether, our results provide guidance for practitioners and researchers\nturning to explanations as tools for model debugging.</p>\n", "tags": [] },
{"key": "adler2016auditing", "year": "2016", "title":"Auditing Black-box Models for Indirect Influence", "abstract": "<p>Data-trained predictive models see widespread use, but for the most part they\nare used as black boxes which output a prediction or score. It is therefore\nhard to acquire a deeper understanding of model behavior, and in particular how\ndifferent features influence the model prediction. This is important when\ninterpreting the behavior of complex models, or asserting that certain\nproblematic attributes (like race or gender) are not unduly influencing\ndecisions.\n  In this paper, we present a technique for auditing black-box models, which\nlets us study the extent to which existing models take advantage of particular\nfeatures in the dataset, without knowing how the models work. Our work focuses\non the problem of indirect influence: how some features might indirectly\ninfluence outcomes via other, related features. As a result, we can find\nattribute influences even in cases where, upon further direct examination of\nthe model, the attribute is not referred to by the model at all.\n  Our approach does not require the black-box model to be retrained. This is\nimportant if (for example) the model is only accessible via an API, and\ncontrasts our work with other methods that investigate feature influence like\nfeature selection. We present experimental evidence for the effectiveness of\nour procedure using a variety of publicly available datasets and models. We\nalso validate our procedure using techniques from interpretable learning and\nfeature selection, as well as against other black-box auditing procedures.</p>\n", "tags": [] },
{"key": "agarwal2020neural", "year": "2020", "title":"Neural Additive Models: Interpretable Machine Learning with Neural Nets", "abstract": "<p>Deep neural networks (DNNs) are powerful black-box predictors that have\nachieved impressive performance on a wide variety of tasks. However, their\naccuracy comes at the cost of intelligibility: it is usually unclear how they\nmake their decisions. This hinders their applicability to high stakes\ndecision-making domains such as healthcare. We propose Neural Additive Models\n(NAMs) which combine some of the expressivity of DNNs with the inherent\nintelligibility of generalized additive models. NAMs learn a linear combination\nof neural networks that each attend to a single input feature. These networks\nare trained jointly and can learn arbitrarily complex relationships between\ntheir input feature and the output. Our experiments on regression and\nclassification datasets show that NAMs are more accurate than widely used\nintelligible models such as logistic regression and shallow decision trees.\nThey perform similarly to existing state-of-the-art generalized additive models\nin accuracy, but are more flexible because they are based on neural nets\ninstead of boosted trees. To demonstrate this, we show how NAMs can be used for\nmultitask learning on synthetic data and on the COMPAS recidivism data due to\ntheir composability, and demonstrate that the differentiability of NAMs allows\nthem to train more complex interpretable models for COVID-19.</p>\n", "tags": [] },
{"key": "agarwal2021towards", "year": "2021", "title":"Towards the Unification and Robustness of Perturbation and Gradient Based Explanations", "abstract": "<p>As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a post\nhoc manner. In this work, we analyze two popular post hoc interpretation\ntechniques: SmoothGrad which is a gradient based method, and a variant of LIME\nwhich is a perturbation based method. More specifically, we derive explicit\nclosed form expressions for the explanations output by these two methods and\nshow that they both converge to the same explanation in expectation, i.e., when\nthe number of perturbed samples used by these methods is large. We then\nleverage this connection to establish other desirable properties, such as\nrobustness, for these techniques. We also derive finite sample complexity\nbounds for the number of perturbations required for these methods to converge\nto their expected explanation. Finally, we empirically validate our theory\nusing extensive experimentation on both synthetic and real world datasets.</p>\n", "tags": [] },
{"key": "ai2022explanatory", "year": "2022", "title":"Explanatory machine learning for sequential human teaching", "abstract": "<p>The topic of comprehensibility of machine-learned theories has recently drawn\nincreasing attention. Inductive Logic Programming (ILP) uses logic programming\nto derive logic theories from small data based on abduction and induction\ntechniques. Learned theories are represented in the form of rules as\ndeclarative descriptions of obtained knowledge. In earlier work, the authors\nprovided the first evidence of a measurable increase in human comprehension\nbased on machine-learned logic rules for simple classification tasks. In a\nlater study, it was found that the presentation of machine-learned explanations\nto humans can produce both beneficial and harmful effects in the context of\ngame learning. We continue our investigation of comprehensibility by examining\nthe effects of the ordering of concept presentations on human comprehension. In\nthis work, we examine the explanatory effects of curriculum order and the\npresence of machine-learned explanations for sequential problem-solving. We\nshow that 1) there exist tasks A and B such that learning A before B has a\nbetter human comprehension with respect to learning B before A and 2) there\nexist tasks A and B such that the presence of explanations when learning A\ncontributes to improved human comprehension when subsequently learning B. We\npropose a framework for the effects of sequential teaching on comprehension\nbased on an existing definition of comprehensibility and provide evidence for\nsupport from data collected in human trials. Empirical results show that\nsequential teaching of concepts with increasing complexity a) has a beneficial\neffect on human comprehension and b) leads to human re-discovery of\ndivide-and-conquer problem-solving strategies, and c) studying machine-learned\nexplanations allows adaptations of human problem-solving strategy with better\nperformance.</p>\n", "tags": [] },
{"key": "akhtar2021attack", "year": "2021", "title":"Attack to Fool and Explain Deep Networks", "abstract": "<p>Deep visual models are susceptible to adversarial perturbations to inputs.\nAlthough these signals are carefully crafted, they still appear noise-like\npatterns to humans. This observation has led to the argument that deep visual\nrepresentation is misaligned with human perception. We counter-argue by\nproviding evidence of human-meaningful patterns in adversarial perturbations.\nWe first propose an attack that fools a network to confuse a whole category of\nobjects (source class) with a target label. Our attack also limits the\nunintended fooling by samples from non-sources classes, thereby circumscribing\nhuman-defined semantic notions for network fooling. We show that the proposed\nattack not only leads to the emergence of regular geometric patterns in the\nperturbations, but also reveals insightful information about the decision\nboundaries of deep models. Exploring this phenomenon further, we alter the\n<code class=\"language-plaintext highlighter-rouge\">adversarial' objective of our attack to use it as a tool to </code>explain’ deep\nvisual representation. We show that by careful channeling and projection of the\nperturbations computed by our method, we can visualize a model’s understanding\nof human-defined semantic notions. Finally, we exploit the explanability\nproperties of our perturbations to perform image generation, inpainting and\ninteractive image manipulation by attacking adversarialy robust\n`classifiers’.In all, our major contribution is a novel pragmatic adversarial\nattack that is subsequently transformed into a tool to interpret the visual\nmodels. The article also makes secondary contributions in terms of establishing\nthe utility of our attack beyond the adversarial objective with multiple\ninteresting applications.</p>\n", "tags": [] },
{"key": "alain2016understanding", "year": "2016", "title":"Understanding intermediate layers using linear classifier probes", "abstract": "<p>Neural network models have a reputation for being black boxes. We propose to\nmonitor the features at every layer of a model and measure how suitable they\nare for classification. We use linear classifiers, which we refer to as\n“probes”, trained entirely independently of the model itself.\n  This helps us better understand the roles and dynamics of the intermediate\nlayers. We demonstrate how this can be used to develop a better intuition about\nmodels and to diagnose potential problems.\n  We apply this technique to the popular models Inception v3 and Resnet-50.\nAmong other things, we observe experimentally that the linear separability of\nfeatures increase monotonically along the depth of the model.</p>\n", "tags": [] },
{"key": "albini2021counterfactual", "year": "2021", "title":"Counterfactual Shapley Additive Explanations", "abstract": "<p>Feature attributions are a common paradigm for model explanations due to\ntheir simplicity in assigning a single numeric score for each input feature to\na model. In the actionable recourse setting, wherein the goal of the\nexplanations is to improve outcomes for model consumers, it is often unclear\nhow feature attributions should be correctly used. With this work, we aim to\nstrengthen and clarify the link between actionable recourse and feature\nattributions. Concretely, we propose a variant of SHAP, Counterfactual SHAP\n(CF-SHAP), that incorporates counterfactual information to produce a background\ndataset for use within the marginal (a.k.a. interventional) Shapley value\nframework. We motivate the need within the actionable recourse setting for\ncareful consideration of background datasets when using Shapley values for\nfeature attributions with numerous synthetic examples. Moreover, we demonstrate\nthe efficacy of CF-SHAP by proposing and justifying a quantitative score for\nfeature attributions, counterfactual-ability, showing that as measured by this\nmetric, CF-SHAP is superior to existing methods when evaluated on public\ndatasets using tree ensembles.</p>\n", "tags": [] },
{"key": "alregib2022explanatory", "year": "2022", "title":"Explanatory Paradigms in Neural Networks", "abstract": "<p>In this article, we present a leap-forward expansion to the study of\nexplainability in neural networks by considering explanations as answers to\nabstract reasoning-based questions. With $P$ as the prediction from a neural\nnetwork, these questions are <code class=\"language-plaintext highlighter-rouge\">Why P?', </code>What if not P?’, and `Why P, rather\nthan Q?’ for a given contrast prediction $Q$. The answers to these questions\nare observed correlations, observed counterfactuals, and observed contrastive\nexplanations respectively. Together, these explanations constitute the\nabductive reasoning scheme. We term the three explanatory schemes as observed\nexplanatory paradigms. The term observed refers to the specific case of\npost-hoc explainability, when an explanatory technique explains the decision\n$P$ after a trained neural network has made the decision $P$. The primary\nadvantage of viewing explanations through the lens of abductive reasoning-based\nquestions is that explanations can be used as reasons while making decisions.\nThe post-hoc field of explainability, that previously only justified decisions,\nbecomes active by being involved in the decision making process and providing\nlimited, but relevant and contextual interventions. The contributions of this\narticle are: ($i$) realizing explanations as reasoning paradigms, ($ii$)\nproviding a probabilistic definition of observed explanations and their\ncompleteness, ($iii$) creating a taxonomy for evaluation of explanations, and\n($iv$) positioning gradient-based complete explanainability’s replicability and\nreproducibility across multiple applications and data modalities, ($v$) code\nrepositories, publicly available at\nhttps://github.com/olivesgatech/Explanatory-Paradigms.</p>\n", "tags": [] },
{"key": "alvarez-melis2018robustness", "year": "2018", "title":"On the Robustness of Interpretability Methods", "abstract": "<p>We argue that robustness of explanations—i.e., that similar inputs should\ngive rise to similar explanations—is a key desideratum for interpretability.\nWe introduce metrics to quantify robustness and demonstrate that current\nmethods do not perform well according to these metrics. Finally, we propose\nways that robustness can be enforced on existing interpretability approaches.</p>\n", "tags": [] },
{"key": "antorán2020getting", "year": "2020", "title":"Getting a CLUE: A Method for Explaining Uncertainty Estimates", "abstract": "<p>Both uncertainty estimation and interpretability are important factors for\ntrustworthy machine learning systems. However, there is little work at the\nintersection of these two areas. We address this gap by proposing a novel\nmethod for interpreting uncertainty estimates from differentiable probabilistic\nmodels, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent\nUncertainty Explanations (CLUE), indicates how to change an input, while\nkeeping it on the data manifold, such that a BNN becomes more confident about\nthe input’s prediction. We validate CLUE through 1) a novel framework for\nevaluating counterfactual explanations of uncertainty, 2) a series of ablation\nexperiments, and 3) a user study. Our experiments show that CLUE outperforms\nbaselines and enables practitioners to better understand which input patterns\nare responsible for predictive uncertainty.</p>\n", "tags": [] },
{"key": "arras2017explaining", "year": "2017", "title":"Explaining Recurrent Neural Network Predictions in Sentiment Analysis", "abstract": "<p>Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown\nto deliver insightful explanations in the form of input space relevances for\nunderstanding feed-forward neural network classification decisions. In the\npresent work, we extend the usage of LRP to recurrent neural networks. We\npropose a specific propagation rule applicable to multiplicative connections as\nthey arise in recurrent network architectures such as LSTMs and GRUs. We apply\nour technique to a word-based bi-directional LSTM model on a five-class\nsentiment prediction task, and evaluate the resulting LRP relevances both\nqualitatively and quantitatively, obtaining better results than a\ngradient-based related method which was used in previous work.</p>\n", "tags": [] },
{"key": "arras2019evaluating", "year": "2019", "title":"Evaluating Recurrent Neural Network Explanations", "abstract": "<p>Recently, several methods have been proposed to explain the predictions of\nrecurrent neural networks (RNNs), in particular of LSTMs. The goal of these\nmethods is to understand the network’s decisions by assigning to each input\nvariable, e.g., a word, a relevance indicating to which extent it contributed\nto a particular prediction. In previous works, some of these methods were not\nyet compared to one another, or were evaluated only qualitatively. We close\nthis gap by systematically and quantitatively comparing these methods in\ndifferent settings, namely (1) a toy arithmetic task which we use as a sanity\ncheck, (2) a five-class sentiment prediction of movie reviews, and besides (3)\nwe explore the usefulness of word relevances to build sentence-level\nrepresentations. Lastly, using the method that performed best in our\nexperiments, we show how specific linguistic phenomena such as the negation in\nsentiment analysis reflect in terms of relevance patterns, and how the\nrelevance visualization can help to understand the misclassification of\nindividual samples.</p>\n", "tags": [] },
{"key": "arras2019explaining", "year": "2019", "title":"Explaining and Interpreting LSTMs", "abstract": "<p>While neural networks have acted as a strong unifying force in the design of\nmodern AI systems, the neural network architectures themselves remain highly\nheterogeneous due to the variety of tasks to be solved. In this chapter, we\nexplore how to adapt the Layer-wise Relevance Propagation (LRP) technique used\nfor explaining the predictions of feed-forward networks to the LSTM\narchitecture used for sequential data modeling and forecasting. The special\naccumulators and gated interactions present in the LSTM require both a new\npropagation scheme and an extension of the underlying theoretical framework to\ndeliver faithful explanations.</p>\n", "tags": [] },
{"key": "arras2020ground", "year": "2020", "title":"Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI", "abstract": "<p>The rise of deep learning in today’s applications entailed an increasing need\nin explaining the model’s decisions beyond prediction performances in order to\nfoster trust and accountability. Recently, the field of explainable AI (XAI)\nhas developed methods that provide such explanations for already trained neural\nnetworks. In computer vision tasks such explanations, termed heatmaps,\nvisualize the contributions of individual pixels to the prediction. So far XAI\nmethods along with their heatmaps were mainly validated qualitatively via\nhuman-based assessment, or evaluated through auxiliary proxy tasks such as\npixel perturbation, weak object localization or randomization tests. Due to the\nlack of an objective and commonly accepted quality measure for heatmaps, it was\ndebatable which XAI method performs best and whether explanations can be\ntrusted at all. In the present work, we tackle the problem by proposing a\nground truth based evaluation framework for XAI methods based on the CLEVR\nvisual question answering task. Our framework provides a (1) selective, (2)\ncontrolled and (3) realistic testbed for the evaluation of neural network\nexplanations. We compare ten different explanation methods, resulting in new\ninsights about the quality and properties of XAI methods, sometimes\ncontradicting with conclusions from previous comparative studies. The CLEVR-XAI\ndataset and the benchmarking code can be found at\nhttps://github.com/ahmedmagdiosman/clevr-xai.</p>\n", "tags": [] },
{"key": "artelt2021evaluating", "year": "2021", "title":"Evaluating Robustness of Counterfactual Explanations", "abstract": "<p>Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system’s behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions – as changes to the input\n– that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput – i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n  In this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.</p>\n", "tags": [] },
{"key": "arun2020assessing", "year": "2020", "title":"Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging", "abstract": "<p>Saliency maps have become a widely used method to make deep learning models\nmore interpretable by providing post-hoc explanations of classifiers through\nidentification of the most pertinent areas of the input medical image. They are\nincreasingly being used in medical imaging to provide clinically plausible\nexplanations for the decisions the neural network makes. However, the utility\nand robustness of these visualization maps has not yet been rigorously examined\nin the context of medical imaging. We posit that trustworthiness in this\ncontext requires 1) localization utility, 2) sensitivity to model weight\nrandomization, 3) repeatability, and 4) reproducibility. Using the localization\ninformation available in two large public radiology datasets, we quantify the\nperformance of eight commonly used saliency map approaches for the above\ncriteria using area under the precision-recall curves (AUPRC) and structural\nsimilarity index (SSIM), comparing their performance to various baseline\nmeasures. Using our framework to quantify the trustworthiness of saliency maps,\nwe show that all eight saliency map techniques fail at least one of the\ncriteria and are, in most cases, less trustworthy when compared to the\nbaselines. We suggest that their usage in the high-risk domain of medical\nimaging warrants additional scrutiny and recommend that detection or\nsegmentation models be used if localization is the desired output of the\nnetwork. Additionally, to promote reproducibility of our findings, we provide\nthe code we used for all tests performed in this work at this link:\nhttps://github.com/QTIM-Lab/Assessing-Saliency-Maps.</p>\n", "tags": [] },
{"key": "arya2019one", "year": "2019", "title":"One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques", "abstract": "<p>As artificial intelligence and machine learning algorithms make further\ninroads into society, calls are increasing from multiple stakeholders for these\nalgorithms to explain their outputs. At the same time, these stakeholders,\nwhether they be affected citizens, government regulators, domain experts, or\nsystem developers, present different requirements for explanations. Toward\naddressing these needs, we introduce AI Explainability 360\n(http://aix360.mybluemix.net/), an open-source software toolkit featuring eight\ndiverse and state-of-the-art explainability methods and two evaluation metrics.\nEqually important, we provide a taxonomy to help entities requiring\nexplanations to navigate the space of explanation methods, not only those in\nthe toolkit but also in the broader literature on explainability. For data\nscientists and other users of the toolkit, we have implemented an extensible\nsoftware architecture that organizes methods according to their place in the AI\nmodeling pipeline. We also discuss enhancements to bring research innovations\ncloser to consumers of explanations, ranging from simplified, more accessible\nversions of algorithms, to tutorials and an interactive web demo to introduce\nAI explainability to different audiences and application domains. Together, our\ntoolkit and taxonomy can help identify gaps where more explainability methods\nare needed and provide a platform to incorporate them as they are developed.</p>\n", "tags": [] },
{"key": "arya2021ai", "year": "2021", "title":"AI Explainability 360: Impact and Design", "abstract": "<p>As artificial intelligence and machine learning algorithms become\nincreasingly prevalent in society, multiple stakeholders are calling for these\nalgorithms to provide explanations. At the same time, these stakeholders,\nwhether they be affected citizens, government regulators, domain experts, or\nsystem developers, have different explanation needs. To address these needs, in\n2019, we created AI Explainability 360 (Arya et al. 2020), an open source\nsoftware toolkit featuring ten diverse and state-of-the-art explainability\nmethods and two evaluation metrics. This paper examines the impact of the\ntoolkit with several case studies, statistics, and community feedback. The\ndifferent ways in which users have experienced AI Explainability 360 have\nresulted in multiple types of impact and improvements in multiple metrics,\nhighlighted by the adoption of the toolkit by the independent LF AI &amp; Data\nFoundation. The paper also describes the flexible design of the toolkit,\nexamples of its use, and the significant educational material and documentation\navailable to its users.</p>\n", "tags": [] },
{"key": "asher2020adequate", "year": "2020", "title":"Adequate and fair explanations", "abstract": "<p>Explaining sophisticated machine-learning based systems is an important issue\nat the foundations of AI. Recent efforts have shown various methods for\nproviding explanations. These approaches can be broadly divided into two\nschools: those that provide a local and human interpreatable approximation of a\nmachine learning algorithm, and logical approaches that exactly characterise\none aspect of the decision. In this paper we focus upon the second school of\nexact explanations with a rigorous logical foundation. There is an\nepistemological problem with these exact methods. While they can furnish\ncomplete explanations, such explanations may be too complex for humans to\nunderstand or even to write down in human readable form. Interpretability\nrequires epistemically accessible explanations, explanations humans can grasp.\nYet what is a sufficiently complete epistemically accessible explanation still\nneeds clarification. We do this here in terms of counterfactuals, following\n[Wachter et al., 2017]. With counterfactual explanations, many of the\nassumptions needed to provide a complete explanation are left implicit. To do\nso, counterfactual explanations exploit the properties of a particular data\npoint or sample, and as such are also local as well as partial explanations. We\nexplore how to move from local partial explanations to what we call complete\nlocal explanations and then to global ones. But to preserve accessibility we\nargue for the need for partiality. This partiality makes it possible to hide\nexplicit biases present in the algorithm that may be injurious or unfair.We\ninvestigate how easy it is to uncover these biases in providing complete and\nfair explanations by exploiting the structure of the set of counterfactuals\nproviding a complete local explanation.</p>\n", "tags": [] },
{"key": "atanasova2020generating", "year": "2020", "title":"Generating Fact Checking Explanations", "abstract": "<p>Most existing work on automated fact checking is concerned with predicting\nthe veracity of claims based on metadata, social network spread, language used\nin claims, and, more recently, evidence supporting or denying claims. A crucial\npiece of the puzzle that is still missing is to understand how to automate the\nmost elaborate part of the process – generating justifications for verdicts on\nclaims. This paper provides the first study of how these explanations can be\ngenerated automatically based on available claim context, and how this task can\nbe modelled jointly with veracity prediction. Our results indicate that\noptimising both objectives at the same time, rather than training them\nseparately, improves the performance of a fact checking system. The results of\na manual evaluation further suggest that the informativeness, coverage and\noverall quality of the generated explanations are also improved in the\nmulti-task model.</p>\n", "tags": [] },
{"key": "aubry2015understanding", "year": "2015", "title":"Understanding deep features with computer-generated imagery", "abstract": "<p>We introduce an approach for analyzing the variation of features generated by\nconvolutional neural networks (CNNs) with respect to scene factors that occur\nin natural images. Such factors may include object style, 3D viewpoint, color,\nand scene lighting configuration. Our approach analyzes CNN feature responses\ncorresponding to different scene factors by controlling for them via rendering\nusing a large database of 3D CAD models. The rendered images are presented to a\ntrained CNN and responses for different layers are studied with respect to the\ninput scene factors. We perform a decomposition of the responses based on\nknowledge of the input scene factors and analyze the resulting components. In\nparticular, we quantify their relative importance in the CNN responses and\nvisualize them using principal component analysis. We show qualitative and\nquantitative results of our study on three CNNs trained on large image\ndatasets: AlexNet, Places, and Oxford VGG. We observe important differences\nacross the networks and CNN layers for different scene factors and object\ncategories. Finally, we demonstrate that our analysis based on\ncomputer-generated imagery translates to the network representation of natural\nimages.</p>\n", "tags": [] },
{"key": "augenstein2021towards", "year": "2021", "title":"Towards Explainable Fact Checking", "abstract": "<p>The past decade has seen a substantial rise in the amount of mis- and\ndisinformation online, from targeted disinformation campaigns to influence\npolitics, to the unintentional spreading of misinformation about public health.\nThis development has spurred research in the area of automatic fact checking,\nfrom approaches to detect check-worthy claims and determining the stance of\ntweets towards claims, to methods to determine the veracity of claims given\nevidence documents. These automatic methods are often content-based, using\nnatural language processing methods, which in turn utilise deep neural networks\nto learn higher-order features from text in order to make predictions. As deep\nneural networks are black-box models, their inner workings cannot be easily\nexplained. At the same time, it is desirable to explain how they arrive at\ncertain decisions, especially if they are to be used for decision making. While\nthis has been known for some time, the issues this raises have been exacerbated\nby models increasing in size, and by EU legislation requiring models to be used\nfor decision making to provide explanations, and, very recently, by legislation\nrequiring online platforms operating in the EU to provide transparent reporting\non their services. Despite this, current solutions for explainability are still\nlacking in the area of fact checking. This thesis presents my research on\nautomatic fact checking, including claim check-worthiness detection, stance\ndetection and veracity prediction. Its contributions go beyond fact checking,\nwith the thesis proposing more general machine learning solutions for natural\nlanguage processing in the area of learning with limited labelled data.\nFinally, the thesis presents some first solutions for explainable fact\nchecking.</p>\n", "tags": [] },
{"key": "bai2019rectified", "year": "2019", "title":"Rectified Decision Trees: Towards Interpretability, Compression and Empirical Soundness", "abstract": "<p>How to obtain a model with good interpretability and performance has always\nbeen an important research topic. In this paper, we propose rectified decision\ntrees (ReDT), a knowledge distillation based decision trees rectification with\nhigh interpretability, small model size, and empirical soundness. Specifically,\nwe extend the impurity calculation and the pure ending condition of the\nclassical decision tree to propose a decision tree extension that allows the\nuse of soft labels generated by a well-trained teacher model in training and\nprediction process. It is worth noting that for the acquisition of soft labels,\nwe propose a new multiple cross-validation based method to reduce the effects\nof randomness and overfitting. These approaches ensure that ReDT retains\nexcellent interpretability and even achieves fewer nodes than the decision tree\nin the aspect of compression while having relatively good performance. Besides,\nin contrast to traditional knowledge distillation, back propagation of the\nstudent model is not necessarily required in ReDT, which is an attempt of a new\nknowledge distillation approach. Extensive experiments are conducted, which\ndemonstrates the superiority of ReDT in interpretability, compression, and\nempirical soundness.</p>\n", "tags": [] },
{"key": "bai2020attentions", "year": "2020", "title":"Why Attentions May Not Be Interpretable?", "abstract": "<p>Attention-based methods have played important roles in model interpretations,\nwhere the calculated attention weights are expected to highlight the critical\nparts of inputs~(e.g., keywords in sentences). However, recent research found\nthat attention-as-importance interpretations often do not work as we expected.\nFor example, learned attention weights sometimes highlight less meaningful\ntokens like “[SEP]”, “,”, and “.”, and are frequently uncorrelated with other\nfeature importance indicators like gradient-based measures. A recent debate\nover whether attention is an explanation or not has drawn considerable\ninterest. In this paper, we demonstrate that one root cause of this phenomenon\nis the combinatorial shortcuts, which means that, in addition to the\nhighlighted parts, the attention weights themselves may carry extra information\nthat could be utilized by downstream models after attention layers. As a\nresult, the attention weights are no longer pure importance indicators. We\ntheoretically analyze combinatorial shortcuts, design one intuitive experiment\nto show their existence, and propose two methods to mitigate this issue. We\nconduct empirical studies on attention-based interpretation models. The results\nshow that the proposed methods can effectively improve the interpretability of\nattention mechanisms.</p>\n", "tags": [] },
{"key": "baniecki2020grammar", "year": "2020", "title":"The Grammar of Interactive Explanatory Model Analysis", "abstract": "<p>The growing need for in-depth analysis of predictive models leads to a series\nof new methods for explaining their local and global properties. Which of these\nmethods is the best? It turns out that this is an ill-posed question. One\ncannot sufficiently explain a black-box machine learning model using a single\nmethod that gives only one perspective. Isolated explanations are prone to\nmisunderstanding, leading to wrong or simplistic reasoning. This problem is\nknown as the Rashomon effect and refers to diverse, even contradictory,\ninterpretations of the same phenomenon. Surprisingly, most methods developed\nfor explainable and responsible machine learning focus on a single-aspect of\nthe model behavior. In contrast, we showcase the problem of explainability as\nan interactive and sequential analysis of a model. This paper proposes how\ndifferent Explanatory Model Analysis (EMA) methods complement each other and\ndiscusses why it is essential to juxtapose them. The introduced process of\nInteractive EMA (IEMA) derives from the algorithmic side of explainable machine\nlearning and aims to embrace ideas developed in cognitive sciences. We\nformalize the grammar of IEMA to describe potential human-model dialogues. It\nis implemented in a widely used human-centered open-source software framework\nthat adopts interactivity, customizability and automation as its main traits.\nWe conduct a user study to evaluate the usefulness of IEMA, which indicates\nthat an interactive sequential analysis of a model increases the performance\nand confidence of human decision making.</p>\n", "tags": [] },
{"key": "bansal2019case", "year": "2019", "title":"A Case for Backward Compatibility for Human-AI Teams", "abstract": "<p>AI systems are being deployed to support human decision making in high-stakes\ndomains. In many cases, the human and AI form a team, in which the human makes\ndecisions after reviewing the AI’s inferences. A successful partnership\nrequires that the human develops insights into the performance of the AI\nsystem, including its failures. We study the influence of updates to an AI\nsystem in this setting. While updates can increase the AI’s predictive\nperformance, they may also lead to changes that are at odds with the user’s\nprior experiences and confidence in the AI’s inferences, hurting therefore the\noverall team performance. We introduce the notion of the compatibility of an AI\nupdate with prior user experience and present methods for studying the role of\ncompatibility in human-AI teams. Empirical results on three high-stakes domains\nshow that current machine learning algorithms do not produce compatible\nupdates. We propose a re-training objective to improve the compatibility of an\nupdate by penalizing new errors. The objective offers full leverage of the\nperformance/compatibility tradeoff, enabling more compatible yet accurate\nupdates.</p>\n", "tags": [] },
{"key": "barbado2019rule", "year": "2019", "title":"Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM", "abstract": "<p>OneClass SVM is a popular method for unsupervised anomaly detection. As many\nother methods, it suffers from the black box problem: it is difficult to\njustify, in an intuitive and simple manner, why the decision frontier is\nidentifying data points as anomalous or non anomalous. Such type of problem is\nbeing widely addressed for supervised models. However, it is still an uncharted\narea for unsupervised learning. In this paper, we evaluate several rule\nextraction techniques over OneClass SVM models, as well as present alternative\ndesigns for some of those algorithms. Together with that, we propose algorithms\nto compute metrics related with eXplainable Artificial Intelligence (XAI)\nregarding the “comprehensibility”, “representativeness”, “stability” and\n“diversity” of the extracted rules. We evaluate our proposals with different\ndatasets, including real-world data coming from industry. With this, our\nproposal contributes to extend XAI techniques to unsupervised machine learning\nmodels.</p>\n", "tags": [] },
{"key": "barr2020towards", "year": "2020", "title":"Towards Ground Truth Explainability on Tabular Data", "abstract": "<p>In data science, there is a long history of using synthetic data for method\ndevelopment, feature selection and feature engineering. Our current interest in\nsynthetic data comes from recent work in explainability. Today’s datasets are\ntypically larger and more complex - requiring less interpretable models. In the\nsetting of \\textit{post hoc} explainability, there is no ground truth for\nexplanations. Inspired by recent work in explaining image classifiers that does\nprovide ground truth, we propose a similar solution for tabular data. Using\ncopulas, a concise specification of the desired statistical properties of a\ndataset, users can build intuition around explainability using controlled data\nsets and experimentation. The current capabilities are demonstrated on three\nuse cases: one dimensional logistic regression, impact of correlation from\ninformative features, impact of correlation from redundant variables.</p>\n", "tags": [] },
{"key": "barr2021counterfactual", "year": "2021", "title":"Counterfactual Explanations via Latent Space Projection and Interpolation", "abstract": "<p>Counterfactual explanations represent the minimal change to a data sample\nthat alters its predicted classification, typically from an unfavorable initial\nclass to a desired target class. Counterfactuals help answer questions such as\n“what needs to change for this application to get accepted for a loan?”. A\nnumber of recently proposed approaches to counterfactual generation give\nvarying definitions of “plausible” counterfactuals and methods to generate\nthem. However, many of these methods are computationally intensive and provide\nunconvincing explanations. Here we introduce SharpShooter, a method for binary\nclassification that starts by creating a projected version of the input that\nclassifies as the target class. Counterfactual candidates are then generated in\nlatent space on the interpolation line between the input and its projection. We\nthen demonstrate that our framework translates core characteristics of a sample\nto its counterfactual through the use of learned representations. Furthermore,\nwe show that SharpShooter is competitive across common quality metrics on\ntabular and image datasets while being orders of magnitude faster than two\ncomparable methods and excels at measures of realism, making it well-suited for\nhigh velocity machine learning applications which require timely explanations.</p>\n", "tags": [] },
{"key": "bastings2020elephant", "year": "2020", "title":"The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?", "abstract": "<p>There is a recent surge of interest in using attention as explanation of\nmodel predictions, with mixed evidence on whether attention can be used as\nsuch. While attention conveniently gives us one weight per input token and is\neasily extracted, it is often unclear toward what goal it is used as\nexplanation. We find that often that goal, whether explicitly stated or not, is\nto find out what input tokens are the most relevant to a prediction, and that\nthe implied user for the explanation is a model developer. For this goal and\nuser, we argue that input saliency methods are better suited, and that there\nare no compelling reasons to use attention, despite the coincidence that it\nprovides a weight for each input. With this position paper, we hope to shift\nsome of the recent focus on attention to saliency methods, and for authors to\nclearly state the goal and user for their explanations.</p>\n", "tags": [] },
{"key": "bastings2021will", "year": "2021", "title":"\"Will You Find These Shortcuts?\" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification", "abstract": "<p>Feature attribution a.k.a. input salience methods which assign an importance\nscore to a feature are abundant but may produce surprisingly different results\nfor the same model on the same input. While differences are expected if\ndisparate definitions of importance are assumed, most methods claim to provide\nfaithful attributions and point at the features most relevant for a model’s\nprediction. Existing work on faithfulness evaluation is not conclusive and does\nnot provide a clear answer as to how different methods are to be compared.\nFocusing on text classification and the model debugging scenario, our main\ncontribution is a protocol for faithfulness evaluation that makes use of\npartially synthetic data to obtain ground truth for feature importance ranking.\nFollowing the protocol, we do an in-depth analysis of four standard salience\nmethod classes on a range of datasets and shortcuts for BERT and LSTM models\nand demonstrate that some of the most popular method configurations provide\npoor results even for simplest shortcuts. We recommend following the protocol\nfor each new task and model combination to find the best method for identifying\nshortcuts.</p>\n", "tags": [] },
{"key": "bau2017network", "year": "2017", "title":"Network Dissection: Quantifying Interpretability of Deep Visual Representations", "abstract": "<p>We propose a general framework called Network Dissection for quantifying the\ninterpretability of latent representations of CNNs by evaluating the alignment\nbetween individual hidden units and a set of semantic concepts. Given any CNN\nmodel, the proposed method draws on a broad data set of visual concepts to\nscore the semantics of hidden units at each intermediate convolutional layer.\nThe units with semantics are given labels across a range of objects, parts,\nscenes, textures, materials, and colors. We use the proposed method to test the\nhypothesis that interpretability of units is equivalent to random linear\ncombinations of units, then we apply our method to compare the latent\nrepresentations of various networks when trained to solve different supervised\nand self-supervised training tasks. We further analyze the effect of training\niterations, compare networks trained with different initializations, examine\nthe impact of network depth and width, and measure the effect of dropout and\nbatch normalization on the interpretability of deep visual representations. We\ndemonstrate that the proposed method can shed light on characteristics of CNN\nmodels and training methods that go beyond measurements of their discriminative\npower.</p>\n", "tags": [] },
{"key": "begley2020explainability", "year": "2020", "title":"Explainability for fair machine learning", "abstract": "<p>As the decisions made or influenced by machine learning models increasingly\nimpact our lives, it is crucial to detect, understand, and mitigate unfairness.\nBut even simply determining what “unfairness” should mean in a given context is\nnon-trivial: there are many competing definitions, and choosing between them\noften requires a deep understanding of the underlying task. It is thus tempting\nto use model explainability to gain insights into model fairness, however\nexisting explainability tools do not reliably indicate whether a model is\nindeed fair. In this work we present a new approach to explaining fairness in\nmachine learning, based on the Shapley value paradigm. Our fairness\nexplanations attribute a model’s overall unfairness to individual input\nfeatures, even in cases where the model does not operate on sensitive\nattributes directly. Moreover, motivated by the linearity of Shapley\nexplainability, we propose a meta algorithm for applying existing training-time\nfairness interventions, wherein one trains a perturbation to the original\nmodel, rather than a new model entirely. By explaining the original model, the\nperturbation, and the fair-corrected model, we gain insight into the\naccuracy-fairness trade-off that is being made by the intervention. We further\nshow that this meta algorithm enjoys both flexibility and stability benefits\nwith no loss in performance.</p>\n", "tags": [] },
{"key": "benchekroun2020need", "year": "2020", "title":"The Need for Standardized Explainability", "abstract": "<p>Explainable AI (XAI) is paramount in industry-grade AI; however existing\nmethods fail to address this necessity, in part due to a lack of\nstandardisation of explainability methods. The purpose of this paper is to\noffer a perspective on the current state of the area of explainability, and to\nprovide novel definitions for Explainability and Interpretability to begin\nstandardising this area of research. To do so, we provide an overview of the\nliterature on explainability, and of the existing methods that are already\nimplemented. Finally, we offer a tentative taxonomy of the different\nexplainability methods, opening the door to future research.</p>\n", "tags": [] },
{"key": "benshmuel2022meet", "year": "2022", "title":"Meet You Halfway: Explaining Deep Learning Mysteries", "abstract": "<p>Deep neural networks perform exceptionally well on various learning tasks\nwith state-of-the-art results. While these models are highly expressive and\nachieve impressively accurate solutions with excellent generalization\nabilities, they are susceptible to minor perturbations. Samples that suffer\nsuch perturbations are known as “adversarial examples”. Even though deep\nlearning is an extensively researched field, many questions about the nature of\ndeep learning models remain unanswered. In this paper, we introduce a new\nconceptual framework attached with a formal description that aims to shed light\non the network’s behavior and interpret the behind-the-scenes of the learning\nprocess. Our framework provides an explanation for inherent questions\nconcerning deep learning. Particularly, we clarify: (1) Why do neural networks\nacquire generalization abilities? (2) Why do adversarial examples transfer\nbetween different models?. We provide a comprehensive set of experiments that\nsupport this new framework, as well as its underlying theory.</p>\n", "tags": [] },
{"key": "berger2020visually", "year": "2020", "title":"Visually Analyzing Contextualized Embeddings", "abstract": "<p>In this paper we introduce a method for visually analyzing contextualized\nembeddings produced by deep neural network-based language models. Our approach\nis inspired by linguistic probes for natural language processing, where tasks\nare designed to probe language models for linguistic structure, such as\nparts-of-speech and named entities. These approaches are largely confirmatory,\nhowever, only enabling a user to test for information known a priori. In this\nwork, we eschew supervised probing tasks, and advocate for unsupervised probes,\ncoupled with visual exploration techniques, to assess what is learned by\nlanguage models. Specifically, we cluster contextualized embeddings produced\nfrom a large text corpus, and introduce a visualization design based on this\nclustering and textual structure - cluster co-occurrences, cluster spans, and\ncluster-word membership - to help elicit the functionality of, and relationship\nbetween, individual clusters. User feedback highlights the benefits of our\ndesign in discovering different types of linguistic structures.</p>\n", "tags": [] },
{"key": "besold2018what", "year": "2018", "title":"The What, the Why, and the How of Artificial Explanations in Automated Decision-Making", "abstract": "<p>The increasing incorporation of Artificial Intelligence in the form of\nautomated systems into decision-making procedures highlights not only the\nimportance of decision theory for automated systems but also the need for these\ndecision procedures to be explainable to the people involved in them.\nTraditional realist accounts of explanation, wherein explanation is a relation\nthat holds (or does not hold) eternally between an explanans and an\nexplanandum, are not adequate to account for the notion of explanation required\nfor artificial decision procedures. We offer an alternative account of\nexplanation as used in the context of automated decision-making that makes\nexplanation an epistemic phenomenon, and one that is dependent on context. This\naccount of explanation better accounts for the way that we talk about, and use,\nexplanations and derived concepts, such as `explanatory power’, and also allows\nus to differentiate between reasons or causes on the one hand, which do not\nneed to have an epistemic aspect, and explanations on the other, which do have\nsuch an aspect. Against this theoretical backdrop we then review existing\napproaches to explanation in Artificial Intelligence and Machine Learning, and\nsuggest desiderata which truly explainable decision systems should fulfill.</p>\n", "tags": [] },
{"key": "bhatt2019explainable", "year": "2019", "title":"Explainable Machine Learning in Deployment", "abstract": "<p>Explainable machine learning offers the potential to provide stakeholders\nwith insights into model behavior by using various methods such as feature\nimportance scores, counterfactual explanations, or influential training data.\nYet there is little understanding of how organizations use these methods in\npractice. This study explores how organizations view and use explainability for\nstakeholder consumption. We find that, currently, the majority of deployments\nare not for end users affected by the model but rather for machine learning\nengineers, who use explainability to debug the model itself. There is thus a\ngap between explainability in practice and the goal of transparency, since\nexplanations primarily serve internal stakeholders rather than external ones.\nOur study synthesizes the limitations of current explainability techniques that\nhamper their use for end users. To facilitate end user interaction, we develop\na framework for establishing clear goals for explainability. We end by\ndiscussing concerns raised regarding explainability.</p>\n", "tags": [] },
{"key": "bhatt2020evaluating", "year": "2020", "title":"Evaluating and Aggregating Feature-based Model Explanations", "abstract": "<p>A feature-based model explanation denotes how much each input feature\ncontributes to a model’s output for a given data point. As the number of\nproposed explanation functions grows, we lack quantitative evaluation criteria\nto help practitioners know when to use which explanation function. This paper\nproposes quantitative evaluation criteria for feature-based explanations: low\nsensitivity, high faithfulness, and low complexity. We devise a framework for\naggregating explanation functions. We develop a procedure for learning an\naggregate explanation function with lower complexity and then derive a new\naggregate Shapley value explanation function that minimizes sensitivity.</p>\n", "tags": [] },
{"key": "biessmann2021quality", "year": "2021", "title":"Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated", "abstract": "<p>The field explainable artificial intelligence (XAI) has brought about an\narsenal of methods to render Machine Learning (ML) predictions more\ninterpretable. But how useful explanations provided by transparent ML methods\nare for humans remains difficult to assess. Here we investigate the quality of\ninterpretable computer vision algorithms using techniques from psychophysics.\nIn crowdsourced annotation tasks we study the impact of different\ninterpretability approaches on annotation accuracy and task time. We compare\nthese quality metrics with classical XAI, automated quality metrics. Our\nresults demonstrate that psychophysical experiments allow for robust quality\nassessment of transparency in machine learning. Interestingly the quality\nmetrics computed without humans in the loop did not provide a consistent\nranking of interpretability methods nor were they representative for how useful\nan explanation was for humans. These findings highlight the potential of\nmethods from classical psychophysics for modern machine learning applications.\nWe hope that our results provide convincing arguments for evaluating\ninterpretability in its natural habitat, human-ML interaction, if the goal is\nto obtain an authentic assessment of interpretability.</p>\n", "tags": [] },
{"key": "bodell2019interpretable", "year": "2019", "title":"Interpretable Word Embeddings via Informative Priors", "abstract": "<p>Word embeddings have demonstrated strong performance on NLP tasks. However,\nlack of interpretability and the unsupervised nature of word embeddings have\nlimited their use within computational social science and digital humanities.\nWe propose the use of informative priors to create interpretable and\ndomain-informed dimensions for probabilistic word embeddings. Experimental\nresults show that sensible priors can capture latent semantic concepts better\nthan or on-par with the current state of the art, while retaining the\nsimplicity and generalizability of using priors.</p>\n", "tags": [] },
{"key": "bodria2021benchmarking", "year": "2021", "title":"Benchmarking and Survey of Explanation Methods for Black Box Models", "abstract": "<p>The widespread adoption of black-box models in Artificial Intelligence has\nenhanced the need for explanation methods to reveal how these obscure models\nreach specific decisions. Retrieving explanations is fundamental to unveil\npossible biases and to resolve practical or ethical issues. Nowadays, the\nliterature is full of methods with different explanations. We provide a\ncategorization of explanation methods based on the type of explanation\nreturned. We present the most recent and widely used explainers, and we show a\nvisual comparison among explanations and a quantitative benchmarking.</p>\n", "tags": [] },
{"key": "braşoveanu2022visualizing", "year": "2022", "title":"Visualizing and Explaining Language Models", "abstract": "<p>During the last decade, Natural Language Processing has become, after\nComputer Vision, the second field of Artificial Intelligence that was massively\nchanged by the advent of Deep Learning. Regardless of the architecture, the\nlanguage models of the day need to be able to process or generate text, as well\nas predict missing words, sentences or relations depending on the task. Due to\ntheir black-box nature, such models are difficult to interpret and explain to\nthird parties. Visualization is often the bridge that language model designers\nuse to explain their work, as the coloring of the salient words and phrases,\nclustering or neuron activations can be used to quickly understand the\nunderlying models. This paper showcases the techniques used in some of the most\npopular Deep Learning for NLP visualizations, with a special focus on\ninterpretability and explainability.</p>\n", "tags": [] },
{"key": "buchholz2022means-end", "year": "2022", "title":"A Means-End Account of Explainable Artificial Intelligence", "abstract": "<p>Explainable artificial intelligence (XAI) seeks to produce explanations for\nthose machine learning methods which are deemed opaque. However, there is\nconsiderable disagreement about what this means and how to achieve it. Authors\ndisagree on what should be explained (topic), to whom something should be\nexplained (stakeholder), how something should be explained (instrument), and\nwhy something should be explained (goal). In this paper, I employ insights from\nmeans-end epistemology to structure the field. According to means-end\nepistemology, different means ought to be rationally adopted to achieve\ndifferent epistemic ends. Applied to XAI, different topics, stakeholders, and\ngoals thus require different instruments. I call this the means-end account of\nXAI. The means-end account has a descriptive and a normative component: on the\none hand, I show how the specific means-end relations give rise to a taxonomy\nof existing contributions to the field of XAI; on the other hand, I argue that\nthe suitability of XAI methods can be assessed by analyzing whether they are\nprescribed by a given topic, stakeholder, and goal.</p>\n", "tags": [] },
{"key": "buhrmester2019analysis", "year": "2019", "title":"Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey", "abstract": "<p>Deep Learning is a state-of-the-art technique to make inference on extensive\nor complex data. As a black box model due to their multilayer nonlinear\nstructure, Deep Neural Networks are often criticized to be non-transparent and\ntheir predictions not traceable by humans. Furthermore, the models learn from\nartificial datasets, often with bias or contaminated discriminating content.\nThrough their increased distribution, decision-making algorithms can contribute\npromoting prejudge and unfairness which is not easy to notice due to lack of\ntransparency. Hence, scientists developed several so-called explanators or\nexplainers which try to point out the connection between input and output to\nrepresent in a simplified way the inner structure of machine learning black\nboxes. In this survey we differ the mechanisms and properties of explaining\nsystems for Deep Neural Networks for Computer Vision tasks. We give a\ncomprehensive overview about taxonomy of related studies and compare several\nsurvey papers that deal with explainability in general. We work out the\ndrawbacks and gaps and summarize further research ideas.</p>\n", "tags": [] },
{"key": "bykov2020much", "year": "2020", "title":"How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks", "abstract": "<p>Explainable AI (XAI) aims to provide interpretations for predictions made by\nlearning machines, such as deep neural networks, in order to make the machines\nmore transparent for the user and furthermore trustworthy also for applications\nin e.g. safety-critical areas. So far, however, no methods for quantifying\nuncertainties of explanations have been conceived, which is problematic in\ndomains where a high confidence in explanations is a prerequisite. We therefore\ncontribute by proposing a new framework that allows to convert any arbitrary\nexplanation method for neural networks into an explanation method for Bayesian\nneural networks, with an in-built modeling of uncertainties. Within the\nBayesian framework a network’s weights follow a distribution that extends\nstandard single explanation scores and heatmaps to distributions thereof, in\nthis manner translating the intrinsic network model uncertainties into a\nquantification of explanation uncertainties. This allows us for the first time\nto carve out uncertainties associated with a model explanation and subsequently\ngauge the appropriate level of explanation confidence for a user (using\npercentiles). We demonstrate the effectiveness and usefulness of our approach\nextensively in various experiments, both qualitatively and quantitatively.</p>\n", "tags": [] },
{"key": "bäuerle2020explornn", "year": "2020", "title":"exploRNN: Understanding Recurrent Neural Networks through Visual Exploration", "abstract": "<p>Due to the success of deep learning (DL) and its growing job market, students\nand researchers from many areas are interested in learning about DL\ntechnologies. Visualization has proven to be of great help during this learning\nprocess. While most current educational visualizations are targeted towards one\nspecific architecture or use case, recurrent neural networks (RNNs), which are\ncapable of processing sequential data, are not covered yet. This is despite the\nfact that tasks on sequential data, such as text and function analysis, are at\nthe forefront of DL research. Therefore, we propose exploRNN, the first\ninteractively explorable educational visualization for RNNs. On the basis of\nmaking learning easier and more fun, we define educational objectives targeted\ntowards understanding RNNs. We use these objectives to form guidelines for the\nvisual design process. By means of exploRNN, which is accessible online, we\nprovide an overview of the training process of RNNs at a coarse level, while\nalso allowing a detailed inspection of the data flow within LSTM cells. In an\nempirical study, we assessed 37 subjects in a between-subjects design to\ninvestigate the learning outcomes and cognitive load of exploRNN compared to a\nclassic text-based learning environment. While learners in the text group are\nahead in superficial knowledge acquisition, exploRNN is particularly helpful\nfor deeper understanding of the learning content. In addition, the complex\ncontent in exploRNN is perceived as significantly easier and causes less\nextraneous load than in the text group. The study shows that for difficult\nlearning material such as recurrent networks, where deep understanding is\nimportant, interactive visualizations such as exploRNN can be helpful.</p>\n", "tags": [] },
{"key": "böhle2021convolutional", "year": "2021", "title":"Convolutional Dynamic Alignment Networks for Interpretable Classifications", "abstract": "<p>We introduce a new family of neural network models called Convolutional\nDynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a\nhigh degree of inherent interpretability. Their core building blocks are\nDynamic Alignment Units (DAUs), which linearly transform their input with\nweight vectors that dynamically align with task-relevant patterns. As a result,\nCoDA-Nets model the classification prediction through a series of\ninput-dependent linear transformations, allowing for linear decomposition of\nthe output into individual input contributions. Given the alignment of the\nDAUs, the resulting contribution maps align with discriminative input patterns.\nThese model-inherent decompositions are of high visual quality and outperform\nexisting attribution methods under quantitative metrics. Further, CoDA-Nets\nconstitute performant classifiers, achieving on par results to ResNet and VGG\nmodels on e.g. CIFAR-10 and TinyImagenet.</p>\n", "tags": [] },
{"key": "böhle2021optimising", "year": "2021", "title":"Optimising for Interpretability: Convolutional Dynamic Alignment Networks", "abstract": "<p>We introduce a new family of neural network models called Convolutional\nDynamic Alignment Networks (CoDA Nets), which are performant classifiers with a\nhigh degree of inherent interpretability. Their core building blocks are\nDynamic Alignment Units (DAUs), which are optimised to transform their inputs\nwith dynamically computed weight vectors that align with task-relevant\npatterns. As a result, CoDA Nets model the classification prediction through a\nseries of input-dependent linear transformations, allowing for linear\ndecomposition of the output into individual input contributions. Given the\nalignment of the DAUs, the resulting contribution maps align with\ndiscriminative input patterns. These model-inherent decompositions are of high\nvisual quality and outperform existing attribution methods under quantitative\nmetrics. Further, CoDA Nets constitute performant classifiers, achieving on par\nresults to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,\nCoDA Nets can be combined with conventional neural network models to yield\npowerful classifiers that more easily scale to complex datasets such as\nImagenet whilst exhibiting an increased interpretable depth, i.e., the output\ncan be explained well in terms of contributions from intermediate layers within\nthe network.</p>\n", "tags": [] },
{"key": "böhle2022b-cos", "year": "2022", "title":"B-cos Networks: Alignment is All We Need for Interpretability", "abstract": "<p>We present a new direction for increasing the interpretability of deep neural\nnetworks (DNNs) by promoting weight-input alignment during training. For this,\nwe propose to replace the linear transforms in DNNs by our B-cos transform. As\nwe show, a sequence (network) of such transforms induces a single linear\ntransform that faithfully summarises the full model computations. Moreover, the\nB-cos transform introduces alignment pressure on the weights during\noptimisation. As a result, those induced linear transforms become highly\ninterpretable and align with task-relevant features. Importantly, the B-cos\ntransform is designed to be compatible with existing architectures and we show\nthat it can easily be integrated into common models such as VGGs, ResNets,\nInceptionNets, and DenseNets, whilst maintaining similar performance on\nImageNet. The resulting explanations are of high visual quality and perform\nwell under quantitative metrics for interpretability. Code available at\nhttps://www.github.com/moboehle/B-cos.</p>\n", "tags": [] },
{"key": "cai2021xproax-local", "year": "2021", "title":"XPROAX-Local explanations for text classification with progressive neighborhood approximation", "abstract": "<p>The importance of the neighborhood for training a local surrogate model to\napproximate the local decision boundary of a black box classifier has been\nalready highlighted in the literature. Several attempts have been made to\nconstruct a better neighborhood for high dimensional data, like texts, by using\ngenerative autoencoders. However, existing approaches mainly generate neighbors\nby selecting purely at random from the latent space and struggle under the\ncurse of dimensionality to learn a good local decision boundary. To overcome\nthis problem, we propose a progressive approximation of the neighborhood using\ncounterfactual instances as initial landmarks and a careful 2-stage sampling\napproach to refine counterfactuals and generate factuals in the neighborhood of\nthe input instance to be explained. Our work focuses on textual data and our\nexplanations consist of both word-level explanations from the original instance\n(intrinsic) and the neighborhood (extrinsic) and factual- and\ncounterfactual-instances discovered during the neighborhood generation process\nthat further reveal the effect of altering certain parts in the input text. Our\nexperiments on real-world datasets demonstrate that our method outperforms the\ncompetitors in terms of usefulness and stability (for the qualitative part) and\ncompleteness, compactness and correctness (for the quantitative part).</p>\n", "tags": [] },
{"key": "camburu2020explaining", "year": "2020", "title":"Explaining Deep Neural Networks", "abstract": "<p>Deep neural networks are becoming more and more popular due to their\nrevolutionary success in diverse areas, such as computer vision, natural\nlanguage processing, and speech recognition. However, the decision-making\nprocesses of these models are generally not interpretable to users. In various\ndomains, such as healthcare, finance, or law, it is critical to know the\nreasons behind a decision made by an artificial intelligence system. Therefore,\nseveral directions for explaining neural models have recently been explored. In\nthis thesis, I investigate two major directions for explaining deep neural\nnetworks. The first direction consists of feature-based post-hoc explanatory\nmethods, that is, methods that aim to explain an already trained and fixed\nmodel (post-hoc), and that provide explanations in terms of input features,\nsuch as tokens for text and superpixels for images (feature-based). The second\ndirection consists of self-explanatory neural models that generate natural\nlanguage explanations, that is, models that have a built-in module that\ngenerates explanations for the predictions of the model.</p>\n", "tags": [] },
{"key": "chajewska2013defining", "year": "2013", "title":"Defining Explanation in Probabilistic Systems", "abstract": "<p>As probabilistic systems gain popularity and are coming into wider use, the\nneed for a mechanism that explains the system’s findings and recommendations\nbecomes more critical. The system will also need a mechanism for ordering\ncompeting explanations. We examine two representative approaches to explanation\nin the literature - one due to G\"ardenfors and one due to Pearl - and show\nthat both suffer from significant problems. We propose an approach to defining\na notion of “better explanation” that combines some of the features of both\ntogether with more recent work by Pearl and others on causality.</p>\n", "tags": [] },
{"key": "chang2020invariant", "year": "2020", "title":"Invariant Rationalization", "abstract": "<p>Selective rationalization improves neural network interpretability by\nidentifying a small subset of input features – the rationale – that best\nexplains or supports the prediction. A typical rationalization criterion, i.e.\nmaximum mutual information (MMI), finds the rationale that maximizes the\nprediction performance based only on the rationale. However, MMI can be\nproblematic because it picks up spurious correlations between the input\nfeatures and the output. Instead, we introduce a game-theoretic invariant\nrationalization criterion where the rationales are constrained to enable the\nsame predictor to be optimal across different environments. We show both\ntheoretically and empirically that the proposed rationales can rule out\nspurious correlations, generalize better to different test scenarios, and align\nbetter with human judgments. Our data and code are available.</p>\n", "tags": [] },
{"key": "chattopadhyay2019neural", "year": "2019", "title":"Neural Network Attributions: A Causal Perspective", "abstract": "<p>We propose a new attribution method for neural networks developed using first\nprinciples of causality (to the best of our knowledge, the first such). The\nneural network architecture is viewed as a Structural Causal Model, and a\nmethodology to compute the causal effect of each feature on the output is\npresented. With reasonable assumptions on the causal structure of the input\ndata, we propose algorithms to efficiently compute the causal effects, as well\nas scale the approach to data with large dimensionality. We also show how this\nmethod can be used for recurrent neural networks. We report experimental\nresults on both simulated and real datasets showcasing the promise and\nusefulness of the proposed algorithm.</p>\n", "tags": [] },
{"key": "che2015distilling", "year": "2015", "title":"Distilling Knowledge from Deep Networks with Applications to Healthcare Domain", "abstract": "<p>Exponential growth in Electronic Healthcare Records (EHR) has resulted in new\nopportunities and urgent needs for discovery of meaningful data-driven\nrepresentations and patterns of diseases in Computational Phenotyping research.\nDeep Learning models have shown superior performance for robust prediction in\ncomputational phenotyping tasks, but suffer from the issue of model\ninterpretability which is crucial for clinicians involved in decision-making.\nIn this paper, we introduce a novel knowledge-distillation approach called\nInterpretable Mimic Learning, to learn interpretable phenotype features for\nmaking robust prediction while mimicking the performance of deep learning\nmodels. Our framework uses Gradient Boosting Trees to learn interpretable\nfeatures from deep learning models such as Stacked Denoising Autoencoder and\nLong Short-Term Memory. Exhaustive experiments on a real-world clinical\ntime-series dataset show that our method obtains similar or better performance\nthan the deep learning models, and it provides interpretable phenotypes for\nclinical decision making.</p>\n", "tags": [] },
{"key": "chefer2020transformer", "year": "2020", "title":"Transformer Interpretability Beyond Attention Visualization", "abstract": "<p>Self-attention techniques, and specifically Transformers, are dominating the\nfield of text processing and are becoming increasingly popular in computer\nvision classification tasks. In order to visualize the parts of the image that\nled to a certain classification, existing methods either rely on the obtained\nattention maps or employ heuristic propagation along the attention graph. In\nthis work, we propose a novel way to compute relevancy for Transformer\nnetworks. The method assigns local relevance based on the Deep Taylor\nDecomposition principle and then propagates these relevancy scores through the\nlayers. This propagation involves attention layers and skip connections, which\nchallenge existing methods. Our solution is based on a specific formulation\nthat is shown to maintain the total relevancy across layers. We benchmark our\nmethod on very recent visual Transformer networks, as well as on a text\nclassification problem, and demonstrate a clear advantage over the existing\nexplainability methods.</p>\n", "tags": [] },
{"key": "chen2018classifier", "year": "2018", "title":"Why Is My Classifier Discriminatory?", "abstract": "<p>Recent attempts to achieve fairness in predictive models focus on the balance\nbetween fairness and accuracy. In sensitive applications such as healthcare or\ncriminal justice, this trade-off is often undesirable as any increase in\nprediction error could have devastating consequences. In this work, we argue\nthat the fairness of predictions should be evaluated in context of the data,\nand that unfairness induced by inadequate samples sizes or unmeasured\npredictive variables should be addressed through data collection, rather than\nby constraining the model. We decompose cost-based metrics of discrimination\ninto bias, variance, and noise, and propose actions aimed at estimating and\nreducing each term. Finally, we perform case-studies on prediction of income,\nmortality, and review ratings, confirming the value of this analysis. We find\nthat data collection is often a means to reduce discrimination without\nsacrificing accuracy.</p>\n", "tags": [] },
{"key": "chen2018interpretable", "year": "2018", "title":"An Interpretable Model with Globally Consistent Explanations for Credit Risk", "abstract": "<p>We propose a possible solution to a public challenge posed by the Fair Isaac\nCorporation (FICO), which is to provide an explainable model for credit risk\nassessment. Rather than present a black box model and explain it afterwards, we\nprovide a globally interpretable model that is as accurate as other neural\nnetworks. Our “two-layer additive risk model” is decomposable into subscales,\nwhere each node in the second layer represents a meaningful subscale, and all\nof the nonlinearities are transparent. We provide three types of explanations\nthat are simpler than, but consistent with, the global model. One of these\nexplanation methods involves solving a minimum set cover problem to find\nhigh-support globally-consistent explanations. We present a new online\nvisualization tool to allow users to explore the global model and its\nexplanations.</p>\n", "tags": [] },
{"key": "chen2018looks", "year": "2018", "title":"This Looks Like That: Deep Learning for Interpretable Image Recognition", "abstract": "<p>When we are faced with challenging image classification tasks, we often\nexplain our reasoning by dissecting the image, and pointing out prototypical\naspects of one class or another. The mounting evidence for each of the classes\nhelps us make our final decision. In this work, we introduce a deep network\narchitecture – prototypical part network (ProtoPNet), that reasons in a\nsimilar way: the network dissects the image by finding prototypical parts, and\ncombines evidence from the prototypes to make a final classification. The model\nthus reasons in a way that is qualitatively similar to the way ornithologists,\nphysicians, and others would explain to people on how to solve challenging\nimage classification tasks. The network uses only image-level labels for\ntraining without any annotations for parts of images. We demonstrate our method\non the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show\nthat ProtoPNet can achieve comparable accuracy with its analogous\nnon-interpretable counterpart, and when several ProtoPNets are combined into a\nlarger network, it can achieve an accuracy that is on par with some of the\nbest-performing deep models. Moreover, ProtoPNet provides a level of\ninterpretability that is absent in other interpretable deep models.</p>\n", "tags": [] },
{"key": "chen2020adaptive", "year": "2020", "title":"Adaptive Explainable Neural Networks (AxNNs)", "abstract": "<p>While machine learning techniques have been successfully applied in several\nfields, the black-box nature of the models presents challenges for interpreting\nand explaining the results. We develop a new framework called Adaptive\nExplainable Neural Networks (AxNN) for achieving the dual goals of good\npredictive performance and model interpretability. For predictive performance,\nwe build a structured neural network made up of ensembles of generalized\nadditive model networks and additive index models (through explainable neural\nnetworks) using a two-stage process. This can be done using either a boosting\nor a stacking ensemble. For interpretability, we show how to decompose the\nresults of AxNN into main effects and higher-order interaction effects. The\ncomputations are inherited from Google’s open source tool AdaNet and can be\nefficiently accelerated by training with distributed computing. The results are\nillustrated on simulated and real datasets.</p>\n", "tags": [] },
{"key": "chen2020concept", "year": "2020", "title":"Concept Whitening for Interpretable Image Recognition", "abstract": "<p>What does a neural network encode about a concept as we traverse through the\nlayers? Interpretability in machine learning is undoubtedly important, but the\ncalculations of neural networks are very challenging to understand. Attempts to\nsee inside their hidden layers can either be misleading, unusable, or rely on\nthe latent space to possess properties that it may not have. In this work,\nrather than attempting to analyze a neural network posthoc, we introduce a\nmechanism, called concept whitening (CW), to alter a given layer of the network\nto allow us to better understand the computation leading up to that layer. When\na concept whitening module is added to a CNN, the axes of the latent space are\naligned with known concepts of interest. By experiment, we show that CW can\nprovide us a much clearer understanding for how the network gradually learns\nconcepts over layers. CW is an alternative to a batch normalization layer in\nthat it normalizes, and also decorrelates (whitens) the latent space. CW can be\nused in any layer of the network without hurting predictive performance.</p>\n", "tags": [] },
{"key": "chen2020generating", "year": "2020", "title":"Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection", "abstract": "<p>Generating explanations for neural networks has become crucial for their\napplications in real-world with respect to reliability and trustworthiness. In\nnatural language processing, existing methods usually provide important\nfeatures which are words or phrases selected from an input text as an\nexplanation, but ignore the interactions between them. It poses challenges for\nhumans to interpret an explanation and connect it to model prediction. In this\nwork, we build hierarchical explanations by detecting feature interactions.\nSuch explanations visualize how words and phrases are combined at different\nlevels of the hierarchy, which can help users understand the decision-making of\nblack-box models. The proposed method is evaluated with three neural text\nclassifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic\nand human evaluations. Experiments show the effectiveness of the proposed\nmethod in providing explanations that are both faithful to models and\ninterpretable to humans.</p>\n", "tags": [] },
{"key": "chen2022use-case-grounded", "year": "2022", "title":"Use-Case-Grounded Simulations for Explanation Evaluation", "abstract": "<p>A growing body of research runs human subject evaluations to study whether\nproviding users with explanations of machine learning models can help them with\npractical real-world use cases. However, running user studies is challenging\nand costly, and consequently each study typically only evaluates a limited\nnumber of different settings, e.g., studies often only evaluate a few\narbitrarily selected explanation methods. To address these challenges and aid\nuser study design, we introduce Use-Case-Grounded Simulated Evaluations\n(SimEvals). SimEvals involve training algorithmic agents that take as input the\ninformation content (such as model explanations) that would be presented to\neach participant in a human subject study, to predict answers to the use case\nof interest. The algorithmic agent’s test set accuracy provides a measure of\nthe predictiveness of the information content for the downstream use case. We\nrun a comprehensive evaluation on three real-world use cases (forward\nsimulation, model debugging, and counterfactual reasoning) to demonstrate that\nSimevals can effectively identify which explanation methods will help humans\nfor each use case. These results provide evidence that SimEvals can be used to\nefficiently screen an important set of user study design decisions, e.g.\nselecting which explanations should be presented to the user, before running a\npotentially costly user study.</p>\n", "tags": [] },
{"key": "cheng2021tsgb", "year": "2021", "title":"TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency", "abstract": "<p>The explanation for deep neural networks has drawn extensive attention in the\ndeep learning community over the past few years. In this work, we study the\nvisual saliency, a.k.a. visual explanation, to interpret convolutional neural\nnetworks. Compared to iteration based saliency methods, single backward pass\nbased saliency methods benefit from faster speed, and they are widely used in\ndownstream visual tasks. Thus, we focus on single backward pass based methods.\nHowever, existing methods in this category struggle to uccessfully produce\nfine-grained saliency maps concentrating on specific target classes. That said,\nproducing faithful saliency maps satisfying both target-selectiveness and\nfine-grainedness using a single backward pass is a challenging problem in the\nfield. To mitigate this problem, we revisit the gradient flow inside the\nnetwork, and find that the entangled semantics and original weights may disturb\nthe propagation of target-relevant saliency. Inspired by those observations, we\npropose a novel visual saliency method, termed Target-Selective Gradient\nBackprop (TSGB), which leverages rectification operations to effectively\nemphasize target classes and further efficiently propagate the saliency to the\nimage space, thereby generating target-selective and fine-grained saliency\nmaps. The proposed TSGB consists of two components, namely, TSGB-Conv and\nTSGB-FC, which rectify the gradients for convolutional layers and\nfully-connected layers, respectively. Extensive qualitative and quantitative\nexperiments on the ImageNet and Pascal VOC datasets show that the proposed\nmethod achieves more accurate and reliable results than the other competitive\nmethods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB.</p>\n", "tags": [] },
{"key": "chou2021counterfactuals", "year": "2021", "title":"Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications", "abstract": "<p>There has been a growing interest in model-agnostic methods that can make\ndeep learning models more transparent and explainable to a user. Some\nresearchers recently argued that for a machine to achieve a certain degree of\nhuman-level explainability, this machine needs to provide human causally\nunderstandable explanations, also known as causability. A specific class of\nalgorithms that have the potential to provide causability are counterfactuals.\nThis paper presents an in-depth systematic review of the diverse existing body\nof literature on counterfactuals and causability for explainable artificial\nintelligence. We performed an LDA topic modelling analysis under a PRISMA\nframework to find the most relevant literature articles. This analysis resulted\nin a novel taxonomy that considers the grounding theories of the surveyed\nalgorithms, together with their underlying properties and applications in\nreal-world data. This research suggests that current model-agnostic\ncounterfactual algorithms for explainable AI are not grounded on a causal\ntheoretical formalism and, consequently, cannot promote causability to a human\ndecision-maker. Our findings suggest that the explanations derived from major\nalgorithms in the literature provide spurious correlations rather than\ncause/effects relationships, leading to sub-optimal, erroneous or even biased\nexplanations. This paper also advances the literature with new directions and\nchallenges on promoting causability in model-agnostic approaches for\nexplainable artificial intelligence.</p>\n", "tags": [] },
{"key": "chrysostomou2021enjoy", "year": "2021", "title":"Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience", "abstract": "<p>Pretrained transformer-based models such as BERT have demonstrated\nstate-of-the-art predictive performance when adapted into a range of natural\nlanguage processing tasks. An open problem is how to improve the faithfulness\nof explanations (rationales) for the predictions of these models. In this\npaper, we hypothesize that salient information extracted a priori from the\ntraining data can complement the task-specific information learned by the model\nduring fine-tuning on a downstream task. In this way, we aim to help BERT not\nto forget assigning importance to informative input tokens when making\npredictions by proposing SaLoss; an auxiliary loss function for guiding the\nmulti-head attention mechanism during training to be close to salient\ninformation extracted a priori using TextRank. Experiments for explanation\nfaithfulness across five datasets, show that models trained with SaLoss\nconsistently provide more faithful explanations across four different feature\nattribution methods compared to vanilla BERT. Using the rationales extracted\nfrom vanilla BERT and SaLoss models to train inherently faithful classifiers,\nwe further show that the latter result in higher predictive performance in\ndownstream tasks.</p>\n", "tags": [] },
{"key": "chyung2019extracting", "year": "2019", "title":"Extracting Interpretable Concept-Based Decision Trees from CNNs", "abstract": "<p>In an attempt to gather a deeper understanding of how convolutional neural\nnetworks (CNNs) reason about human-understandable concepts, we present a method\nto infer labeled concept data from hidden layer activations and interpret the\nconcepts through a shallow decision tree. The decision tree can provide\ninformation about which concepts a model deems important, as well as provide an\nunderstanding of how the concepts interact with each other. Experiments\ndemonstrate that the extracted decision tree is capable of accurately\nrepresenting the original CNN’s classifications at low tree depths, thus\nencouraging human-in-the-loop understanding of discriminative concepts.</p>\n", "tags": [] },
{"key": "ciocan2018interpretable", "year": "2018", "title":"Interpretable Optimal Stopping", "abstract": "<p>Optimal stopping is the problem of deciding when to stop a stochastic system\nto obtain the greatest reward, arising in numerous application areas such as\nfinance, healthcare and marketing. State-of-the-art methods for\nhigh-dimensional optimal stopping involve approximating the value function or\nthe continuation value, and then using that approximation within a greedy\npolicy. Although such policies can perform very well, they are generally not\nguaranteed to be interpretable; that is, a decision maker may not be able to\neasily see the link between the current system state and the policy’s action.\nIn this paper, we propose a new approach to optimal stopping, wherein the\npolicy is represented as a binary tree, in the spirit of naturally\ninterpretable tree models commonly used in machine learning. We show that the\nclass of tree policies is rich enough to approximate the optimal policy. We\nformulate the problem of learning such policies from observed trajectories of\nthe stochastic system as a sample average approximation (SAA) problem. We prove\nthat the SAA problem converges under mild conditions as the sample size\nincreases, but that computationally even immediate simplifications of the SAA\nproblem are theoretically intractable. We thus propose a tractable heuristic\nfor approximately solving the SAA problem, by greedily constructing the tree\nfrom the top down. We demonstrate the value of our approach by applying it to\nthe canonical problem of option pricing, using both synthetic instances and\ninstances using real S&amp;P-500 data. Our method obtains policies that (1)\noutperform state-of-the-art non-interpretable methods, based on\nsimulation-regression and martingale duality, and (2) possess a remarkably\nsimple and intuitive structure.</p>\n", "tags": [] },
{"key": "colin2021cannot", "year": "2021", "title":"What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods", "abstract": "<p>A multitude of explainability methods and associated fidelity performance\nmetrics have been proposed to help better understand how modern AI systems make\ndecisions. However, much of the current work has remained theoretical –\nwithout much consideration for the human end-user. In particular, it is not yet\nknown (1) how useful current explainability methods are in practice for more\nreal-world scenarios and (2) how well associated performance metrics accurately\npredict how much knowledge individual explanations contribute to a human\nend-user trying to understand the inner-workings of the system. To fill this\ngap, we conducted psychophysics experiments at scale to evaluate the ability of\nhuman participants to leverage representative attribution methods for\nunderstanding the behavior of different image classifiers representing three\nreal-world scenarios: identifying bias in an AI system, characterizing the\nvisual strategy it uses for tasks that are too difficult for an untrained\nnon-expert human observer as well as understanding its failure cases. Our\nresults demonstrate that the degree to which individual attribution methods\nhelp human participants better understand an AI system varied widely across\nthese scenarios. This suggests a critical need for the field to move past\nquantitative improvements of current attribution methods towards the\ndevelopment of complementary approaches that provide qualitatively different\nsources of information to human end-users.</p>\n", "tags": [] },
{"key": "crabbé2021explaining", "year": "2021", "title":"Explaining Latent Representations with a Corpus of Examples", "abstract": "<p>Modern machine learning models are complicated. Most of them rely on\nconvoluted latent representations of their input to issue a prediction. To\nachieve greater transparency than a black-box that connects inputs to\npredictions, it is necessary to gain a deeper understanding of these latent\nrepresentations. To that aim, we propose SimplEx: a user-centred method that\nprovides example-based explanations with reference to a freely selected set of\nexamples, called the corpus. SimplEx uses the corpus to improve the user’s\nunderstanding of the latent space with post-hoc explanations answering two\nquestions: (1) Which corpus examples explain the prediction issued for a given\ntest example? (2) What features of these corpus examples are relevant for the\nmodel to relate them to the test example? SimplEx provides an answer by\nreconstructing the test latent representation as a mixture of corpus latent\nrepresentations. Further, we propose a novel approach, the Integrated Jacobian,\nthat allows SimplEx to make explicit the contribution of each corpus feature in\nthe mixture. Through experiments on tasks ranging from mortality prediction to\nimage classification, we demonstrate that these decompositions are robust and\naccurate. With illustrative use cases in medicine, we show that SimplEx\nempowers the user by highlighting relevant patterns in the corpus that explain\nmodel representations. Moreover, we demonstrate how the freedom in choosing the\ncorpus allows the user to have personalized explanations in terms of examples\nthat are meaningful for them.</p>\n", "tags": [] },
{"key": "craye2018exploring", "year": "2018", "title":"Exploring to learn visual saliency: The RL-IAC approach", "abstract": "<p>The problem of object localization and recognition on autonomous mobile\nrobots is still an active topic. In this context, we tackle the problem of\nlearning a model of visual saliency directly on a robot. This model, learned\nand improved on-the-fly during the robot’s exploration provides an efficient\ntool for localizing relevant objects within their environment. The proposed\napproach includes two intertwined components. On the one hand, we describe a\nmethod for learning and incrementally updating a model of visual saliency from\na depth-based object detector. This model of saliency can also be exploited to\nproduce bounding box proposals around objects of interest. On the other hand,\nwe investigate an autonomous exploration technique to efficiently learn such a\nsaliency model. The proposed exploration, called Reinforcement\nLearning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot’s\nexploration so that samples selected by the robot are likely to improve the\ncurrent model of saliency. We then demonstrate that such a saliency model\nlearned directly on a robot outperforms several state-of-the-art saliency\ntechniques, and that RL-IAC can drastically decrease the required time for\nlearning a reliable saliency model.</p>\n", "tags": [] },
{"key": "cui2019chip", "year": "2019", "title":"CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks", "abstract": "<p>With the widespread applications of deep convolutional neural networks\n(DCNNs), it becomes increasingly important for DCNNs not only to make accurate\npredictions but also to explain how they make their decisions. In this work, we\npropose a CHannel-wise disentangled InterPretation (CHIP) model to give the\nvisual interpretation to the predictions of DCNNs. The proposed model distills\nthe class-discriminative importance of channels in networks by utilizing the\nsparse regularization. Here, we first introduce the network perturbation\ntechnique to learn the model. The proposed model is capable to not only distill\nthe global perspective knowledge from networks but also present the\nclass-discriminative visual interpretation for specific predictions of\nnetworks. It is noteworthy that the proposed model is able to interpret\ndifferent layers of networks without re-training. By combining the distilled\ninterpretation knowledge in different layers, we further propose the Refined\nCHIP visual interpretation that is both high-resolution and\nclass-discriminative. Experimental results on the standard dataset demonstrate\nthat the proposed model provides promising visual interpretation for the\npredictions of networks in image classification task compared with existing\nvisual interpretation methods. Besides, the proposed method outperforms related\napproaches in the application of ILSVRC 2015 weakly-supervised localization\ntask.</p>\n", "tags": [] },
{"key": "custode2020evolutionary", "year": "2020", "title":"Evolutionary learning of interpretable decision trees", "abstract": "<p>Reinforcement learning techniques achieved human-level performance in several\ntasks in the last decade. However, in recent years, the need for\ninterpretability emerged: we want to be able to understand how a system works\nand the reasons behind its decisions. Not only we need interpretability to\nassess the safety of the produced systems, we also need it to extract knowledge\nabout unknown problems. While some techniques that optimize decision trees for\nreinforcement learning do exist, they usually employ greedy algorithms or they\ndo not exploit the rewards given by the environment. This means that these\ntechniques may easily get stuck in local optima. In this work, we propose a\nnovel approach to interpretable reinforcement learning that uses decision\ntrees. We present a two-level optimization scheme that combines the advantages\nof evolutionary algorithms with the advantages of Q-learning. This way we\ndecompose the problem into two sub-problems: the problem of finding a\nmeaningful and useful decomposition of the state space, and the problem of\nassociating an action to each state. We test the proposed method on three\nwell-known reinforcement learning benchmarks, on which it results competitive\nwith respect to the state-of-the-art in both performance and interpretability.\nFinally, we perform an ablation study that confirms that using the two-level\noptimization scheme gives a boost in performance in non-trivial environments\nwith respect to a one-layer optimization technique.</p>\n", "tags": [] },
{"key": "dalvi2022towards", "year": "2022", "title":"Towards Teachable Reasoning Systems", "abstract": "<p>Our goal is a teachable reasoning system for question-answering (QA), where a\nuser can interact with faithful answer explanations, and correct errors so that\nthe system improves over time. Our approach is three-fold: First, generated\nchains of reasoning show how answers are implied by the system’s own internal\nbeliefs. Second, users can interact with the explanations to identify erroneous\nmodel beliefs and provide corrections. Third, we augment the model with a\ndynamic memory of such corrections. Retrievals from memory are used as\nadditional context for QA, to help avoid previous mistakes in similar new\nsituations - a novel type of memory-based continuous learning. To our\nknowledge, this is the first system to generate chains that are both faithful\n(the answer follows from the reasoning) and truthful (the chain reflects the\nsystem’s own beliefs, as ascertained by self-querying). In evaluation, users\njudge that a majority (65%+) of generated chains clearly show how an answer\nfollows from a set of facts - substantially better than a high-performance\nbaseline. We also find that using simulated feedback, our system (called\nEntailmentWriter) continually improves with time, requiring feedback on only\n25% of training examples to reach within 1% of the upper-bound (feedback on all\nexamples). We observe a similar trend with real users. This suggests new\nopportunities for using language models in an interactive setting where users\ncan inspect, debug, correct, and improve a system’s performance over time.</p>\n", "tags": [] },
{"key": "danilevsky2020survey", "year": "2020", "title":"A Survey of the State of Explainable AI for Natural Language Processing", "abstract": "<p>Recent years have seen important advances in the quality of state-of-the-art\nmodels, but this has come at the expense of models becoming less interpretable.\nThis survey presents an overview of the current state of Explainable AI (XAI),\nconsidered within the domain of Natural Language Processing (NLP). We discuss\nthe main categorization of explanations, as well as the various ways\nexplanations can be arrived at and visualized. We detail the operations and\nexplainability techniques currently available for generating explanations for\nNLP model predictions, to serve as a resource for model developers in the\ncommunity. Finally, we point out the current gaps and encourage directions for\nfuture work in this important research area.</p>\n", "tags": [] },
{"key": "das2016human", "year": "2016", "title":"Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?", "abstract": "<p>We conduct large-scale studies on `human attention’ in Visual Question\nAnswering (VQA) to understand where humans choose to look to answer questions\nabout images. We design and test multiple game-inspired novel\nattention-annotation interfaces that require the subject to sharpen regions of\na blurred image to answer a question. Thus, we introduce the VQA-HAT (Human\nATtention) dataset. We evaluate attention maps generated by state-of-the-art\nVQA models against human attention both qualitatively (via visualizations) and\nquantitatively (via rank-order correlation). Overall, our experiments show that\ncurrent attention models in VQA do not seem to be looking at the same regions\nas humans.</p>\n", "tags": [] },
{"key": "das2020leveraging", "year": "2020", "title":"Leveraging Rationales to Improve Human Task Performance", "abstract": "<p>Machine learning (ML) systems across many application areas are increasingly\ndemonstrating performance that is beyond that of humans. In response to the\nproliferation of such models, the field of Explainable AI (XAI) has sought to\ndevelop techniques that enhance the transparency and interpretability of\nmachine learning methods. In this work, we consider a question not previously\nexplored within the XAI and ML communities: Given a computational system whose\nperformance exceeds that of its human user, can explainable AI capabilities be\nleveraged to improve the performance of the human? We study this question in\nthe context of the game of Chess, for which computational game engines that\nsurpass the performance of the average player are widely available. We\nintroduce the Rationale-Generating Algorithm, an automated technique for\ngenerating rationales for utility-based computational methods, which we\nevaluate with a multi-day user study against two baselines. The results show\nthat our approach produces rationales that lead to statistically significant\nimprovement in human task performance, demonstrating that rationales\nautomatically generated from an AI’s internal task model can be used not only\nto explain what the system is doing, but also to instruct the user and\nultimately improve their task performance.</p>\n", "tags": [] },
{"key": "davis2020measure", "year": "2020", "title":"Measure Utility, Gain Trust: Practical Advice for XAI Researcher", "abstract": "<p>Research into the explanation of machine learning models, i.e., explainable\nAI (XAI), has seen a commensurate exponential growth alongside deep artificial\nneural networks throughout the past decade. For historical reasons, explanation\nand trust have been intertwined. However, the focus on trust is too narrow, and\nhas led the research community astray from tried and true empirical methods\nthat produced more defensible scientific knowledge about people and\nexplanations. To address this, we contribute a practical path forward for\nresearchers in the XAI field. We recommend researchers focus on the utility of\nmachine learning explanations instead of trust. We outline five broad use cases\nwhere explanations are useful and, for each, we describe pseudo-experiments\nthat rely on objective empirical measurements and falsifiable hypotheses. We\nbelieve that this experimental rigor is necessary to contribute to scientific\nknowledge in the field of XAI.</p>\n", "tags": [] },
{"key": "delaney2020instance-based", "year": "2020", "title":"Instance-based Counterfactual Explanations for Time Series Classification", "abstract": "<p>In recent years, there has been a rapidly expanding focus on explaining the\npredictions made by black-box AI systems that handle image and tabular data.\nHowever, considerably less attention has been paid to explaining the\npredictions of opaque AI systems handling time series data. In this paper, we\nadvance a novel model-agnostic, case-based technique – Native Guide – that\ngenerates counterfactual explanations for time series classifiers. Given a\nquery time series, $T_{q}$, for which a black-box classification system\npredicts class, $c$, a counterfactual time series explanation shows how $T_{q}$\ncould change, such that the system predicts an alternative class, $c’$. The\nproposed instance-based technique adapts existing counterfactual instances in\nthe case-base by highlighting and modifying discriminative areas of the time\nseries that underlie the classification. Quantitative and qualitative results\nfrom two comparative experiments indicate that Native Guide generates\nplausible, proximal, sparse and diverse explanations that are better than those\nproduced by key benchmark counterfactual methods.</p>\n", "tags": [] },
{"key": "dhamdhere2019shapley", "year": "2019", "title":"The Shapley Taylor Interaction Index", "abstract": "<p>The attribution problem, that is the problem of attributing a model’s\nprediction to its base features, is well-studied. We extend the notion of\nattribution to also apply to feature interactions.\n  The Shapley value is a commonly used method to attribute a model’s prediction\nto its base features. We propose a generalization of the Shapley value called\nShapley-Taylor index that attributes the model’s prediction to interactions of\nsubsets of features up to some size k. The method is analogous to how the\ntruncated Taylor Series decomposes the function value at a certain point using\nits derivatives at a different point. In fact, we show that the Shapley Taylor\nindex is equal to the Taylor Series of the multilinear extension of the\nset-theoretic behavior of the model.\n  We axiomatize this method using the standard Shapley axioms – linearity,\ndummy, symmetry and efficiency – and an additional axiom that we call the\ninteraction distribution axiom. This new axiom explicitly characterizes how\ninteractions are distributed for a class of functions that model pure\ninteraction.\n  We contrast the Shapley-Taylor index against the previously proposed Shapley\nInteraction index (cf. [9]) from the cooperative game theory literature. We\nalso apply the Shapley Taylor index to three models and identify interesting\nqualitative insights.</p>\n", "tags": [] },
{"key": "dieber2020model", "year": "2020", "title":"Why model why? Assessing the strengths and limitations of LIME", "abstract": "<p>When it comes to complex machine learning models, commonly referred to as\nblack boxes, understanding the underlying decision making process is crucial\nfor domains such as healthcare and financial services, and also when it is used\nin connection with safety critical systems such as autonomous vehicles. As such\ninterest in explainable artificial intelligence (xAI) tools and techniques has\nincreased in recent years. However, the effectiveness of existing xAI\nframeworks, especially concerning algorithms that work with data as opposed to\nimages, is still an open research question. In order to address this gap, in\nthis paper we examine the effectiveness of the Local Interpretable\nModel-Agnostic Explanations (LIME) xAI framework, one of the most popular model\nagnostic frameworks found in the literature, with a specific focus on its\nperformance in terms of making tabular models more interpretable. In\nparticular, we apply several state of the art machine learning algorithms on a\ntabular dataset, and demonstrate how LIME can be used to supplement\nconventional performance assessment methods. In addition, we evaluate the\nunderstandability of the output produced by LIME both via a usability study,\ninvolving participants who are not familiar with LIME, and its overall\nusability via an assessment framework, which is derived from the International\nOrganisation for Standardisation 9241-11:1998 standard.</p>\n", "tags": [] },
{"key": "ding2021evaluating", "year": "2021", "title":"Evaluating Saliency Methods for Neural Language Models", "abstract": "<p>Saliency methods are widely used to interpret neural network predictions, but\ndifferent variants of saliency methods often disagree even on the\ninterpretations of the same prediction made by the same model. In these cases,\nhow do we identify when are these interpretations trustworthy enough to be used\nin analyses? To address this question, we conduct a comprehensive and\nquantitative evaluation of saliency methods on a fundamental category of NLP\nmodels: neural language models. We evaluate the quality of prediction\ninterpretations from two perspectives that each represents a desirable property\nof these interpretations: plausibility and faithfulness. Our evaluation is\nconducted on four different datasets constructed from the existing human\nannotation of syntactic and semantic agreements, on both sentence-level and\ndocument-level. Through our evaluation, we identified various ways saliency\nmethods could yield interpretations of low quality. We recommend that future\nwork deploying such methods to neural language models should carefully validate\ntheir interpretations before drawing insights.</p>\n", "tags": [] },
{"key": "dong2016characterizing", "year": "2016", "title":"Characterizing Driving Styles with Deep Learning", "abstract": "<p>Characterizing driving styles of human drivers using vehicle sensor data,\ne.g., GPS, is an interesting research problem and an important real-world\nrequirement from automotive industries. A good representation of driving\nfeatures can be highly valuable for autonomous driving, auto insurance, and\nmany other application scenarios. However, traditional methods mainly rely on\nhandcrafted features, which limit machine learning algorithms to achieve a\nbetter performance. In this paper, we propose a novel deep learning solution to\nthis problem, which could be the first attempt of extending deep learning to\ndriving behavior analysis based on GPS data. The proposed approach can\neffectively extract high level and interpretable features describing complex\ndriving patterns. It also requires significantly less human experience and\nwork. The power of the learned driving style representations are validated\nthrough the driver identification problem using a large real dataset.</p>\n", "tags": [] },
{"key": "dong2017improving", "year": "2017", "title":"Improving Interpretability of Deep Neural Networks with Semantic Information", "abstract": "<p>Interpretability of deep neural networks (DNNs) is essential since it enables\nusers to understand the overall strengths and weaknesses of the models, conveys\nan understanding of how the models will behave in the future, and how to\ndiagnose and correct potential problems. However, it is challenging to reason\nabout what a DNN actually does due to its opaque or black-box nature. To\naddress this issue, we propose a novel technique to improve the\ninterpretability of DNNs by leveraging the rich semantic information embedded\nin human descriptions. By concentrating on the video captioning task, we first\nextract a set of semantically meaningful topics from the human descriptions\nthat cover a wide range of visual concepts, and integrate them into the model\nwith an interpretive loss. We then propose a prediction difference maximization\nalgorithm to interpret the learned features of each neuron. Experimental\nresults demonstrate its effectiveness in video captioning using the\ninterpretable features, which can also be transferred to video action\nrecognition. By clearly understanding the learned features, users can easily\nrevise false predictions via a human-in-the-loop procedure.</p>\n", "tags": [] },
{"key": "dong2017towards", "year": "2017", "title":"Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples", "abstract": "<p>Deep neural networks (DNNs) have demonstrated impressive performance on a\nwide array of tasks, but they are usually considered opaque since internal\nstructure and learned parameters are not interpretable. In this paper, we\nre-examine the internal representations of DNNs using adversarial images, which\nare generated by an ensemble-optimization algorithm. We find that: (1) the\nneurons in DNNs do not truly detect semantic objects/parts, but respond to\nobjects/parts only as recurrent discriminative patches; (2) deep visual\nrepresentations are not robust distributed codes of visual concepts because the\nrepresentations of adversarial images are largely not consistent with those of\nreal images, although they have similar visual appearance, both of which are\ndifferent from previous findings. To further improve the interpretability of\nDNNs, we propose an adversarial training scheme with a consistent loss such\nthat the neurons are endowed with human-interpretable concepts. The induced\ninterpretable representations enable us to trace eventual outcomes back to\ninfluential neurons. Therefore, human users can know how the models make\npredictions, as well as when and why they make errors.</p>\n", "tags": [] },
{"key": "dong2019towards", "year": "2019", "title":"Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples", "abstract": "<p>Sometimes it is not enough for a DNN to produce an outcome. For example, in\napplications such as healthcare, users need to understand the rationale of the\ndecisions. Therefore, it is imperative to develop algorithms to learn models\nwith good interpretability (Doshi-Velez 2017). An important factor that leads\nto the lack of interpretability of DNNs is the ambiguity of neurons, where a\nneuron may fire for various unrelated concepts. This work aims to increase the\ninterpretability of DNNs on the whole image space by reducing the ambiguity of\nneurons. In this paper, we make the following contributions:\n  1) We propose a metric to evaluate the consistency level of neurons in a\nnetwork quantitatively.\n  2) We find that the learned features of neurons are ambiguous by leveraging\nadversarial examples.\n  3) We propose to improve the consistency of neurons on adversarial example\nsubset by an adversarial training algorithm with a consistent loss.</p>\n", "tags": [] },
{"key": "donnelly2021deformable", "year": "2021", "title":"Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes", "abstract": "<p>We present a deformable prototypical part network (Deformable ProtoPNet), an\ninterpretable image classifier that integrates the power of deep learning and\nthe interpretability of case-based reasoning. This model classifies input\nimages by comparing them with prototypes learned during training, yielding\nexplanations in the form of “this looks like that.” However, while previous\nmethods use spatially rigid prototypes, we address this shortcoming by\nproposing spatially flexible prototypes. Each prototype is made up of several\nprototypical parts that adaptively change their relative spatial positions\ndepending on the input image. Consequently, a Deformable ProtoPNet can\nexplicitly capture pose variations and context, improving both model accuracy\nand the richness of explanations provided. Compared to other case-based\ninterpretable models using prototypes, our approach achieves state-of-the-art\naccuracy and gives an explanation with greater context. The code is available\nat https://github.com/jdonnelly36/Deformable-ProtoPNet.</p>\n", "tags": [] },
{"key": "doshi-velez2017accountability", "year": "2017", "title":"Accountability of AI Under the Law: The Role of Explanation", "abstract": "<p>The ubiquity of systems using artificial intelligence or “AI” has brought\nincreasing attention to how those systems should be regulated. The choice of\nhow to regulate AI systems will require care. AI systems have the potential to\nsynthesize large amounts of data, allowing for greater levels of\npersonalization and precision than ever before—applications range from\nclinical decision support to autonomous driving and predictive policing. That\nsaid, there exist legitimate concerns about the intentional and unintentional\nnegative consequences of AI systems. There are many ways to hold AI systems\naccountable. In this work, we focus on one: explanation. Questions about a\nlegal right to explanation from AI systems was recently debated in the EU\nGeneral Data Protection Regulation, and thus thinking carefully about when and\nhow explanation from AI systems might improve accountability is timely. In this\nwork, we review contexts in which explanation is currently required under the\nlaw, and then list the technical considerations that must be considered if we\ndesired AI systems that could provide kinds of explanations that are currently\nrequired of humans.</p>\n", "tags": [] },
{"key": "doshi-velez2017towards", "year": "2017", "title":"Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "<p>As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.</p>\n", "tags": [] },
{"key": "dosovitskiy2015inverting", "year": "2015", "title":"Inverting Visual Representations with Convolutional Networks", "abstract": "<p>Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities.</p>\n", "tags": [] },
{"key": "du2018techniques", "year": "2018", "title":"Techniques for Interpretable Machine Learning", "abstract": "<p>Interpretable machine learning tackles the important problem that humans\ncannot understand the behaviors of complex machine learning models and how\nthese models arrive at a particular decision. Although many approaches have\nbeen proposed, a comprehensive understanding of the achievements and challenges\nis still lacking. We provide a survey covering existing techniques to increase\nthe interpretability of machine learning models. We also discuss crucial issues\nthat the community should consider in future work such as designing\nuser-friendly explanations and developing comprehensive evaluation metrics to\nfurther push forward the area of interpretable machine learning.</p>\n", "tags": [] },
{"key": "du2018towards", "year": "2018", "title":"Towards Explanation of DNN-based Prediction with Guided Feature Inversion", "abstract": "<p>While deep neural networks (DNN) have become an effective computational tool,\nthe prediction results are often criticized by the lack of interpretability,\nwhich is essential in many real-world applications such as health informatics.\nExisting attempts based on local interpretations aim to identify relevant\nfeatures contributing the most to the prediction of DNN by monitoring the\nneighborhood of a given input. They usually simply ignore the intermediate\nlayers of the DNN that might contain rich information for interpretation. To\nbridge the gap, in this paper, we propose to investigate a guided feature\ninversion framework for taking advantage of the deep architectures towards\neffective interpretation. The proposed framework not only determines the\ncontribution of each feature in the input but also provides insights into the\ndecision-making process of DNN models. By further interacting with the neuron\nof the target category at the output layer of the DNN, we enforce the\ninterpretation result to be class-discriminative. We apply the proposed\ninterpretation model to different CNN architectures to provide explanations for\nimage data and conduct extensive experiments on ImageNet and PASCAL VOC07\ndatasets. The interpretation results demonstrate the effectiveness of our\nproposed framework in providing class-discriminative interpretation for\nDNN-based prediction.</p>\n", "tags": [] },
{"key": "du2019attribution", "year": "2019", "title":"On Attribution of Recurrent Neural Network Predictions via Additive Decomposition", "abstract": "<p>RNN models have achieved the state-of-the-art performance in a wide range of\ntext mining tasks. However, these models are often regarded as black-boxes and\nare criticized due to the lack of interpretability. In this paper, we enhance\nthe interpretability of RNNs by providing interpretable rationales for RNN\npredictions. Nevertheless, interpreting RNNs is a challenging problem. Firstly,\nunlike existing methods that rely on local approximation, we aim to provide\nrationales that are more faithful to the decision making process of RNN models.\nSecondly, a flexible interpretation method should be able to assign\ncontribution scores to text segments of varying lengths, instead of only to\nindividual words. To tackle these challenges, we propose a novel attribution\nmethod, called REAT, to provide interpretations to RNN predictions. REAT\ndecomposes the final prediction of a RNN into additive contribution of each\nword in the input text. This additive decomposition enables REAT to further\nobtain phrase-level attribution scores. In addition, REAT is generally\napplicable to various RNN architectures, including GRU, LSTM and their\nbidirectional versions. Experimental results demonstrate the faithfulness and\ninterpretability of the proposed attribution method. Comprehensive analysis\nshows that our attribution method could unveil the useful linguistic knowledge\ncaptured by RNNs. Some analysis further demonstrates our method could be\nutilized as a debugging tool to examine the vulnerability and failure reasons\nof RNNs, which may lead to several promising future directions to promote\ngeneralization ability of RNNs.</p>\n", "tags": [] },
{"key": "eckstein2021discriminative", "year": "2021", "title":"Discriminative Attribution from Counterfactuals", "abstract": "<p>We present a method for neural network interpretability by combining feature\nattribution with counterfactual explanations to generate attribution maps that\nhighlight the most discriminative features between pairs of classes. We show\nthat this method can be used to quantitatively evaluate the performance of\nfeature attribution methods in an objective manner, thus preventing potential\nobserver bias. We evaluate the proposed method on three diverse datasets,\nincluding a challenging artificial dataset and real-world biological data. We\nshow quantitatively and qualitatively that the highlighted features are\nsubstantially more discriminative than those extracted using conventional\nattribution methods and argue that this type of explanation is better suited\nfor understanding fine grained class differences as learned by a deep neural\nnetwork.</p>\n", "tags": [] },
{"key": "ehsan2021explainability", "year": "2021", "title":"Explainability Pitfalls: Beyond Dark Patterns in Explainable AI", "abstract": "<p>To make Explainable AI (XAI) systems trustworthy, understanding harmful\neffects is just as important as producing well-designed explanations. In this\npaper, we address an important yet unarticulated type of negative effect in\nXAI. We introduce explainability pitfalls(EPs), unanticipated negative\ndownstream effects from AI explanations manifesting even when there is no\nintention to manipulate users. EPs are different from, yet related to, dark\npatterns, which are intentionally deceptive practices. We articulate the\nconcept of EPs by demarcating it from dark patterns and highlighting the\nchallenges arising from uncertainties around pitfalls. We situate and\noperationalize the concept using a case study that showcases how, despite best\nintentions, unsuspecting negative effects such as unwarranted trust in\nnumerical explanations can emerge. We propose proactive and preventative\nstrategies to address EPs at three interconnected levels: research, design, and\norganizational.</p>\n", "tags": [] },
{"key": "etheredge2020decontextualized", "year": "2020", "title":"Decontextualized learning for interpretable hierarchical representations of visual patterns", "abstract": "<p>Apart from discriminative models for classification and object detection\ntasks, the application of deep convolutional neural networks to basic research\nutilizing natural imaging data has been somewhat limited; particularly in cases\nwhere a set of interpretable features for downstream analysis is needed, a key\nrequirement for many scientific investigations. We present an algorithm and\ntraining paradigm designed specifically to address this: decontextualized\nhierarchical representation learning (DHRL). By combining a generative model\nchaining procedure with a ladder network architecture and latent space\nregularization for inference, DHRL address the limitations of small datasets\nand encourages a disentangled set of hierarchically organized features. In\naddition to providing a tractable path for analyzing complex hierarchal\npatterns using variation inference, this approach is generative and can be\ndirectly combined with empirical and theoretical approaches. To highlight the\nextensibility and usefulness of DHRL, we demonstrate this method in application\nto a question from evolutionary biology.</p>\n", "tags": [] },
{"key": "fabbri2011explanation-based", "year": "2011", "title":"Explanation-Based Auditing", "abstract": "<p>To comply with emerging privacy laws and regulations, it has become common\nfor applications like electronic health records systems (EHRs) to collect\naccess logs, which record each time a user (e.g., a hospital employee) accesses\na piece of sensitive data (e.g., a patient record). Using the access log, it is\neasy to answer simple queries (e.g., Who accessed Alice’s medical record?), but\nthis often does not provide enough information. In addition to learning who\naccessed their medical records, patients will likely want to understand why\neach access occurred. In this paper, we introduce the problem of generating\nexplanations for individual records in an access log. The problem is motivated\nby user-centric auditing applications, and it also provides a novel approach to\nmisuse detection. We develop a framework for modeling explanations which is\nbased on a fundamental observation: For certain classes of databases, including\nEHRs, the reason for most data accesses can be inferred from data stored\nelsewhere in the database. For example, if Alice has an appointment with Dr.\nDave, this information is stored in the database, and it explains why Dr. Dave\nlooked at Alice’s record. Large numbers of data accesses can be explained using\ngeneral forms called explanation templates. Rather than requiring an\nadministrator to manually specify explanation templates, we propose a set of\nalgorithms for automatically discovering frequent templates from the database\n(i.e., those that explain a large number of accesses). We also propose\ntechniques for inferring collaborative user groups, which can be used to\nenhance the quality of the discovered explanations. Finally, we have evaluated\nour proposed techniques using an access log and data from the University of\nMichigan Health System. Our results demonstrate that in practice we can provide\nexplanations for over 94% of data accesses in the log.</p>\n", "tags": [] },
{"key": "fan2020trust", "year": "2020", "title":"Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis", "abstract": "<p>With the rapid growth of Android malware, many machine learning-based malware\nanalysis approaches are proposed to mitigate the severe phenomenon. However,\nsuch classifiers are opaque, non-intuitive, and difficult for analysts to\nunderstand the inner decision reason. For this reason, a variety of explanation\napproaches are proposed to interpret predictions by providing important\nfeatures. Unfortunately, the explanation results obtained in the malware\nanalysis domain cannot achieve a consensus in general, which makes the analysts\nconfused about whether they can trust such results. In this work, we propose\nprincipled guidelines to assess the quality of five explanation approaches by\ndesigning three critical quantitative metrics to measure their stability,\nrobustness, and effectiveness. Furthermore, we collect five widely-used malware\ndatasets and apply the explanation approaches on them in two tasks, including\nmalware detection and familial identification. Based on the generated\nexplanation results, we conduct a sanity check of such explanation approaches\nin terms of the three metrics. The results demonstrate that our metrics can\nassess the explanation approaches and help us obtain the knowledge of most\ntypical malicious behaviors for malware analysis.</p>\n", "tags": [] },
{"key": "fel2020good", "year": "2020", "title":"How Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks", "abstract": "<p>A plethora of methods have been proposed to explain how deep neural networks\nreach their decisions but comparatively, little effort has been made to ensure\nthat the explanations produced by these methods are objectively relevant. While\nseveral desirable properties for trustworthy explanations have been formulated,\nobjective measures have been harder to derive. Here, we propose two new\nmeasures to evaluate explanations borrowed from the field of algorithmic\nstability: mean generalizability MeGe and relative consistency ReCo. We conduct\nextensive experiments on different network architectures, common explainability\nmethods, and several image datasets to demonstrate the benefits of the proposed\nmeasures.In comparison to ours, popular fidelity measures are not sufficient to\nguarantee trustworthy explanations.Finally, we found that 1-Lipschitz networks\nproduce explanations with higher MeGe and ReCo than common neural networks\nwhile reaching similar accuracy. This suggests that 1-Lipschitz networks are a\nrelevant direction towards predictors that are more explainable and\ntrustworthy.</p>\n", "tags": [] },
{"key": "folke2021explainable", "year": "2021", "title":"Explainable AI for Natural Adversarial Images", "abstract": "<p>Adversarial images highlight how vulnerable modern image classifiers are to\nperturbations outside of their training set. Human oversight might mitigate\nthis weakness, but depends on humans understanding the AI well enough to\npredict when it is likely to make a mistake. In previous work we have found\nthat humans tend to assume that the AI’s decision process mirrors their own.\nHere we evaluate if methods from explainable AI can disrupt this assumption to\nhelp participants predict AI classifications for adversarial and standard\nimages. We find that both saliency maps and examples facilitate catching AI\nerrors, but their effects are not additive, and saliency maps are more\neffective than examples.</p>\n", "tags": [] },
{"key": "fong2017interpretable", "year": "2017", "title":"Interpretable Explanations of Black Boxes by Meaningful Perturbation", "abstract": "<p>As machine learning algorithms are increasingly applied to high impact yet\nhigh risk tasks, such as medical diagnosis or autonomous driving, it is\ncritical that researchers can explain how such algorithms arrived at their\npredictions. In recent years, a number of image saliency methods have been\ndeveloped to summarize where highly complex neural networks “look” in an image\nfor evidence for their predictions. However, these techniques are limited by\ntheir heuristic nature and architectural constraints. In this paper, we make\ntwo main contributions: First, we propose a general framework for learning\ndifferent kinds of explanations for any black box algorithm. Second, we\nspecialise the framework to find the part of an image most responsible for a\nclassifier decision. Unlike previous works, our method is model-agnostic and\ntestable because it is grounded in explicit and interpretable image\nperturbations.</p>\n", "tags": [] },
{"key": "freiesleben2020intriguing", "year": "2020", "title":"The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples", "abstract": "<p>The same method that creates adversarial examples (AEs) to fool\nimage-classifiers can be used to generate counterfactual explanations (CEs)\nthat explain algorithmic decisions. This observation has led researchers to\nconsider CEs as AEs by another name. We argue that the relationship to the true\nlabel and the tolerance with respect to proximity are two properties that\nformally distinguish CEs and AEs. Based on these arguments, we introduce CEs,\nAEs, and related concepts mathematically in a common framework. Furthermore, we\nshow connections between current methods for generating CEs and AEs, and\nestimate that the fields will merge more and more as the number of common\nuse-cases grows.</p>\n", "tags": [] },
{"key": "främling2020explainable", "year": "2020", "title":"Explainable AI without Interpretable Model", "abstract": "<p>Explainability has been a challenge in AI for as long as AI has existed. With\nthe recently increased use of AI in society, it has become more important than\never that AI systems would be able to explain the reasoning behind their\nresults also to end-users in situations such as being eliminated from a\nrecruitment process or having a bank loan application refused by an AI system.\nEspecially if the AI system has been trained using Machine Learning, it tends\nto contain too many parameters for them to be analysed and understood, which\nhas caused them to be called `black-box’ systems. Most Explainable AI (XAI)\nmethods are based on extracting an interpretable model that can be used for\nproducing explanations. However, the interpretable model does not necessarily\nmap accurately to the original black-box model. Furthermore, the\nunderstandability of interpretable models for an end-user remains questionable.\nThe notions of Contextual Importance and Utility (CIU) presented in this paper\nmake it possible to produce human-like explanations of black-box outcomes\ndirectly, without creating an interpretable model. Therefore, CIU explanations\nmap accurately to the black-box model itself. CIU is completely model-agnostic\nand can be used with any black-box system. In addition to feature importance,\nthe utility concept that is well-known in Decision Theory provides a new\ndimension to explanations compared to most existing XAI methods. Finally, CIU\ncan produce explanations at any level of abstraction and using different\nvocabularies and other means of interaction, which makes it possible to adjust\nexplanations and interaction according to the context and to the target users.</p>\n", "tags": [] },
{"key": "fu2020interactive", "year": "2020", "title":"Interactive Knowledge Distillation", "abstract": "<p>Knowledge distillation is a standard teacher-student learning framework to\ntrain a light-weight student network under the guidance of a well-trained large\nteacher network. As an effective teaching strategy, interactive teaching has\nbeen widely employed at school to motivate students, in which teachers not only\nprovide knowledge but also give constructive feedback to students upon their\nresponses, to improve their learning performance. In this work, we propose an\nInterActive Knowledge Distillation (IAKD) scheme to leverage the interactive\nteaching strategy for efficient knowledge distillation. In the distillation\nprocess, the interaction between teacher and student networks is implemented by\na swapping-in operation: randomly replacing the blocks in the student network\nwith the corresponding blocks in the teacher network. In the way, we directly\ninvolve the teacher’s powerful feature transformation ability to largely boost\nthe student’s performance. Experiments with typical settings of teacher-student\nnetworks demonstrate that the student networks trained by our IAKD achieve\nbetter performance than those trained by conventional knowledge distillation\nmethods on diverse image classification datasets.</p>\n", "tags": [] },
{"key": "gamboa2017deep", "year": "2017", "title":"Deep Learning for Time-Series Analysis", "abstract": "<p>In many real-world application, e.g., speech recognition or sleep stage\nclassification, data are captured over the course of time, constituting a\nTime-Series. Time-Series often contain temporal dependencies that cause two\notherwise identical points of time to belong to different classes or predict\ndifferent behavior. This characteristic generally increases the difficulty of\nanalysing them. Existing techniques often depended on hand-crafted features\nthat were expensive to create and required expert knowledge of the field. With\nthe advent of Deep Learning new models of unsupervised learning of features for\nTime-series analysis and forecast have been developed. Such new developments\nare the topic of this paper: a review of the main Deep Learning techniques is\npresented, and some applications on Time-Series analysis are summaried. The\nresults make it clear that Deep Learning has a lot to contribute to the field.</p>\n", "tags": [] },
{"key": "garreau2020looking", "year": "2020", "title":"Looking Deeper into Tabular LIME", "abstract": "<p>In this paper, we present a thorough theoretical analysis of the default\nimplementation of LIME in the case of tabular data. We prove that in the large\nsample limit, the interpretable coefficients provided by Tabular LIME can be\ncomputed in an explicit way as a function of the algorithm parameters and some\nexpectation computations related to the black-box model. When the function to\nexplain has some nice algebraic structure (linear, multiplicative, or sparsely\ndepending on a subset of the coordinates), our analysis provides interesting\ninsights into the explanations provided by LIME. These can be applied to a\nrange of machine learning models including Gaussian kernels or CART random\nforests. As an example, for linear functions we show that LIME has the\ndesirable property to provide explanations that are proportional to the\ncoefficients of the function to explain and to ignore coordinates that are not\nused by the function to explain. For partition-based regressors, on the other\nside, we show that LIME produces undesired artifacts that may provide\nmisleading explanations.</p>\n", "tags": [] },
{"key": "garreau2021lime", "year": "2021", "title":"What does LIME really see in images?", "abstract": "<p>The performance of modern algorithms on certain computer vision tasks such as\nobject recognition is now close to that of humans. This success was achieved at\nthe price of complicated architectures depending on millions of parameters and\nit has become quite challenging to understand how particular predictions are\nmade. Interpretability methods propose to give us this understanding. In this\npaper, we study LIME, perhaps one of the most popular. On the theoretical side,\nwe show that when the number of generated examples is large, LIME explanations\nare concentrated around a limit explanation for which we give an explicit\nexpression. We further this study for elementary shape detectors and linear\nmodels. As a consequence of this analysis, we uncover a connection between LIME\nand integrated gradients, another explanation method. More precisely, the LIME\nexplanations are similar to the sum of integrated gradients over the\nsuperpixels used in the preprocessing step of LIME.</p>\n", "tags": [] },
{"key": "gautam2021looks", "year": "2021", "title":"This looks more like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation", "abstract": "<p>Current machine learning models have shown high efficiency in solving a wide\nvariety of real-world problems. However, their black box character poses a\nmajor challenge for the understanding and traceability of the underlying\ndecision-making strategies. As a remedy, many post-hoc explanation and\nself-explanatory methods have been developed to interpret the models’ behavior.\nThese methods, in addition, enable the identification of artifacts that can be\nlearned by the model as class-relevant features. In this work, we provide a\ndetailed case study of the self-explaining network, ProtoPNet, in the presence\nof a spectrum of artifacts. Accordingly, we identify the main drawbacks of\nProtoPNet, especially, its coarse and spatially imprecise explanations. We\naddress these limitations by introducing Prototypical Relevance Propagation\n(PRP), a novel method for generating more precise model-aware explanations.\nFurthermore, in order to obtain a clean dataset, we propose to use multi-view\nclustering strategies for segregating the artifact images using the PRP\nexplanations, thereby suppressing the potential artifact learning in the\nmodels.</p>\n", "tags": [] },
{"key": "ge2022explainable", "year": "2022", "title":"Explainable Fairness in Recommendation", "abstract": "<p>Existing research on fairness-aware recommendation has mainly focused on the\nquantification of fairness and the development of fair recommendation models,\nneither of which studies a more substantial problem–identifying the underlying\nreason of model disparity in recommendation. This information is critical for\nrecommender system designers to understand the intrinsic recommendation\nmechanism and provides insights on how to improve model fairness to decision\nmakers. Fortunately, with the rapid development of Explainable AI, we can use\nmodel explainability to gain insights into model (un)fairness. In this paper,\nwe study the problem of explainable fairness, which helps to gain insights\nabout why a system is fair or unfair, and guides the design of fair recommender\nsystems with a more informed and unified methodology. Particularly, we focus on\na common setting with feature-aware recommendation and exposure unfairness, but\nthe proposed explainable fairness framework is general and can be applied to\nother recommendation settings and fairness definitions. We propose a\nCounterfactual Explainable Fairness framework, called CEF, which generates\nexplanations about model fairness that can improve the fairness without\nsignificantly hurting the performance.The CEF framework formulates an\noptimization problem to learn the “minimal” change of the input features that\nchanges the recommendation results to a certain level of fairness. Based on the\ncounterfactual recommendation result of each feature, we calculate an\nexplainability score in terms of the fairness-utility trade-off to rank all the\nfeature-based explanations, and select the top ones as fairness explanations.</p>\n", "tags": [] },
{"key": "geada2021trustyai", "year": "2021", "title":"TrustyAI Explainability Toolkit", "abstract": "<p>Artificial intelligence (AI) is becoming increasingly more popular and can be\nfound in workplaces and homes around the world. The decisions made by such\n“black box” systems are often opaque; that is, so complex as to be functionally\nimpossible to understand. How do we ensure that these systems are behaving as\ndesired? TrustyAI is an initiative which looks into explainable artificial\nintelligence (XAI) solutions to address this issue of explainability in the\ncontext of both AI models and decision services. This paper presents the\nTrustyAI Explainability Toolkit, a Java and Python library that provides XAI\nexplanations of decision services and predictive models for both enterprise and\ndata science use-cases. We describe the TrustyAI implementations and extensions\nto techniques such as LIME, SHAP and counterfactuals, which are benchmarked\nagainst existing implementations in a variety of experiments.</p>\n", "tags": [] },
{"key": "gerlings2021explainable", "year": "2021", "title":"Explainable AI, but explainable to whom?", "abstract": "<p>Advances in AI technologies have resulted in superior levels of AI-based\nmodel performance. However, this has also led to a greater degree of model\ncomplexity, resulting in ‘black box’ models. In response to the AI black box\nproblem, the field of explainable AI (xAI) has emerged with the aim of\nproviding explanations catered to human understanding, trust, and transparency.\nYet, we still have a limited understanding of how xAI addresses the need for\nexplainable AI in the context of healthcare. Our research explores the\ndiffering explanation needs amongst stakeholders during the development of an\nAI-system for classifying COVID-19 patients for the ICU. We demonstrate that\nthere is a constellation of stakeholders who have different explanation needs,\nnot just the ‘user’. Further, the findings demonstrate how the need for xAI\nemerges through concerns associated with specific stakeholder groups i.e., the\ndevelopment team, subject matter experts, decision makers, and the audience.\nOur findings contribute to the expansion of xAI by highlighting that different\nstakeholders have different explanation needs. From a practical perspective,\nthe study provides insights on how AI systems can be adjusted to support\ndifferent stakeholders needs, ensuring better implementation and operation in a\nhealthcare context.</p>\n", "tags": [] },
{"key": "ghorbani2017interpretation", "year": "2017", "title":"Interpretation of Neural Networks is Fragile", "abstract": "<p>In order for machine learning to be deployed and trusted in many\napplications, it is crucial to be able to reliably explain why the machine\nlearning algorithm makes certain predictions. For example, if an algorithm\nclassifies a given pathology image to be a malignant tumor, then the doctor may\nneed to know which parts of the image led the algorithm to this classification.\nHow to interpret black-box predictors is thus an important and active area of\nresearch. A fundamental question is: how much can we trust the interpretation\nitself? In this paper, we show that interpretation of deep learning predictions\nis extremely fragile in the following sense: two perceptively indistinguishable\ninputs with the same predicted label can be assigned very different\ninterpretations. We systematically characterize the fragility of several\nwidely-used feature-importance interpretation methods (saliency maps, relevance\npropagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that\neven small random perturbation can change the feature importance and new\nsystematic perturbations can lead to dramatically different interpretations\nwithout changing the label. We extend these results to show that\ninterpretations based on exemplars (e.g. influence functions) are similarly\nfragile. Our analysis of the geometry of the Hessian matrix gives insight on\nwhy fragility could be a fundamental challenge to the current interpretation\napproaches.</p>\n", "tags": [] },
{"key": "ghosh2017towards", "year": "2017", "title":"Towards a New Interpretation of Separable Convolutions", "abstract": "<p>In recent times, the use of separable convolutions in deep convolutional\nneural network architectures has been explored. Several researchers, most\nnotably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in\ntheir deep architectures and have demonstrated state of the art or close to\nstate of the art performance. However, the underlying mechanism of action of\nseparable convolutions are still not fully understood. Although their\nmathematical definition is well understood as a depthwise convolution followed\nby a pointwise convolution, deeper interpretations such as the extreme\nInception hypothesis (Chollet, 2016) have failed to provide a thorough\nexplanation of their efficacy. In this paper, we propose a hybrid\ninterpretation that we believe is a better model for explaining the efficacy of\nseparable convolutions.</p>\n", "tags": [] },
{"key": "ghosh2019generating", "year": "2019", "title":"Generating Natural Language Explanations for Visual Question Answering using Scene Graphs and Visual Attention", "abstract": "<p>In this paper, we present a novel approach for the task of eXplainable\nQuestion Answering (XQA), i.e., generating natural language (NL) explanations\nfor the Visual Question Answering (VQA) problem. We generate NL explanations\ncomprising of the evidence to support the answer to a question asked to an\nimage using two sources of information: (a) annotations of entities in an image\n(e.g., object labels, region descriptions, relation phrases) generated from the\nscene graph of the image, and (b) the attention map generated by a VQA model\nwhen answering the question. We show how combining the visual attention map\nwith the NL representation of relevant scene graph entities, carefully selected\nusing a language model, can give reasonable textual explanations without the\nneed of any additional collected data (explanation captions, etc). We run our\nalgorithms on the Visual Genome (VG) dataset and conduct internal user-studies\nto demonstrate the efficacy of our approach over a strong baseline. We have\nalso released a live web demo showcasing our VQA and textual explanation\ngeneration using scene graphs and visual attention.</p>\n", "tags": [] },
{"key": "gilpin2019explaining", "year": "2019", "title":"Explaining Explanations to Society", "abstract": "<p>There is a disconnect between explanatory artificial intelligence (XAI)\nmethods and the types of explanations that are useful for and demanded by\nsociety (policy makers, government officials, etc.) Questions that experts in\nartificial intelligence (AI) ask opaque systems provide inside explanations,\nfocused on debugging, reliability, and validation. These are different from\nthose that society will ask of these systems to build trust and confidence in\ntheir decisions. Although explanatory AI systems can answer many questions that\nexperts desire, they often don’t explain why they made decisions in a way that\nis precise (true to the model) and understandable to humans. These outside\nexplanations can be used to build trust, comply with regulatory and policy\nchanges, and act as external validation. In this paper, we focus on XAI methods\nfor deep neural networks (DNNs) because of DNNs’ use in decision-making and\ninherent opacity. We explore the types of questions that explanatory DNN\nsystems can answer and discuss challenges in building explanatory systems that\nprovide outside explanations for societal requirements and benefit.</p>\n", "tags": [] },
{"key": "ginsparg2004information", "year": "2004", "title":"Information, please... ?", "abstract": "<p>Stephen Hawking’s recent concession that black holes do not irretrievably\neradicate information after all has garnered much attention. It is refreshing\nto see the public focused, if just for a moment, on an important conundrum that\nhas fascinated theoretical physicists for three decades, and prompted much\nconceptual progress. The scientific issues, however, remain much less settled\nthan Dr. Hawking’s celebrated wager on the question.</p>\n", "tags": [] },
{"key": "gosiewska2019trust", "year": "2019", "title":"Do Not Trust Additive Explanations", "abstract": "<p>Explainable Artificial Intelligence (XAI)has received a great deal of\nattention recently. Explainability is being presented as a remedy for the\ndistrust of complex and opaque models. Model agnostic methods such as LIME,\nSHAP, or Break Down promise instance-level interpretability for any complex\nmachine learning model. But how faithful are these additive explanations? Can\nwe rely on additive explanations for non-additive models?\n  In this paper, we (1) examine the behavior of the most popular instance-level\nexplanations under the presence of interactions, (2) introduce a new method\nthat detects interactions for instance-level explanations, (3) perform a large\nscale benchmark to see how frequently additive explanations may be misleading.</p>\n", "tags": [] },
{"key": "goyal2019counterfactual", "year": "2019", "title":"Counterfactual Visual Explanations", "abstract": "<p>In this work, we develop a technique to produce counterfactual visual\nexplanations. Given a ‘query’ image $I$ for which a vision system predicts\nclass $c$, a counterfactual visual explanation identifies how $I$ could change\nsuch that the system would output a different specified class $c’$. To do this,\nwe select a ‘distractor’ image $I’$ that the system predicts as class $c’$ and\nidentify spatial regions in $I$ and $I’$ such that replacing the identified\nregion in $I$ with the identified region in $I’$ would push the system towards\nclassifying $I$ as $c’$. We apply our approach to multiple image classification\ndatasets generating qualitative results showcasing the interpretability and\ndiscriminativeness of our counterfactual explanations. To explore the\neffectiveness of our explanations in teaching humans, we present machine\nteaching experiments for the task of fine-grained bird classification. We find\nthat users trained to distinguish bird species fare better when given access to\ncounterfactual explanations in addition to training examples.</p>\n", "tags": [] },
{"key": "graziotin2015feel", "year": "2015", "title":"How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance", "abstract": "<p>Affects—emotions and moods—have an impact on cognitive activities and the\nworking performance of individuals. Development tasks are undertaken through\ncognitive processes, yet software engineering research lacks theory on affects\nand their impact on software development activities. In this paper, we report\non an interpretive study aimed at broadening our understanding of the\npsychology of programming in terms of the experience of affects while\nprogramming, and the impact of affects on programming performance. We conducted\na qualitative interpretive study based on: face-to-face open-ended interviews,\nin-field observations, and e-mail exchanges. This enabled us to construct a\nnovel explanatory theory of the impact of affects on development performance.\nThe theory is explicated using an established taxonomy framework. The proposed\ntheory builds upon the concepts of events, affects, attractors, focus, goals,\nand performance. Theoretical and practical implications are given.</p>\n", "tags": [] },
{"key": "gu2020interpretable", "year": "2020", "title":"Interpretable Graph Capsule Networks for Object Recognition", "abstract": "<p>Capsule Networks, as alternatives to Convolutional Neural Networks, have been\nproposed to recognize objects from images. The current literature demonstrates\nmany advantages of CapsNets over CNNs. However, how to create explanations for\nindividual classifications of CapsNets has not been well explored. The widely\nused saliency methods are mainly proposed for explaining CNN-based\nclassifications; they create saliency map explanations by combining activation\nvalues and the corresponding gradients, e.g., Grad-CAM. These saliency methods\nrequire a specific architecture of the underlying classifiers and cannot be\ntrivially applied to CapsNets due to the iterative routing mechanism therein.\nTo overcome the lack of interpretability, we can either propose new post-hoc\ninterpretation methods for CapsNets or modifying the model to have build-in\nexplanations. In this work, we explore the latter. Specifically, we propose\ninterpretable Graph Capsule Networks (GraCapsNets), where we replace the\nrouting part with a multi-head attention-based Graph Pooling approach. In the\nproposed model, individual classification explanations can be created\neffectively and efficiently. Our model also demonstrates some unexpected\nbenefits, even though it replaces the fundamental part of CapsNets. Our\nGraCapsNets achieve better classification performance with fewer parameters and\nbetter adversarial robustness, when compared to CapsNets. Besides, GraCapsNets\nalso keep other advantages of CapsNets, namely, disentangled representations\nand affine transformation robustness.</p>\n", "tags": [] },
{"key": "guidotti2018survey", "year": "2018", "title":"A Survey Of Methods For Explaining Black Box Models", "abstract": "<p>In the last years many accurate decision support systems have been\nconstructed as black boxes, that is as systems that hide their internal logic\nto the user. This lack of explanation constitutes both a practical and an\nethical issue. The literature reports many approaches aimed at overcoming this\ncrucial weakness sometimes at the cost of scarifying accuracy for\ninterpretability. The applications in which black box decision systems can be\nused are various, and each approach is typically developed to provide a\nsolution for a specific problem and, as a consequence, delineating explicitly\nor implicitly its own definition of interpretability and explanation. The aim\nof this paper is to provide a classification of the main problems addressed in\nthe literature with respect to the notion of explanation and the type of black\nbox system. Given a problem definition, a black box type, and a desired\nexplanation this survey should help the researcher to find the proposals more\nuseful for his own work. The proposed classification of approaches to open\nblack box models should also be useful for putting the many research open\nquestions in perspective.</p>\n", "tags": [] },
{"key": "guo2018visualizing", "year": "2018", "title":"Visualizing and Understanding Deep Neural Networks in CTR Prediction", "abstract": "<p>Although deep learning techniques have been successfully applied to many\ntasks, interpreting deep neural network models is still a big challenge to us.\nRecently, many works have been done on visualizing and analyzing the mechanism\nof deep neural networks in the areas of image processing and natural language\nprocessing. In this paper, we present our approaches to visualize and\nunderstand deep neural networks for a very important commercial task–CTR\n(Click-through rate) prediction. We conduct experiments on the productive data\nfrom our online advertising system with daily varying distribution. To\nunderstand the mechanism and the performance of the model, we inspect the\nmodel’s inner status at neuron level. Also, a probe approach is implemented to\nmeasure the layer-wise performance of the model. Moreover, to measure the\ninfluence from the input features, we calculate saliency scores based on the\nback-propagated gradients. Practical applications are also discussed, for\nexample, in understanding, monitoring, diagnosing and refining models and\nalgorithms.</p>\n", "tags": [] },
{"key": "guo2019exploring", "year": "2019", "title":"Exploring Interpretable LSTM Neural Networks over Multi-Variable Data", "abstract": "<p>For recurrent neural networks trained on time series with target and\nexogenous variables, in addition to accurate prediction, it is also desired to\nprovide interpretable insights into the data. In this paper, we explore the\nstructure of LSTM recurrent neural networks to learn variable-wise hidden\nstates, with the aim to capture different dynamics in multi-variable time\nseries and distinguish the contribution of variables to the prediction. With\nthese variable-wise hidden states, a mixture attention mechanism is proposed to\nmodel the generative process of the target. Then we develop associated training\nmethods to jointly learn network parameters, variable and temporal importance\nw.r.t the prediction of the target variable. Extensive experiments on real\ndatasets demonstrate enhanced prediction performance by capturing the dynamics\nof different variables. Meanwhile, we evaluate the interpretation results both\nqualitatively and quantitatively. It exhibits the prospect as an end-to-end\nframework for both forecasting and knowledge extraction over multi-variable\ndata.</p>\n", "tags": [] },
{"key": "gupta2019simple", "year": "2019", "title":"A Simple Saliency Method That Passes the Sanity Checks", "abstract": "<p>There is great interest in “saliency methods” (also called “attribution\nmethods”), which give “explanations” for a deep net’s decision, by assigning a\n“score” to each feature/pixel in the input. Their design usually involves\ncredit-assignment via the gradient of the output with respect to input.\nRecently Adebayo et al. [arXiv:1810.03292] questioned the validity of many of\nthese methods since they do not pass simple <em>sanity checks</em> which test whether\nthe scores shift/vanish when layers of the trained net are randomized, or when\nthe net is retrained using random labels for inputs.\n  We propose a simple fix to existing saliency methods that helps them pass\nsanity checks, which we call “competition for pixels”. This involves computing\nsaliency maps for all possible labels in the classification task, and using a\nsimple competition among them to identify and remove less relevant pixels from\nthe map. The simplest variant of this is “Competitive Gradient $\\odot$ Input\n(CGI)”: it is efficient, requires no additional training, and uses only the\ninput and gradient. Some theoretical justification is provided for it\n(especially for ReLU networks) and its performance is empirically demonstrated.</p>\n", "tags": [] },
{"key": "halpern2002causes", "year": "2002", "title":"Causes and Explanations: A Structural-Model Approach. Part II: Explanations", "abstract": "<p>We propose new definitions of (causal) explanation, using structural\nequations to model counterfactuals. The definition is based on the notion of\nactual cause, as defined and motivated in a companion paper. Essentially, an\nexplanation is a fact that is not known for certain but, if found to be true,\nwould constitute an actual cause of the fact to be explained, regardless of the\nagent’s initial uncertainty. We show that the definition handles well a number\nof problematic examples from the literature.</p>\n", "tags": [] },
{"key": "han2020explaining", "year": "2020", "title":"Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions", "abstract": "<p>Modern deep learning models for NLP are notoriously opaque. This has\nmotivated the development of methods for interpreting such models, e.g., via\ngradient-based saliency maps or the visualization of attention weights. Such\napproaches aim to provide explanations for a particular model prediction by\nhighlighting important words in the corresponding input text. While this might\nbe useful for tasks where decisions are explicitly influenced by individual\ntokens in the input, we suspect that such highlighting is not suitable for\ntasks where model decisions should be driven by more complex reasoning. In this\nwork, we investigate the use of influence functions for NLP, providing an\nalternative approach to interpreting neural text classifiers. Influence\nfunctions explain the decisions of a model by identifying influential training\nexamples. Despite the promise of this approach, influence functions have not\nyet been extensively evaluated in the context of NLP, a gap addressed by this\nwork. We conduct a comparison between influence functions and common\nword-saliency methods on representative tasks. As suspected, we find that\ninfluence functions are particularly useful for natural language inference, a\ntask in which ‘saliency maps’ may not have clear interpretation. Furthermore,\nwe develop a new quantitative measure based on influence functions that can\nreveal artifacts in training data.</p>\n", "tags": [] },
{"key": "hase2020evaluating", "year": "2020", "title":"Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?", "abstract": "<p>Algorithmic approaches to interpreting machine learning models have\nproliferated in recent years. We carry out human subject tests that are the\nfirst of their kind to isolate the effect of algorithmic explanations on a key\naspect of model interpretability, simulatability, while avoiding important\nconfounding experimental factors. A model is simulatable when a person can\npredict its behavior on new inputs. Through two kinds of simulation tests\ninvolving text and tabular data, we evaluate five explanations methods: (1)\nLIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a\nComposite approach that combines explanations from each method. Clear evidence\nof method effectiveness is found in very few cases: LIME improves\nsimulatability in tabular classification, and our Prototype method is effective\nin counterfactual simulation tests. We also collect subjective ratings of\nexplanations, but we do not find that ratings are predictive of how helpful\nexplanations are. Our results provide the first reliable and comprehensive\nestimates of how explanations influence simulatability across a variety of\nexplanation methods and data domains. We show that (1) we need to be careful\nabout the metrics we use to evaluate explanation methods, and (2) there is\nsignificant room for improvement in current methods. All our supporting code,\ndata, and models are publicly available at:\nhttps://github.com/peterbhase/InterpretableNLP-ACL2020</p>\n", "tags": [] },
{"key": "hase2021models", "year": "2021", "title":"When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data", "abstract": "<p>Many methods now exist for conditioning model outputs on task instructions,\nretrieved documents, and user-provided explanations and feedback. Rather than\nrelying solely on examples of task inputs and outputs, these approaches use\nvaluable additional data for improving model correctness and aligning learned\nmodels with human priors. Meanwhile, a growing body of evidence suggests that\nsome language models can (1) store a large amount of knowledge in their\nparameters, and (2) perform inference over tasks in textual inputs at test\ntime. These results raise the possibility that, for some tasks, humans cannot\nexplain to a model any more about the task than it already knows or could infer\non its own. In this paper, we study the circumstances under which explanations\nof individual data points can (or cannot) improve modeling performance. In\norder to carefully control important properties of the data and explanations,\nwe introduce a synthetic dataset for experiments, and we also make use of three\nexisting datasets with explanations: e-SNLI, TACRED, and SemEval. We first give\na formal framework for the available modeling approaches, in which explanation\ndata can be used as model inputs, as targets, or as a prior. After arguing that\nthe most promising role for explanation data is as model inputs, we propose to\nuse a retrieval-based method and show that it solves our synthetic task with\naccuracies upwards of 95%, while baselines without explanation data achieve\nbelow 65% accuracy. We then identify properties of datasets for which\nretrieval-based modeling fails. With the three existing datasets, we find no\nimprovements from explanation retrieval. Drawing on findings from our synthetic\ntask, we suggest that at least one of six preconditions for successful modeling\nfails to hold with these datasets. Our code is publicly available at\nhttps://github.com/peterbhase/ExplanationRoles</p>\n", "tags": [] },
{"key": "he2019understanding", "year": "2019", "title":"Understanding and Visualizing Deep Visual Saliency Models", "abstract": "<p>Recently, data-driven deep saliency models have achieved high performance and\nhave outperformed classical saliency models, as demonstrated by results on\ndatasets such as the MIT300 and SALICON. Yet, there remains a large gap between\nthe performance of these models and the inter-human baseline. Some outstanding\nquestions include what have these models learned, how and where they fail, and\nhow they can be improved. This article attempts to answer these questions by\nanalyzing the representations learned by individual neurons located at the\nintermediate layers of deep saliency models. To this end, we follow the steps\nof existing deep saliency models, that is borrowing a pre-trained model of\nobject recognition to encode the visual features and learning a decoder to\ninfer the saliency. We consider two cases when the encoder is used as a fixed\nfeature extractor and when it is fine-tuned, and compare the inner\nrepresentations of the network. To study how the learned representations depend\non the task, we fine-tune the same network using the same image set but for two\ndifferent tasks: saliency prediction versus scene classification. Our analyses\nreveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are\nalready encoded within various layers of the network pre-trained for object\nrecognition, 2) using modern datasets, we find that fine-tuning pre-trained\nmodels for saliency prediction makes them favor some categories (e.g. head)\nover some others (e.g. text), 3) although deep models of saliency outperform\nclassical models on natural images, the converse is true for synthetic stimuli\n(e.g. pop-out search arrays), an evidence of significant difference between\nhuman and data-driven saliency models, and 4) we confirm that, after-fine\ntuning, the change in inner-representations is mostly due to the task and not\nthe domain shift in the data.</p>\n", "tags": [] },
{"key": "hendricks2016generating", "year": "2016", "title":"Generating Visual Explanations", "abstract": "<p>Clearly explaining a rationale for a classification decision to an end-user\ncan be as important as the decision itself. Existing approaches for deep visual\nrecognition are generally opaque and do not output any justification text;\ncontemporary vision-language models can describe image content but fail to take\ninto account class-discriminative image aspects which justify visual\npredictions. We propose a new model that focuses on the discriminating\nproperties of the visible object, jointly predicts a class label, and explains\nwhy the predicted label is appropriate for the image. We propose a novel loss\nfunction based on sampling and reinforcement learning that learns to generate\nsentences that realize a global sentence property, such as class specificity.\nOur results on a fine-grained bird species classification dataset show that our\nmodel is able to generate explanations which are not only consistent with an\nimage but also more discriminative than descriptions produced by existing\ncaptioning methods.</p>\n", "tags": [] },
{"key": "herm2022stop", "year": "2022", "title":"Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability", "abstract": "<p>Machine learning algorithms enable advanced decision making in contemporary\nintelligent systems. Research indicates that there is a tradeoff between their\nmodel performance and explainability. Machine learning models with higher\nperformance are often based on more complex algorithms and therefore lack\nexplainability and vice versa. However, there is little to no empirical\nevidence of this tradeoff from an end user perspective. We aim to provide\nempirical evidence by conducting two user experiments. Using two distinct\ndatasets, we first measure the tradeoff for five common classes of machine\nlearning algorithms. Second, we address the problem of end user perceptions of\nexplainable artificial intelligence augmentations aimed at increasing the\nunderstanding of the decision logic of high-performing complex models. Our\nresults diverge from the widespread assumption of a tradeoff curve and indicate\nthat the tradeoff between model performance and explainability is much less\ngradual in the end user’s perception. This is a stark contrast to assumed\ninherent model interpretability. Further, we found the tradeoff to be\nsituational for example due to data complexity. Results of our second\nexperiment show that while explainable artificial intelligence augmentations\ncan be used to increase explainability, the type of explanation plays an\nessential role in end user perception.</p>\n", "tags": [] },
{"key": "herman2017promise", "year": "2017", "title":"The Promise and Peril of Human Evaluation for Model Interpretability", "abstract": "<p>Transparency, user trust, and human comprehension are popular ethical\nmotivations for interpretable machine learning. In support of these goals,\nresearchers evaluate model explanation performance using humans and real world\napplications. This alone presents a challenge in many areas of artificial\nintelligence. In this position paper, we propose a distinction between\ndescriptive and persuasive explanations. We discuss reasoning suggesting that\nfunctional interpretability may be correlated with cognitive function and user\npreferences. If this is indeed the case, evaluation and optimization using\nfunctional metrics could perpetuate implicit cognitive bias in explanations\nthat threaten transparency. Finally, we propose two potential research\ndirections to disambiguate cognitive function and explanation models, retaining\ncontrol over the tradeoff between accuracy and interpretability.</p>\n", "tags": [] },
{"key": "hernandez2022natural", "year": "2022", "title":"Natural Language Descriptions of Deep Visual Features", "abstract": "<p>Some neurons in deep networks specialize in recognizing highly specific\nperceptual, structural, or semantic features of inputs. In computer vision,\ntechniques exist for identifying neurons that respond to individual concept\ncategories like colors, textures, and object classes. But these techniques are\nlimited in scope, labeling only a small subset of neurons and behaviors in any\nnetwork. Is a richer characterization of neuron-level computation possible? We\nintroduce a procedure (called MILAN, for mutual-information-guided linguistic\nannotation of neurons) that automatically labels neurons with open-ended,\ncompositional, natural language descriptions. Given a neuron, MILAN generates a\ndescription by searching for a natural language string that maximizes pointwise\nmutual information with the image regions in which the neuron is active. MILAN\nproduces fine-grained descriptions that capture categorical, relational, and\nlogical structure in learned features. These descriptions obtain high agreement\nwith human-generated feature descriptions across a diverse set of model\narchitectures and tasks, and can aid in understanding and controlling learned\nmodels. We highlight three applications of natural language neuron\ndescriptions. First, we use MILAN for analysis, characterizing the distribution\nand importance of neurons selective for attribute, category, and relational\ninformation in vision models. Second, we use MILAN for auditing, surfacing\nneurons sensitive to human faces in datasets designed to obscure them. Finally,\nwe use MILAN for editing, improving robustness in an image classifier by\ndeleting neurons sensitive to text features spuriously correlated with class\nlabels.</p>\n", "tags": [] },
{"key": "hoffmann2021looks", "year": "2021", "title":"This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks", "abstract": "<p>Deep neural networks that yield human interpretable decisions by\narchitectural design have lately become an increasingly popular alternative to\npost hoc interpretation of traditional black-box models. Among these networks,\nthe arguably most widespread approach is so-called prototype learning, where\nsimilarities to learned latent prototypes serve as the basis of classifying an\nunseen data point. In this work, we point to an important shortcoming of such\napproaches. Namely, there is a semantic gap between similarity in latent space\nand similarity in input space, which can corrupt interpretability. We design\ntwo experiments that exemplify this issue on the so-called ProtoPNet.\nSpecifically, we find that this network’s interpretability mechanism can be led\nastray by intentionally crafted or even JPEG compression artefacts, which can\nproduce incomprehensible decisions. We argue that practitioners ought to have\nthis shortcoming in mind when deploying prototype-based models in practice.</p>\n", "tags": [] },
{"key": "hohman2018visual", "year": "2018", "title":"Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers", "abstract": "<p>Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W’s and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.</p>\n", "tags": [] },
{"key": "holzinger2017need", "year": "2017", "title":"What do we need to build explainable AI systems for the medical domain?", "abstract": "<p>Artificial intelligence (AI) generally and machine learning (ML) specifically\ndemonstrate impressive practical success in many different application domains,\ne.g. in autonomous driving, speech recognition, or recommender systems. Deep\nlearning approaches, trained on extremely large data sets or using\nreinforcement learning methods have even exceeded human performance in visual\ntasks, particularly on playing games such as Atari, or mastering the game of\nGo. Even in the medical domain there are remarkable results. The central\nproblem of such models is that they are regarded as black-box models and even\nif we understand the underlying mathematical principles, they lack an explicit\ndeclarative knowledge representation, hence have difficulty in generating the\nunderlying explanatory structures. This calls for systems enabling to make\ndecisions transparent, understandable and explainable. A huge motivation for\nour approach are rising legal and privacy aspects. The new European General\nData Protection Regulation entering into force on May 25th 2018, will make\nblack-box approaches difficult to use in business. This does not imply a ban on\nautomatic learning approaches or an obligation to explain everything all the\ntime, however, there must be a possibility to make the results re-traceable on\ndemand. In this paper we outline some of our research topics in the context of\nthe relatively new area of explainable-AI with a focus on the application in\nmedicine, which is a very special domain. This is due to the fact that medical\nprofessionals are working mostly with distributed heterogeneous and complex\nsources of data. In this paper we concentrate on three sources: images, *omics\ndata and text. We argue that research in explainable-AI would generally help to\nfacilitate the implementation of AI/ML in the medical domain, and specifically\nhelp to facilitate transparency and trust.</p>\n", "tags": [] },
{"key": "homdee2020actionable", "year": "2020", "title":"Actionable Interpretation of Machine Learning Models for Sequential Data: Dementia-related Agitation Use Case", "abstract": "<p>Machine learning has shown successes for complex learning problems in which\ndata/parameters can be multidimensional and too complex for a first-principles\nbased analysis. Some applications that utilize machine learning require human\ninterpretability, not just to understand a particular result (classification,\ndetection, etc.) but also for humans to take action based on that result.\nBlack-box machine learning model interpretation has been studied, but recent\nwork has focused on validation and improving model performance. In this work,\nan actionable interpretation of black-box machine learning models is presented.\nThe proposed technique focuses on the extraction of actionable measures to help\nusers make a decision or take an action. Actionable interpretation can be\nimplemented in most traditional black-box machine learning models. It uses the\nalready trained model, used training data, and data processing techniques to\nextract actionable items from the model outcome and its time-series inputs. An\nimplementation of the actionable interpretation is shown with a use case:\ndementia-related agitation prediction and the ambient environment. It is shown\nthat actionable items can be extracted, such as the decreasing of in-home light\nlevel, which is triggering an agitation episode. This use case of actionable\ninterpretation can help dementia caregivers take action to intervene and\nprevent agitation.</p>\n", "tags": [] },
{"key": "honeycutt2020soliciting", "year": "2020", "title":"Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy", "abstract": "<p>Mixed-initiative systems allow users to interactively provide feedback to\npotentially improve system performance. Human feedback can correct model errors\nand update model parameters to dynamically adapt to changing data.\nAdditionally, many users desire the ability to have a greater level of control\nand fix perceived flaws in systems they rely on. However, how the ability to\nprovide feedback to autonomous systems influences user trust is a largely\nunexplored area of research. Our research investigates how the act of providing\nfeedback can affect user understanding of an intelligent system and its\naccuracy. We present a controlled experiment using a simulated object detection\nsystem with image data to study the effects of interactive feedback collection\non user impressions. The results show that providing human-in-the-loop feedback\nlowered both participants’ trust in the system and their perception of system\naccuracy, regardless of whether the system accuracy improved in response to\ntheir feedback. These results highlight the importance of considering the\neffects of allowing end-user feedback on user trust when designing intelligent\nsystems.</p>\n", "tags": [] },
{"key": "hong2020interpretable", "year": "2020", "title":"Interpretable Sequence Classification Via Prototype Trajectory", "abstract": "<p>We propose a novel interpretable deep neural network for text classification,\ncalled ProtoryNet, based on a new concept of prototype trajectories. Motivated\nby the prototype theory in modern linguistics, ProtoryNet makes a prediction by\nfinding the most similar prototype for each sentence in a text sequence and\nfeeding an RNN backbone with the proximity of each sentence to the\ncorresponding active prototype. The RNN backbone then captures the temporal\npattern of the prototypes, which we refer to as prototype trajectories.\nPrototype trajectories enable intuitive and fine-grained interpretation of the\nreasoning process of the RNN model, in resemblance to how humans analyze texts.\nWe also design a prototype pruning procedure to reduce the total number of\nprototypes used by the model for better interpretability. Experiments on\nmultiple public data sets show that ProtoryNet is more accurate than the\nbaseline prototype-based deep neural net and reduces the performance gap\ncompared to state-of-the-art black-box models. In addition, after prototype\npruning, the resulting ProtoryNet models only need less than or around 20\nprototypes for all datasets, which significantly benefits interpretability.\nFurthermore, we report a survey result indicating that human users find\nProtoryNet more intuitive and easier to understand than other prototype-based\nmethods.</p>\n", "tags": [] },
{"key": "hooker2018benchmark", "year": "2018", "title":"A Benchmark for Interpretability Methods in Deep Neural Networks", "abstract": "<p>We propose an empirical measure of the approximate accuracy of feature\nimportance estimates in deep neural networks. Our results across several\nlarge-scale image classification datasets show that many popular\ninterpretability methods produce estimates of feature importance that are not\nbetter than a random designation of feature importance. Only certain ensemble\nbased approaches—VarGrad and SmoothGrad-Squared—outperform such a random\nassignment of importance. The manner of ensembling remains critical, we show\nthat some approaches do no better then the underlying method but carry a far\nhigher computational burden.</p>\n", "tags": [] },
{"key": "hsieh2020evaluations", "year": "2020", "title":"Evaluations and Methods for Explanation through Robustness Analysis", "abstract": "<p>Feature based explanations, that provide importance of each feature towards\nthe model prediction, is arguably one of the most intuitive ways to explain a\nmodel. In this paper, we establish a novel set of evaluation criteria for such\nfeature based explanations by robustness analysis. In contrast to existing\nevaluations which require us to specify some way to “remove” features that\ncould inevitably introduces biases and artifacts, we make use of the subtler\nnotion of smaller adversarial perturbations. By optimizing towards our proposed\nevaluation criteria, we obtain new explanations that are loosely necessary and\nsufficient for a prediction. We further extend the explanation to extract the\nset of features that would move the current prediction to a target class by\nadopting targeted adversarial attack for the robustness analysis. Through\nexperiments across multiple domains and a user study, we validate the\nusefulness of our evaluation criteria and our derived explanations.</p>\n", "tags": [] },
{"key": "hsu2017unsupervised", "year": "2017", "title":"Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data", "abstract": "<p>We present a factorized hierarchical variational autoencoder, which learns\ndisentangled and interpretable representations from sequential data without\nsupervision. Specifically, we exploit the multi-scale nature of information in\nsequential data by formulating it explicitly within a factorized hierarchical\ngraphical model that imposes sequence-dependent priors and sequence-independent\npriors to different sets of latent variables. The model is evaluated on two\nspeech corpora to demonstrate, qualitatively, its ability to transform speakers\nor linguistic content by manipulating different sets of latent variables; and\nquantitatively, its ability to outperform an i-vector baseline for speaker\nverification and reduce the word error rate by as much as 35% in mismatched\ntrain/test scenarios for automatic speech recognition tasks.</p>\n", "tags": [] },
{"key": "hu2016harnessing", "year": "2016", "title":"Harnessing Deep Neural Networks with Logic Rules", "abstract": "<p>Combining deep neural networks with structured logic rules is desirable to\nharness flexibility and reduce uninterpretability of the neural models. We\npropose a general framework capable of enhancing various types of neural\nnetworks (e.g., CNNs and RNNs) with declarative first-order logic rules.\nSpecifically, we develop an iterative distillation method that transfers the\nstructured information of logic rules into the weights of neural networks. We\ndeploy the framework on a CNN for sentiment analysis, and an RNN for named\nentity recognition. With a few highly intuitive rules, we obtain substantial\nimprovements and achieve state-of-the-art or comparable results to previous\nbest-performing systems.</p>\n", "tags": [] },
{"key": "huang2019understanding", "year": "2019", "title":"Understanding Generalization through Visualizations", "abstract": "<p>The power of neural networks lies in their ability to generalize to unseen\ndata, yet the underlying reasons for this phenomenon remain elusive. Numerous\nrigorous attempts have been made to explain generalization, but available\nbounds are still quite loose, and analysis does not always lead to true\nunderstanding. The goal of this work is to make generalization more intuitive.\nUsing visualization methods, we discuss the mystery of generalization, the\ngeometry of loss landscapes, and how the curse (or, rather, the blessing) of\ndimensionality causes optimizers to settle into minima that generalize well.</p>\n", "tags": [] },
{"key": "huang2021physically", "year": "2021", "title":"Physically Explainable CNN for SAR Image Classification", "abstract": "<p>Integrating the special electromagnetic characteristics of Synthetic Aperture\nRadar (SAR) in deep neural networks is essential in order to enhance the\nexplainability and physics awareness of deep learning. In this paper, we first\npropose a novel physically explainable convolutional neural network for SAR\nimage classification, namely physics guided and injected learning (PGIL). It\ncomprises three parts: (1) explainable models (XM) to provide prior physics\nknowledge, (2) physics guided network (PGN) to encode the knowledge into\nphysics-aware features, and (3) physics injected network (PIN) to adaptively\nintroduce the physics-aware features into classification pipeline for label\nprediction. A hybrid Image-Physics SAR dataset format is proposed for\nevaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The\nresults show that the proposed PGIL substantially improve the classification\nperformance in case of limited labeled data compared with the counterpart\ndata-driven CNN and other pre-training methods. Additionally, the physics\nexplanations are discussed to indicate the interpretability and the physical\nconsistency preserved in the predictions. We deem the proposed method would\npromote the development of physically explainable deep learning in SAR image\ninterpretation field.</p>\n", "tags": [] },
{"key": "hvilshøj2021quantitative", "year": "2021", "title":"On Quantitative Evaluations of Counterfactuals", "abstract": "<p>As counterfactual examples become increasingly popular for explaining\ndecisions of deep learning models, it is essential to understand what\nproperties quantitative evaluation metrics do capture and equally important\nwhat they do not capture. Currently, such understanding is lacking, potentially\nslowing down scientific progress. In this paper, we consolidate the work on\nevaluating visual counterfactual examples through an analysis and experiments.\nWe find that while most metrics behave as intended for sufficiently simple\ndatasets, some fail to tell the difference between good and bad counterfactuals\nwhen the complexity increases. We observe experimentally that metrics give good\nscores to tiny adversarial-like changes, wrongly identifying such changes as\nsuperior counterfactual examples. To mitigate this issue, we propose two new\nmetrics, the Label Variation Score and the Oracle score, which are both less\nvulnerable to such tiny changes. We conclude that a proper quantitative\nevaluation of visual counterfactual examples should combine metrics to ensure\nthat all aspects of good counterfactuals are quantified.</p>\n", "tags": [] },
{"key": "höllig2022tsinterpret", "year": "2022", "title":"TSInterpret: A unified framework for time series interpretability", "abstract": "<p>With the increasing application of deep learning algorithms to time series\nclassification, especially in high-stake scenarios, the relevance of\ninterpreting those algorithms becomes key. Although research in time series\ninterpretability has grown, accessibility for practitioners is still an\nobstacle. Interpretability approaches and their visualizations are diverse in\nuse without a unified API or framework. To close this gap, we introduce\nTSInterpret an easily extensible open-source Python library for interpreting\npredictions of time series classifiers that combines existing interpretation\napproaches into one unified framework. The library features (i)\nstate-of-the-art interpretability algorithms, (ii) exposes a unified API\nenabling users to work with explanations consistently and provides (iii)\nsuitable visualizations for each explanation.</p>\n", "tags": [] },
{"key": "ignatiev2020relating", "year": "2020", "title":"On Relating 'Why?' and 'Why Not?' Explanations", "abstract": "<p>Explanations of Machine Learning (ML) models often address a ‘Why?’ question.\nSuch explanations can be related with selecting feature-value pairs which are\nsufficient for the prediction. Recent work has investigated explanations that\naddress a ‘Why Not?’ question, i.e. finding a change of feature values that\nguarantee a change of prediction. Given their goals, these two forms of\nexplaining predictions of ML models appear to be mostly unrelated. However,\nthis paper demonstrates otherwise, and establishes a rigorous formal\nrelationship between ‘Why?’ and ‘Why Not?’ explanations. Concretely, the paper\nproves that, for any given instance, ‘Why?’ explanations are minimal hitting\nsets of ‘Why Not?’ explanations and vice-versa. Furthermore, the paper devises\nnovel algorithms for extracting and enumerating both forms of explanations.</p>\n", "tags": [] },
{"key": "islam2019enabling", "year": "2019", "title":"Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural Networks", "abstract": "<p>Information fusion is an essential part of numerous engineering systems and\nbiological functions, e.g., human cognition. Fusion occurs at many levels,\nranging from the low-level combination of signals to the high-level aggregation\nof heterogeneous decision-making processes. While the last decade has witnessed\nan explosion of research in deep learning, fusion in neural networks has not\nobserved the same revolution. Specifically, most neural fusion approaches are\nad hoc, are not understood, are distributed versus localized, and/or\nexplainability is low (if present at all). Herein, we prove that the fuzzy\nChoquet integral (ChI), a powerful nonlinear aggregation function, can be\nrepresented as a multi-layer network, referred to hereafter as ChIMP. We also\nput forth an improved ChIMP (iChIMP) that leads to a stochastic gradient\ndescent-based optimization in light of the exponential number of ChI inequality\nconstraints. An additional benefit of ChIMP/iChIMP is that it enables\neXplainable AI (XAI). Synthetic validation experiments are provided and iChIMP\nis applied to the fusion of a set of heterogeneous architecture deep models in\nremote sensing. We show an improvement in model accuracy and our previously\nestablished XAI indices shed light on the quality of our data, model, and its\ndecisions.</p>\n", "tags": [] },
{"key": "islam2021explainable", "year": "2021", "title":"Explainable Artificial Intelligence Approaches: A Survey", "abstract": "<p>The lack of explainability of a decision from an Artificial Intelligence (AI)\nbased “black box” system/model, despite its superiority in many real-world\napplications, is a key stumbling block for adopting AI in many high stakes\napplications of different domain or industry. While many popular Explainable\nArtificial Intelligence (XAI) methods or approaches are available to facilitate\na human-friendly explanation of the decision, each has its own merits and\ndemerits, with a plethora of open challenges. We demonstrate popular XAI\nmethods with a mutual case study/task (i.e., credit default prediction),\nanalyze for competitive advantages from multiple perspectives (e.g., local,\nglobal), provide meaningful insight on quantifying explainability, and\nrecommend paths towards responsible or human-centered AI using XAI as a medium.\nPractitioners can use this work as a catalog to understand, compare, and\ncorrelate competitive advantages of popular XAI methods. In addition, this\nsurvey elicits future research directions towards responsible or human-centric\nAI systems, which is crucial to adopt AI in high stakes applications.</p>\n", "tags": [] },
{"key": "ismail2019input-cell", "year": "2019", "title":"Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks", "abstract": "<p>Recent efforts to improve the interpretability of deep neural networks use\nsaliency to characterize the importance of input features to predictions made\nby models. Work on interpretability using saliency-based methods on Recurrent\nNeural Networks (RNNs) has mostly targeted language tasks, and their\napplicability to time series data is less understood. In this work we analyze\nsaliency-based methods for RNNs, both classical and gated cell architectures.\nWe show that RNN saliency vanishes over time, biasing detection of salient\nfeatures only to later time steps and are, therefore, incapable of reliably\ndetecting important features at arbitrary time intervals. To address this\nvanishing saliency problem, we propose a novel RNN cell structure (input-cell\nattention), which can extend any RNN cell architecture. At each time step,\ninstead of only looking at the current input vector, input-cell attention uses\na fixed-size matrix embedding, each row of the matrix attending to different\ninputs from current or previous time steps. Using synthetic data, we show that\nthe saliency map produced by the input-cell attention RNN is able to faithfully\ndetect important features regardless of their occurrence in time. We also apply\nthe input-cell attention RNN on a neuroscience task analyzing functional\nMagnetic Resonance Imaging (fMRI) data for human subjects performing a variety\nof tasks. In this case, we use saliency to characterize brain regions (input\nfeatures) for which activity is important to distinguish between tasks. We show\nthat standard RNN architectures are only capable of detecting important brain\nregions in the last few time steps of the fMRI data, while the input-cell\nattention model is able to detect important brain region activity across time\nwithout latter time step biases.</p>\n", "tags": [] },
{"key": "ismail2020benchmarking", "year": "2020", "title":"Benchmarking Deep Learning Interpretability in Time Series Predictions", "abstract": "<p>Saliency methods are used extensively to highlight the importance of input\nfeatures in model predictions. These methods are mostly used in vision and\nlanguage tasks, and their applications to time series data is relatively\nunexplored. In this paper, we set out to extensively compare the performance of\nvarious saliency-based interpretability methods across diverse neural\narchitectures, including Recurrent Neural Network, Temporal Convolutional\nNetworks, and Transformers in a new benchmark of synthetic time series data. We\npropose and report multiple metrics to empirically evaluate the performance of\nsaliency methods for detecting feature importance over time using both\nprecision (i.e., whether identified features contain meaningful signals) and\nrecall (i.e., the number of features with signal identified as important).\nThrough several experiments, we show that (i) in general, network architectures\nand saliency methods fail to reliably and accurately identify feature\nimportance over time in time series data, (ii) this failure is mainly due to\nthe conflation of time and feature domains, and (iii) the quality of saliency\nmaps can be improved substantially by using our proposed two-step temporal\nsaliency rescaling (TSR) approach that first calculates the importance of each\ntime step before calculating the importance of each feature at a time step.</p>\n", "tags": [] },
{"key": "ivankay2022fooling", "year": "2022", "title":"Fooling Explanations in Text Classifiers", "abstract": "<p>State-of-the-art text classification models are becoming increasingly reliant\non deep neural networks (DNNs). Due to their black-box nature, faithful and\nrobust explanation methods need to accompany classifiers for deployment in\nreal-life scenarios. However, it has been shown in vision applications that\nexplanation methods are susceptible to local, imperceptible perturbations that\ncan significantly alter the explanations without changing the predicted\nclasses. We show here that the existence of such perturbations extends to text\nclassifiers as well. Specifically, we introduceTextExplanationFooler (TEF), a\nnovel explanation attack algorithm that alters text input samples imperceptibly\nso that the outcome of widely-used explanation methods changes considerably\nwhile leaving classifier predictions unchanged. We evaluate the performance of\nthe attribution robustness estimation performance in TEF on five sequence\nclassification datasets, utilizing three DNN architectures and three\ntransformer architectures for each dataset. TEF can significantly decrease the\ncorrelation between unchanged and perturbed input attributions, which shows\nthat all models and explanation methods are susceptible to TEF perturbations.\nMoreover, we evaluate how the perturbations transfer to other model\narchitectures and attribution methods, and show that TEF perturbations are also\neffective in scenarios where the target model and explanation method are\nunknown. Finally, we introduce a semi-universal attack that is able to compute\nfast, computationally light perturbations with no knowledge of the attacked\nclassifier nor explanation method. Overall, our work shows that explanations in\ntext classifiers are very fragile and users need to carefully address their\nrobustness before relying on them in critical applications.</p>\n", "tags": [] },
{"key": "izza2020explaining", "year": "2020", "title":"On Explaining Decision Trees", "abstract": "<p>Decision trees (DTs) epitomize what have become to be known as interpretable\nmachine learning (ML) models. This is informally motivated by paths in DTs\nbeing often much smaller than the total number of features. This paper shows\nthat in some settings DTs can hardly be deemed interpretable, with paths in a\nDT being arbitrarily larger than a PI-explanation, i.e. a subset-minimal set of\nfeature values that entails the prediction. As a result, the paper proposes a\nnovel model for computing PI-explanations of DTs, which enables computing one\nPI-explanation in polynomial time. Moreover, it is shown that enumeration of\nPI-explanations can be reduced to the enumeration of minimal hitting sets.\nExperimental results were obtained on a wide range of publicly available\ndatasets with well-known DT-learning tools, and confirm that in most cases DTs\nhave paths that are proper supersets of PI-explanations.</p>\n", "tags": [] },
{"key": "jacovi2020aligning", "year": "2020", "title":"Aligning Faithful Interpretations with their Social Attribution", "abstract": "<p>We find that the requirement of model interpretations to be faithful is vague\nand incomplete. With interpretation by textual highlights as a case-study, we\npresent several failure cases. Borrowing concepts from social science, we\nidentify that the problem is a misalignment between the causal chain of\ndecisions (causal attribution) and the attribution of human behavior to the\ninterpretation (social attribution). We re-formulate faithfulness as an\naccurate attribution of causality to the model, and introduce the concept of\naligned faithfulness: faithful causal chains that are aligned with their\nexpected social behavior. The two steps of causal attribution and social\nattribution together complete the process of explaining behavior. With this\nformalization, we characterize various failures of misaligned faithful\nhighlight interpretations, and propose an alternative causal chain to remedy\nthe issues. Finally, we implement highlight explanations of the proposed causal\nformat using contrastive explanations.</p>\n", "tags": [] },
{"key": "jacovi2020towards", "year": "2020", "title":"Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?", "abstract": "<p>With the growing popularity of deep-learning based NLP models, comes a need\nfor interpretable systems. But what is interpretability, and what constitutes a\nhigh-quality interpretation? In this opinion piece we reflect on the current\nstate of interpretability evaluation research. We call for more clearly\ndifferentiating between different desired criteria an interpretation should\nsatisfy, and focus on the faithfulness criteria. We survey the literature with\nrespect to faithfulness evaluation, and arrange the current approaches around\nthree assumptions, providing an explicit form to how faithfulness is “defined”\nby the community. We provide concrete guidelines on how evaluation of\ninterpretation methods should and should not be conducted. Finally, we claim\nthat the current binary definition for faithfulness sets a potentially\nunrealistic bar for being considered faithful. We call for discarding the\nbinary notion of faithfulness in favor of a more graded one, which we believe\nwill be of greater practical utility.</p>\n", "tags": [] },
{"key": "jacovi2021contrastive", "year": "2021", "title":"Contrastive Explanations for Model Interpretability", "abstract": "<p>Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model’s decision.</p>\n", "tags": [] },
{"key": "jain2019attention", "year": "2019", "title":"Attention is not Explanation", "abstract": "<p>Attention mechanisms have seen wide adoption in neural NLP models. In\naddition to improving predictive performance, these are often touted as\naffording transparency: models equipped with attention provide a distribution\nover attended-to input units, and this is often presented (at least implicitly)\nas communicating the relative importance of inputs. However, it is unclear what\nrelationship exists between attention weights and model outputs. In this work,\nwe perform extensive experiments across a variety of NLP tasks that aim to\nassess the degree to which attention weights provide meaningful `explanations’\nfor predictions. We find that they largely do not. For example, learned\nattention weights are frequently uncorrelated with gradient-based measures of\nfeature importance, and one can identify very different attention distributions\nthat nonetheless yield equivalent predictions. Our findings show that standard\nattention modules do not provide meaningful explanations and should not be\ntreated as though they do. Code for all experiments is available at\nhttps://github.com/successar/AttentionExplanation.</p>\n", "tags": [] },
{"key": "jain2020learning", "year": "2020", "title":"Learning to Faithfully Rationalize by Construction", "abstract": "<p>In many settings it is important for one to be able to understand why a model\nmade a particular prediction. In NLP this often entails extracting snippets of\nan input text <code class=\"language-plaintext highlighter-rouge\">responsible for' corresponding model output; when such a snippet\ncomprises tokens that indeed informed the model's prediction, it is a faithful\nexplanation. In some settings, faithfulness may be critical to ensure\ntransparency. Lei et al. (2016) proposed a model to produce faithful rationales\nfor neural text classification by defining independent snippet extraction and\nprediction modules. However, the discrete selection over input tokens performed\nby this method complicates training, leading to high variance and requiring\ncareful hyperparameter tuning. We propose a simpler variant of this approach\nthat provides faithful explanations by construction. In our scheme, named\nFRESH, arbitrary feature importance scores (e.g., gradients from a trained\nmodel) are used to induce binary labels over token inputs, which an extractor\ncan be trained to predict. An independent classifier module is then trained\nexclusively on snippets provided by the extractor; these snippets thus\nconstitute faithful explanations, even if the classifier is arbitrarily\ncomplex. In both automatic and manual evaluations we find that variants of this\nsimple framework yield predictive performance superior to </code>end-to-end’\napproaches, while being more general and easier to train. Code is available at\nhttps://github.com/successar/FRESH</p>\n", "tags": [] },
{"key": "jalwana2021cameras", "year": "2021", "title":"CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency", "abstract": "<p>Backpropagation image saliency aims at explaining model predictions by\nestimating model-centric importance of individual pixels in the input. However,\nclass-insensitivity of the earlier layers in a network only allows saliency\ncomputation with low resolution activation maps of the deeper layers, resulting\nin compromised image saliency. Remedifying this can lead to sanity failures. We\npropose CAMERAS, a technique to compute high-fidelity backpropagation saliency\nmaps without requiring any external priors and preserving the map sanity. Our\nmethod systematically performs multi-scale accumulation and fusion of the\nactivation maps and backpropagated gradients to compute precise saliency maps.\nFrom accurate image saliency to articulation of relative importance of input\nfeatures for different models, and precise discrimination between model\nperception of visually similar objects, our high-resolution mapping offers\nmultiple novel insights into the black-box deep visual models, which are\npresented in the paper. We also demonstrate the utility of our saliency maps in\nadversarial setup by drastically reducing the norm of attack signals by\nfocusing them on the precise regions identified by our maps. Our method also\ninspires new evaluation metrics and a sanity check for this developing research\ndirection. Code is available here https://github.com/VisMIL/CAMERAS</p>\n", "tags": [] },
{"key": "jiang2018trust", "year": "2018", "title":"To Trust Or Not To Trust A Classifier", "abstract": "<p>Knowing when a classifier’s prediction can be trusted is useful in many\napplications and critical for safely using AI. While the bulk of the effort in\nmachine learning research has been towards improving classifier performance,\nunderstanding when a classifier’s predictions should and should not be trusted\nhas received far less attention. The standard approach is to use the\nclassifier’s discriminant or confidence score; however, we show there exists an\nalternative that is more effective in many situations. We propose a new score,\ncalled the trust score, which measures the agreement between the classifier and\na modified nearest-neighbor classifier on the testing example. We show\nempirically that high (low) trust scores produce surprisingly high precision at\nidentifying correctly (incorrectly) classified examples, consistently\noutperforming the classifier’s confidence score as well as many other\nbaselines. Further, under some mild distributional assumptions, we show that if\nthe trust score for an example is high (low), the classifier will likely agree\n(disagree) with the Bayes-optimal classifier. Our guarantees consist of\nnon-asymptotic rates of statistical consistency under various nonparametric\nsettings and build on recent developments in topological data analysis.</p>\n", "tags": [] },
{"key": "joshi2020explainable", "year": "2020", "title":"Explainable Disease Classification via weakly-supervised segmentation", "abstract": "<p>Deep learning based approaches to Computer Aided Diagnosis (CAD) typically\npose the problem as an image classification (Normal or Abnormal) problem. These\nsystems achieve high to very high accuracy in specific disease detection for\nwhich they are trained but lack in terms of an explanation for the provided\ndecision/classification result. The activation maps which correspond to\ndecisions do not correlate well with regions of interest for specific diseases.\nThis paper examines this problem and proposes an approach which mimics the\nclinical practice of looking for an evidence prior to diagnosis. A CAD model is\nlearnt using a mixed set of information: class labels for the entire training\nset of images plus a rough localisation of suspect regions as an extra input\nfor a smaller subset of training images for guiding the learning. The proposed\napproach is illustrated with detection of diabetic macular edema (DME) from OCT\nslices. Results of testing on on a large public dataset show that with just a\nthird of images with roughly segmented fluid filled regions, the classification\naccuracy is on par with state of the art methods while providing a good\nexplanation in the form of anatomically accurate heatmap /region of interest.\nThe proposed solution is then adapted to Breast Cancer detection from\nmammographic images. Good evaluation results on public datasets underscores the\ngeneralisability of the proposed solution.</p>\n", "tags": [] },
{"key": "joshi2021review", "year": "2021", "title":"A Review on Explainability in Multimodal Deep Neural Nets", "abstract": "<p>Artificial Intelligence techniques powered by deep neural nets have achieved\nmuch success in several application domains, most significantly and notably in\nthe Computer Vision applications and Natural Language Processing tasks.\nSurpassing human-level performance propelled the research in the applications\nwhere different modalities amongst language, vision, sensory, text play an\nimportant role in accurate predictions and identification. Several multimodal\nfusion methods employing deep learning models are proposed in the literature.\nDespite their outstanding performance, the complex, opaque and black-box nature\nof the deep neural nets limits their social acceptance and usability. This has\ngiven rise to the quest for model interpretability and explainability, more so\nin the complex tasks involving multimodal AI methods. This paper extensively\nreviews the present literature to present a comprehensive survey and commentary\non the explainability in multimodal deep neural nets, especially for the vision\nand language tasks. Several topics on multimodal AI and its applications for\ngeneric domains have been covered in this paper, including the significance,\ndatasets, fundamental building blocks of the methods and techniques,\nchallenges, applications, and future trends in this domain</p>\n", "tags": [] },
{"key": "kakogeorgiou2021evaluating", "year": "2021", "title":"Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing", "abstract": "<p>Although deep neural networks hold the state-of-the-art in several remote\nsensing tasks, their black-box operation hinders the understanding of their\ndecisions, concealing any bias and other shortcomings in datasets and model\nperformance. To this end, we have applied explainable artificial intelligence\n(XAI) methods in remote sensing multi-label classification tasks towards\nproducing human-interpretable explanations and improve transparency. In\nparticular, we utilized and trained deep learning models with state-of-the-art\nperformance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods\nwere employed towards understanding and interpreting models’ predictions, along\nwith quantitative metrics to assess and compare their performance. Numerous\nexperiments were performed to assess the overall performance of XAI methods for\nstraightforward prediction cases, competing multiple labels, as well as\nmisclassification cases. According to our findings, Occlusion, Grad-CAM and\nLime were the most interpretable and reliable XAI methods. However, none\ndelivers high-resolution outputs, while apart from Grad-CAM, both Lime and\nOcclusion are computationally expensive. We also highlight different aspects of\nXAI performance and elaborate with insights on black-box decisions in order to\nimprove transparency, understand their behavior and reveal, as well, datasets’\nparticularities.</p>\n", "tags": [] },
{"key": "kang2019interpreting", "year": "2019", "title":"Interpreting Undesirable Pixels for Image Classification on Black-Box Models", "abstract": "<p>In an effort to interpret black-box models, researches for developing\nexplanation methods have proceeded in recent years. Most studies have tried to\nidentify input pixels that are crucial to the prediction of a classifier. While\nthis approach is meaningful to analyse the characteristic of blackbox models,\nit is also important to investigate pixels that interfere with the prediction.\nTo tackle this issue, in this paper, we propose an explanation method that\nvisualizes undesirable regions to classify an image as a target class. To be\nspecific, we divide the concept of undesirable regions into two terms: (1)\nfactors for a target class, which hinder that black-box models identify\nintrinsic characteristics of a target class and (2) factors for non-target\nclasses that are important regions for an image to be classified as other\nclasses. We visualize such undesirable regions on heatmaps to qualitatively\nvalidate the proposed method. Furthermore, we present an evaluation metric to\nprovide quantitative results on ImageNet.</p>\n", "tags": [] },
{"key": "kang2019towards", "year": "2019", "title":"Towards Interpretable Deep Extreme Multi-label Learning", "abstract": "<p>Many Machine Learning algorithms, such as deep neural networks, have long\nbeen criticized for being “black-boxes”-a kind of models unable to provide how\nit arrive at a decision without further efforts to interpret. This problem has\nraised concerns on model applications’ trust, safety, nondiscrimination, and\nother ethical issues. In this paper, we discuss the machine learning\ninterpretability of a real-world application, eXtreme Multi-label Learning\n(XML), which involves learning models from annotated data with many pre-defined\nlabels. We propose a two-step XML approach that combines deep non-negative\nautoencoder with other multi-label classifiers to tackle different data\napplications with a large number of labels. Our experimental result shows that\nthe proposed approach is able to cope with many-label problems as well as to\nprovide interpretable label hierarchies and dependencies that helps us\nunderstand how the model recognizes the existences of objects in an image.</p>\n", "tags": [] },
{"key": "kang2020multivariate", "year": "2020", "title":"Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs", "abstract": "<p>Information visualization significantly enhances human perception by\ngraphically representing complex data sets. The variety of visualization\ndesigns makes it challenging to efficiently evaluate all possible designs\ncatering to users’ preferences and characteristics. Most of existing evaluation\nmethods perform user studies to obtain multivariate qualitative responses from\nusers via questionnaires and interviews. However, these methods cannot support\nonline evaluation of designs as they are often time-consuming. A statistical\nmodel is desired to predict users’ preferences on visualization designs based\non non-interference measurements (i.e., wearable sensor signals). In this work,\nwe propose a multivariate regression of mixed responses (MRMR) to facilitate\nquantitative evaluation of visualization designs. The proposed MRMR method is\nable to provide accurate model prediction with meaningful variable selection. A\nsimulation study and a user study of evaluating visualization designs with 14\neffective participants are conducted to illustrate the merits of the proposed\nmodel.</p>\n", "tags": [] },
{"key": "karimi2019model", "year": "2019", "title":"Model-Agnostic Counterfactual Explanations for Consequential Decisions", "abstract": "<p>Predictive models are being increasingly used to support consequential\ndecision making at the individual level in contexts such as pretrial bail and\nloan approval. As a result, there is increasing social and legal pressure to\nprovide explanations that help the affected individuals not only to understand\nwhy a prediction was output, but also how to act to obtain a desired outcome.\nTo this end, several works have proposed optimization-based methods to generate\nnearest counterfactual explanations. However, these methods are often\nrestricted to a particular subset of models (e.g., decision trees or linear\nmodels) and differentiable distance functions. In contrast, we build on\nstandard theory and tools from formal verification and propose a novel\nalgorithm that solves a sequence of satisfiability problems, where both the\ndistance function (objective) and predictive model (constraints) are\nrepresented as logic formulae. As shown by our experiments on real-world data,\nour algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable,\n{non-}convex); ii) data-type-agnostic (heterogeneous features); iii)\ndistance-agnostic ($\\ell_0, \\ell_1, \\ell_\\infty$, and combinations thereof);\niv) able to generate plausible and diverse counterfactuals for any sample\n(i.e., 100% coverage); and v) at provably optimal distances.</p>\n", "tags": [] },
{"key": "karimi2020survey", "year": "2020", "title":"A survey of algorithmic recourse: definitions, formulations, solutions, and prospects", "abstract": "<p>Machine learning is increasingly used to inform decision-making in sensitive\nsituations where decisions have consequential effects on individuals’ lives. In\nthese settings, in addition to requiring models to be accurate and robust,\nsocially relevant values such as fairness, privacy, accountability, and\nexplainability play an important role for the adoption and impact of said\ntechnologies. In this work, we focus on algorithmic recourse, which is\nconcerned with providing explanations and recommendations to individuals who\nare unfavourably treated by automated decision-making systems. We first perform\nan extensive literature review, and align the efforts of many authors by\npresenting unified definitions, formulations, and solutions to recourse. Then,\nwe provide an overview of the prospective research directions towards which the\ncommunity may engage, challenging existing assumptions and making explicit\nconnections to other ethical challenges such as security, privacy, and\nfairness.</p>\n", "tags": [] },
{"key": "karpathy2015visualizing", "year": "2015", "title":"Visualizing and Understanding Recurrent Networks", "abstract": "<p>Recurrent Neural Networks (RNNs), and specifically a variant with Long\nShort-Term Memory (LSTM), are enjoying renewed interest as a result of\nsuccessful applications in a wide range of machine learning problems that\ninvolve sequential data. However, while LSTMs provide exceptional results in\npractice, the source of their performance and their limitations remain rather\npoorly understood. Using character-level language models as an interpretable\ntestbed, we aim to bridge this gap by providing an analysis of their\nrepresentations, predictions and error types. In particular, our experiments\nreveal the existence of interpretable cells that keep track of long-range\ndependencies such as line lengths, quotes and brackets. Moreover, our\ncomparative analysis with finite horizon n-gram models traces the source of the\nLSTM improvements to long-range structural dependencies. Finally, we provide\nanalysis of the remaining errors and suggests areas for further study.</p>\n", "tags": [] },
{"key": "kasirzadeh2019mathematical", "year": "2019", "title":"Mathematical decisions and non-causal elements of explainable AI", "abstract": "<p>The social implications of algorithmic decision-making in sensitive contexts\nhave generated lively debates among multiple stakeholders, such as moral and\npolitical philosophers, computer scientists, and the public. Yet, the lack of a\ncommon language and a conceptual framework for an appropriate bridging of the\nmoral, technical, and political aspects of the debate prevents the discussion\nto be as effective as it can be. Social scientists and psychologists are\ncontributing to this debate by gathering a wealth of empirical data, yet a\nphilosophical analysis of the social implications of algorithmic\ndecision-making remains comparatively impoverished. In attempting to address\nthis lacuna, this paper argues that a hierarchy of different types of\nexplanations for why and how an algorithmic decision outcome is achieved can\nestablish the relevant connection between the moral and technical aspects of\nalgorithmic decision-making. In particular, I offer a multi-faceted conceptual\nframework for the explanations and the interpretations of algorithmic\ndecisions, and I claim that this framework can lay the groundwork for a focused\ndiscussion among multiple stakeholders about the social implications of\nalgorithmic decision-making, as well as AI governance and ethics more\ngenerally.</p>\n", "tags": [] },
{"key": "kauffmann2018towards", "year": "2018", "title":"Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models", "abstract": "<p>A common machine learning task is to discriminate between normal and\nanomalous data points. In practice, it is not always sufficient to reach high\naccuracy at this task, one also would like to understand why a given data point\nhas been predicted in a certain way. We present a new principled approach for\none-class SVMs that decomposes outlier predictions in terms of input variables.\nThe method first recomposes the one-class model as a neural network with\ndistance functions and min-pooling, and then performs a deep Taylor\ndecomposition (DTD) of the model output. The proposed One-Class DTD is\napplicable to a number of common distance-based SVM kernels and is able to\nreliably explain a wide set of data anomalies. Furthermore, it outperforms\nbaselines such as sensitivity analysis, nearest neighbor, or simple edge\ndetection.</p>\n", "tags": [] },
{"key": "kaur2022sensible", "year": "2022", "title":"Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory", "abstract": "<p>Understanding how ML models work is a prerequisite for responsibly designing,\ndeploying, and using ML-based systems. With interpretability approaches, ML can\nnow offer explanations for its outputs to aid human understanding. Though these\napproaches rely on guidelines for how humans explain things to each other, they\nultimately solve for improving the artifact – an explanation. In this paper,\nwe propose an alternate framework for interpretability grounded in Weick’s\nsensemaking theory, which focuses on who the explanation is intended for.\nRecent work has advocated for the importance of understanding stakeholders’\nneeds – we build on this by providing concrete properties (e.g., identity,\nsocial context, environmental cues, etc.) that shape human understanding. We\nuse an application of sensemaking in organizations as a template for discussing\ndesign guidelines for Sensible AI, AI that factors in the nuances of human\ncognition when trying to explain itself.</p>\n", "tags": [] },
{"key": "keane2019case", "year": "2019", "title":"How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins", "abstract": "<p>This paper surveys an approach to the XAI problem, using post-hoc explanation\nby example, that hinges on twinning Artificial Neural Networks (ANNs) with\nCase-Based Reasoning (CBR) systems, so-called ANN-CBR twins. A systematic\nsurvey of 1100+ papers was carried out to identify the fragmented literature on\nthis topic and to trace it influence through to more recent work involving Deep\nNeural Networks (DNNs). The paper argues that this twin-system approach,\nespecially using ANN-CBR twins, presents one possible coherent, generic\nsolution to the XAI problem (and, indeed, XCBR problem). The paper concludes by\nroad-mapping some future directions for this XAI solution involving (i) further\ntests of feature-weighting techniques, (iii) explorations of how explanatory\ncases might best be deployed (e.g., in counterfactuals, near-miss cases, a\nfortori cases), and (iii) the raising of the unwelcome and, much ignored, issue\nof human user evaluation.</p>\n", "tags": [] },
{"key": "keane2019twin-system", "year": "2019", "title":"The Twin-System Approach as One Generic Solution for XAI: An Overview of ANN-CBR Twins for Explaining Deep Learning", "abstract": "<p>The notion of twin systems is proposed to address the eXplainable AI (XAI)\nproblem, where an uninterpretable black-box system is mapped to a white-box\n‘twin’ that is more interpretable. In this short paper, we overview very recent\nwork that advances a generic solution to the XAI problem, the so called twin\nsystem approach. The most popular twinning in the literature is that between an\nArtificial Neural Networks (ANN ) as a black box and Case Based Reasoning (CBR)\nsystem as a white-box, where the latter acts as an interpretable proxy for the\nformer. We outline how recent work reviving this idea has applied it to deep\nlearning methods. Furthermore, we detail the many fruitful directions in which\nthis work may be taken; such as, determining the most (i) accurate\nfeature-weighting methods to be used, (ii) appropriate deployments for\nexplanatory cases, (iii) useful cases of explanatory value to users.</p>\n", "tags": [] },
{"key": "keane2021twin", "year": "2021", "title":"Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation", "abstract": "<p>Recently, it has been proposed that fruitful synergies may exist between Deep\nLearning (DL) and Case Based Reasoning (CBR); that there are insights to be\ngained by applying CBR ideas to problems in DL (what could be called DeepCBR).\nIn this paper, we report on a program of research that applies CBR solutions to\nthe problem of Explainable AI (XAI) in the DL. We describe a series of\ntwin-systems pairings of opaque DL models with transparent CBR models that\nallow the latter to explain the former using factual, counterfactual and\nsemi-factual explanation strategies. This twinning shows that functional\nabstractions of DL (e.g., feature weights, feature importance and decision\nboundaries) can be used to drive these explanatory solutions. We also raise the\nprospect that this research also applies to the problem of Data Augmentation in\nDL, underscoring the fecundity of these DeepCBR ideas.</p>\n", "tags": [] },
{"key": "khakzar2022explanations", "year": "2022", "title":"Do Explanations Explain? Model Knows Best", "abstract": "<p>It is a mystery which input features contribute to a neural network’s output.\nVarious explanation (feature attribution) methods are proposed in the\nliterature to shed light on the problem. One peculiar observation is that these\nexplanations (attributions) point to different features as being important. The\nphenomenon raises the question, which explanation to trust? We propose a\nframework for evaluating the explanations using the neural network model\nitself. The framework leverages the network to generate input features that\nimpose a particular behavior on the output. Using the generated features, we\ndevise controlled experimental setups to evaluate whether an explanation method\nconforms to an axiom. Thus we propose an empirical framework for axiomatic\nevaluation of explanation methods. We evaluate well-known and promising\nexplanation solutions using the proposed framework. The framework provides a\ntoolset to reveal properties and drawbacks within existing and future\nexplanation solutions.</p>\n", "tags": [] },
{"key": "kim2015bayesian", "year": "2015", "title":"The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification", "abstract": "<p>We present the Bayesian Case Model (BCM), a general framework for Bayesian\ncase-based reasoning (CBR) and prototype classification and clustering. BCM\nbrings the intuitive power of CBR to a Bayesian generative framework. The BCM\nlearns prototypes, the “quintessential” observations that best represent\nclusters in a dataset, by performing joint inference on cluster labels,\nprototypes and important features. Simultaneously, BCM pursues sparsity by\nlearning subspaces, the sets of features that play important roles in the\ncharacterization of the prototypes. The prototype and subspace representation\nprovides quantitative benefits in interpretability while preserving\nclassification accuracy. Human subject experiments verify statistically\nsignificant improvements to participants’ understanding when using explanations\nproduced by BCM, compared to those given by prior art.</p>\n", "tags": [] },
{"key": "kim2019learning", "year": "2019", "title":"Learning Interpretable Models with Causal Guarantees", "abstract": "<p>Machine learning has shown much promise in helping improve the quality of\nmedical, legal, and financial decision-making. In these applications, machine\nlearning models must satisfy two important criteria: (i) they must be causal,\nsince the goal is typically to predict individual treatment effects, and (ii)\nthey must be interpretable, so that human decision makers can validate and\ntrust the model predictions. There has recently been much progress along each\ndirection independently, yet the state-of-the-art approaches are fundamentally\nincompatible. We propose a framework for learning interpretable models from\nobservational data that can be used to predict individual treatment effects\n(ITEs). In particular, our framework converts any supervised learning algorithm\ninto an algorithm for estimating ITEs. Furthermore, we prove an error bound on\nthe treatment effects predicted by our model. Finally, in an experiment on\nreal-world data, we show that the models trained using our framework\nsignificantly outperform a number of baselines.</p>\n", "tags": [] },
{"key": "kim2020interpretation", "year": "2020", "title":"Interpretation of NLP models through input marginalization", "abstract": "<p>To demystify the “black box” property of deep neural networks for natural\nlanguage processing (NLP), several methods have been proposed to interpret\ntheir predictions by measuring the change in prediction probability after\nerasing each token of an input. Since existing methods replace each token with\na predefined value (i.e., zero), the resulting sentence lies out of the\ntraining data distribution, yielding misleading interpretations. In this study,\nwe raise the out-of-distribution problem induced by the existing interpretation\nmethods and present a remedy; we propose to marginalize each token out. We\ninterpret various NLP models trained for sentiment analysis and natural\nlanguage inference using the proposed method.</p>\n", "tags": [] },
{"key": "kim2021sanity", "year": "2021", "title":"Sanity Simulations for Saliency Methods", "abstract": "<p>Saliency methods are a popular class of feature attribution explanation\nmethods that aim to capture a model’s predictive reasoning by identifying\n“important” pixels in an input image. However, the development and adoption of\nthese methods are hindered by the lack of access to ground-truth model\nreasoning, which prevents accurate evaluation. In this work, we design a\nsynthetic benchmarking framework, SMERF, that allows us to perform\nground-truth-based evaluation while controlling the complexity of the model’s\nreasoning. Experimentally, SMERF reveals significant limitations in existing\nsaliency methods and, as a result, represents a useful tool for the development\nof new saliency methods.</p>\n", "tags": [] },
{"key": "kindermans2017learning", "year": "2017", "title":"Learning how to explain neural networks: PatternNet and PatternAttribution", "abstract": "<p>DeConvNet, Guided BackProp, LRP, were invented to better understand deep\nneural networks. We show that these methods do not produce the theoretically\ncorrect explanation for a linear model. Yet they are used on multi-layer\nnetworks with millions of parameters. This is a cause for concern since linear\nmodels are simple neural networks. We argue that explanation methods for neural\nnets should work reliably in the limit of simplicity, the linear models. Based\non our analysis of linear models we propose a generalization that yields two\nexplanation techniques (PatternNet and PatternAttribution) that are\ntheoretically sound for linear models and produce improved explanations for\ndeep networks.</p>\n", "tags": [] },
{"key": "kindermans2017unreliability", "year": "2017", "title":"The (Un)reliability of saliency methods", "abstract": "<p>Saliency methods aim to explain the predictions of deep neural networks.\nThese methods lack reliability when the explanation is sensitive to factors\nthat do not contribute to the model prediction. We use a simple and common\npre-processing step —adding a constant shift to the input data— to show\nthat a transformation with no effect on the model can cause numerous methods to\nincorrectly attribute. In order to guarantee reliability, we posit that methods\nshould fulfill input invariance, the requirement that a saliency method mirror\nthe sensitivity of the model with respect to transformations of the input. We\nshow, through several examples, that saliency methods that do not satisfy input\ninvariance result in misleading attribution.</p>\n", "tags": [] },
{"key": "koh2017understanding", "year": "2017", "title":"Understanding Black-box Predictions via Influence Functions", "abstract": "<p>How can we explain the predictions of a black-box model? In this paper, we\nuse influence functions – a classic technique from robust statistics – to\ntrace a model’s prediction through the learning algorithm and back to its\ntraining data, thereby identifying training points most responsible for a given\nprediction. To scale up influence functions to modern machine learning\nsettings, we develop a simple, efficient implementation that requires only\noracle access to gradients and Hessian-vector products. We show that even on\nnon-convex and non-differentiable models where the theory breaks down,\napproximations to influence functions can still provide valuable information.\nOn linear models and convolutional neural networks, we demonstrate that\ninfluence functions are useful for multiple purposes: understanding model\nbehavior, debugging models, detecting dataset errors, and even creating\nvisually-indistinguishable training-set attacks.</p>\n", "tags": [] },
{"key": "kokhlikyan2021investigating", "year": "2021", "title":"Investigating sanity checks for saliency maps with image and text classification", "abstract": "<p>Saliency maps have shown to be both useful and misleading for explaining\nmodel predictions especially in the context of images. In this paper, we\nperform sanity checks for text modality and show that the conclusions made for\nimage do not directly transfer to text. We also analyze the effects of the\ninput multiplier in certain saliency maps using similarity scores,\nmax-sensitivity and infidelity evaluation metrics. Our observations reveal that\nthe input multiplier carries input’s structural patterns in explanation maps,\nthus leading to similar results regardless of the choice of model parameters.\nWe also show that the smoothness of a Neural Network (NN) function can affect\nthe quality of saliency-based explanations. Our investigations reveal that\nreplacing ReLUs with Softplus and MaxPool with smoother variants such as\nLogSumExp (LSE) can lead to explanations that are more reliable based on the\ninfidelity evaluation metric.</p>\n", "tags": [] },
{"key": "kong2021deeprare", "year": "2021", "title":"DeepRare: Generic Unsupervised Visual Attention Models", "abstract": "<p>Human visual system is modeled in engineering field providing\nfeature-engineered methods which detect contrasted/surprising/unusual data into\nimages. This data is “interesting” for humans and leads to numerous\napplications. Deep learning (DNNs) drastically improved the algorithms\nefficiency on the main benchmark datasets. However, DNN-based models are\ncounter-intuitive: surprising or unusual data is by definition difficult to\nlearn because of its low occurrence probability. In reality, DNN-based models\nmainly learn top-down features such as faces, text, people, or animals which\nusually attract human attention, but they have low efficiency in extracting\nsurprising or unusual data in the images. In this paper, we propose a new\nvisual attention model called DeepRare2021 (DR21) which uses the power of DNNs\nfeature extraction and the genericity of feature-engineered algorithms. This\nalgorithm is an evolution of a previous version called DeepRare2019 (DR19)\nbased on a common framework. DR21 1) does not need any training and uses the\ndefault ImageNet training, 2) is fast even on CPU, 3) is tested on four very\ndifferent eye-tracking datasets showing that the DR21 is generic and is always\nin the within the top models on all datasets and metrics while no other model\nexhibits such a regularity and genericity. Finally DR21 4) is tested with\nseveral network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2\n(MN2) and 5) it provides explanation and transparency on which parts of the\nimage are the most surprising at different levels despite the use of a\nDNN-based feature extractor. DeepRare2021 code can be found at\nhttps://github.com/numediart/VisualAttention-RareFamil}.</p>\n", "tags": [] },
{"key": "konstantinov2021attention", "year": "2021", "title":"Attention-like feature explanation for tabular data", "abstract": "<p>A new method for local and global explanation of the machine learning\nblack-box model predictions by tabular data is proposed. It is implemented as a\nsystem called AFEX (Attention-like Feature EXplanation) and consisting of two\nmain parts. The first part is a set of the one-feature neural subnetworks which\naim to get a specific representation for every feature in the form of a basis\nof shape functions. The subnetworks use shortcut connections with trainable\nparameters to improve the network performance. The second part of AFEX produces\nshape functions of features as the weighted sum of the basis shape functions\nwhere weights are computed by using an attention-like mechanism. AFEX\nidentifies pairwise interactions between features based on pairwise\nmultiplications of shape functions corresponding to different features. A\nmodification of AFEX with incorporating an additional surrogate model which\napproximates the black-box model is proposed. AFEX is trained end-to-end on a\nwhole dataset only once such that it does not require to train neural networks\nagain in the explanation stage. Numerical experiments with synthetic and real\ndata illustrate AFEX.</p>\n", "tags": [] },
{"key": "krakovna2016increasing", "year": "2016", "title":"Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models", "abstract": "<p>As deep neural networks continue to revolutionize various application\ndomains, there is increasing interest in making these powerful models more\nunderstandable and interpretable, and narrowing down the causes of good and bad\npredictions. We focus on recurrent neural networks, state of the art models in\nspeech recognition and translation. Our approach to increasing interpretability\nis by combining a long short-term memory (LSTM) model with a hidden Markov\nmodel (HMM), a simpler and more transparent model. We add the HMM state\nprobabilities to the output layer of the LSTM, and then train the HMM and LSTM\neither sequentially or jointly. The LSTM can make use of the information from\nthe HMM, and fill in the gaps when the HMM is not performing well. A small\nhybrid model usually performs better than a standalone LSTM of the same size,\nespecially on smaller data sets. We test the algorithms on text data and\nmedical time series data, and find that the LSTM and HMM learn complementary\ninformation about the features in the text.</p>\n", "tags": [] },
{"key": "krause2017workflow", "year": "2017", "title":"A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations", "abstract": "<p>Human-in-the-loop data analysis applications necessitate greater transparency\nin machine learning models for experts to understand and trust their decisions.\nTo this end, we propose a visual analytics workflow to help data scientists and\ndomain experts explore, diagnose, and understand the decisions made by a binary\nclassifier. The approach leverages “instance-level explanations”, measures of\nlocal feature relevance that explain single instances, and uses them to build a\nset of visual representations that guide the users in their investigation. The\nworkflow is based on three main visual representations and steps: one based on\naggregate statistics to see how data distributes across correct / incorrect\ndecisions; one based on explanations to understand which features are used to\nmake these decisions; and one based on raw data, to derive insights on\npotential root causes for the observed patterns. The workflow is derived from a\nlong-term collaboration with a group of machine learning and healthcare\nprofessionals who used our method to make sense of machine learning models they\ndeveloped. The case study from this collaboration demonstrates that the\nproposed workflow helps experts derive useful knowledge about the model and the\nphenomena it describes, thus experts can generate useful hypotheses on how a\nmodel can be improved.</p>\n", "tags": [] },
{"key": "krishna2022disagreement", "year": "2022", "title":"The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective", "abstract": "<p>As various post hoc explanation methods are increasingly being leveraged to\nexplain complex models in high-stakes settings, it becomes critical to develop\na deeper understanding of if and when the explanations output by these methods\ndisagree with each other, and how such disagreements are resolved in practice.\nHowever, there is little to no research that provides answers to these critical\nquestions. In this work, we introduce and study the disagreement problem in\nexplainable machine learning. More specifically, we formalize the notion of\ndisagreement between explanations, analyze how often such disagreements occur\nin practice, and how do practitioners resolve these disagreements. To this end,\nwe first conduct interviews with data scientists to understand what constitutes\ndisagreement between explanations generated by different methods for the same\nmodel prediction, and introduce a novel quantitative framework to formalize\nthis understanding. We then leverage this framework to carry out a rigorous\nempirical analysis with four real-world datasets, six state-of-the-art post hoc\nexplanation methods, and eight different predictive models, to measure the\nextent of disagreement between the explanations generated by various popular\nexplanation methods. In addition, we carry out an online user study with data\nscientists to understand how they resolve the aforementioned disagreements. Our\nresults indicate that state-of-the-art explanation methods often disagree in\nterms of the explanations they output. Our findings also underscore the\nimportance of developing principled evaluation metrics that enable\npractitioners to effectively compare explanations.</p>\n", "tags": [] },
{"key": "kumar2016understanding", "year": "2016", "title":"Understanding Anatomy Classification Through Attentive Response Maps", "abstract": "<p>One of the main challenges for broad adoption of deep learning based models\nsuch as convolutional neural networks (CNN), is the lack of understanding of\ntheir decisions. In many applications, a simpler, less capable model that can\nbe easily understood is favorable to a black-box model that has superior\nperformance. In this paper, we present an approach for designing CNNs based on\nvisualization of the internal activations of the model. We visualize the\nmodel’s response through attentive response maps obtained using a fractional\nstride convolution technique and compare the results with known imaging\nlandmarks from the medical literature. We show that sufficiently deep and\ncapable models can be successfully trained to use the same medical landmarks a\nhuman expert would use. Our approach allows for communicating the model\ndecision process well, but also offers insight towards detecting biases.</p>\n", "tags": [] },
{"key": "lage2019human", "year": "2019", "title":"Human Evaluation of Models Built for Interpretability", "abstract": "<p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>\n", "tags": ["evaluation"] },
{"key": "lai2019many", "year": "2019", "title":"Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification", "abstract": "<p>Feature importance is commonly used to explain machine predictions. While\nfeature importance can be derived from a machine learning model with a variety\nof methods, the consistency of feature importance via different methods remains\nunderstudied. In this work, we systematically compare feature importance from\nbuilt-in mechanisms in a model such as attention values and post-hoc methods\nthat approximate model behavior such as LIME. Using text classification as a\ntestbed, we find that 1) no matter which method we use, important features from\ntraditional models such as SVM and XGBoost are more similar with each other,\nthan with deep learning models; 2) post-hoc methods tend to generate more\nsimilar important features for two models than built-in methods. We further\ndemonstrate how such similarity varies across instances. Notably, important\nfeatures do not always resemble each other better when two models agree on the\npredicted label than when they disagree.</p>\n", "tags": [] },
{"key": "lakkaraju2017interpretable", "year": "2017", "title":"Interpretable & Explorable Approximations of Black Box Models", "abstract": "<p>We propose Black Box Explanations through Transparent Approximations (BETA),\na novel model agnostic framework for explaining the behavior of any black-box\nclassifier by simultaneously optimizing for fidelity to the original model and\ninterpretability of the explanation. To this end, we develop a novel objective\nfunction which allows us to learn (with optimality guarantees), a small number\nof compact decision sets each of which explains the behavior of the black box\nmodel in unambiguous, well-defined regions of feature space. Furthermore, our\nframework also is capable of accepting user input when generating these\napproximations, thus allowing users to interactively explore how the black-box\nmodel behaves in different subspaces that are of interest to the user. To the\nbest of our knowledge, this is the first approach which can produce global\nexplanations of the behavior of any given black box model through joint\noptimization of unambiguity, fidelity, and interpretability, while also\nallowing users to explore model behavior based on their preferences.\nExperimental evaluation with real-world datasets and user studies demonstrates\nthat our approach can generate highly compact, easy-to-understand, yet accurate\napproximations of various kinds of predictive models compared to\nstate-of-the-art baselines.</p>\n", "tags": [] },
{"key": "lakkaraju2019how", "year": "2019", "title":"\"How do I fool you?\": Manipulating User Trust via Misleading Black Box Explanations", "abstract": "<p>As machine learning black boxes are increasingly being deployed in critical\ndomains such as healthcare and criminal justice, there has been a growing\nemphasis on developing techniques for explaining these black boxes in a human\ninterpretable manner. It has recently become apparent that a high-fidelity\nexplanation of a black box ML model may not accurately reflect the biases in\nthe black box. As a consequence, explanations have the potential to mislead\nhuman users into trusting a problematic black box. In this work, we rigorously\nexplore the notion of misleading explanations and how they influence user trust\nin black-box models. More specifically, we propose a novel theoretical\nframework for understanding and generating misleading explanations, and carry\nout a user study with domain experts to demonstrate how these explanations can\nbe used to mislead users. Our work is the first to empirically establish how\nuser trust in black box models can be manipulated via misleading explanations.</p>\n", "tags": [] },
{"key": "lam2021finding", "year": "2021", "title":"Finding Representative Interpretations on Convolutional Neural Networks", "abstract": "<p>Interpreting the decision logic behind effective deep convolutional neural\nnetworks (CNN) on images complements the success of deep learning models.\nHowever, the existing methods can only interpret some specific decision logic\non individual or a small number of images. To facilitate human\nunderstandability and generalization ability, it is important to develop\nrepresentative interpretations that interpret common decision logics of a CNN\non a large group of similar images, which reveal the common semantics data\ncontributes to many closely related predictions. In this paper, we develop a\nnovel unsupervised approach to produce a highly representative interpretation\nfor a large number of similar images. We formulate the problem of finding\nrepresentative interpretations as a co-clustering problem, and convert it into\na submodular cost submodular cover problem based on a sample of the linear\ndecision boundaries of a CNN. We also present a visualization and similarity\nranking method. Our extensive experiments demonstrate the excellent performance\nof our method.</p>\n", "tags": [] },
{"key": "lampinen2021tell", "year": "2021", "title":"Tell me why! Explanations support learning relational and causal structure", "abstract": "<p>Inferring the abstract relational and causal structure of the world is a\nmajor challenge for reinforcement-learning (RL) agents. For humans,\nlanguage–particularly in the form of explanations–plays a considerable role\nin overcoming this challenge. Here, we show that language can play a similar\nrole for deep RL agents in complex environments. While agents typically\nstruggle to acquire relational and causal knowledge, augmenting their\nexperience by training them to predict language descriptions and explanations\ncan overcome these limitations. We show that language can help agents learn\nchallenging relational tasks, and examine which aspects of language contribute\nto its benefits. We then show that explanations can help agents to infer not\nonly relational but also causal structure. Language can shape the way that\nagents to generalize out-of-distribution from ambiguous, causally-confounded\ntraining, and explanations even allow agents to learn to perform experimental\ninterventions to identify causal relationships. Our results suggest that\nlanguage description and explanation may be powerful tools for improving agent\nlearning and generalization.</p>\n", "tags": [] },
{"key": "langer2021want", "year": "2021", "title":"What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research", "abstract": "<p>Previous research in Explainable Artificial Intelligence (XAI) suggests that\na main aim of explainability approaches is to satisfy specific interests,\ngoals, expectations, needs, and demands regarding artificial systems (we call\nthese stakeholders’ desiderata) in a variety of contexts. However, the\nliterature on XAI is vast, spreads out across multiple largely disconnected\ndisciplines, and it often remains unclear how explainability approaches are\nsupposed to achieve the goal of satisfying stakeholders’ desiderata. This paper\ndiscusses the main classes of stakeholders calling for explainability of\nartificial systems and reviews their desiderata. We provide a model that\nexplicitly spells out the main concepts and relations necessary to consider and\ninvestigate when evaluating, adjusting, choosing, and developing explainability\napproaches that aim to satisfy stakeholders’ desiderata. This model can serve\nresearchers from the variety of different disciplines involved in XAI as a\ncommon ground. It emphasizes where there is interdisciplinary potential in the\nevaluation and the development of explainability approaches.</p>\n", "tags": [] },
{"key": "leandro2016reverse", "year": "2016", "title":"Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz Fuzzy Logics using Linear Neural Networks", "abstract": "<p>This work describes a methodology to combine logic-based systems and\nconnectionist systems. Our approach uses finite truth valued {\\L}ukasiewicz\nlogic, where we take advantage of fact what in this type of logics every\nconnective can be define by a neuron in an artificial network having by\nactivation function the identity truncated to zero and one. This allowed the\ninjection of first-order formulas in a network architecture, and also\nsimplified symbolic rule extraction.\n  Our method trains a neural network using Levenderg-Marquardt algorithm, where\nwe restrict the knowledge dissemination in the network structure. We show how\nthis reduces neural networks plasticity without damage drastically the learning\nperformance. Making the descriptive power of produced neural networks similar\nto the descriptive power of {\\L}ukasiewicz logic language, simplifying the\ntranslation between symbolic and connectionist structures.\n  This method is used in the reverse engineering problem of finding the formula\nused on generation of a truth table for a multi-valued {\\L}ukasiewicz logic.\nFor real data sets the method is particularly useful for attribute selection,\non binary classification problems defined using nominal attribute. After\nattribute selection and possible data set completion in the resulting\nconnectionist model: neurons are directly representable using a disjunctive or\nconjunctive formulas, in the {\\L}ukasiewicz logic, or neurons are\ninterpretations which can be approximated by symbolic rules. This fact is\nexemplified, extracting symbolic knowledge from connectionist models generated\nfor the data set Mushroom from UCI Machine Learning Repository.</p>\n", "tags": [] },
{"key": "leavitt2020towards", "year": "2020", "title":"Towards falsifiable interpretability research", "abstract": "<p>Methods for understanding the decisions of and mechanisms underlying deep\nneural networks (DNNs) typically rely on building intuition by emphasizing\nsensory or semantic features of individual examples. For instance, methods aim\nto visualize the components of an input which are “important” to a network’s\ndecision, or to measure the semantic properties of single neurons. Here, we\nargue that interpretability research suffers from an over-reliance on\nintuition-based approaches that risk-and in some cases have caused-illusory\nprogress and misleading conclusions. We identify a set of limitations that we\nargue impede meaningful progress in interpretability research, and examine two\npopular classes of interpretability methods-saliency and single-neuron-based\napproaches-that serve as case studies for how overreliance on intuition and\nlack of falsifiability can undermine interpretability research. To address\nthese concerns, we propose a strategy to address these impediments in the form\nof a framework for strongly falsifiable interpretability research. We encourage\nresearchers to use their intuitions as a starting point to develop and test\nclear, falsifiable hypotheses, and hope that our framework yields robust,\nevidence-based interpretability methods that generate meaningful advances in\nour understanding of DNNs.</p>\n", "tags": [] },
{"key": "lecue2018semantic", "year": "2018", "title":"Semantic Explanations of Predictions", "abstract": "<p>The main objective of explanations is to transmit knowledge to humans. This\nwork proposes to construct informative explanations for predictions made from\nmachine learning models. Motivated by the observations from social sciences,\nour approach selects data points from the training sample that exhibit special\ncharacteristics crucial for explanation, for instance, ones contrastive to the\nclassification prediction and ones representative of the models. Subsequently,\nsemantic concepts are derived from the selected data points through the use of\ndomain ontologies. These concepts are filtered and ranked to produce\ninformative explanations that improves human understanding. The main features\nof our approach are that (1) knowledge about explanations is captured in the\nform of ontological concepts, (2) explanations include contrastive evidences in\naddition to normal evidences, and (3) explanations are user relevant.</p>\n", "tags": [] },
{"key": "lee2016going", "year": "2016", "title":"Going Deeper with Contextual CNN for Hyperspectral Image Classification", "abstract": "<p>In this paper, we describe a novel deep convolutional neural network (CNN)\nthat is deeper and wider than other existing deep networks for hyperspectral\nimage classification. Unlike current state-of-the-art approaches in CNN-based\nhyperspectral image classification, the proposed network, called contextual\ndeep CNN, can optimally explore local contextual interactions by jointly\nexploiting local spatio-spectral relationships of neighboring individual pixel\nvectors. The joint exploitation of the spatio-spectral information is achieved\nby a multi-scale convolutional filter bank used as an initial component of the\nproposed CNN pipeline. The initial spatial and spectral feature maps obtained\nfrom the multi-scale filter bank are then combined together to form a joint\nspatio-spectral feature map. The joint feature map representing rich spectral\nand spatial properties of the hyperspectral image is then fed through a fully\nconvolutional network that eventually predicts the corresponding label of each\npixel vector. The proposed approach is tested on three benchmark datasets: the\nIndian Pines dataset, the Salinas dataset and the University of Pavia dataset.\nPerformance comparison shows enhanced classification performance of the\nproposed approach over the current state-of-the-art on the three datasets.</p>\n", "tags": [] },
{"key": "lee2018understanding", "year": "2018", "title":"Understanding Learned Models by Identifying Important Features at the Right Resolution", "abstract": "<p>In many application domains, it is important to characterize how complex\nlearned models make their decisions across the distribution of instances. One\nway to do this is to identify the features and interactions among them that\ncontribute to a model’s predictive accuracy. We present a model-agnostic\napproach to this task that makes the following specific contributions. Our\napproach (i) tests feature groups, in addition to base features, and tries to\ndetermine the level of resolution at which important features can be\ndetermined, (ii) uses hypothesis testing to rigorously assess the effect of\neach feature on the model’s loss, (iii) employs a hierarchical approach to\ncontrol the false discovery rate when testing feature groups and individual\nbase features for importance, and (iv) uses hypothesis testing to identify\nimportant interactions among features and feature groups. We evaluate our\napproach by analyzing random forest and LSTM neural network models learned in\ntwo challenging biomedical applications.</p>\n", "tags": [] },
{"key": "lertvittayakumjorn2019humangrounded", "year": "2019", "title":"Human-grounded Evaluations of Explanation Methods for Text Classification", "abstract": "<p>Due to the black-box nature of deep learning models, methods for explaining\nthe models’ results are crucial to gain trust from humans and support\ncollaboration between AIs and humans. In this paper, we consider several\nmodel-agnostic and model-specific explanation methods for CNNs for text\nclassification and conduct three human-grounded evaluations, focusing on\ndifferent purposes of explanations: (1) revealing model behavior, (2)\njustifying model predictions, and (3) helping humans investigate uncertain\npredictions. The results highlight dissimilar qualities of the various\nexplanation methods we consider and show the degree to which these methods\ncould serve for each purpose.</p>\n", "tags": [] },
{"key": "lertvittayakumjorn2022argumentative", "year": "2022", "title":"Argumentative Explanations for Pattern-Based Text Classifiers", "abstract": "<p>Recent works in Explainable AI mostly address the transparency issue of\nblack-box models or create explanations for any kind of models (i.e., they are\nmodel-agnostic), while leaving explanations of interpretable models largely\nunderexplored. In this paper, we fill this gap by focusing on explanations for\na specific interpretable model, namely pattern-based logistic regression (PLR)\nfor binary text classification. We do so because, albeit interpretable, PLR is\nchallenging when it comes to explanations. In particular, we found that a\nstandard way to extract explanations from this model does not consider\nrelations among the features, making the explanations hardly plausible to\nhumans. Hence, we propose AXPLR, a novel explanation method using (forms of)\ncomputational argumentation to generate explanations (for outputs computed by\nPLR) which unearth model agreements and disagreements among the features.\nSpecifically, we use computational argumentation as follows: we see features\n(patterns) in PLR as arguments in a form of quantified bipolar argumentation\nframeworks (QBAFs) and extract attacks and supports between arguments based on\nspecificity of the arguments; we understand logistic regression as a gradual\nsemantics for these QBAFs, used to determine the arguments’ dialectic strength;\nand we study standard properties of gradual semantics for QBAFs in the context\nof our argumentative re-interpretation of PLR, sanctioning its suitability for\nexplanatory purposes. We then show how to extract intuitive explanations (for\noutputs computed by PLR) from the constructed QBAFs. Finally, we conduct an\nempirical evaluation and two experiments in the context of human-AI\ncollaboration to demonstrate the advantages of our resulting AXPLR method.</p>\n", "tags": [] },
{"key": "letham2015interpretable", "year": "2015", "title":"Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model", "abstract": "<p>We aim to produce predictive models that are not only accurate, but are also\ninterpretable to human experts. Our models are decision lists, which consist of\na series of if…then… statements (e.g., if high blood pressure, then stroke)\nthat discretize a high-dimensional, multivariate feature space into a series of\nsimple, readily interpretable decision statements. We introduce a generative\nmodel called Bayesian Rule Lists that yields a posterior distribution over\npossible decision lists. It employs a novel prior structure to encourage\nsparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy\non par with the current top algorithms for prediction in machine learning. Our\nmethod is motivated by recent developments in personalized medicine, and can be\nused to produce highly accurate and interpretable medical scoring systems. We\ndemonstrate this by producing an alternative to the CHADS$_2$ score, actively\nused in clinical practice for estimating the risk of stroke in patients that\nhave atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more\naccurate.</p>\n", "tags": [] },
{"key": "li2015visual", "year": "2015", "title":"Visual Saliency Based on Multiscale Deep Features", "abstract": "<p>Visual saliency is a fundamental problem in both cognitive and computational\nsciences, including computer vision. In this CVPR 2015 paper, we discover that\na high-quality visual saliency model can be trained with multiscale features\nextracted using a popular deep learning architecture, convolutional neural\nnetworks (CNNs), which have had many successes in visual recognition tasks. For\nlearning such saliency models, we introduce a neural network architecture,\nwhich has fully connected layers on top of CNNs responsible for extracting\nfeatures at three different scales. We then propose a refinement method to\nenhance the spatial coherence of our saliency results. Finally, aggregating\nmultiple saliency maps computed for different levels of image segmentation can\nfurther boost the performance, yielding saliency maps better than those\ngenerated from a single segmentation. To promote further research and\nevaluation of visual saliency models, we also construct a new large database of\n4447 challenging images and their pixelwise saliency annotation. Experimental\nresults demonstrate that our proposed method is capable of achieving\nstate-of-the-art performance on all public benchmarks, improving the F-Measure\nby 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset\n(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively\non these two datasets.</p>\n", "tags": [] },
{"key": "li2015visualizing", "year": "2015", "title":"Visualizing and Understanding Neural Models in NLP", "abstract": "<p>While neural networks have been successfully applied to many NLP tasks the\nresulting vector-based models are very difficult to interpret. For example it’s\nnot clear how they achieve {\\em compositionality}, building sentence meaning\nfrom the meanings of words and phrases. In this paper we describe four\nstrategies for visualizing compositionality in neural models for NLP, inspired\nby similar work in computer vision. We first plot unit values to visualize\ncompositionality of negation, intensification, and concessive clauses, allow us\nto see well-known markedness asymmetries in negation. We then introduce three\nsimple and straightforward methods for visualizing a unit’s {\\em salience}, the\namount it contributes to the final composed meaning: (1) gradient\nback-propagation, (2) the variance of a token from the average word node, (3)\nLSTM-style gates that measure information flow. We test our methods on\nsentiment using simple recurrent nets and LSTMs. Our general-purpose methods\nmay have wide applications for understanding compositionality and other\nsemantic properties of deep networks , and also shed light on why LSTMs\noutperform simple recurrent nets,</p>\n", "tags": [] },
{"key": "li2016understanding", "year": "2016", "title":"Understanding Neural Networks through Representation Erasure", "abstract": "<p>While neural networks have been successfully applied to many natural language\nprocessing tasks, they come at the cost of interpretability. In this paper, we\npropose a general methodology to analyze and interpret decisions from a neural\nmodel by observing the effects on the model of erasing various parts of the\nrepresentation, such as input word-vector dimensions, intermediate hidden\nunits, or input words. We present several approaches to analyzing the effects\nof such erasure, from computing the relative difference in evaluation metrics,\nto using reinforcement learning to erase the minimum set of input words in\norder to flip a neural model’s decision. In a comprehensive analysis of\nmultiple NLP tasks, including linguistic feature classification, sentence-level\nsentiment analysis, and document level sentiment aspect prediction, we show\nthat the proposed methodology not only offers clear explanations about neural\nmodel decisions, but also provides a way to conduct error analysis on neural\nmodels.</p>\n", "tags": [] },
{"key": "li2019deepgcns", "year": "2019", "title":"DeepGCNs: Can GCNs Go as Deep as CNNs?", "abstract": "<p>Convolutional Neural Networks (CNNs) achieve impressive performance in a wide\nvariety of fields. Their success benefited from a massive boost when very deep\nCNN models were able to be reliably trained. Despite their merits, CNNs fail to\nproperly address problems with non-Euclidean data. To overcome this challenge,\nGraph Convolutional Networks (GCNs) build graphs to represent non-Euclidean\ndata, borrow concepts from CNNs, and apply them in training. GCNs show\npromising results, but they are usually limited to very shallow models due to\nthe vanishing gradient problem. As a result, most state-of-the-art GCN models\nare no deeper than 3 or 4 layers. In this work, we present new ways to\nsuccessfully train very deep GCNs. We do this by borrowing concepts from CNNs,\nspecifically residual/dense connections and dilated convolutions, and adapting\nthem to GCN architectures. Extensive experiments show the positive effect of\nthese deep GCN frameworks. Finally, we use these new concepts to build a very\ndeep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU\nover state-of-the-art) in the task of point cloud semantic segmentation. We\nbelieve that the community can greatly benefit from this work, as it opens up\nmany opportunities for advancing GCN-based research.</p>\n", "tags": [] },
{"key": "li2019interpretable", "year": "2019", "title":"Interpretable Neural Network Decoupling", "abstract": "<p>The remarkable performance of convolutional neural networks (CNNs) is\nentangled with their huge number of uninterpretable parameters, which has\nbecome the bottleneck limiting the exploitation of their full potential.\nTowards network interpretation, previous endeavors mainly resort to the single\nfilter analysis, which however ignores the relationship between filters. In\nthis paper, we propose a novel architecture decoupling method to interpret the\nnetwork from a perspective of investigating its calculation paths. More\nspecifically, we introduce a novel architecture controlling module in each\nlayer to encode the network architecture by a vector. By maximizing the mutual\ninformation between the vectors and input images, the module is trained to\nselect specific filters to distill a unique calculation path for each input.\nFurthermore, to improve the interpretability and compactness of the decoupled\nnetwork, the output of each layer is encoded to align the architecture encoding\nvector with the constraint of sparsity regularization. Unlike conventional\npixel-level or filter-level network interpretation methods, we propose a\npath-level analysis to explore the relationship between the combination of\nfilter and semantic concepts, which is more suitable to interpret the working\nrationale of the decoupled network. Extensive experiments show that the\ndecoupled network achieves several applications, i.e., network interpretation,\nnetwork acceleration, and adversarial samples detection.</p>\n", "tags": [] },
{"key": "li2021interpretable", "year": "2021", "title":"Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond", "abstract": "<p>Deep neural networks have been well-known for their superb handling of\nvarious machine learning and artificial intelligence tasks. However, due to\ntheir over-parameterized black-box nature, it is often difficult to understand\nthe prediction results of deep models. In recent years, many interpretation\ntools have been proposed to explain or reveal how deep models make decisions.\nIn this paper, we review this line of research and try to make a comprehensive\nsurvey. Specifically, we first introduce and clarify two basic concepts –\ninterpretations and interpretability – that people usually get confused about.\nTo address the research efforts in interpretations, we elaborate the designs of\na number of interpretation algorithms, from different perspectives, by\nproposing a new taxonomy. Then, to understand the interpretation results, we\nalso survey the performance metrics for evaluating interpretation algorithms.\nFurther, we summarize the current works in evaluating models’ interpretability\nusing “trustworthy” interpretation algorithms. Finally, we review and discuss\nthe connections between deep models’ interpretations and other factors, such as\nadversarial robustness and learning from interpretations, and we introduce\nseveral open-source libraries for interpretation algorithms and evaluation\napproaches.</p>\n", "tags": [] },
{"key": "liang2017interpretable", "year": "2017", "title":"Interpretable Structure-Evolving LSTM", "abstract": "<p>This paper develops a general framework for learning interpretable data\nrepresentation via Long Short-Term Memory (LSTM) recurrent neural networks over\nhierarchal graph structures. Instead of learning LSTM models over the pre-fixed\nstructures, we propose to further learn the intermediate interpretable\nmulti-level graph structures in a progressive and stochastic way from data\nduring the LSTM network optimization. We thus call this model the\nstructure-evolving LSTM. In particular, starting with an initial element-level\ngraph representation where each node is a small data element, the\nstructure-evolving LSTM gradually evolves the multi-level graph representations\nby stochastically merging the graph nodes with high compatibilities along the\nstacked LSTM layers. In each LSTM layer, we estimate the compatibility of two\nconnected nodes from their corresponding LSTM gate outputs, which is used to\ngenerate a merging probability. The candidate graph structures are accordingly\ngenerated where the nodes are grouped into cliques with their merging\nprobabilities. We then produce the new graph structure with a\nMetropolis-Hasting algorithm, which alleviates the risk of getting stuck in\nlocal optimums by stochastic sampling with an acceptance probability. Once a\ngraph structure is accepted, a higher-level graph is then constructed by taking\nthe partitioned cliques as its nodes. During the evolving process,\nrepresentation becomes more abstracted in higher-levels where redundant\ninformation is filtered out, allowing more efficient propagation of long-range\ndata dependencies. We evaluate the effectiveness of structure-evolving LSTM in\nthe application of semantic object parsing and demonstrate its advantage over\nstate-of-the-art LSTM models on standard benchmarks.</p>\n", "tags": [] },
{"key": "lim2019temporal", "year": "2019", "title":"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting", "abstract": "<p>Multi-horizon forecasting problems often contain a complex mix of inputs –\nincluding static (i.e. time-invariant) covariates, known future inputs, and\nother exogenous time series that are only observed historically – without any\nprior information on how they interact with the target. While several deep\nlearning models have been proposed for multi-step prediction, they typically\ncomprise black-box models which do not account for the full range of inputs\npresent in common scenarios. In this paper, we introduce the Temporal Fusion\nTransformer (TFT) – a novel attention-based architecture which combines\nhigh-performance multi-horizon forecasting with interpretable insights into\ntemporal dynamics. To learn temporal relationships at different scales, the TFT\nutilizes recurrent layers for local processing and interpretable self-attention\nlayers for learning long-term dependencies. The TFT also uses specialized\ncomponents for the judicious selection of relevant features and a series of\ngating layers to suppress unnecessary components, enabling high performance in\na wide range of regimes. On a variety of real-world datasets, we demonstrate\nsignificant performance improvements over existing benchmarks, and showcase\nthree practical interpretability use-cases of TFT.</p>\n", "tags": [] },
{"key": "lim2020time", "year": "2020", "title":"Time Series Forecasting With Deep Learning: A Survey", "abstract": "<p>Numerous deep learning architectures have been developed to accommodate the\ndiversity of time series datasets across different domains. In this article, we\nsurvey common encoder and decoder designs used in both one-step-ahead and\nmulti-horizon time series forecasting – describing how temporal information is\nincorporated into predictions by each model. Next, we highlight recent\ndevelopments in hybrid deep learning models, which combine well-studied\nstatistical models with neural network components to improve pure methods in\neither category. Lastly, we outline some ways in which deep learning can also\nfacilitate decision support with time series data.</p>\n", "tags": [] },
{"key": "lin2020see", "year": "2020", "title":"What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors", "abstract": "<p>EXplainable AI (XAI) methods have been proposed to interpret how a deep\nneural network predicts inputs through model saliency explanations that\nhighlight the parts of the inputs deemed important to arrive a decision at a\nspecific target. However, it remains challenging to quantify correctness of\ntheir interpretability as current evaluation approaches either require\nsubjective input from humans or incur high computation cost with automated\nevaluation. In this paper, we propose backdoor trigger patterns–hidden\nmalicious functionalities that cause misclassification–to automate the\nevaluation of saliency explanations. Our key observation is that triggers\nprovide ground truth for inputs to evaluate whether the regions identified by\nan XAI method are truly relevant to its output. Since backdoor triggers are the\nmost important features that cause deliberate misclassification, a robust XAI\nmethod should reveal their presence at inference time. We introduce three\ncomplementary metrics for systematic evaluation of explanations that an XAI\nmethod generates and evaluate seven state-of-the-art model-free and\nmodel-specific posthoc methods through 36 models trojaned with specifically\ncrafted triggers using color, shape, texture, location, and size. We discovered\nsix methods that use local explanation and feature relevance fail to completely\nhighlight trigger regions, and only a model-free approach can uncover the\nentire trigger region.</p>\n", "tags": [] },
{"key": "lipton2015learning", "year": "2015", "title":"Learning to Diagnose with LSTM Recurrent Neural Networks", "abstract": "<p>Clinical medical data, especially in the intensive care unit (ICU), consist\nof multivariate time series of observations. For each patient visit (or\nepisode), sensor data and lab test results are recorded in the patient’s\nElectronic Health Record (EHR). While potentially containing a wealth of\ninsights, the data is difficult to mine effectively, owing to varying length,\nirregular sampling and missing data. Recurrent Neural Networks (RNNs),\nparticularly those using Long Short-Term Memory (LSTM) hidden units, are\npowerful and increasingly popular models for learning from sequence data. They\neffectively model varying length sequences and capture long range dependencies.\nWe present the first study to empirically evaluate the ability of LSTMs to\nrecognize patterns in multivariate time series of clinical measurements.\nSpecifically, we consider multilabel classification of diagnoses, training a\nmodel to classify 128 diagnoses given 13 frequently but irregularly sampled\nclinical measurements. First, we establish the effectiveness of a simple LSTM\nnetwork for modeling clinical data. Then we demonstrate a straightforward and\neffective training strategy in which we replicate targets at each sequence\nstep. Trained only on raw time series, our models outperform several strong\nbaselines, including a multilayer perceptron trained on hand-engineered\nfeatures.</p>\n", "tags": [] },
{"key": "lipton2017mythos", "year": "2017", "title":"The Mythos of Model Interpretability", "abstract": "<p>Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.</p>\n", "tags": ["understanding"] },
{"key": "liu2017contextual", "year": "2017", "title":"Contextual Outlier Interpretation", "abstract": "<p>Outlier detection plays an essential role in many data-driven applications to\nidentify isolated instances that are different from the majority. While many\nstatistical learning and data mining techniques have been used for developing\nmore effective outlier detection algorithms, the interpretation of detected\noutliers does not receive much attention. Interpretation is becoming\nincreasingly important to help people trust and evaluate the developed models\nthrough providing intrinsic reasons why the certain outliers are chosen. It is\ndifficult, if not impossible, to simply apply feature selection for explaining\noutliers due to the distinct characteristics of various detection models,\ncomplicated structures of data in certain applications, and imbalanced\ndistribution of outliers and normal instances. In addition, the role of\ncontrastive contexts where outliers locate, as well as the relation between\noutliers and contexts, are usually overlooked in interpretation. To tackle the\nissues above, in this paper, we propose a novel Contextual Outlier\nINterpretation (COIN) method to explain the abnormality of existing outliers\nspotted by detectors. The interpretability for an outlier is achieved from\nthree aspects: outlierness score, attributes that contribute to the\nabnormality, and contextual description of its neighborhoods. Experimental\nresults on various types of datasets demonstrate the flexibility and\neffectiveness of the proposed framework compared with existing interpretation\napproaches.</p>\n", "tags": [] },
{"key": "liu2019towards", "year": "2019", "title":"Towards Visually Explaining Variational Autoencoders", "abstract": "<p>Recent advances in Convolutional Neural Network (CNN) model interpretability\nhave led to impressive progress in visualizing and understanding model\npredictions. In particular, gradient-based visual attention methods have driven\nmuch recent effort in using visual attention maps as a means for visual\nexplanations. A key problem, however, is these methods are designed for\nclassification and categorization tasks, and their extension to explaining\ngenerative models, e.g. variational autoencoders (VAE) is not trivial. In this\nwork, we take a step towards bridging this crucial gap, proposing the first\ntechnique to visually explain VAEs by means of gradient-based attention. We\npresent methods to generate visual attention from the learned latent space, and\nalso demonstrate such attention explanations serve more than just explaining\nVAE predictions. We show how these attention maps can be used to localize\nanomalies in images, demonstrating state-of-the-art performance on the MVTec-AD\ndataset. We also show how they can be infused into model training, helping\nbootstrap the VAE into learning improved latent space disentanglement,\ndemonstrated on the Dsprites dataset.</p>\n", "tags": [] },
{"key": "liu2021going", "year": "2021", "title":"Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models", "abstract": "<p>Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer’s\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.</p>\n", "tags": [] },
{"key": "liu2021synthetic", "year": "2021", "title":"Synthetic Benchmarks for Scientific Research in Explainable Machine Learning", "abstract": "<p>As machine learning models grow more complex and their applications become\nmore high-stakes, tools for explaining model predictions have become\nincreasingly important. This has spurred a flurry of research in model\nexplainability and has given rise to feature attribution methods such as LIME\nand SHAP. Despite their widespread use, evaluating and comparing different\nfeature attribution methods remains challenging: evaluations ideally require\nhuman studies, and empirical evaluation metrics are often data-intensive or\ncomputationally prohibitive on real-world datasets. In this work, we address\nthis issue by releasing XAI-Bench: a suite of synthetic datasets along with a\nlibrary for benchmarking feature attribution algorithms. Unlike real-world\ndatasets, synthetic datasets allow the efficient computation of conditional\nexpected values that are needed to evaluate ground-truth Shapley values and\nother metrics. The synthetic datasets we release offer a wide variety of\nparameters that can be configured to simulate real-world data. We demonstrate\nthe power of our library by benchmarking popular explainability techniques\nacross several evaluation metrics and across a variety of settings. The\nversatility and efficiency of our library will help researchers bring their\nexplainability methods from development to deployment. Our code is available at\nhttps://github.com/abacusai/xai-bench.</p>\n", "tags": [] },
{"key": "liu2022generating", "year": "2022", "title":"Generating and Visualizing Trace Link Explanations", "abstract": "<p>Recent breakthroughs in deep-learning (DL) approaches have resulted in the\ndynamic generation of trace links that are far more accurate than was\npreviously possible. However, DL-generated links lack clear explanations, and\ntherefore non-experts in the domain can find it difficult to understand the\nunderlying semantics of the link, making it hard for them to evaluate the\nlink’s correctness or suitability for a specific software engineering task. In\nthis paper we present a novel NLP pipeline for generating and visualizing trace\nlink explanations. Our approach identifies domain-specific concepts, retrieves\na corpus of concept-related sentences, mines concept definitions and usage\nexamples, and identifies relations between cross-artifact concepts in order to\nexplain the links. It applies a post-processing step to prioritize the most\nlikely acronyms and definitions and to eliminate non-relevant ones. We evaluate\nour approach using project artifacts from three different domains of\ninterstellar telescopes, positive train control, and electronic health-care\nsystems, and then report coverage, correctness, and potential utility of the\ngenerated definitions. We design and utilize an explanation interface which\nleverages concept definitions and relations to visualize and explain trace link\nrationales, and we report results from a user study that was conducted to\nevaluate the effectiveness of the explanation interface. Results show that the\nexplanations presented in the interface helped non-experts to understand the\nunderlying semantics of a trace link and improved their ability to vet the\ncorrectness of the link.</p>\n", "tags": [] },
{"key": "looveren2021conditional", "year": "2021", "title":"Conditional Generative Models for Counterfactual Explanations", "abstract": "<p>Counterfactual instances offer human-interpretable insight into the local\nbehaviour of machine learning models. We propose a general framework to\ngenerate sparse, in-distribution counterfactual model explanations which match\na desired target prediction with a conditional generative model, allowing\nbatches of counterfactual instances to be generated with a single forward pass.\nThe method is flexible with respect to the type of generative model used as\nwell as the task of the underlying predictive model. This allows\nstraightforward application of the framework to different modalities such as\nimages, time series or tabular data as well as generative model paradigms such\nas GANs or autoencoders and predictive tasks like classification or regression.\nWe illustrate the effectiveness of our method on image (CelebA), time series\n(ECG) and mixed-type tabular (Adult Census) data.</p>\n", "tags": [] },
{"key": "lu2021crowdsourcing", "year": "2021", "title":"Crowdsourcing Evaluation of Saliency-based XAI Methods", "abstract": "<p>Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, “Peek-a-boom”,\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.</p>\n", "tags": [] },
{"key": "lundberg2017unified", "year": "2017", "title":"A Unified Approach to Interpreting Model Predictions", "abstract": "<p>Understanding why a model makes a certain prediction can be as crucial as the\nprediction’s accuracy in many applications. However, the highest accuracy for\nlarge modern datasets is often achieved by complex models that even experts\nstruggle to interpret, such as ensemble or deep learning models, creating a\ntension between accuracy and interpretability. In response, various methods\nhave recently been proposed to help users interpret the predictions of complex\nmodels, but it is often unclear how these methods are related and when one\nmethod is preferable over another. To address this problem, we present a\nunified framework for interpreting predictions, SHAP (SHapley Additive\nexPlanations). SHAP assigns each feature an importance value for a particular\nprediction. Its novel components include: (1) the identification of a new class\nof additive feature importance measures, and (2) theoretical results showing\nthere is a unique solution in this class with a set of desirable properties.\nThe new class unifies six existing methods, notable because several recent\nmethods in the class lack the proposed desirable properties. Based on insights\nfrom this unification, we present new methods that show improved computational\nperformance and/or better consistency with human intuition than previous\napproaches.</p>\n", "tags": [] },
{"key": "mahendran2014understanding", "year": "2014", "title":"Understanding Deep Image Representations by Inverting Them", "abstract": "<p>Image representations, from SIFT and Bag of Visual Words to Convolutional\nNeural Networks (CNNs), are a crucial component of almost any image\nunderstanding system. Nevertheless, our understanding of them remains limited.\nIn this paper we conduct a direct analysis of the visual information contained\nin representations by asking the following question: given an encoding of an\nimage, to which extent is it possible to reconstruct the image itself? To\nanswer this question we contribute a general framework to invert\nrepresentations. We show that this method can invert representations such as\nHOG and SIFT more accurately than recent alternatives while being applicable to\nCNNs too. We then use this technique to study the inverse of recent\nstate-of-the-art CNN image representations for the first time. Among our\nfindings, we show that several layers in CNNs retain photographically accurate\ninformation about the image, with different degrees of geometric and\nphotometric invariance.</p>\n", "tags": [] },
{"key": "makinwa2021detection", "year": "2021", "title":"Detection Accuracy for Evaluating Compositional Explanations of Units", "abstract": "<p>The recent success of deep learning models in solving complex problems and in\ndifferent domains has increased interest in understanding what they learn.\nTherefore, different approaches have been employed to explain these models, one\nof which uses human-understandable concepts as explanations. Two examples of\nmethods that use this approach are Network Dissection and Compositional\nexplanations. The former explains units using atomic concepts, while the latter\nmakes explanations more expressive, replacing atomic concepts with logical\nforms. While intuitively, logical forms are more informative than atomic\nconcepts, it is not clear how to quantify this improvement, and their\nevaluation is often based on the same metric that is optimized during the\nsearch-process and on the usage of hyper-parameters to be tuned. In this paper,\nwe propose to use as evaluation metric the Detection Accuracy, which measures\nunits’ consistency of detection of their assigned explanations. We show that\nthis metric (1) evaluates explanations of different lengths effectively, (2)\ncan be used as a stopping criterion for the compositional explanation search,\neliminating the explanation length hyper-parameter, and (3) exposes new\nspecialized units whose length 1 explanations are the perceptual abstractions\nof their longer explanations.</p>\n", "tags": [] },
{"key": "mangla2020saliency", "year": "2020", "title":"On Saliency Maps and Adversarial Robustness", "abstract": "<p>A Very recent trend has emerged to couple the notion of interpretability and\nadversarial robustness, unlike earlier efforts which solely focused on good\ninterpretations or robustness against adversaries. Works have shown that\nadversarially trained models exhibit more interpretable saliency maps than\ntheir non-robust counterparts, and that this behavior can be quantified by\nconsidering the alignment between input image and saliency map. In this work,\nwe provide a different perspective to this coupling, and provide a method,\nSaliency based Adversarial training (SAT), to use saliency maps to improve\nadversarial robustness of a model. In particular, we show that using\nannotations such as bounding boxes and segmentation masks, already provided\nwith a dataset, as weak saliency maps, suffices to improve adversarial\nrobustness with no additional effort to generate the perturbations themselves.\nOur empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17\ndatasets consistently corroborate our claim, by showing improved adversarial\nrobustness using our method. saliency maps. We also show how using finer and\nstronger saliency maps leads to more robust models, and how integrating SAT\nwith existing adversarial training methods, further boosts performance of these\nexisting methods.</p>\n", "tags": [] },
{"key": "manica2022gt4sd", "year": "2022", "title":"GT4SD: Generative Toolkit for Scientific Discovery", "abstract": "<p>With the growing availability of data within various scientific domains,\ngenerative models hold enormous potential to accelerate scientific discovery at\nevery step of the scientific method. Perhaps their most valuable application\nlies in the speeding up of what has traditionally been the slowest and most\nchallenging step of coming up with a hypothesis. Powerful representations are\nnow being learned from large volumes of data to generate novel hypotheses,\nwhich is making a big impact on scientific discovery applications ranging from\nmaterial design to drug discovery. The GT4SD\n(https://github.com/GT4SD/gt4sd-core) is an extensible open-source library that\nenables scientists, developers and researchers to train and use\nstate-of-the-art generative models for hypothesis generation in scientific\ndiscovery. GT4SD supports a variety of uses of generative models across\nmaterial science and drug discovery, including molecule discovery and design\nbased on properties related to target proteins, omic profiles, scaffold\ndistances, binding energies and more.</p>\n", "tags": [] },
{"key": "marcos2020contextual", "year": "2020", "title":"Contextual Semantic Interpretability", "abstract": "<p>Convolutional neural networks (CNN) are known to learn an image\nrepresentation that captures concepts relevant to the task, but do so in an\nimplicit way that hampers model interpretability. However, one could argue that\nsuch a representation is hidden in the neurons and can be made explicit by\nteaching the model to recognize semantically interpretable attributes that are\npresent in the scene. We call such an intermediate layer a \\emph{semantic\nbottleneck}. Once the attributes are learned, they can be re-combined to reach\nthe final decision and provide both an accurate prediction and an explicit\nreasoning behind the CNN decision. In this paper, we look into semantic\nbottlenecks that capture context: we want attributes to be in groups of a few\nmeaningful elements and participate jointly to the final decision. We use a\ntwo-layer semantic bottleneck that gathers attributes into interpretable,\nsparse groups, allowing them contribute differently to the final output\ndepending on the context. We test our contextual semantic interpretable\nbottleneck (CSIB) on the task of landscape scenicness estimation and train the\nsemantic interpretable bottleneck using an auxiliary database (SUN Attributes).\nOur model yields in predictions as accurate as a non-interpretable baseline\nwhen applied to a real-world test set of Flickr images, all while providing\nclear and interpretable explanations for each prediction.</p>\n", "tags": [] },
{"key": "mardaoui2020analysis", "year": "2020", "title":"An Analysis of LIME for Text Data", "abstract": "<p>Text data are increasingly handled in an automated fashion by machine\nlearning algorithms. But the models handling these data are not always\nwell-understood due to their complexity and are more and more often referred to\nas “black-boxes.” Interpretability methods aim to explain how these models\noperate. Among them, LIME has become one of the most popular in recent years.\nHowever, it comes without theoretical guarantees: even for simple models, we\nare not sure that LIME behaves accurately. In this paper, we provide a first\ntheoretical analysis of LIME for text data. As a consequence of our theoretical\nfindings, we show that LIME indeed provides meaningful explanations for simple\nmodels, namely decision trees and linear models.</p>\n", "tags": [] },
{"key": "markus2020role", "year": "2020", "title":"The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies", "abstract": "<p>Artificial intelligence (AI) has huge potential to improve the health and\nwell-being of people, but adoption in clinical practice is still limited. Lack\nof transparency is identified as one of the main barriers to implementation, as\nclinicians should be confident the AI system can be trusted. Explainable AI has\nthe potential to overcome this issue and can be a step towards trustworthy AI.\nIn this paper we review the recent literature to provide guidance to\nresearchers and practitioners on the design of explainable AI systems for the\nhealth-care domain and contribute to formalization of the field of explainable\nAI. We argue the reason to demand explainability determines what should be\nexplained as this determines the relative importance of the properties of\nexplainability (i.e. interpretability and fidelity). Based on this, we propose\na framework to guide the choice between classes of explainable AI methods\n(explainable modelling versus post-hoc explanation; model-based,\nattribution-based, or example-based explanations; global and local\nexplanations). Furthermore, we find that quantitative evaluation metrics, which\nare important for objective standardized evaluation, are still lacking for some\nproperties (e.g. clarity) and types of explanations (e.g. example-based\nmethods). We conclude that explainable modelling can contribute to trustworthy\nAI, but the benefits of explainability still need to be proven in practice and\ncomplementary measures might be needed to create trustworthy AI in health care\n(e.g. reporting data quality, performing extensive (external) validation, and\nregulation).</p>\n", "tags": [] },
{"key": "marx2019disentangling", "year": "2019", "title":"Disentangling Influence: Using Disentangled Representations to Audit Model Predictions", "abstract": "<p>Motivated by the need to audit complex and black box models, there has been\nextensive research on quantifying how data features influence model\npredictions. Feature influence can be direct (a direct influence on model\noutcomes) and indirect (model outcomes are influenced via proxy features).\nFeature influence can also be expressed in aggregate over the training or test\ndata or locally with respect to a single point. Current research has typically\nfocused on one of each of these dimensions. In this paper, we develop\ndisentangled influence audits, a procedure to audit the indirect influence of\nfeatures. Specifically, we show that disentangled representations provide a\nmechanism to identify proxy features in the dataset, while allowing an explicit\ncomputation of feature influence on either individual outcomes or\naggregate-level outcomes. We show through both theory and experiments that\ndisentangled influence audits can both detect proxy features and show, for each\nindividual or in aggregate, which of these proxy features affects the\nclassifier being audited the most. In this respect, our method is more powerful\nthan existing methods for ascertaining feature influence.</p>\n", "tags": [] },
{"key": "mayor-torres2021interpretable", "year": "2021", "title":"Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity", "abstract": "<p>Machine learning methods, such as deep learning, show promising results in\nthe medical domain. However, the lack of interpretability of these algorithms\nmay hinder their applicability to medical decision support systems. This paper\nstudies an interpretable deep learning technique, called SincNet. SincNet is a\nconvolutional neural network that efficiently learns customized band-pass\nfilters through trainable sinc-functions. In this study, we use SincNet to\nanalyze the neural activity of individuals with Autism Spectrum Disorder (ASD),\nwho experience characteristic differences in neural oscillatory activity. In\nparticular, we propose a novel SincNet-based neural network for detecting\nemotions in ASD patients using EEG signals. The learned filters can be easily\ninspected to detect which part of the EEG spectrum is used for predicting\nemotions. We found that our system automatically learns the high-$\\alpha$ (9-13\nHz) and $\\beta$ (13-30 Hz) band suppression often present in individuals with\nASD. This result is consistent with recent neuroscience studies on emotion\nrecognition, which found an association between these band suppressions and the\nbehavioral deficits observed in individuals with ASD. The improved\ninterpretability of SincNet is achieved without sacrificing performance in\nemotion recognition.</p>\n", "tags": [] },
{"key": "meade2019exploring", "year": "2019", "title":"Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls", "abstract": "<p>Performance RNN is a machine-learning system designed primarily for the\ngeneration of solo piano performances using an event-based (rather than audio)\nrepresentation. More specifically, Performance RNN is a long short-term memory\n(LSTM) based recurrent neural network that models polyphonic music with\nexpressive timing and dynamics (Oore et al., 2018). The neural network uses a\nsimple language model based on the Musical Instrument Digital Interface (MIDI)\nfile format. Performance RNN is trained on the e-Piano Junior Competition\nDataset (International Piano e-Competition, 2018), a collection of solo piano\nperformances by expert pianists. As an artistic tool, one of the limitations of\nthe original model has been the lack of useable controls. The standard form of\nPerformance RNN can generate interesting pieces, but little control is provided\nover what specifically is generated. This paper explores a set of\nconditioning-based controls used to influence the generation process.</p>\n", "tags": [] },
{"key": "miller2017explanation", "year": "2017", "title":"Explanation in Artificial Intelligence: Insights from the Social Sciences", "abstract": "<p>There has been a recent resurgence in the area of explainable artificial\nintelligence as researchers and practitioners seek to make their algorithms\nmore understandable. Much of this research is focused on explicitly explaining\ndecisions or actions to a human observer, and it should not be controversial to\nsay that looking at how humans explain to each other can serve as a useful\nstarting point for explanation in artificial intelligence. However, it is fair\nto say that most work in explainable artificial intelligence uses only the\nresearchers’ intuition of what constitutes a `good’ explanation. There exists\nvast and valuable bodies of research in philosophy, psychology, and cognitive\nscience of how people define, generate, select, evaluate, and present\nexplanations, which argues that people employ certain cognitive biases and\nsocial expectations towards the explanation process. This paper argues that the\nfield of explainable artificial intelligence should build on this existing\nresearch, and reviews relevant papers from philosophy, cognitive\npsychology/science, and social psychology, which study these topics. It draws\nout some important findings, and discusses ways that these can be infused with\nwork on explainable artificial intelligence.</p>\n", "tags": [] },
{"key": "miller2018contrastive", "year": "2018", "title":"Contrastive Explanation: A Structural-Model Approach", "abstract": "<p>This paper presents a model of contrastive explanation using structural\ncasual models. The topic of causal explanation in artificial intelligence has\ngathered interest in recent years as researchers and practitioners aim to\nincrease trust and understanding of intelligent decision-making. While\ndifferent sub-fields of artificial intelligence have looked into this problem\nwith a sub-field-specific view, there are few models that aim to capture\nexplanation more generally. One general model is based on structural causal\nmodels. It defines an explanation as a fact that, if found to be true, would\nconstitute an actual cause of a specific event. However, research in philosophy\nand social sciences shows that explanations are contrastive: that is, when\npeople ask for an explanation of an event – the fact – they (sometimes\nimplicitly) are asking for an explanation relative to some contrast case; that\nis, “Why P rather than Q?”. In this paper, we extend the structural causal\nmodel approach to define two complementary notions of contrastive explanation,\nand demonstrate them on two classical problems in artificial intelligence:\nclassification and planning. We believe that this model can help researchers in\nsubfields of artificial intelligence to better understand contrastive\nexplanation.</p>\n", "tags": [] },
{"key": "ming2019interpretable", "year": "2019", "title":"Interpretable and Steerable Sequence Learning via Prototypes", "abstract": "<p>One of the major challenges in machine learning nowadays is to provide\npredictions with not only high accuracy but also user-friendly explanations.\nAlthough in recent years we have witnessed increasingly popular use of deep\nneural networks for sequence modeling, it is still challenging to explain the\nrationales behind the model outputs, which is essential for building trust and\nsupporting the domain experts to validate, critique and refine the model. We\npropose ProSeNet, an interpretable and steerable deep sequence model with\nnatural explanations derived from case-based reasoning. The prediction is\nobtained by comparing the inputs to a few prototypes, which are exemplar cases\nin the problem domain. For better interpretability, we define several criteria\nfor constructing the prototypes, including simplicity, diversity, and sparsity\nand propose the learning objective and the optimization procedure. ProSeNet\nalso provides a user-friendly approach to model steering: domain experts\nwithout any knowledge on the underlying model or parameters can easily\nincorporate their intuition and experience by manually refining the prototypes.\nWe conduct experiments on a wide range of real-world applications, including\npredictive diagnostics for automobiles, ECG, and protein sequence\nclassification and sentiment analysis on texts. The result shows that ProSeNet\ncan achieve accuracy on par with state-of-the-art deep learning models. We also\nevaluate the interpretability of the results with concrete case studies.\nFinally, through user study on Amazon Mechanical Turk (MTurk), we demonstrate\nthat the model selects high-quality prototypes which align well with human\nknowledge and can be interactively refined for better interpretability without\nloss of performance.</p>\n", "tags": [] },
{"key": "mittelstadt2018explaining", "year": "2018", "title":"Explaining Explanations in AI", "abstract": "<p>Recent work on interpretability in machine learning and AI has focused on the\nbuilding of simplified models that approximate the true criteria used to make\ndecisions. These models are a useful pedagogical device for teaching trained\nprofessionals how to predict what decisions will be made by the complex system,\nand most importantly how the system might break. However, when considering any\nsuch model it’s important to remember Box’s maxim that “All models are wrong\nbut some are useful.” We focus on the distinction between these models and\nexplanations in philosophy and sociology. These models can be understood as a\n“do it yourself kit” for explanations, allowing a practitioner to directly\nanswer “what if questions” or generate contrastive explanations without\nexternal assistance. Although a valuable ability, giving these models as\nexplanations appears more difficult than necessary, and other forms of\nexplanation may not have the same trade-offs. We contrast the different schools\nof thought on what makes an explanation, and suggest that machine learning\nmight benefit from viewing the problem more broadly.</p>\n", "tags": [] },
{"key": "mohankumar2020towards", "year": "2020", "title":"Towards Transparent and Explainable Attention Models", "abstract": "<p>Recent studies on interpretability of attention distributions have led to\nnotions of faithful and plausible explanations for a model’s predictions.\nAttention distributions can be considered a faithful explanation if a higher\nattention weight implies a greater impact on the model’s prediction. They can\nbe considered a plausible explanation if they provide a human-understandable\njustification for the model’s predictions. In this work, we first explain why\ncurrent attention mechanisms in LSTM based encoders can neither provide a\nfaithful nor a plausible explanation of the model’s predictions. We observe\nthat in LSTM based encoders the hidden representations at different time-steps\nare very similar to each other (high conicity) and attention weights in these\nsituations do not carry much meaning because even a random permutation of the\nattention weights does not affect the model’s predictions. Based on experiments\non a wide variety of tasks and datasets, we observe attention distributions\noften attribute the model’s predictions to unimportant words such as\npunctuation and fail to offer a plausible explanation for the predictions. To\nmake attention mechanisms more faithful and plausible, we propose a modified\nLSTM cell with a diversity-driven training objective that ensures that the\nhidden representations learned at different time steps are diverse. We show\nthat the resulting attention distributions offer more transparency as they (i)\nprovide a more precise importance ranking of the hidden states (ii) are better\nindicative of words important for the model’s predictions (iii) correlate\nbetter with gradient-based attribution methods. Human evaluations indicate that\nthe attention distributions learned by our model offer a plausible explanation\nof the model’s predictions. Our code has been made publicly available at\nhttps://github.com/akashkm99/Interpretable-Attention</p>\n", "tags": [] },
{"key": "mohseni2018humangrounded", "year": "2018", "title":"A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning", "abstract": "<p>Research in interpretable machine learning proposes different computational\nand human subject approaches to evaluate model saliency explanations. These\napproaches measure different qualities of explanations to achieve diverse goals\nin designing interpretable machine learning systems. In this paper, we propose\na human attention benchmark for image and text domains using multi-layer human\nattention masks aggregated from multiple human annotators. We then present an\nevaluation study to evaluate model saliency explanations obtained using\nGrad-cam and LIME techniques. We demonstrate our benchmark’s utility for\nquantitative evaluation of model explanations by comparing it with human\nsubjective ratings and ground-truth single-layer segmentation masks\nevaluations. Our study results show that our threshold agnostic evaluation\nmethod with the human attention baseline is more effective than single-layer\nobject segmentation masks to ground truth. Our experiments also reveal user\nbiases in the subjective rating of model saliency explanations.</p>\n", "tags": [] },
{"key": "molnar2020interpretable", "year": "2020", "title":"Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges", "abstract": "<p>We present a brief history of the field of interpretable machine learning\n(IML), give an overview of state-of-the-art interpretation methods, and discuss\nchallenges. Research in IML has boomed in recent years. As young as the field\nis, it has over 200 years old roots in regression modeling and rule-based\nmachine learning, starting in the 1960s. Recently, many new IML methods have\nbeen proposed, many of them model-agnostic, but also interpretation techniques\nspecific to deep learning and tree-based ensembles. IML methods either directly\nanalyze model components, study sensitivity to input perturbations, or analyze\nlocal or global surrogate approximations of the ML model. The field approaches\na state of readiness and stability, with many methods not only proposed in\nresearch, but also implemented in open-source software. But many important\nchallenges remain for IML, such as dealing with dependent features, causal\ninterpretation, and uncertainty estimation, which need to be resolved for its\nsuccessful application to scientific problems. A further challenge is a missing\nrigorous definition of interpretability, which is accepted by the community. To\naddress the challenges and advance the field, we urge to recall our roots of\ninterpretable, data-driven modeling in statistics and (rule-based) ML, but also\nto consider other areas such as sensitivity analysis, causal inference, and the\nsocial sciences.</p>\n", "tags": [] },
{"key": "moorman2022people", "year": "2022", "title":"Do People Trust Robots that Learn in the Home?", "abstract": "<p>It is not scalable for assistive robotics to have all functionalities\npre-programmed prior to user introduction. Instead, it is more realistic for\nagents to perform supplemental on site learning. This opportunity to learn user\nand environment particularities is especially helpful for care robots that\nassist with individualized caregiver activities in residential or nursing home\nenvironments. Many assistive robots, ranging in complexity from Roomba to\nPepper, already conduct some of their learning in the home, observable to the\nuser. We lack an understanding of how witnessing this learning impacts the\nuser. Thus, we propose to assess end-user attitudes towards the concept of\nembodied robots that conduct some learning in the home as compared to robots\nthat are delivered fully-capable. In this virtual, between-subjects study, we\nrecruit end users (care-givers and care-takers) from nursing homes, and\ninvestigate user trust in three different domains: navigation, manipulation,\nand preparation. Informed by the first study where we identify agent learning\nas a key factor in determining trust, we propose a second study to explore how\nto modulate that trust. This second, in-person study investigates the\neffectiveness of apologies, explanations of robot failure, and transparency of\nlearning at improving trust in embodied learning robots.</p>\n", "tags": [] },
{"key": "moradi2020explaining", "year": "2020", "title":"Explaining Black-box Models for Biomedical Text Classification", "abstract": "<p>In this paper, we propose a novel method named Biomedical Confident Itemsets\nExplanation (BioCIE), aiming at post-hoc explanation of black-box machine\nlearning models for biomedical text classification. Using sources of domain\nknowledge and a confident itemset mining method, BioCIE discretizes the\ndecision space of a black-box into smaller subspaces and extracts semantic\nrelationships between the input text and class labels in different subspaces.\nConfident itemsets discover how biomedical concepts are related to class labels\nin the black-box’s decision space. BioCIE uses the itemsets to approximate the\nblack-box’s behavior for individual predictions. Optimizing fidelity,\ninterpretability, and coverage measures, BioCIE produces class-wise\nexplanations that represent decision boundaries of the black-box. Results of\nevaluations on various biomedical text classification tasks and black-box\nmodels demonstrated that BioCIE can outperform perturbation-based and decision\nset methods in terms of producing concise, accurate, and interpretable\nexplanations. BioCIE improved the fidelity of instance-wise and class-wise\nexplanations by 11.6% and 7.5%, respectively. It also improved the\ninterpretability of explanations by 8%. BioCIE can be effectively used to\nexplain how a black-box biomedical text classification model semantically\nrelates input texts to class labels. The source code and supplementary material\nare available at https://github.com/mmoradi-iut/BioCIE.</p>\n", "tags": [] },
{"key": "mothilal2019explaining", "year": "2019", "title":"Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations", "abstract": "<p>Post-hoc explanations of machine learning models are crucial for people to\nunderstand and act on algorithmic predictions. An intriguing class of\nexplanations is through counterfactuals, hypothetical examples that show people\nhow to obtain a different prediction. We posit that effective counterfactual\nexplanations should satisfy two properties: feasibility of the counterfactual\nactions given user context and constraints, and diversity among the\ncounterfactuals presented. To this end, we propose a framework for generating\nand evaluating a diverse set of counterfactual explanations based on\ndeterminantal point processes. To evaluate the actionability of\ncounterfactuals, we provide metrics that enable comparison of\ncounterfactual-based methods to other local explanation methods. We further\naddress necessary tradeoffs and point to causal implications in optimizing for\ncounterfactuals. Our experiments on four real-world datasets show that our\nframework can generate a set of counterfactuals that are diverse and well\napproximate local decision boundaries, outperforming prior approaches to\ngenerating diverse counterfactuals. We provide an implementation of the\nframework at https://github.com/microsoft/DiCE.</p>\n", "tags": [] },
{"key": "mougan2021desiderata", "year": "2021", "title":"Desiderata for Explainable AI in statistical production systems of the European Central Bank", "abstract": "<p>Explainable AI constitutes a fundamental step towards establishing fairness\nand addressing bias in algorithmic decision-making. Despite the large body of\nwork on the topic, the benefit of solutions is mostly evaluated from a\nconceptual or theoretical point of view and the usefulness for real-world use\ncases remains uncertain. In this work, we aim to state clear user-centric\ndesiderata for explainable AI reflecting common explainability needs\nexperienced in statistical production systems of the European Central Bank. We\nlink the desiderata to archetypical user roles and give examples of techniques\nand methods which can be used to address the user’s needs. To this end, we\nprovide two concrete use cases from the domain of statistical data production\nin central banks: the detection of outliers in the Centralised Securities\nDatabase and the data-driven identification of data quality checks for the\nSupervisory Banking data system.</p>\n", "tags": [] },
{"key": "mueller2019explanation", "year": "2019", "title":"Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI", "abstract": "<p>This is an integrative review that address the question, “What makes for a\ngood explanation?” with reference to AI systems. Pertinent literatures are\nvast. Thus, this review is necessarily selective. That said, most of the key\nconcepts and issues are expressed in this Report. The Report encapsulates the\nhistory of computer science efforts to create systems that explain and instruct\n(intelligent tutoring systems and expert systems). The Report expresses the\nexplainability issues and challenges in modern AI, and presents capsule views\nof the leading psychological theories of explanation. Certain articles stand\nout by virtue of their particular relevance to XAI, and their methods, results,\nand key points are highlighted. It is recommended that AI/XAI researchers be\nencouraged to include in their research reports fuller details on their\nempirical or experimental methods, in the fashion of experimental psychology\nresearch reports: details on Participants, Instructions, Procedures, Tasks,\nDependent Variables (operational definitions of the measures and metrics),\nIndependent Variables (conditions), and Control Conditions.</p>\n", "tags": [] },
{"key": "mukhopadhyay2020decoding", "year": "2020", "title":"Decoding CNN based Object Classifier Using Visualization", "abstract": "<p>This paper investigates how working of Convolutional Neural Network (CNN) can\nbe explained through visualization in the context of machine perception of\nautonomous vehicles. We visualize what type of features are extracted in\ndifferent convolution layers of CNN that helps to understand how CNN gradually\nincreases spatial information in every layer. Thus, it concentrates on region\nof interests in every transformation. Visualizing heat map of activation helps\nus to understand how CNN classifies and localizes different objects in image.\nThis study also helps us to reason behind low accuracy of a model helps to\nincrease trust on object detection module.</p>\n", "tags": [] },
{"key": "mundhenk2019efficient", "year": "2019", "title":"Efficient Saliency Maps for Explainable AI", "abstract": "<p>We describe an explainable AI saliency map method for use with deep\nconvolutional neural networks (CNN) that is much more efficient than popular\nfine-resolution gradient methods. It is also quantitatively similar or better\nin accuracy. Our technique works by measuring information at the end of each\nnetwork scale which is then combined into a single saliency map. We describe\nhow saliency measures can be made more efficient by exploiting Saliency Map\nOrder Equivalence. We visualize individual scale/layer contributions by using a\nLayer Ordered Visualization of Information. This provides an interesting\ncomparison of scale information contributions within the network not provided\nby other saliency map methods. Using our method instead of Guided Backprop,\ncoarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem\nto yield demonstrably superior results without sacrificing speed. This will\nmake fine-resolution saliency methods feasible on resource limited platforms\nsuch as robots, cell phones, low-cost industrial devices, astronomy and\nsatellite imagery.</p>\n", "tags": [] },
{"key": "murata2001magical", "year": "2001", "title":"Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition in Japanese and English Sentences", "abstract": "<p>George A. Miller said that human beings have only seven chunks in short-term\nmemory, plus or minus two. We counted the number of bunsetsus (phrases) whose\nmodifiees are undetermined in each step of an analysis of the dependency\nstructure of Japanese sentences, and which therefore must be stored in\nshort-term memory. The number was roughly less than nine, the upper bound of\nseven plus or minus two. We also obtained similar results with English\nsentences under the assumption that human beings recognize a series of words,\nsuch as a noun phrase (NP), as a unit. This indicates that if we assume that\nthe human cognitive units in Japanese and English are bunsetsu and NP\nrespectively, analysis will support Miller’s $7 \\pm 2$ theory.</p>\n", "tags": [] },
{"key": "murdoch2017automatic", "year": "2017", "title":"Automatic Rule Extraction from Long Short Term Memory Networks", "abstract": "<p>Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.</p>\n", "tags": [] },
{"key": "murdoch2018beyond", "year": "2018", "title":"Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs", "abstract": "<p>The driving force behind the recent success of LSTMs has been their ability\nto learn complex and non-linear relationships. Consequently, our inability to\ndescribe these relationships has led to LSTMs being characterized as black\nboxes. To this end, we introduce contextual decomposition (CD), an\ninterpretation algorithm for analysing individual predictions made by standard\nLSTMs, without any changes to the underlying model. By decomposing the output\nof a LSTM, CD captures the contributions of combinations of words or variables\nto the final prediction of an LSTM. On the task of sentiment analysis with the\nYelp and SST data sets, we show that CD is able to reliably identify words and\nphrases of contrasting sentiment, and how they are combined to yield the LSTM’s\nfinal prediction. Using the phrase-level labels in SST, we also demonstrate\nthat CD is able to successfully extract positive and negative negations from an\nLSTM, something which has not previously been done.</p>\n", "tags": [] },
{"key": "müller2022interactive", "year": "2022", "title":"An Interactive Explanatory AI System for Industrial Quality Control", "abstract": "<p>Machine learning based image classification algorithms, such as deep neural\nnetwork approaches, will be increasingly employed in critical settings such as\nquality control in industry, where transparency and comprehensibility of\ndecisions are crucial. Therefore, we aim to extend the defect detection task\ntowards an interactive human-in-the-loop approach that allows us to integrate\nrich background knowledge and the inference of complex relationships going\nbeyond traditional purely data-driven approaches. We propose an approach for an\ninteractive support system for classifications in an industrial quality control\nsetting that combines the advantages of both (explainable) knowledge-driven and\ndata-driven machine learning methods, in particular inductive logic programming\nand convolutional neural networks, with human expertise and control. The\nresulting system can assist domain experts with decisions, provide transparent\nexplanations for results, and integrate feedback from users; thus reducing\nworkload for humans while both respecting their expertise and without removing\ntheir agency or accountability.</p>\n", "tags": [] },
{"key": "naik2020explanation", "year": "2020", "title":"Explanation from Specification", "abstract": "<p>Explainable components in XAI algorithms often come from a familiar set of\nmodels, such as linear models or decision trees. We formulate an approach where\nthe type of explanation produced is guided by a specification. Specifications\nare elicited from the user, possibly using interaction with the user and\ncontributions from other areas. Areas where a specification could be obtained\ninclude forensic, medical, and scientific applications. Providing a menu of\npossible types of specifications in an area is an exploratory knowledge\nrepresentation and reasoning task for the algorithm designer, aiming at\nunderstanding the possibilities and limitations of efficiently computable modes\nof explanations. Two examples are discussed: explanations for Bayesian networks\nusing the theory of argumentation, and explanations for graph neural networks.\nThe latter case illustrates the possibility of having a representation\nformalism available to the user for specifying the type of explanation\nrequested, for example, a chemical query language for classifying molecules.\nThe approach is motivated by a theory of explanation in the philosophy of\nscience, and it is related to current questions in the philosophy of science on\nthe role of machine learning.</p>\n", "tags": [] },
{"key": "nallbani2021resvgae", "year": "2021", "title":"ResVGAE: Going Deeper with Residual Modules for Link Prediction", "abstract": "<p>Graph autoencoders are efficient at embedding graph-based data sets. Most\ngraph autoencoder architectures have shallow depths which limits their ability\nto capture meaningful relations between nodes separated by multi-hops. In this\npaper, we propose Residual Variational Graph Autoencoder, ResVGAE, a deep\nvariational graph autoencoder model with multiple residual modules. We show\nthat our multiple residual modules, a convolutional layer with residual\nconnection, improve the average precision of the graph autoencoders.\nExperimental results suggest that our proposed model with residual modules\noutperforms the models without residual modules and achieves similar results\nwhen compared with other state-of-the-art methods.</p>\n", "tags": [] },
{"key": "narayanan2018humans", "year": "2018", "title":"How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation", "abstract": "<p>Recent years have seen a boom in interest in machine learning systems that\ncan provide a human-understandable rationale for their predictions or\ndecisions. However, exactly what kinds of explanation are truly\nhuman-interpretable remains poorly understood. This work advances our\nunderstanding of what makes explanations interpretable in the specific context\nof verification. Suppose we have a machine learning system that predicts X, and\nwe provide rationale for this prediction X. Given an input, an explanation, and\nan output, is the output consistent with the input and the supposed rationale?\nVia a series of user-studies, we identify what kinds of increases in complexity\nhave the greatest effect on the time it takes for humans to verify the\nrationale, and which seem relatively insensitive.</p>\n", "tags": [] },
{"key": "nauta2020looks", "year": "2020", "title":"This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition", "abstract": "<p>Image recognition with prototypes is considered an interpretable alternative\nfor black box deep learning models. Classification depends on the extent to\nwhich a test image “looks like” a prototype. However, perceptual similarity for\nhumans can be different from the similarity learned by the classification\nmodel. Hence, only visualising prototypes can be insufficient for a user to\nunderstand what a prototype exactly represents, and why the model considers a\nprototype and an image to be similar. We address this ambiguity and argue that\nprototypes should be explained. We improve interpretability by automatically\nenhancing visual prototypes with textual quantitative information about visual\ncharacteristics deemed important by the classification model. Specifically, our\nmethod clarifies the meaning of a prototype by quantifying the influence of\ncolour hue, shape, texture, contrast and saturation and can generate both\nglobal and local explanations. Because of the generality of our approach, it\ncan improve the interpretability of any similarity-based method for\nprototypical image recognition. In our experiments, we apply our method to the\nexisting Prototypical Part Network (ProtoPNet). Our analysis confirms that the\nglobal explanations are generalisable, and often correspond to the visually\nperceptible properties of a prototype. Our explanations are especially relevant\nfor prototypes which might have been interpreted incorrectly otherwise. By\nexplaining such ‘misleading’ prototypes, we improve the interpretability and\nsimulatability of a prototype-based classification model. We also use our\nmethod to check whether visually similar prototypes have similar explanations,\nand are able to discover redundancy. Code is available at\nhttps://github.com/M-Nauta/Explaining_Prototypes .</p>\n", "tags": [] },
{"key": "nauta2022anecdotal", "year": "2022", "title":"From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI", "abstract": "<p>The rising popularity of explainable artificial intelligence (XAI) to\nunderstand high-performing black boxes, also raised the question of how to\nevaluate explanations of machine learning (ML) models. While interpretability\nand explainability are often presented as a subjectively validated binary\nproperty, we consider it a multi-faceted concept. We identify 12 conceptual\nproperties, such as Compactness and Correctness, that should be evaluated for\ncomprehensively assessing the quality of an explanation. Our so-called Co-12\nproperties serve as categorization scheme for systematically reviewing the\nevaluation practice of more than 300 papers published in the last 7 years at\nmajor AI and ML conferences that introduce an XAI method. We find that 1 in 3\npapers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate\nwith users. We also contribute to the call for objective, quantifiable\nevaluation methods by presenting an extensive overview of quantitative XAI\nevaluation methods. This systematic collection of evaluation methods provides\nresearchers and practitioners with concrete tools to thoroughly validate,\nbenchmark and compare new and existing XAI methods. This also opens up\nopportunities to include quantitative metrics as optimization criteria during\nmodel training in order to optimize for accuracy and interpretability\nsimultaneously.</p>\n", "tags": [] },
{"key": "nguyen2016plug", "year": "2016", "title":"Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space", "abstract": "<p>Generating high-resolution, photo-realistic images has been a long-standing\ngoal in machine learning. Recently, Nguyen et al. (2016) showed one interesting\nway to synthesize novel images by performing gradient ascent in the latent\nspace of a generator network to maximize the activations of one or multiple\nneurons in a separate classifier network. In this paper we extend this method\nby introducing an additional prior on the latent code, improving both sample\nquality and sample diversity, leading to a state-of-the-art generative model\nthat produces high quality images at higher resolutions (227x227) than previous\ngenerative models, and does so for all 1000 ImageNet categories. In addition,\nwe provide a unified probabilistic interpretation of related activation\nmaximization methods and call the general class of models “Plug and Play\nGenerative Networks”. PPGNs are composed of 1) a generator network G that is\ncapable of drawing a wide range of image types and 2) a replaceable “condition”\nnetwork C that tells the generator what to draw. We demonstrate the generation\nof images conditioned on a class (when C is an ImageNet or MIT Places\nclassification network) and also conditioned on a caption (when C is an image\ncaptioning network). Our method also improves the state of the art of\nMultifaceted Feature Visualization, which generates the set of synthetic inputs\nthat activate a neuron in order to better understand how deep neural networks\noperate. Finally, we show that our model performs reasonably well at the task\nof image inpainting. While image models are used in this paper, the approach is\nmodality-agnostic and can be applied to many types of data.</p>\n", "tags": [] },
{"key": "nguyen2020interpretable", "year": "2020", "title":"Interpretable Time Series Classification using Linear Models and Multi-resolution Multi-domain Symbolic Representations", "abstract": "<p>The time series classification literature has expanded rapidly over the last\ndecade, with many new classification approaches published each year. Prior\nresearch has mostly focused on improving the accuracy and efficiency of\nclassifiers, with interpretability being somewhat neglected. This aspect of\nclassifiers has become critical for many application domains and the\nintroduction of the EU GDPR legislation in 2018 is likely to further emphasize\nthe importance of interpretable learning algorithms. Currently,\nstate-of-the-art classification accuracy is achieved with very complex models\nbased on large ensembles (COTE) or deep neural networks (FCN). These approaches\nare not efficient with regard to either time or space, are difficult to\ninterpret and cannot be applied to variable-length time series, requiring\npre-processing of the original series to a set fixed-length. In this paper we\npropose new time series classification algorithms to address these gaps. Our\napproach is based on symbolic representations of time series, efficient\nsequence mining algorithms and linear classification models. Our linear models\nare as accurate as deep learning models but are more efficient regarding\nrunning time and memory, can work with variable-length time series and can be\ninterpreted by highlighting the discriminative symbolic features on the\noriginal time series. We show that our multi-resolution multi-domain linear\nclassifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art\nCOTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a\nfraction of the time and memory required by either COTE or deep models. To\nfurther analyse the interpretability of our classifier, we present a case study\non a human motion dataset collected by the authors. We release all the results,\nsource code and data to encourage reproducibility.</p>\n", "tags": [] },
{"key": "nguyen2020towards", "year": "2020", "title":"Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees", "abstract": "<p>On the one hand, artificial neural networks (ANNs) are commonly labelled as\nblack-boxes, lacking interpretability; an issue that hinders human\nunderstanding of ANNs’ behaviors. A need exists to generate a meaningful\nsequential logic of the ANN for interpreting a production process of a specific\noutput. On the other hand, decision trees exhibit better interpretability and\nexpressive power due to their representation language and the existence of\nefficient algorithms to transform the trees into rules. However, growing a\ndecision tree based on the available data could produce larger than necessary\ntrees or trees that do not generalise well. In this paper, we introduce two\nnovel multivariate decision tree (MDT) algorithms for rule extraction from\nANNs: an Exact-Convertible Decision Tree (EC-DT) and an Extended C-Net\nalgorithm. They both transform a neural network with Rectified Linear Unit\nactivation functions into a representative tree, which can further be used to\nextract multivariate rules for reasoning. While the EC-DT translates an ANN in\na layer-wise manner to represent exactly the decision boundaries implicitly\nlearned by the hidden layers of the network, the Extended C-Net combines the\ndecompositional approach from EC-DT with a C5 tree learning algorithm to form\ndecision rules. The results suggest that while EC-DT is superior in preserving\nthe structure and the fidelity of ANN, Extended C-Net generates the most\ncompact and highly effective trees from ANN. Both proposed MDT algorithms\ngenerate rules including combinations of multiple attributes for precise\ninterpretations for decision-making.</p>\n", "tags": [] },
{"key": "nguyen2021effectiveness", "year": "2021", "title":"The effectiveness of feature attribution methods and its correlation with automatic evaluation scores", "abstract": "<p>Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et\nal. 2018). In this paper, we conduct the first user study to measure\nattribution map effectiveness in assisting humans in ImageNet classification\nand Stanford Dogs fine-grained classification, and when an image is natural or\nadversarial (i.e., contains adversarial perturbations). Overall, feature\nattribution is surprisingly not more effective than showing humans nearest\ntraining-set examples. On a harder task of fine-grained dog categorization,\npresenting attribution maps to humans does not help, but instead hurts the\nperformance of human-AI teams compared to AI alone. Importantly, we found\nautomatic attribution-map evaluation measures to correlate poorly with the\nactual human-AI team performance. Our findings encourage the community to\nrigorously test their methods on the downstream human-in-the-loop applications\nand to rethink the existing evaluation metrics.</p>\n", "tags": [] },
{"key": "nguyen2022visual", "year": "2022", "title":"Visual correspondence-based explanations improve AI robustness and human-AI team accuracy", "abstract": "<p>Explaining artificial intelligence (AI) predictions is increasingly important\nand even imperative in many high-stakes applications where humans are the\nultimate decision-makers. In this work, we propose two novel architectures of\nself-interpretable image classifiers that first explain, and then predict (as\nopposed to post-hoc explanations) by harnessing the visual correspondences\nbetween a query image and exemplars. Our models consistently improve (by 1 to 4\npoints) on out-of-distribution (OOD) datasets while performing marginally worse\n(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest\nneighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,\nour correspondence-based explanations are found to be more useful to users than\nkNN explanations. Our explanations help users more accurately reject AI’s wrong\ndecisions than all other tested methods. Interestingly, for the first time, we\nshow that it is possible to achieve complementary human-AI team accuracy (i.e.,\nthat is higher than either AI-alone or human-alone), in ImageNet and CUB image\nclassification tasks.</p>\n", "tags": [] },
{"key": "niu2021explainable", "year": "2021", "title":"Explainable Student Performance Prediction With Personalized Attention for Explaining Why A Student Fails", "abstract": "<p>As student failure rates continue to increase in higher education, predicting\nstudent performance in the following semester has become a significant demand.\nPersonalized student performance prediction helps educators gain a\ncomprehensive view of student status and effectively intervene in advance.\nHowever, existing works scarcely consider the explainability of student\nperformance prediction, which educators are most concerned about. In this\npaper, we propose a novel Explainable Student performance prediction method\nwith Personalized Attention (ESPA) by utilizing relationships in student\nprofiles and prior knowledge of related courses. The designed Bidirectional\nLong Short-Term Memory (BiLSTM) architecture extracts the semantic information\nin the paths with specific patterns. As for leveraging similar paths’ internal\nrelations, a local and global-level attention mechanism is proposed to\ndistinguish the influence of different students or courses for making\npredictions. Hence, valid reasoning on paths can be applied to predict the\nperformance of students. The ESPA consistently outperforms the other\nstate-of-the-art models for student performance prediction, and the results are\nintuitively explainable. This work can help educators better understand the\ndifferent impacts of behavior on students’ studies.</p>\n", "tags": [] },
{"key": "norelli2022explanatory", "year": "2022", "title":"Explanatory Learning: Beyond Empiricism in Neural Networks", "abstract": "<p>We introduce Explanatory Learning (EL), a framework to let machines use\nexisting knowledge buried in symbolic sequences – e.g. explanations written in\nhieroglyphic – by autonomously learning to interpret them. In EL, the burden\nof interpreting symbols is not left to humans or rigid human-coded compilers,\nas done in Program Synthesis. Rather, EL calls for a learned interpreter, built\nupon a limited collection of symbolic sequences paired with observations of\nseveral phenomena. This interpreter can be used to make predictions on a novel\nphenomenon given its explanation, and even to find that explanation using only\na handful of observations, like human scientists do. We formulate the EL\nproblem as a simple binary classification task, so that common end-to-end\napproaches aligned with the dominant empiricist view of machine learning could,\nin principle, solve it. To these models, we oppose Critical Rationalist\nNetworks (CRNs), which instead embrace a rationalist view on the acquisition of\nknowledge. CRNs express several desired properties by construction, they are\ntruly explainable, can adjust their processing at test-time for harder\ninferences, and can offer strong confidence guarantees on their predictions. As\na final contribution, we introduce Odeen, a basic EL environment that simulates\na small flatland-style universe full of phenomena to explain. Using Odeen as a\ntestbed, we show how CRNs outperform empiricist end-to-end approaches of\nsimilar size and architecture (Transformers) in discovering explanations for\nnovel phenomena.</p>\n", "tags": [] },
{"key": "nushi2018towards", "year": "2018", "title":"Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure", "abstract": "<p>As machine learning systems move from computer-science laboratories into the\nopen world, their accountability becomes a high priority problem.\nAccountability requires deep understanding of system behavior and its failures.\nCurrent evaluation methods such as single-score error metrics and confusion\nmatrices provide aggregate views of system performance that hide important\nshortcomings. Understanding details about failures is important for identifying\npathways for refinement, communicating the reliability of systems in different\nsettings, and for specifying appropriate human oversight and engagement.\nCharacterization of failures and shortcomings is particularly complex for\nsystems composed of multiple machine learned components. For such systems,\nexisting evaluation methods have limited expressiveness in describing and\nexplaining the relationship among input content, the internal states of system\ncomponents, and final output quality. We present Pandora, a set of hybrid\nhuman-machine methods and tools for describing and explaining system failures.\nPandora leverages both human and system-generated observations to summarize\nconditions of system malfunction with respect to the input content and system\narchitecture. We share results of a case study with a machine learning pipeline\nfor image captioning that show how detailed performance views can be beneficial\nfor analysis and debugging.</p>\n", "tags": [] },
{"key": "pan2016shallow", "year": "2016", "title":"Shallow and Deep Convolutional Networks for Saliency Prediction", "abstract": "<p>The prediction of salient areas in images has been traditionally addressed\nwith hand-crafted features based on neuroscience principles. This paper,\nhowever, addresses the problem with a completely data-driven approach by\ntraining a convolutional neural network (convnet). The learning process is\nformulated as a minimization of a loss function that measures the Euclidean\ndistance of the predicted saliency map with the provided ground truth. The\nrecent publication of large datasets of saliency prediction has provided enough\ndata to train end-to-end architectures that are both fast and accurate. Two\ndesigns are proposed: a shallow convnet trained from scratch, and a another\ndeeper solution whose first three layers are adapted from another network\ntrained for classification. To the authors knowledge, these are the first\nend-to-end CNNs trained and tested for the purpose of saliency prediction.</p>\n", "tags": [] },
{"key": "pan2021definitions", "year": "2021", "title":"The Definitions of Interpretability and Learning of Interpretable Models", "abstract": "<p>As machine learning algorithms getting adopted in an ever-increasing number\nof applications, interpretation has emerged as a crucial desideratum. In this\npaper, we propose a mathematical definition for the human-interpretable model.\nIn particular, we define interpretability between two information process\nsystems. If a prediction model is interpretable by a human recognition system\nbased on the above interpretability definition, the prediction model is defined\nas a completely human-interpretable model. We further design a practical\nframework to train a completely human-interpretable model by user interactions.\nExperiments on image datasets show the advantages of our proposed model in two\naspects: 1) The completely human-interpretable model can provide an entire\ndecision-making process that is human-understandable; 2) The completely\nhuman-interpretable model is more robust against adversarial attacks.</p>\n", "tags": [] },
{"key": "pang2018learning", "year": "2018", "title":"Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection", "abstract": "<p>Learning expressive low-dimensional representations of ultrahigh-dimensional\ndata, e.g., data with thousands/millions of features, has been a major way to\nenable learning methods to address the curse of dimensionality. However,\nexisting unsupervised representation learning methods mainly focus on\npreserving the data regularity information and learning the representations\nindependently of subsequent outlier detection methods, which can result in\nsuboptimal and unstable performance of detecting irregularities (i.e.,\noutliers).\n  This paper introduces a ranking model-based framework, called RAMODO, to\naddress this issue. RAMODO unifies representation learning and outlier\ndetection to learn low-dimensional representations that are tailored for a\nstate-of-the-art outlier detection approach - the random distance-based\napproach. This customized learning yields more optimal and stable\nrepresentations for the targeted outlier detectors. Additionally, RAMODO can\nleverage little labeled data as prior knowledge to learn more expressive and\napplication-relevant representations. We instantiate RAMODO to an efficient\nmethod called REPEN to demonstrate the performance of RAMODO.\n  Extensive empirical results on eight real-world ultrahigh dimensional data\nsets show that REPEN (i) enables a random distance-based detector to obtain\nsignificantly better AUC performance and two orders of magnitude speedup; (ii)\nperforms substantially better and more stably than four state-of-the-art\nrepresentation learning methods; and (iii) leverages less than 1% labeled data\nto achieve up to 32% AUC improvement.</p>\n", "tags": [] },
{"key": "papernot2018deep", "year": "2018", "title":"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning", "abstract": "<p>Deep neural networks (DNNs) enable innovative applications of machine\nlearning like image recognition, machine translation, or malware detection.\nHowever, deep learning is often criticized for its lack of robustness in\nadversarial settings (e.g., vulnerability to adversarial inputs) and general\ninability to rationalize its predictions. In this work, we exploit the\nstructure of deep learning to enable new learning-based inference and decision\nstrategies that achieve desirable properties such as robustness and\ninterpretability. We take a first step in this direction and introduce the Deep\nk-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest\nneighbors algorithm with representations of the data learned by each layer of\nthe DNN: a test input is compared to its neighboring training points according\nto the distance that separates them in the representations. We show the labels\nof these neighboring points afford confidence estimates for inputs outside the\nmodel’s training manifold, including on malicious inputs like adversarial\nexamples–and therein provides protections against inputs that are outside the\nmodels understanding. This is because the nearest neighbors can be used to\nestimate the nonconformity of, i.e., the lack of support for, a prediction in\nthe training data. The neighbors also constitute human-interpretable\nexplanations of predictions. We evaluate the DkNN algorithm on several\ndatasets, and show the confidence estimates accurately identify inputs outside\nthe model, and that the explanations provided by nearest neighbors are\nintuitive and useful in understanding model failures.</p>\n", "tags": [] },
{"key": "park2016attentive", "year": "2016", "title":"Attentive Explanations: Justifying Decisions and Pointing to the Evidence", "abstract": "<p>Deep models are the defacto standard in visual decision models due to their\nimpressive performance on a wide array of visual tasks. However, they are\nfrequently seen as opaque and are unable to explain their decisions. In\ncontrast, humans can justify their decisions with natural language and point to\nthe evidence in the visual world which led to their decisions. We postulate\nthat deep models can do this as well and propose our Pointing and Justification\n(PJ-X) model which can justify its decision with a sentence and point to the\nevidence by introspecting its decision and explanation process using an\nattention mechanism. Unfortunately there is no dataset available with reference\nexplanations for visual decision making. We thus collect two datasets in two\ndomains where it is interesting and challenging to explain decisions. First, we\nextend the visual question answering task to not only provide an answer but\nalso a natural language explanation for the answer. Second, we focus on\nexplaining human activities which is traditionally more challenging than object\nclassification. We extensively evaluate our PJ-X model, both on the\njustification and pointing tasks, by comparing it to prior models and ablations\nusing both automatic and human evaluations.</p>\n", "tags": [] },
{"key": "park2022vision", "year": "2022", "title":"How Do Vision Transformers Work?", "abstract": "<p>The success of multi-head self-attentions (MSAs) for computer vision is now\nindisputable. However, little is known about how MSAs work. We present\nfundamental explanations to help better understand the nature of MSAs. In\nparticular, we demonstrate the following properties of MSAs and Vision\nTransformers (ViTs): (1) MSAs improve not only accuracy but also generalization\nby flattening the loss landscapes. Such improvement is primarily attributable\nto their data specificity, not long-range dependency. On the other hand, ViTs\nsuffer from non-convex losses. Large datasets and loss landscape smoothing\nmethods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.\nFor example, MSAs are low-pass filters, but Convs are high-pass filters.\nTherefore, MSAs and Convs are complementary; (3) Multi-stage neural networks\nbehave like a series connection of small individual models. In addition, MSAs\nat the end of a stage play a key role in prediction. Based on these insights,\nwe propose AlterNet, a model in which Conv blocks at the end of a stage are\nreplaced with MSA blocks. AlterNet outperforms CNNs not only in large data\nregimes but also in small data regimes. The code is available at\nhttps://github.com/xxxnell/how-do-vits-work.</p>\n", "tags": [] },
{"key": "patro2019explanation", "year": "2019", "title":"Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA", "abstract": "<p>In this paper, we aim to obtain improved attention for a visual question\nanswering (VQA) task. It is challenging to provide supervision for attention.\nAn observation we make is that visual explanations as obtained through class\nactivation mappings (specifically Grad-CAM) that are meant to explain the\nperformance of various networks could form a means of supervision. However, as\nthe distributions of attention maps and that of Grad-CAMs differ, it would not\nbe suitable to directly use these as a form of supervision. Rather, we propose\nthe use of a discriminator that aims to distinguish samples of visual\nexplanation and attention maps. The use of adversarial training of the\nattention regions as a two-player game between attention and explanation serves\nto bring the distributions of attention maps and visual explanations closer.\nSignificantly, we observe that providing such a means of supervision also\nresults in attention maps that are more closely related to human attention\nresulting in a substantial improvement over baseline stacked attention network\n(SAN) models. It also results in a good improvement in rank correlation metric\non the VQA task. This method can also be combined with recent MCB based methods\nand results in consistent improvement. We also provide comparisons with other\nmeans for learning distributions such as based on Correlation Alignment\n(Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and\nobserve that the adversarial loss outperforms the other forms of learning the\nattention maps. Visualization of the results also confirms our hypothesis that\nattention maps improve using this form of supervision.</p>\n", "tags": [] },
{"key": "peri2019show,", "year": "2019", "title":"Show, Translate and Tell", "abstract": "<p>Humans have an incredible ability to process and understand information from\nmultiple sources such as images, video, text, and speech. Recent success of\ndeep neural networks has enabled us to develop algorithms which give machines\nthe ability to understand and interpret this information. There is a need to\nboth broaden their applicability and develop methods which correlate visual\ninformation along with semantic content. We propose a unified model which\njointly trains on images and captions, and learns to generate new captions\ngiven either an image or a caption query. We evaluate our model on three\ndifferent tasks namely cross-modal retrieval, image captioning, and sentence\nparaphrasing. Our model gains insight into cross-modal vector embeddings,\ngeneralizes well on multiple tasks and is competitive to state of the art\nmethods on retrieval.</p>\n", "tags": [] },
{"key": "pfitzinger2021interpretable", "year": "2021", "title":"An Interpretable Neural Network for Parameter Inference", "abstract": "<p>Adoption of deep neural networks in fields such as economics or finance has\nbeen constrained by the lack of interpretability of model outcomes. This paper\nproposes a generative neural network architecture - the parameter encoder\nneural network (PENN) - capable of estimating local posterior distributions for\nthe parameters of a regression model. The parameters fully explain predictions\nin terms of the inputs and permit visualization, interpretation and inference\nin the presence of complex heterogeneous effects and feature dependencies. The\nuse of Bayesian inference techniques offers an intuitive mechanism to\nregularize local parameter estimates towards a stable solution, and to reduce\nnoise-fitting in settings of limited data availability. The proposed neural\nnetwork is particularly well-suited to applications in economics and finance,\nwhere parameter inference plays an important role. An application to an asset\npricing problem demonstrates how the PENN can be used to explore nonlinear risk\ndynamics in financial markets, and to compare empirical nonlinear effects to\nbehavior posited by financial theory.</p>\n", "tags": [] },
{"key": "phillips2017interpretable", "year": "2017", "title":"Interpretable Active Learning", "abstract": "<p>Active learning has long been a topic of study in machine learning. However,\nas increasingly complex and opaque models have become standard practice, the\nprocess of active learning, too, has become more opaque. There has been little\ninvestigation into interpreting what specific trends and patterns an active\nlearning strategy may be exploring. This work expands on the Local\nInterpretable Model-agnostic Explanations framework (LIME) to provide\nexplanations for active learning recommendations. We demonstrate how LIME can\nbe used to generate locally faithful explanations for an active learning\nstrategy, and how these explanations can be used to understand how different\nmodels and datasets explore a problem space over time. In order to quantify the\nper-subgroup differences in how an active learning strategy queries spatial\nregions, we introduce a notion of uncertainty bias (based on disparate impact)\nto measure the discrepancy in the confidence for a model’s predictions between\none subgroup and another. Using the uncertainty bias measure, we show that our\nquery explanations accurately reflect the subgroup focus of the active learning\nqueries, allowing for an interpretable explanation of what is being learned as\npoints with similar sources of uncertainty have their uncertainty bias\nresolved. We demonstrate that this technique can be applied to track\nuncertainty bias over user-defined clusters or automatically generated clusters\nbased on the source of uncertainty.</p>\n", "tags": [] },
{"key": "phuong2021towards", "year": "2021", "title":"Towards Understanding Knowledge Distillation", "abstract": "<p>Knowledge distillation, i.e., one classifier being trained on the outputs of\nanother classifier, is an empirically very successful technique for knowledge\ntransfer between classifiers. It has even been observed that classifiers learn\nmuch faster and more reliably if trained with the outputs of another classifier\nas soft labels, instead of from ground truth data. So far, however, there is no\nsatisfactory theoretical explanation of this phenomenon. In this work, we\nprovide the first insights into the working mechanisms of distillation by\nstudying the special case of linear and deep linear classifiers. Specifically,\nwe prove a generalization bound that establishes fast convergence of the\nexpected risk of a distillation-trained linear classifier. From the bound and\nits proof we extract three key factors that determine the success of\ndistillation: * data geometry – geometric properties of the data distribution,\nin particular class separation, has a direct influence on the convergence speed\nof the risk; * optimization bias – gradient descent optimization finds a very\nfavorable minimum of the distillation objective; and * strong monotonicity –\nthe expected risk of the student classifier always decreases when the size of\nthe training set grows.</p>\n", "tags": [] },
{"key": "poerner2018evaluating", "year": "2018", "title":"Evaluating neural network explanation methods using hybrid documents and morphological agreement", "abstract": "<p>The behavior of deep neural networks (DNNs) is hard to understand. This makes\nit necessary to explore post hoc explanation methods. We conduct the first\ncomprehensive evaluation of explanation methods for NLP. To this end, we design\ntwo novel evaluation paradigms that cover two important classes of NLP\nproblems: small context and large context problems. Both paradigms require no\nmanual annotation and are therefore broadly applicable. We also introduce\nLIMSSE, an explanation method inspired by LIME that is designed for NLP. We\nshow empirically that LIMSSE, LRP and DeepLIFT are the most effective\nexplanation methods and recommend them for explaining DNNs in NLP.</p>\n", "tags": [] },
{"key": "poursabzisangdeh2021manipulating", "year": "2021", "title":"Manipulating and Measuring Model Interpretability", "abstract": "<p>With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.</p>\n", "tags": ["measuring"] },
{"key": "pruthi2019learning", "year": "2019", "title":"Learning to Deceive with Attention-Based Explanations", "abstract": "<p>Attention mechanisms are ubiquitous components in neural architectures\napplied to natural language processing. In addition to yielding gains in\npredictive accuracy, attention weights are often claimed to confer\ninterpretability, purportedly useful both for providing insights to\npractitioners and for explaining why a model makes its decisions to\nstakeholders. We call the latter use of attention mechanisms into question by\ndemonstrating a simple method for training models to produce deceptive\nattention masks. Our method diminishes the total weight assigned to designated\nimpermissible tokens, even when the models can be shown to nevertheless rely on\nthese features to drive predictions. Across multiple models and tasks, our\napproach manipulates attention weights while paying surprisingly little cost in\naccuracy. Through a human study, we show that our manipulated attention-based\nexplanations deceive people into thinking that predictions from a model biased\nagainst gender minorities do not rely on the gender. Consequently, our results\ncast doubt on attention’s reliability as a tool for auditing algorithms in the\ncontext of fairness and accountability.</p>\n", "tags": [] },
{"key": "pruthi2020evaluating", "year": "2020", "title":"Evaluating Explanations: How much do explanations from the teacher aid students?", "abstract": "<p>While many methods purport to explain predictions by highlighting salient\nfeatures, what aims these explanations serve and how they ought to be evaluated\noften go unstated. In this work, we introduce a framework to quantify the value\nof explanations via the accuracy gains that they confer on a student model\ntrained to simulate a teacher model. Crucially, the explanations are available\nto the student during training, but are not available at test time. Compared to\nprior proposals, our approach is less easily gamed, enabling principled,\nautomatic, model-agnostic evaluation of attributions. Using our framework, we\ncompare numerous attribution methods for text classification and question\nanswering, and observe quantitative differences that are consistent (to a\nmoderate to high degree) across different student model architectures and\nlearning strategies.</p>\n", "tags": [] },
{"key": "puiutta2020explainable", "year": "2020", "title":"Explainable Reinforcement Learning: A Survey", "abstract": "<p>Explainable Artificial Intelligence (XAI), i.e., the development of more\ntransparent and interpretable AI models, has gained increased traction over the\nlast few years. This is due to the fact that, in conjunction with their growth\ninto powerful and ubiquitous tools, AI models exhibit one detrimential\ncharacteristic: a performance-transparency trade-off. This describes the fact\nthat the more complex a model’s inner workings, the less clear it is how its\npredictions or decisions were achieved. But, especially considering Machine\nLearning (ML) methods like Reinforcement Learning (RL) where the system learns\nautonomously, the necessity to understand the underlying reasoning for their\ndecisions becomes apparent. Since, to the best of our knowledge, there exists\nno single work offering an overview of Explainable Reinforcement Learning (XRL)\nmethods, this survey attempts to address this gap. We give a short summary of\nthe problem, a definition of important terms, and offer a classification and\nassessment of current XRL methods. We found that a) the majority of XRL methods\nfunction by mimicking and simplifying a complex model instead of designing an\ninherently simple one, and b) XRL (and XAI) methods often neglect to consider\nthe human side of the equation, not taking into account research from related\nfields like psychology or philosophy. Thus, an interdisciplinary effort is\nneeded to adapt the generated explanations to a (non-expert) human user in\norder to effectively progress in the field of XRL and XAI in general.</p>\n", "tags": [] },
{"key": "rajani2020explaining", "year": "2020", "title":"Explaining and Improving Model Behavior with k Nearest Neighbor Representations", "abstract": "<p>Interpretability techniques in NLP have mainly focused on understanding\nindividual predictions using attention visualization or gradient-based saliency\nmaps over tokens. We propose using k nearest neighbor (kNN) representations to\nidentify training examples responsible for a model’s predictions and obtain a\ncorpus-level understanding of the model’s behavior. Apart from\ninterpretability, we show that kNN representations are effective at uncovering\nlearned spurious associations, identifying mislabeled examples, and improving\nthe fine-tuned model’s performance. We focus on Natural Language Inference\n(NLI) as a case study and experiment with multiple datasets. Our method deploys\nbackoff to kNN for BERT and RoBERTa on examples with low model confidence\nwithout any update to the model parameters. Our results indicate that the kNN\napproach makes the finetuned model more robust to adversarial inputs.</p>\n", "tags": [] },
{"key": "ralekar2021understanding", "year": "2021", "title":"Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks", "abstract": "<p>Human observers engage in selective information uptake when classifying\nvisual patterns. The same is true of deep neural networks, which currently\nconstitute the best performing artificial vision systems. Our goal is to\nexamine the congruence, or lack thereof, in the information-gathering\nstrategies of the two systems. We have operationalized our investigation as a\ncharacter recognition task. We have used eye-tracking to assay the spatial\ndistribution of information hotspots for humans via fixation maps and an\nactivation mapping technique for obtaining analogous distributions for deep\nnetworks through visualization maps. Qualitative comparison between\nvisualization maps and fixation maps reveals an interesting correlate of\ncongruence. The deep learning model considered similar regions in character,\nwhich humans have fixated in the case of correctly classified characters. On\nthe other hand, when the focused regions are different for humans and deep\nnets, the characters are typically misclassified by the latter. Hence, we\npropose to use the visual fixation maps obtained from the eye-tracking\nexperiment as a supervisory input to align the model’s focus on relevant\ncharacter regions. We find that such supervision improves the model’s\nperformance significantly and does not require any additional parameters. This\napproach has the potential to find applications in diverse domains such as\nmedical analysis and surveillance in which explainability helps to determine\nsystem fidelity.</p>\n", "tags": [] },
{"key": "ramamurthy2022analogies", "year": "2022", "title":"Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners", "abstract": "<p>Post-hoc explanations for black box models have been studied extensively in\nclassification and regression settings. However, explanations for models that\noutput similarity between two inputs have received comparatively lesser\nattention. In this paper, we provide model agnostic local explanations for\nsimilarity learners applicable to tabular and text data. We first propose a\nmethod that provides feature attributions to explain the similarity between a\npair of inputs as determined by a black box similarity learner. We then propose\nanalogies as a new form of explanation in machine learning. Here the goal is to\nidentify diverse analogous pairs of examples that share the same level of\nsimilarity as the input pair and provide insight into (latent) factors\nunderlying the model’s prediction. The selection of analogies can optionally\nleverage feature attributions, thus connecting the two forms of explanation\nwhile still maintaining complementarity. We prove that our analogy objective\nfunction is submodular, making the search for good-quality analogies efficient.\nWe apply the proposed approaches to explain similarities between sentences as\npredicted by a state-of-the-art sentence encoder, and between patients in a\nhealthcare utilization application. Efficacy is measured through quantitative\nevaluations, a careful user study, and examples of explanations.</p>\n", "tags": [] },
{"key": "rao2021first", "year": "2021", "title":"A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations", "abstract": "<p>Explainable deep learning models are advantageous in many situations. Prior\nwork mostly provide unimodal explanations through post-hoc approaches not part\nof the original system design. Explanation mechanisms also ignore useful\ntextual information present in images. In this paper, we propose MTXNet, an\nend-to-end trainable multimodal architecture to generate multimodal\nexplanations, which focuses on the text in the image. We curate a novel dataset\nTextVQA-X, containing ground truth visual and multi-reference textual\nexplanations that can be leveraged during both training and evaluation. We then\nquantitatively show that training with multimodal explanations complements\nmodel performance and surpasses unimodal baselines by up to 7% in CIDEr scores\nand 2% in IoU. More importantly, we demonstrate that the multimodal\nexplanations are consistent with human interpretations, help justify the\nmodels’ decision, and provide useful insights to help diagnose an incorrect\nprediction. Finally, we describe a real-world e-commerce application for using\nthe generated multimodal explanations.</p>\n", "tags": [] },
{"key": "rao2022towards", "year": "2022", "title":"Towards Better Understanding Attribution Methods", "abstract": "<p>Deep neural networks are very successful on many vision tasks, but hard to\ninterpret due to their black box nature. To overcome this, various post-hoc\nattribution methods have been proposed to identify image regions most\ninfluential to the models’ decisions. Evaluating such methods is challenging\nsince no ground truth attributions exist. We thus propose three novel\nevaluation schemes to more reliably measure the faithfulness of those methods,\nto make comparisons between them more fair, and to make visual inspection more\nsystematic. To address faithfulness, we propose a novel evaluation setting\n(DiFull) in which we carefully control which parts of the input can influence\nthe output in order to distinguish possible from impossible attributions. To\naddress fairness, we note that different methods are applied at different\nlayers, which skews any comparison, and so evaluate all methods on the same\nlayers (ML-Att) and discuss how this impacts their performance on quantitative\nmetrics. For more systematic visualizations, we propose a scheme (AggAtt) to\nqualitatively evaluate the methods on complete datasets. We use these\nevaluation schemes to study strengths and shortcomings of some widely used\nattribution methods. Finally, we propose a post-processing smoothing step that\nsignificantly improves the performance of some attribution methods, and discuss\nits applicability.</p>\n", "tags": [] },
{"key": "ras2020explainable", "year": "2020", "title":"Explainable Deep Learning: A Field Guide for the Uninitiated", "abstract": "<p>Deep neural networks (DNNs) have become a proven and indispensable machine\nlearning tool. As a black-box model, it remains difficult to diagnose what\naspects of the model’s input drive the decisions of a DNN. In countless\nreal-world domains, from legislation and law enforcement to healthcare, such\ndiagnosis is essential to ensure that DNN decisions are driven by aspects\nappropriate in the context of its use. The development of methods and studies\nenabling the explanation of a DNN’s decisions has thus blossomed into an\nactive, broad area of research. A practitioner wanting to study explainable\ndeep learning may be intimidated by the plethora of orthogonal directions the\nfield has taken. This complexity is further exacerbated by competing\ndefinitions of what it means <code class=\"language-plaintext highlighter-rouge\">to explain'' the actions of a DNN and to\nevaluate an approach's</code>ability to explain’’. This article offers a field\nguide to explore the space of explainable deep learning aimed at those\nuninitiated in the field. The field guide: i) Introduces three simple\ndimensions defining the space of foundational methods that contribute to\nexplainable deep learning, ii) discusses the evaluations for model\nexplanations, iii) places explainability in the context of other related deep\nlearning research areas, and iv) finally elaborates on user-oriented\nexplanation designing and potential future directions on explainable deep\nlearning. We hope the guide is used as an easy-to-digest starting point for\nthose just embarking on research in this field.</p>\n", "tags": [] },
{"key": "ravanelli2018interpretable", "year": "2018", "title":"Interpretable Convolutional Filters with SincNet", "abstract": "<p>Deep learning is currently playing a crucial role toward higher levels of\nartificial intelligence. This paradigm allows neural networks to learn complex\nand abstract representations, that are progressively obtained by combining\nsimpler ones. Nevertheless, the internal “black-box” representations\nautomatically discovered by current neural architectures often suffer from a\nlack of interpretability, making of primary interest the study of explainable\nmachine learning techniques. This paper summarizes our recent efforts to\ndevelop a more interpretable neural model for directly processing speech from\nthe raw waveform. In particular, we propose SincNet, a novel Convolutional\nNeural Network (CNN) that encourages the first layer to discover more\nmeaningful filters by exploiting parametrized sinc functions. In contrast to\nstandard CNNs, which learn all the elements of each filter, only low and high\ncutoff frequencies of band-pass filters are directly learned from data. This\ninductive bias offers a very compact way to derive a customized filter-bank\nfront-end, that only depends on some parameters with a clear physical meaning.\nOur experiments, conducted on both speaker and speech recognition, show that\nthe proposed architecture converges faster, performs better, and is more\ninterpretable than standard CNNs.</p>\n", "tags": [] },
{"key": "ravfogel2021counterfactual", "year": "2021", "title":"Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction", "abstract": "<p>When language models process syntactically complex sentences, do they use\ntheir representations of syntax in a manner that is consistent with the grammar\nof the language? We propose AlterRep, an intervention-based method to address\nthis question. For any linguistic feature of a given sentence, AlterRep\ngenerates counterfactual representations by altering how the feature is\nencoded, while leaving intact all other aspects of the original representation.\nBy measuring the change in a model’s word prediction behavior when these\ncounterfactual representations are substituted for the original ones, we can\ndraw conclusions about the causal effect of the linguistic feature in question\non the model’s behavior. We apply this method to study how BERT models of\ndifferent sizes process relative clauses (RCs). We find that BERT variants use\nRC boundary information during word prediction in a manner that is consistent\nwith the rules of English grammar; this RC boundary information generalizes to\na considerable extent across different RC types, suggesting that BERT\nrepresents RCs as an abstract linguistic category.</p>\n", "tags": [] },
{"key": "ray2019explain", "year": "2019", "title":"Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval", "abstract": "<p>While there have been many proposals on making AI algorithms explainable, few\nhave attempted to evaluate the impact of AI-generated explanations on human\nperformance in conducting human-AI collaborative tasks. To bridge the gap, we\npropose a Twenty-Questions style collaborative image retrieval game,\nExplanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy\nof explanations (visual evidence or textual justification) in the context of\nVisual Question Answering (VQA). In our proposed ExAG, a human user needs to\nguess a secret image picked by the VQA agent by asking natural language\nquestions to it. We show that overall, when AI explains its answers, users\nsucceed more often in guessing the secret image correctly. Notably, a few\ncorrect explanations can readily improve human performance when VQA answers are\nmostly incorrect as compared to no-explanation games. Furthermore, we also show\nthat while explanations rated as “helpful” significantly improve human\nperformance, “incorrect” and “unhelpful” explanations can degrade performance\nas compared to no-explanation games. Our experiments, therefore, demonstrate\nthat ExAG is an effective means to evaluate the efficacy of AI-generated\nexplanations on a human-AI collaborative task.</p>\n", "tags": [] },
{"key": "repetto2021multicriteria", "year": "2021", "title":"Multicriteria interpretability driven Deep Learning", "abstract": "<p>Deep Learning methods are renowned for their performances, yet their lack of\ninterpretability prevents them from high-stakes contexts. Recent model agnostic\nmethods address this problem by providing post-hoc interpretability methods by\nreverse-engineering the model’s inner workings. However, in many regulated\nfields, interpretability should be kept in mind from the start, which means\nthat post-hoc methods are valid only as a sanity check after model training.\nInterpretability from the start, in an abstract setting, means posing a set of\nsoft constraints on the model’s behavior by injecting knowledge and\nannihilating possible biases. We propose a Multicriteria technique that allows\nto control the feature effects on the model’s outcome by injecting knowledge in\nthe objective function. We then extend the technique by including a non-linear\nknowledge function to account for more complex effects and local lack of\nknowledge. The result is a Deep Learning model that embodies interpretability\nfrom the start and aligns with the recent regulations. A practical empirical\nexample based on credit risk, suggests that our approach creates performant yet\nrobust models capable of overcoming biases derived from data scarcity.</p>\n", "tags": [] },
{"key": "ribeiro2016why", "year": "2016", "title":"\"Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "abstract": "<p>Despite widespread adoption, machine learning models remain mostly black\nboxes. Understanding the reasons behind predictions is, however, quite\nimportant in assessing trust, which is fundamental if one plans to take action\nbased on a prediction, or when choosing whether to deploy a new model. Such\nunderstanding also provides insights into the model, which can be used to\ntransform an untrustworthy model or prediction into a trustworthy one. In this\nwork, we propose LIME, a novel explanation technique that explains the\npredictions of any classifier in an interpretable and faithful manner, by\nlearning an interpretable model locally around the prediction. We also propose\na method to explain models by presenting representative individual predictions\nand their explanations in a non-redundant way, framing the task as a submodular\noptimization problem. We demonstrate the flexibility of these methods by\nexplaining different models for text (e.g. random forests) and image\nclassification (e.g. neural networks). We show the utility of explanations via\nnovel experiments, both simulated and with human subjects, on various scenarios\nthat require trust: deciding if one should trust a prediction, choosing between\nmodels, improving an untrustworthy classifier, and identifying why a classifier\nshould not be trusted.</p>\n", "tags": [] },
{"key": "rieger2019aggregating", "year": "2019", "title":"Aggregating explanation methods for stable and robust explainability", "abstract": "<p>Despite a growing literature on explaining neural networks, no consensus has\nbeen reached on how to explain a neural network decision or how to evaluate an\nexplanation. Our contributions in this paper are twofold. First, we investigate\nschemes to combine explanation methods and reduce model uncertainty to obtain a\nsingle aggregated explanation. We provide evidence that the aggregation is\nbetter at identifying important features, than on individual methods.\nAdversarial attacks on explanations is a recent active research topic. As our\nsecond contribution, we present evidence that aggregate explanations are much\nmore robust to attacks than individual explanation methods.</p>\n", "tags": [] },
{"key": "rieger2019interpretations", "year": "2019", "title":"Interpretations are useful: penalizing explanations to align neural networks with prior knowledge", "abstract": "<p>For an explanation of a deep learning model to be effective, it must provide\nboth insight into a model and suggest a corresponding action in order to\nachieve some objective. Too often, the litany of proposed explainable deep\nlearning methods stop at the first step, providing practitioners with insight\ninto a model, but no way to act on it. In this paper, we propose contextual\ndecomposition explanation penalization (CDEP), a method which enables\npractitioners to leverage existing explanation methods in order to increase the\npredictive accuracy of deep learning models. In particular, when shown that a\nmodel has incorrectly assigned importance to some features, CDEP enables\npractitioners to correct these errors by directly regularizing the provided\nexplanations. Using explanations provided by contextual decomposition (CD)\n(Murdoch et al., 2018), we demonstrate the ability of our method to increase\nperformance on an array of toy and real datasets.</p>\n", "tags": [] },
{"key": "roscher2019explainable", "year": "2019", "title":"Explainable Machine Learning for Scientific Insights and Discoveries", "abstract": "<p>Machine learning methods have been remarkably successful for a wide range of\napplication areas in the extraction of essential information from data. An\nexciting and relatively recent development is the uptake of machine learning in\nthe natural sciences, where the major goal is to obtain novel scientific\ninsights and discoveries from observational or simulated data. A prerequisite\nfor obtaining a scientific outcome is domain knowledge, which is needed to gain\nexplainability, but also to enhance scientific consistency. In this article we\nreview explainable machine learning in view of applications in the natural\nsciences and discuss three core elements which we identified as relevant in\nthis context: transparency, interpretability, and explainability. With respect\nto these core elements, we provide a survey of recent scientific works that\nincorporate machine learning and the way that explainable machine learning is\nused in combination with domain knowledge from the application areas.</p>\n", "tags": [] },
{"key": "ross2017improving", "year": "2017", "title":"Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients", "abstract": "<p>Deep neural networks have proven remarkably effective at solving many\nclassification problems, but have been criticized recently for two major\nweaknesses: the reasons behind their predictions are uninterpretable, and the\npredictions themselves can often be fooled by small adversarial perturbations.\nThese problems pose major obstacles for the adoption of neural networks in\ndomains that require security or transparency. In this work, we evaluate the\neffectiveness of defenses that differentiably penalize the degree to which\nsmall changes in inputs can alter model predictions. Across multiple attacks,\narchitectures, defenses, and datasets, we find that neural networks trained\nwith this input gradient regularization exhibit robustness to transferred\nadversarial examples generated to fool all of the other models. We also find\nthat adversarial examples generated to fool gradient-regularized models fool\nall other models equally well, and actually lead to more “legitimate,”\ninterpretable misclassifications as rated by people (which we confirm in a\nhuman subject experiment). Finally, we demonstrate that regularizing input\ngradients makes them more naturally interpretable as rationales for model\npredictions. We conclude by discussing this relationship between\ninterpretability and robustness in deep neural networks.</p>\n", "tags": [] },
{"key": "ross2017right", "year": "2017", "title":"Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations", "abstract": "<p>Neural networks are among the most accurate supervised learning methods in\nuse today, but their opacity makes them difficult to trust in critical\napplications, especially when conditions in training differ from those in test.\nRecent work on explanations for black-box models has produced tools (e.g. LIME)\nto show the implicit rules behind predictions, which can help us identify when\nmodels are right for the wrong reasons. However, these methods do not scale to\nexplaining entire datasets and cannot correct the problems they reveal. We\nintroduce a method for efficiently explaining and regularizing differentiable\nmodels by examining and selectively penalizing their input gradients, which\nprovide a normal to the decision boundary. We apply these penalties both based\non expert annotation and in an unsupervised fashion that encourages diverse\nmodels with qualitatively different decision boundaries for the same\nclassification problem. On multiple datasets, we show our approach generates\nfaithful explanations and models that generalize much better when conditions\ndiffer between training and test.</p>\n", "tags": [] },
{"key": "roxlo2018opening", "year": "2018", "title":"Opening the black box of neural nets: case studies in stop/top discrimination", "abstract": "<p>We introduce techniques for exploring the functionality of a neural network\nand extracting simple, human-readable approximations to its performance. By\nperforming gradient ascent on the input space of the network, we are able to\nproduce large populations of artificial events which strongly excite a given\nclassifier. By studying the populations of these events, we then directly\nproduce what are essentially contour maps of the network’s classification\nfunction. Combined with a suite of tools for identifying the input dimensions\ndeemed most important by the network, we can utilize these maps to efficiently\ninterpret the dominant criteria by which the network makes its classification.\n  As a test case, we study networks trained to discriminate supersymmetric stop\nproduction in the dilepton channel from Standard Model backgrounds. In the case\nof a heavy stop decaying to a light neutralino, we find individual neurons with\nlarge mutual information with $m_{T2}^{\\ell\\ell}$, a human-designed variable\nfor optimizing the analysis. The network selects events with significant\nmissing $p_T$ oriented azimuthally away from both leptons, efficiently\nrejecting $t\\overline{t}$ background. In the case of a light stop with\nthree-body decays to $Wb{\\widetilde \\chi}$ and little phase space, we find\nneurons that smoothly interpolate between a similar top-rejection strategy and\nan ISR-tagging strategy allowing for more missing momentum. We also find that a\nneural network trained on a stealth stop parameter point learns novel angular\ncorrelations.</p>\n", "tags": [] },
{"key": "rudin2018stop", "year": "2018", "title":"Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead", "abstract": "<p>Black box machine learning models are currently being used for high stakes\ndecision-making throughout society, causing problems throughout healthcare,\ncriminal justice, and in other domains. People have hoped that creating methods\nfor explaining these black box models will alleviate some of these problems,\nbut trying to \\textit{explain} black box models, rather than creating models\nthat are \\textit{interpretable} in the first place, is likely to perpetuate bad\npractices and can potentially cause catastrophic harm to society. There is a\nway forward – it is to design models that are inherently interpretable. This\nmanuscript clarifies the chasm between explaining black boxes and using\ninherently interpretable models, outlines several key reasons why explainable\nblack boxes should be avoided in high-stakes decisions, identifies challenges\nto interpretable machine learning, and provides several example applications\nwhere interpretable models could potentially replace black box models in\ncriminal justice, healthcare, and computer vision.</p>\n", "tags": [] },
{"key": "rudin2021interpretable", "year": "2021", "title":"Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges", "abstract": "<p>Interpretability in machine learning (ML) is crucial for high stakes\ndecisions and troubleshooting. In this work, we provide fundamental principles\nfor interpretable ML, and dispel common misunderstandings that dilute the\nimportance of this crucial topic. We also identify 10 technical challenge areas\nin interpretable machine learning and provide history and background on each\nproblem. Some of these problems are classically important, and some are recent\nproblems that have arisen in the last few years. These problems are: (1)\nOptimizing sparse logical models such as decision trees; (2) Optimization of\nscoring systems; (3) Placing constraints into generalized additive models to\nencourage sparsity and better interpretability; (4) Modern case-based\nreasoning, including neural networks and matching for causal inference; (5)\nComplete supervised disentanglement of neural networks; (6) Complete or even\npartial unsupervised disentanglement of neural networks; (7) Dimensionality\nreduction for data visualization; (8) Machine learning models that can\nincorporate physics and other generative or causal constraints; (9)\nCharacterization of the “Rashomon set” of good models; and (10) Interpretable\nreinforcement learning. This survey is suitable as a starting point for\nstatisticians and computer scientists interested in working in interpretable\nmachine learning.</p>\n", "tags": [] },
{"key": "räuker2022toward", "year": "2022", "title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks", "abstract": "<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities, and deep neural networks (DNNs) are increasingly being deployed\nacross a wide range of domains. However, the inner workings of DNNs are\ngenerally difficult to understand, raising concerns about the safety of using\nthese systems without a rigorous understanding of how they function. In this\nsurvey, we review literature on techniques for interpreting the inner\ncomponents of DNNs, which we call “inner” interpretability methods.\nSpecifically, we review methods for interpreting weights, neurons, subnetworks,\nand latent representations with a focus on how these techniques relate to the\ngoal of designing safer, more trustworthy AI systems. We also highlight\nconnections between interpretability and work in modularity, adversarial\nrobustness, continual learning, network compression, and studying the human\nvisual system. Finally, we discuss key challenges and argue for future work in\ninterpretability for AI safety that focuses on diagnostics, benchmarking, and\nrobustness.</p>\n", "tags": [] },
{"key": "sabour2017dynamic", "year": "2017", "title":"Dynamic Routing Between Capsules", "abstract": "<p>A capsule is a group of neurons whose activity vector represents the\ninstantiation parameters of a specific type of entity such as an object or an\nobject part. We use the length of the activity vector to represent the\nprobability that the entity exists and its orientation to represent the\ninstantiation parameters. Active capsules at one level make predictions, via\ntransformation matrices, for the instantiation parameters of higher-level\ncapsules. When multiple predictions agree, a higher level capsule becomes\nactive. We show that a discrimininatively trained, multi-layer capsule system\nachieves state-of-the-art performance on MNIST and is considerably better than\na convolutional net at recognizing highly overlapping digits. To achieve these\nresults we use an iterative routing-by-agreement mechanism: A lower-level\ncapsule prefers to send its output to higher level capsules whose activity\nvectors have a big scalar product with the prediction coming from the\nlower-level capsule.</p>\n", "tags": [] },
{"key": "samek2015evaluating", "year": "2015", "title":"Evaluating the visualization of what a Deep Neural Network has learned", "abstract": "<p>Deep Neural Networks (DNNs) have demonstrated impressive performance in\ncomplex machine learning tasks such as image classification or speech\nrecognition. However, due to their multi-layer nonlinear structure, they are\nnot transparent, i.e., it is hard to grasp what makes them arrive at a\nparticular classification or recognition decision given a new unseen data\nsample. Recently, several approaches have been proposed enabling one to\nunderstand and interpret the reasoning embodied in a DNN for a single test\nimage. These methods quantify the ‘‘importance’’ of individual pixels wrt the\nclassification decision and allow a visualization in terms of a heatmap in\npixel/input space. While the usefulness of heatmaps can be judged subjectively\nby a human, an objective quality measure is missing. In this paper we present a\ngeneral methodology based on region perturbation for evaluating ordered\ncollections of pixels such as heatmaps. We compare heatmaps computed by three\ndifferent methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main\nresult is that the recently proposed Layer-wise Relevance Propagation (LRP)\nalgorithm qualitatively and quantitatively provides a better explanation of\nwhat made a DNN arrive at a particular classification decision than the\nsensitivity-based approach or the deconvolution method. We provide theoretical\narguments to explain this result and discuss its practical implications.\nFinally, we investigate the use of heatmaps for unsupervised assessment of\nneural network performance.</p>\n", "tags": [] },
{"key": "samek2019towards", "year": "2019", "title":"Towards Explainable Artificial Intelligence", "abstract": "<p>In recent years, machine learning (ML) has become a key enabling technology\nfor the sciences and industry. Especially through improvements in methodology,\nthe availability of large databases and increased computational power, today’s\nML algorithms are able to achieve excellent performance (at times even\nexceeding the human level) on an increasing number of complex tasks. Deep\nlearning models are at the forefront of this development. However, due to their\nnested non-linear structure, these powerful models have been generally\nconsidered “black boxes”, not providing any information about what exactly\nmakes them arrive at their predictions. Since in many applications, e.g., in\nthe medical domain, such lack of transparency may be not acceptable, the\ndevelopment of methods for visualizing, explaining and interpreting deep\nlearning models has recently attracted increasing attention. This introductory\npaper presents recent developments and applications in this field and makes a\nplea for a wider use of explainable learning algorithms in practice.</p>\n", "tags": [] },
{"key": "samuel2021evaluation", "year": "2021", "title":"Evaluation of Saliency-based Explainability Method", "abstract": "<p>A particular class of Explainable AI (XAI) methods provide saliency maps to\nhighlight part of the image a Convolutional Neural Network (CNN) model looks at\nto classify the image as a way to explain its working. These methods provide an\nintuitive way for users to understand predictions made by CNNs. Other than\nquantitative computational tests, the vast majority of evidence to highlight\nthat the methods are valuable is anecdotal. Given that humans would be the\nend-users of such methods, we devise three human subject experiments through\nwhich we gauge the effectiveness of these saliency-based explainability\nmethods.</p>\n", "tags": [] },
{"key": "sanakoyeu2018deep", "year": "2018", "title":"Deep Unsupervised Learning of Visual Similarities", "abstract": "<p>Exemplar learning of visual similarities in an unsupervised manner is a\nproblem of paramount importance to Computer Vision. In this context, however,\nthe recent breakthrough in deep learning could not yet unfold its full\npotential. With only a single positive sample, a great imbalance between one\npositive and many negatives, and unreliable relationships between most samples,\ntraining of Convolutional Neural networks is impaired. In this paper we use\nweak estimates of local similarities and propose a single optimization problem\nto extract batches of samples with mutually consistent relations. Conflicting\nrelations are distributed over different batches and similar samples are\ngrouped into compact groups. Learning visual similarities is then framed as a\nsequence of categorization tasks. The CNN then consolidates transitivity\nrelations within and between groups and learns a single representation for all\nsamples without the need for labels. The proposed unsupervised approach has\nshown competitive performance on detailed posture analysis and object\nclassification.</p>\n", "tags": [] },
{"key": "santos2021impact", "year": "2021", "title":"On the Impact of Interpretability Methods in Active Image Augmentation Method", "abstract": "<p>Robustness is a significant constraint in machine learning models. The\nperformance of the algorithms must not deteriorate when training and testing\nwith slightly different data. Deep neural network models achieve awe-inspiring\nresults in a wide range of applications of computer vision. Still, in the\npresence of noise or region occlusion, some models exhibit inaccurate\nperformance even with data handled in training. Besides, some experiments\nsuggest deep learning models sometimes use incorrect parts of the input\ninformation to perform inference. Activate Image Augmentation (ADA) is an\naugmentation method that uses interpretability methods to augment the training\ndata and improve its robustness to face the described problems. Although ADA\npresented interesting results, its original version only used the Vanilla\nBackpropagation interpretability to train the U-Net model. In this work, we\npropose an extensive experimental analysis of the interpretability method’s\nimpact on ADA. We use five interpretability methods: Vanilla Backpropagation,\nGuided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The\nresults show that all methods achieve similar performance at the ending of\ntraining, but when combining ADA with GradCam, the U-Net model presented an\nimpressive fast convergence.</p>\n", "tags": [] },
{"key": "scafarto2022calibrate", "year": "2022", "title":"Calibrate to Interpret", "abstract": "<p>Trustworthy machine learning is driving a large number of ML community works\nin order to improve ML acceptance and adoption. The main aspect of trustworthy\nmachine learning are the followings: fairness, uncertainty, robustness,\nexplainability and formal guaranties. Each of these individual domains gains\nthe ML community interest, visible by the number of related publications.\nHowever few works tackle the interconnection between these fields. In this\npaper we show a first link between uncertainty and explainability, by studying\nthe relation between calibration and interpretation. As the calibration of a\ngiven model changes the way it scores samples, and interpretation approaches\noften rely on these scores, it seems safe to assume that the\nconfidence-calibration of a model interacts with our ability to interpret such\nmodel. In this paper, we show, in the context of networks trained on image\nclassification tasks, to what extent interpretations are sensitive to\nconfidence-calibration. It leads us to suggest a simple practice to improve the\ninterpretation outcomes: Calibrate to Interpret.</p>\n", "tags": [] },
{"key": "schmidt2019quantifying", "year": "2019", "title":"Quantifying Interpretability and Trust in Machine Learning Systems", "abstract": "<p>Decisions by Machine Learning (ML) models have become ubiquitous. Trusting\nthese decisions requires understanding how algorithms take them. Hence\ninterpretability methods for ML are an active focus of research. A central\nproblem in this context is that both the quality of interpretability methods as\nwell as trust in ML predictions are difficult to measure. Yet evaluations,\ncomparisons and improvements of trust and interpretability require quantifiable\nmeasures. Here we propose a quantitative measure for the quality of\ninterpretability methods. Based on that we derive a quantitative measure of\ntrust in ML decisions. Building on previous work we propose to measure\nintuitive understanding of algorithmic decisions using the information transfer\nrate at which humans replicate ML model predictions. We provide empirical\nevidence from crowdsourcing experiments that the proposed metric robustly\ndifferentiates interpretability methods. The proposed metric also demonstrates\nthe value of interpretability for ML assisted human decision making: in our\nexperiments providing explanations more than doubled productivity in annotation\ntasks. However unbiased human judgement is critical for doctors, judges, policy\nmakers and others. Here we derive a trust metric that identifies when human\ndecisions are overly biased towards ML predictions. Our results complement\nexisting qualitative work on trust and interpretability by quantifiable\nmeasures that can serve as objectives for further improving methods in this\nfield of research.</p>\n", "tags": [] },
{"key": "schneider2019humantoai", "year": "2019", "title":"Human-to-AI Coach: Improving Human Inputs to AI Systems", "abstract": "<p>Humans increasingly interact with Artificial intelligence(AI) systems. AI\nsystems are optimized for objectives such as minimum computation or minimum\nerror rate in recognizing and interpreting inputs from humans. In contrast,\ninputs created by humans are often treated as a given. We investigate how\ninputs of humans can be altered to reduce misinterpretation by the AI system\nand to improve efficiency of input generation for the human while altered\ninputs should remain as similar as possible to the original inputs. These\nobjectives result in trade-offs that are analyzed for a deep learning system\nclassifying handwritten digits. To create examples that serve as demonstrations\nfor humans to improve, we develop a model based on a conditional convolutional\nautoencoder (CCAE). Our quantitative and qualitative evaluation shows that in\nmany occasions the generated proposals lead to lower error rates, require less\neffort to create and differ only modestly from the original samples.</p>\n", "tags": [] },
{"key": "schneider2020deceptive", "year": "2020", "title":"Deceptive AI Explanations: Creation and Detection", "abstract": "<p>Artificial intelligence (AI) comes with great opportunities but can also pose\nsignificant risks. Automatically generated explanations for decisions can\nincrease transparency and foster trust, especially for systems based on\nautomated predictions by AI models. However, given, e.g., economic incentives\nto create dishonest AI, to what extent can we trust explanations? To address\nthis issue, our work investigates how AI models (i.e., deep learning, and\nexisting instruments to increase transparency regarding AI decisions) can be\nused to create and detect deceptive explanations. As an empirical evaluation,\nwe focus on text classification and alter the explanations generated by\nGradCAM, a well-established explanation technique in neural networks. Then, we\nevaluate the effect of deceptive explanations on users in an experiment with\n200 participants. Our findings confirm that deceptive explanations can indeed\nfool humans. However, one can deploy machine learning (ML) methods to detect\nseemingly minor deception attempts with accuracy exceeding 80% given sufficient\ndomain knowledge. Without domain knowledge, one can still infer inconsistencies\nin the explanations in an unsupervised manner, given basic knowledge of the\npredictive model under scrutiny.</p>\n", "tags": [] },
{"key": "schramowski2020making", "year": "2020", "title":"Making deep neural networks right for the right scientific reasons by interacting with their explanations", "abstract": "<p>Deep neural networks have shown excellent performances in many real-world\napplications. Unfortunately, they may show “Clever Hans”-like behavior—making\nuse of confounding factors within datasets—to achieve high performance. In\nthis work, we introduce the novel learning setting of “explanatory interactive\nlearning” (XIL) and illustrate its benefits on a plant phenotyping research\ntask. XIL adds the scientist into the training loop such that she interactively\nrevises the original model via providing feedback on its explanations. Our\nexperimental results demonstrate that XIL can help avoiding Clever Hans moments\nin machine learning and encourages (or discourages, if appropriate) trust into\nthe underlying model.</p>\n", "tags": [] },
{"key": "schwalbe2021comprehensive", "year": "2021", "title":"A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts", "abstract": "<p>In the meantime, a wide variety of terminologies, motivations, approaches,\nand evaluation criteria have been developed within the research field of\nexplainable artificial intelligence (XAI). With the amount of XAI methods\nvastly growing, a taxonomy of methods is needed by researchers as well as\npractitioners: To grasp the breadth of the topic, compare methods, and to\nselect the right XAI method based on traits required by a specific use-case\ncontext. Many taxonomies for XAI methods of varying level of detail and depth\ncan be found in the literature. While they often have a different focus, they\nalso exhibit many points of overlap. This paper unifies these efforts and\nprovides a complete taxonomy of XAI methods with respect to notions present in\nthe current state of research. In a structured literature analysis and\nmeta-study, we identified and reviewed more than 50 of the most cited and\ncurrent surveys on XAI methods, metrics, and method traits. After summarizing\nthem in a survey of surveys, we merge terminologies and concepts of the\narticles into a unified structured taxonomy. Single concepts therein are\nillustrated by more than 50 diverse selected example methods in total, which we\ncategorize accordingly. The taxonomy may serve both beginners, researchers, and\npractitioners as a reference and wide-ranging overview of XAI method traits and\naspects. Hence, it provides foundations for targeted, use-case-oriented, and\ncontext-sensitive future research.</p>\n", "tags": [] },
{"key": "senel2017semantic", "year": "2017", "title":"Semantic Structure and Interpretability of Word Embeddings", "abstract": "<p>Dense word embeddings, which encode semantic meanings of words to low\ndimensional vector spaces have become very popular in natural language\nprocessing (NLP) research due to their state-of-the-art performances in many\nNLP tasks. Word embeddings are substantially successful in capturing semantic\nrelations among words, so a meaningful semantic structure must be present in\nthe respective vector spaces. However, in many cases, this semantic structure\nis broadly and heterogeneously distributed across the embedding dimensions,\nwhich makes interpretation a big challenge. In this study, we propose a\nstatistical method to uncover the latent semantic structure in the dense word\nembeddings. To perform our analysis we introduce a new dataset (SEMCAT) that\ncontains more than 6500 words semantically grouped under 110 categories. We\nfurther propose a method to quantify the interpretability of the word\nembeddings; the proposed method is a practical alternative to the classical\nword intrusion test that requires human intervention.</p>\n", "tags": [] },
{"key": "seo2018regional", "year": "2018", "title":"Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks", "abstract": "<p>Recently, many methods to interpret and visualize deep neural network\npredictions have been proposed and significant progress has been made. However,\na more class-discriminative and visually pleasing explanation is required.\nThus, this paper proposes a region-based approach that estimates feature\nimportance in terms of appropriately segmented regions. By fusing the saliency\nmaps generated from multi-scale segmentations, a more class-discriminative and\nvisually pleasing map is obtained. We incorporate this regional multi-scale\nconcept into a prediction difference method that is model-agnostic. An input\nimage is segmented in several scales using the super-pixel method, and\nexclusion of a region is simulated by sampling a normal distribution\nconstructed using the boundary prior. The experimental results demonstrate that\nthe regional multi-scale method produces much more class-discriminative and\nvisually pleasing saliency maps.</p>\n", "tags": [] },
{"key": "shankaranarayana2019alime", "year": "2019", "title":"ALIME: Autoencoder Based Approach for Local Interpretability", "abstract": "<p>Machine learning and especially deep learning have garneredtremendous\npopularity in recent years due to their increased performanceover other\nmethods. The availability of large amount of data has aidedin the progress of\ndeep learning. Nevertheless, deep learning models areopaque and often seen as\nblack boxes. Thus, there is an inherent need tomake the models interpretable,\nespecially so in the medical domain. Inthis work, we propose a locally\ninterpretable method, which is inspiredby one of the recent tools that has\ngained a lot of interest, called localinterpretable model-agnostic explanations\n(LIME). LIME generates singleinstance level explanation by artificially\ngenerating a dataset aroundthe instance (by randomly sampling and using\nperturbations) and thentraining a local linear interpretable model. One of the\nmajor issues inLIME is the instability in the generated explanation, which is\ncaused dueto the randomly generated dataset. Another issue in these kind of\nlocalinterpretable models is the local fidelity. We propose novel\nmodificationsto LIME by employing an autoencoder, which serves as a better\nweightingfunction for the local model. We perform extensive comparisons\nwithdifferent datasets and show that our proposed method results in\nbothimproved stability, as well as local fidelity.</p>\n", "tags": [] },
{"key": "shen2020explain", "year": "2020", "title":"To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles", "abstract": "<p>Explainable AI, in the context of autonomous systems, like self driving cars,\nhas drawn broad interests from researchers. Recent studies have found that\nproviding explanations for an autonomous vehicle actions has many benefits,\ne.g., increase trust and acceptance, but put little emphasis on when an\nexplanation is needed and how the content of explanation changes with context.\nIn this work, we investigate which scenarios people need explanations and how\nthe critical degree of explanation shifts with situations and driver types.\nThrough a user experiment, we ask participants to evaluate how necessary an\nexplanation is and measure the impact on their trust in the self driving cars\nin different contexts. We also present a self driving explanation dataset with\nfirst person explanations and associated measure of the necessity for 1103\nvideo clips, augmenting the Berkeley Deep Drive Attention dataset.\nAdditionally, we propose a learning based model that predicts how necessary an\nexplanation for a given situation in real time, using camera data inputs. Our\nresearch reveals that driver types and context dictates whether or not an\nexplanation is necessary and what is helpful for improved interaction and\nunderstanding.</p>\n", "tags": [] },
{"key": "shen2020interfacegan", "year": "2020", "title":"InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs", "abstract": "<p>Although Generative Adversarial Networks (GANs) have made significant\nprogress in face synthesis, there lacks enough understanding of what GANs have\nlearned in the latent representation to map a random code to a photo-realistic\nimage. In this work, we propose a framework called InterFaceGAN to interpret\nthe disentangled face representation learned by the state-of-the-art GAN models\nand study the properties of the facial semantics encoded in the latent space.\nWe first find that GANs learn various semantics in some linear subspaces of the\nlatent space. After identifying these subspaces, we can realistically\nmanipulate the corresponding facial attributes without retraining the model. We\nthen conduct a detailed study on the correlation between different semantics\nand manage to better disentangle them via subspace projection, resulting in\nmore precise control of the attribute manipulation. Besides manipulating the\ngender, age, expression, and presence of eyeglasses, we can even alter the face\npose and fix the artifacts accidentally made by GANs. Furthermore, we perform\nan in-depth face identity analysis and a layer-wise analysis to evaluate the\nediting results quantitatively. Finally, we apply our approach to real face\nediting by employing GAN inversion approaches and explicitly training\nfeed-forward models based on the synthetic data established by InterFaceGAN.\nExtensive experimental results suggest that learning to synthesize faces\nspontaneously brings a disentangled and controllable face representation.</p>\n", "tags": [] },
{"key": "shen2020useful", "year": "2020", "title":"How Useful Are the Machine-Generated Interpretations to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels", "abstract": "<p>Explaining to users why automated systems make certain mistakes is important\nand challenging. Researchers have proposed ways to automatically produce\ninterpretations for deep neural network models. However, it is unclear how\nuseful these interpretations are in helping users figure out why they are\ngetting an error. If an interpretation effectively explains to users how the\nunderlying deep neural network model works, people who were presented with the\ninterpretation should be better at predicting the model’s outputs than those\nwho were not. This paper presents an investigation on whether or not showing\nmachine-generated visual interpretations helps users understand the incorrectly\npredicted labels produced by image classifiers. We showed the images and the\ncorrect labels to 150 online crowd workers and asked them to select the\nincorrectly predicted labels with or without showing them the machine-generated\nvisual interpretations. The results demonstrated that displaying the visual\ninterpretations did not increase, but rather decreased, the average guessing\naccuracy by roughly 10%.</p>\n", "tags": [] },
{"key": "shen2021interpretable", "year": "2021", "title":"Interpretable Compositional Convolutional Neural Networks", "abstract": "<p>The reasonable definition of semantic interpretability presents the core\nchallenge in explainable AI. This paper proposes a method to modify a\ntraditional convolutional neural network (CNN) into an interpretable\ncompositional CNN, in order to learn filters that encode meaningful visual\npatterns in intermediate convolutional layers. In a compositional CNN, each\nfilter is supposed to consistently represent a specific compositional object\npart or image region with a clear meaning. The compositional CNN learns from\nimage labels for classification without any annotations of parts or regions for\nsupervision. Our method can be broadly applied to different types of CNNs.\nExperiments have demonstrated the effectiveness of our method.</p>\n", "tags": [] },
{"key": "shickel2017deep", "year": "2017", "title":"Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis", "abstract": "<p>The past decade has seen an explosion in the amount of digital information\nstored in electronic health records (EHR). While primarily designed for\narchiving patient clinical information and administrative healthcare tasks,\nmany researchers have found secondary use of these records for various clinical\ninformatics tasks. Over the same period, the machine learning community has\nseen widespread advances in deep learning techniques, which also have been\nsuccessfully applied to the vast amount of EHR data. In this paper, we review\nthese deep EHR systems, examining architectures, technical aspects, and\nclinical applications. We also identify shortcomings of current techniques and\ndiscuss avenues of future research for EHR-based deep learning.</p>\n", "tags": [] },
{"key": "shickel2020sequential", "year": "2020", "title":"Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data", "abstract": "<p>Deep learning continues to revolutionize an ever-growing number of critical\napplication areas including healthcare, transportation, finance, and basic\nsciences. Despite their increased predictive power, model transparency and\nhuman explainability remain a significant challenge due to the “black box”\nnature of modern deep learning models. In many cases the desired balance\nbetween interpretability and performance is predominately task specific.\nHuman-centric domains such as healthcare necessitate a renewed focus on\nunderstanding how and why these frameworks are arriving at critical and\npotentially life-or-death decisions. Given the quantity of research and\nempirical successes of deep learning for computer vision, most of the existing\ninterpretability research has focused on image processing techniques.\nComparatively, less attention has been paid to interpreting deep learning\nframeworks using sequential data. Given recent deep learning advancements in\nhighly sequential domains such as natural language processing and physiological\nsignal processing, the need for deep sequential explanations is at an all-time\nhigh. In this paper, we review current techniques for interpreting deep\nlearning techniques involving sequential data, identify similarities to\nnon-sequential methods, and discuss current limitations and future avenues of\nsequential interpretability research.</p>\n", "tags": [] },
{"key": "shrikumar2016black", "year": "2016", "title":"Not Just a Black Box: Learning Important Features Through Propagating Activation Differences", "abstract": "<p>Note: This paper describes an older version of DeepLIFT. See\nhttps://arxiv.org/abs/1704.02685 for the newer version. Original abstract\nfollows: The purported “black box” nature of neural networks is a barrier to\nadoption in applications where interpretability is essential. Here we present\nDeepLIFT (Learning Important FeaTures), an efficient and effective method for\ncomputing importance scores in a neural network. DeepLIFT compares the\nactivation of each neuron to its ‘reference activation’ and assigns\ncontribution scores according to the difference. We apply DeepLIFT to models\ntrained on natural images and genomic data, and show significant advantages\nover gradient-based methods.</p>\n", "tags": [] },
{"key": "shrikumar2017learning", "year": "2017", "title":"Learning Important Features Through Propagating Activation Differences", "abstract": "<p>The purported “black box” nature of neural networks is a barrier to adoption\nin applications where interpretability is essential. Here we present DeepLIFT\n(Deep Learning Important FeaTures), a method for decomposing the output\nprediction of a neural network on a specific input by backpropagating the\ncontributions of all neurons in the network to every feature of the input.\nDeepLIFT compares the activation of each neuron to its ‘reference activation’\nand assigns contribution scores according to the difference. By optionally\ngiving separate consideration to positive and negative contributions, DeepLIFT\ncan also reveal dependencies which are missed by other approaches. Scores can\nbe computed efficiently in a single backward pass. We apply DeepLIFT to models\ntrained on MNIST and simulated genomic data, and show significant advantages\nover gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:\nbit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:\nhttp://goo.gl/RM8jvH.</p>\n", "tags": [] },
{"key": "shvo2020interpretable", "year": "2020", "title":"Interpretable Sequence Classification via Discrete Optimization", "abstract": "<p>Sequence classification is the task of predicting a class label given a\nsequence of observations. In many applications such as healthcare monitoring or\nintrusion detection, early classification is crucial to prompt intervention. In\nthis work, we learn sequence classifiers that favour early classification from\nan evolving observation trace. While many state-of-the-art sequence classifiers\nare neural networks, and in particular LSTMs, our classifiers take the form of\nfinite state automata and are learned via discrete optimization. Our\nautomata-based classifiers are interpretable—supporting explanation,\ncounterfactual reasoning, and human-in-the-loop modification—and have strong\nempirical performance. Experiments over a suite of goal recognition and\nbehaviour classification datasets show our learned automata-based classifiers\nto have comparable test performance to LSTM-based classifiers, with the added\nadvantage of being interpretable.</p>\n", "tags": [] },
{"key": "shvo2020towards", "year": "2020", "title":"Towards the Role of Theory of Mind in Explanation", "abstract": "<p>Theory of Mind is commonly defined as the ability to attribute mental states\n(e.g., beliefs, goals) to oneself, and to others. A large body of previous work</p>\n<ul>\n  <li>from the social sciences to artificial intelligence - has observed that\nTheory of Mind capabilities are central to providing an explanation to another\nagent or when explaining that agent’s behaviour. In this paper, we build and\nexpand upon previous work by providing an account of explanation in terms of\nthe beliefs of agents and the mechanism by which agents revise their beliefs\ngiven possible explanations. We further identify a set of desiderata for\nexplanations that utilize Theory of Mind. These desiderata inform our\nbelief-based account of explanation.</li>\n</ul>\n", "tags": [] },
{"key": "siddiqui2015sequential", "year": "2015", "title":"Sequential Feature Explanations for Anomaly Detection", "abstract": "<p>In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation’s quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches.</p>\n", "tags": [] },
{"key": "simonyan2013deep", "year": "2013", "title":"Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "abstract": "<p>This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013].</p>\n", "tags": [] },
{"key": "sindhgatta2019exploring", "year": "2019", "title":"Exploring Interpretability for Predictive Process Analytics", "abstract": "<p>Modern predictive analytics underpinned by machine learning techniques has\nbecome a key enabler to the automation of data-driven decision making. In the\ncontext of business process management, predictive analytics has been applied\nto making predictions about the future state of an ongoing business process\ninstance, for example, when will the process instance complete and what will be\nthe outcome upon completion. Machine learning models can be trained on event\nlog data recording historical process execution to build the underlying\npredictive models. Multiple techniques have been proposed so far which encode\nthe information available in an event log and construct input features required\nto train a predictive model. While accuracy has been a dominant criterion in\nthe choice of various techniques, they are often applied as a black-box in\nbuilding predictive models. In this paper, we derive explanations using\ninterpretable machine learning techniques to compare and contrast the\nsuitability of multiple predictive models of high accuracy. The explanations\nallow us to gain an understanding of the underlying reasons for a prediction\nand highlight scenarios where accuracy alone may not be sufficient in assessing\nthe suitability of techniques used to encode event log data to features used by\na predictive model. Findings from this study motivate the need and importance\nto incorporate interpretability in predictive process analytics.</p>\n", "tags": [] },
{"key": "slack2021counterfactual", "year": "2021", "title":"Counterfactual Explanations Can Be Manipulated", "abstract": "<p>Counterfactual explanations are emerging as an attractive option for\nproviding recourse to individuals adversely impacted by algorithmic decisions.\nAs they are deployed in critical applications (e.g. law enforcement, financial\nlending), it becomes important to ensure that we clearly understand the\nvulnerabilities of these methods and find ways to address them. However, there\nis little understanding of the vulnerabilities and shortcomings of\ncounterfactual explanations. In this work, we introduce the first framework\nthat describes the vulnerabilities of counterfactual explanations and shows how\nthey can be manipulated. More specifically, we show counterfactual explanations\nmay converge to drastically different counterfactuals under a small\nperturbation indicating they are not robust. Leveraging this insight, we\nintroduce a novel objective to train seemingly fair models where counterfactual\nexplanations find much lower cost recourse under a slight perturbation. We\ndescribe how these models can unfairly provide low-cost recourse for specific\nsubgroups in the data while appearing fair to auditors. We perform experiments\non loan and violent crime prediction data sets where certain subgroups achieve\nup to 20x lower cost recourse under the perturbation. These results raise\nconcerns regarding the dependability of current counterfactual explanation\ntechniques, which we hope will inspire investigations in robust counterfactual\nexplanations.</p>\n", "tags": [] },
{"key": "sokol2019explainability", "year": "2019", "title":"Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches", "abstract": "<p>Explanations in Machine Learning come in many forms, but a consensus\nregarding their desired properties is yet to emerge. In this paper we introduce\na taxonomy and a set of descriptors that can be used to characterise and\nsystematically assess explainable systems along five key dimensions:\nfunctional, operational, usability, safety and validation. In order to design a\ncomprehensive and representative taxonomy and associated descriptors we\nsurveyed the eXplainable Artificial Intelligence literature, extracting the\ncriteria and desiderata that other authors have proposed or implicitly used in\ntheir research. The survey includes papers introducing new explainability\nalgorithms to see what criteria are used to guide their development and how\nthese algorithms are evaluated, as well as papers proposing such criteria from\nboth computer science and social science perspectives. This novel framework\nallows to systematically compare and contrast explainability approaches, not\njust to better understand their capabilities but also to identify discrepancies\nbetween their theoretical qualities and properties of their implementations. We\ndeveloped an operationalisation of the framework in the form of Explainability\nFact Sheets, which enable researchers and practitioners alike to quickly grasp\ncapabilities and limitations of a particular explainable method. When used as a\nWork Sheet, our taxonomy can guide the development of new explainability\napproaches by aiding in their critical evaluation along the five proposed\ndimensions.</p>\n", "tags": [] },
{"key": "sokol2020towards", "year": "2020", "title":"Towards Faithful and Meaningful Interpretable Representations", "abstract": "<p>Interpretable representations are the backbone of many black-box explainers.\nThey translate the low-level data representation necessary for good predictive\nperformance into high-level human-intelligible concepts used to convey the\nexplanation. Notably, the explanation type and its cognitive complexity are\ndirectly controlled by the interpretable representation, allowing to target a\nparticular audience and use case. However, many explainers that rely on\ninterpretable representations overlook their merit and fall back on default\nsolutions, which may introduce implicit assumptions, thereby degrading the\nexplanatory power of such techniques. To address this problem, we study\nproperties of interpretable representations that encode presence and absence of\nhuman-comprehensible concepts. We show how they are operationalised for\ntabular, image and text data, discussing their strengths and weaknesses.\nFinally, we analyse their explanatory properties in the context of tabular\ndata, where a linear model is used to quantify the importance of interpretable\nconcepts.</p>\n", "tags": [] },
{"key": "springenberg2014striving", "year": "2014", "title":"Striving for Simplicity: The All Convolutional Net", "abstract": "<p>Most modern convolutional neural networks (CNNs) used for object recognition\nare built using the same principles: Alternating convolution and max-pooling\nlayers followed by a small number of fully connected layers. We re-evaluate the\nstate of the art for object recognition from small images with convolutional\nnetworks, questioning the necessity of different components in the pipeline. We\nfind that max-pooling can simply be replaced by a convolutional layer with\nincreased stride without loss in accuracy on several image recognition\nbenchmarks. Following this finding – and building on other recent work for\nfinding simple network structures – we propose a new architecture that\nconsists solely of convolutional layers and yields competitive or state of the\nart performance on several object recognition datasets (CIFAR-10, CIFAR-100,\nImageNet). To analyze the network we introduce a new variant of the\n“deconvolution approach” for visualizing features learned by CNNs, which can be\napplied to a broader range of network structures than existing approaches.</p>\n", "tags": [] },
{"key": "stalder2022see", "year": "2022", "title":"What You See is What You Classify: Black Box Attributions", "abstract": "<p>An important step towards explaining deep image classifiers lies in the\nidentification of image regions that contribute to individual class scores in\nthe model’s output. However, doing this accurately is a difficult task due to\nthe black-box nature of such networks. Most existing approaches find such\nattributions either using activations and gradients or by repeatedly perturbing\nthe input. We instead address this challenge by training a second deep network,\nthe Explainer, to predict attributions for a pre-trained black-box classifier,\nthe Explanandum. These attributions are in the form of masks that only show the\nclassifier-relevant parts of an image, masking out the rest. Our approach\nproduces sharper and more boundary-precise masks when compared to the saliency\nmaps generated by other methods. Moreover, unlike most existing approaches,\nours is capable of directly generating very distinct class-specific masks.\nFinally, the proposed method is very efficient for inference since it only\ntakes a single forward pass through the Explainer to generate all\nclass-specific masks. We show that our attributions are superior to established\nmethods both visually and quantitatively, by evaluating them on the PASCAL\nVOC-2007 and Microsoft COCO-2014 datasets.</p>\n", "tags": [] },
{"key": "strobelt2016lstmvis:", "year": "2016", "title":"LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks", "abstract": "<p>Recurrent neural networks, and in particular long short-term memory (LSTM)\nnetworks, are a remarkably effective tool for sequence modeling that learn a\ndense black-box hidden representation of their sequential input. Researchers\ninterested in better understanding these models have studied the changes in\nhidden state representations over time and noticed some interpretable patterns\nbut also significant noise. In this work, we present LSTMVIS, a visual analysis\ntool for recurrent neural networks with a focus on understanding these hidden\nstate dynamics. The tool allows users to select a hypothesis input range to\nfocus on local state changes, to match these states changes to similar patterns\nin a large data set, and to align these results with structural annotations\nfrom their domain. We show several use cases of the tool for analyzing specific\nhidden state properties on dataset containing nesting, phrase structure, and\nchord progressions, and demonstrate how the tool can be used to isolate\npatterns for further statistical analysis. We characterize the domain, the\ndifferent stakeholders, and their goals and tasks.</p>\n", "tags": [] },
{"key": "sturm2016interpretable", "year": "2016", "title":"Interpretable Deep Neural Networks for Single-Trial EEG Classification", "abstract": "<p>Background: In cognitive neuroscience the potential of Deep Neural Networks\n(DNNs) for solving complex classification tasks is yet to be fully exploited.\nThe most limiting factor is that DNNs as notorious ‘black boxes’ do not provide\ninsight into neurophysiological phenomena underlying a decision. Layer-wise\nRelevance Propagation (LRP) has been introduced as a novel method to explain\nindividual network decisions. New Method: We propose the application of DNNs\nwith LRP for the first time for EEG data analysis. Through LRP the single-trial\nDNN decisions are transformed into heatmaps indicating each data point’s\nrelevance for the outcome of the decision. Results: DNN achieves classification\naccuracies comparable to those of CSP-LDA. In subjects with low performance\nsubject-to-subject transfer of trained DNNs can improve the results. The\nsingle-trial LRP heatmaps reveal neurophysiologically plausible patterns,\nresembling CSP-derived scalp maps. Critically, while CSP patterns represent\nclass-wise aggregated information, LRP heatmaps pinpoint neural patterns to\nsingle time points in single trials. Comparison with Existing Method(s): We\ncompare the classification performance of DNNs to that of linear CSP-LDA on two\ndata sets related to motor-imaginery BCI. Conclusion: We have demonstrated that\nDNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of\nhigh-resolution assessment of neural activity can be reached. LRP is a\npotential remedy for the lack of interpretability of DNNs that has limited\ntheir utility in neuroscientific applications. The extreme specificity of the\nLRP-derived heatmaps opens up new avenues for investigating neural activity\nunderlying complex perception or decision-related processes.</p>\n", "tags": [] },
{"key": "stylianou2019visualizing", "year": "2019", "title":"Visualizing Deep Similarity Networks", "abstract": "<p>For convolutional neural network models that optimize an image embedding, we\npropose a method to highlight the regions of images that contribute most to\npairwise similarity. This work is a corollary to the visualization tools\ndeveloped for classification networks, but applicable to the problem domains\nbetter suited to similarity learning. The visualization shows how similarity\nnetworks that are fine-tuned learn to focus on different features. We also\ngeneralize our approach to embedding networks that use different pooling\nstrategies and provide a simple mechanism to support image similarity searches\non objects or sub-regions in the query image.</p>\n", "tags": [] },
{"key": "su2017one", "year": "2017", "title":"One pixel attack for fooling deep neural networks", "abstract": "<p>Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness.</p>\n", "tags": [] },
{"key": "su2020sanity", "year": "2020", "title":"Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot", "abstract": "<p>Network pruning is a method for reducing test-time computational resource\nrequirements with minimal performance degradation. Conventional wisdom of\npruning algorithms suggests that: (1) Pruning methods exploit information from\ntraining data to find good subnetworks; (2) The architecture of the pruned\nnetwork is crucial for good performance. In this paper, we conduct sanity\nchecks for the above beliefs on several recent unstructured pruning methods and\nsurprisingly find that: (1) A set of methods which aims to find good\nsubnetworks of the randomly-initialized network (which we call “initial\ntickets”), hardly exploits any information from the training data; (2) For the\npruned networks obtained by these methods, randomly changing the preserved\nweights in each layer, while keeping the total number of preserved weights\nunchanged per layer, does not affect the final performance. These findings\ninspire us to choose a series of simple \\emph{data-independent} prune ratios\nfor each layer, and randomly prune each layer accordingly to get a subnetwork\n(which we call “random tickets”). Experimental results show that our zero-shot\nrandom tickets outperform or attain a similar performance compared to existing\n“initial tickets”. In addition, we identify one existing pruning method that\npasses our sanity checks. We hybridize the ratios in our random ticket with\nthis method and propose a new method called “hybrid tickets”, which achieves\nfurther improvement. (Our code is publicly available at\nhttps://github.com/JingtongSu/sanity-checking-pruning)</p>\n", "tags": [] },
{"key": "subramanian2020obtaining", "year": "2020", "title":"Obtaining Faithful Interpretations from Compositional Neural Networks", "abstract": "<p>Neural module networks (NMNs) are a popular approach for modeling\ncompositionality: they achieve high accuracy when applied to problems in\nlanguage and vision, while reflecting the compositional structure of the\nproblem in the network architecture. However, prior work implicitly assumed\nthat the structure of the network modules, describing the abstract reasoning\nprocess, provides a faithful explanation of the model’s reasoning; that is,\nthat all modules perform their intended behaviour. In this work, we propose and\nconduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2\nand DROP, two datasets which require composing multiple reasoning steps. We\nfind that the intermediate outputs differ from the expected output,\nillustrating that the network structure does not provide a faithful explanation\nof model behaviour. To remedy that, we train the model with auxiliary\nsupervision and propose particular choices for module architecture that yield\nmuch better faithfulness, at a minimal cost to accuracy.</p>\n", "tags": [] },
{"key": "sudhakar2021ada", "year": "2021", "title":"Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks", "abstract": "<p>Explainable AI (XAI) is an active research area to interpret a neural\nnetwork’s decision by ensuring transparency and trust in the task-specified\nlearned models. Recently, perturbation-based model analysis has shown better\ninterpretation, but backpropagation techniques are still prevailing because of\ntheir computational efficiency. In this work, we combine both approaches as a\nhybrid visual explanation algorithm and propose an efficient interpretation\nmethod for convolutional neural networks. Our method adaptively selects the\nmost critical features that mainly contribute towards a prediction to probe the\nmodel by finding the activated features. Experimental results show that the\nproposed method can reduce the execution time up to 30% while enhancing\ncompetitive interpretability without compromising the quality of explanation\ngenerated.</p>\n", "tags": [] },
{"key": "sundararajan2017axiomatic", "year": "2017", "title":"Axiomatic Attribution for Deep Networks", "abstract": "<p>We study the problem of attributing the prediction of a deep network to its\ninput features, a problem previously studied by several other works. We\nidentify two fundamental axioms—Sensitivity and Implementation Invariance\nthat attribution methods ought to satisfy. We show that they are not satisfied\nby most known attribution methods, which we consider to be a fundamental\nweakness of those methods. We use the axioms to guide the design of a new\nattribution method called Integrated Gradients. Our method requires no\nmodification to the original network and is extremely simple to implement; it\njust needs a few calls to the standard gradient operator. We apply this method\nto a couple of image models, a couple of text models and a chemistry model,\ndemonstrating its ability to debug networks, to extract rules from a network,\nand to enable users to engage with models better.</p>\n", "tags": [] },
{"key": "suresh2021beyond", "year": "2021", "title":"Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs", "abstract": "<p>To ensure accountability and mitigate harm, it is critical that diverse\nstakeholders can interrogate black-box automated systems and find information\nthat is understandable, relevant, and useful to them. In this paper, we eschew\nprior expertise- and role-based categorizations of interpretability\nstakeholders in favor of a more granular framework that decouples stakeholders’\nknowledge from their interpretability needs. We characterize stakeholders by\ntheir formal, instrumental, and personal knowledge and how it manifests in the\ncontexts of machine learning, the data domain, and the general milieu. We\nadditionally distill a hierarchical typology of stakeholder needs that\ndistinguishes higher-level domain goals from lower-level interpretability\ntasks. In assessing the descriptive, evaluative, and generative powers of our\nframework, we find our more nuanced treatment of stakeholders reveals gaps and\nopportunities in the interpretability literature, adds precision to the design\nand comparison of user studies, and facilitates a more reflexive approach to\nconducting this research.</p>\n", "tags": [] },
{"key": "swamy2021interpreting", "year": "2021", "title":"Interpreting Language Models Through Knowledge Graph Extraction", "abstract": "<p>Transformer-based language models trained on large text corpora have enjoyed\nimmense popularity in the natural language processing community and are\ncommonly used as a starting point for downstream tasks. While these models are\nundeniably useful, it is a challenge to quantify their performance beyond\ntraditional accuracy metrics. In this paper, we compare BERT-based language\nmodels through snapshots of acquired knowledge at sequential stages of the\ntraining process. Structured relationships from training corpora may be\nuncovered through querying a masked language model with probing tasks. We\npresent a methodology to unveil a knowledge acquisition timeline by generating\nknowledge graph extracts from cloze “fill-in-the-blank” statements at various\nstages of RoBERTa’s early training. We extend this analysis to a comparison of\npretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This\nwork proposes a quantitative framework to compare language models through\nknowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech\nanalysis (POSOR) to identify the linguistic strengths of each model variant.\nUsing these metrics, machine learning practitioners can compare models,\ndiagnose their models’ behavioral strengths and weaknesses, and identify new\ntargeted datasets to improve model performance.</p>\n", "tags": [] },
{"key": "swamy2022evaluating", "year": "2022", "title":"Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs", "abstract": "<p>Neural networks are ubiquitous in applied machine learning for education.\nTheir pervasive success in predictive performance comes alongside a severe\nweakness, the lack of explainability of their decisions, especially relevant in\nhuman-centric fields. We implement five state-of-the-art methodologies for\nexplaining black-box machine learning models (LIME, PermutationSHAP,\nKernelSHAP, DiCE, CEM) and examine the strengths of each approach on the\ndownstream task of student performance prediction for five massive open online\ncourses. Our experiments demonstrate that the families of explainers do not\nagree with each other on feature importance for the same Bidirectional LSTM\nmodels with the same representative set of students. We use Principal Component\nAnalysis, Jensen-Shannon distance, and Spearman’s rank-order correlation to\nquantitatively cross-examine explanations across methods and courses.\nFurthermore, we validate explainer performance across curriculum-based\nprerequisite relationships. Our results come to the concerning conclusion that\nthe choice of explainer is an important decision and is in fact paramount to\nthe interpretation of the predictive results, even more so than the course the\nmodel is trained on. Source code and models are released at\nhttp://github.com/epfl-ml4ed/evaluating-explainers.</p>\n", "tags": [] },
{"key": "szegedy2013intriguing", "year": "2013", "title":"Intriguing properties of neural networks", "abstract": "<p>Deep neural networks are highly expressive models that have recently achieved\nstate of the art performance on speech and visual recognition tasks. While\ntheir expressiveness is the reason they succeed, it also causes them to learn\nuninterpretable solutions that could have counter-intuitive properties. In this\npaper we report two such properties.\n  First, we find that there is no distinction between individual high level\nunits and random linear combinations of high level units, according to various\nmethods of unit analysis. It suggests that it is the space, rather than the\nindividual units, that contains of the semantic information in the high layers\nof neural networks.\n  Second, we find that deep neural networks learn input-output mappings that\nare fairly discontinuous to a significant extend. We can cause the network to\nmisclassify an image by applying a certain imperceptible perturbation, which is\nfound by maximizing the network’s prediction error. In addition, the specific\nnature of these perturbations is not a random artifact of learning: the same\nperturbation can cause a different network, that was trained on a different\nsubset of the dataset, to misclassify the same input.</p>\n", "tags": [] },
{"key": "tan2020locality", "year": "2020", "title":"Locality Guided Neural Networks for Explainable Artificial Intelligence", "abstract": "<p>In current deep network architectures, deeper layers in networks tend to\ncontain hundreds of independent neurons which makes it hard for humans to\nunderstand how they interact with each other. By organizing the neurons by\ncorrelation, humans can observe how clusters of neighbouring neurons interact\nwith each other. In this paper, we propose a novel algorithm for back\npropagation, called Locality Guided Neural Network(LGNN) for training networks\nthat preserves locality between neighbouring neurons within each layer of a\ndeep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to\nenforce a local topology on each layer of a deep network such that neighbouring\nneurons are highly correlated with each other. This method contributes to the\ndomain of Explainable Artificial Intelligence (XAI), which aims to alleviate\nthe black-box nature of current AI methods and make them understandable by\nhumans. Our method aims to achieve XAI in deep learning without changing the\nstructure of current models nor requiring any post processing. This paper\nfocuses on Convolutional Neural Networks (CNNs), but can theoretically be\napplied to any type of deep learning architecture. In our experiments, we train\nvarious VGG and Wide ResNet (WRN) networks for image classification on\nCIFAR100. In depth analyses presenting both qualitative and quantitative\nresults demonstrate that our method is capable of enforcing a topology on each\nlayer while achieving a small increase in classification accuracy</p>\n", "tags": [] },
{"key": "teso2018why", "year": "2018", "title":"\"Why Should I Trust Interactive Learners?\" Explaining Interactive Queries of Classifiers to Users", "abstract": "<p>Although interactive learning puts the user into the loop, the learner\nremains mostly a black box for the user. Understanding the reasons behind\nqueries and predictions is important when assessing how the learner works and,\nin turn, trust. Consequently, we propose the novel framework of explanatory\ninteractive learning: in each step, the learner explains its interactive query\nto the user, and she queries of any active classifier for visualizing\nexplanations of the corresponding predictions. We demonstrate that this can\nboost the predictive and explanatory powers of and the trust into the learned\nmodel, using text (e.g. SVMs) and image classification (e.g. neural networks)\nexperiments as well as a user study.</p>\n", "tags": [] },
{"key": "tjoa2020quantifying", "year": "2020", "title":"Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset", "abstract": "<p>Post-hoc analysis is a popular category in eXplainable artificial\nintelligence (XAI) study. In particular, methods that generate heatmaps have\nbeen used to explain the deep neural network (DNN), a black-box model. Heatmaps\ncan be appealing due to the intuitive and visual ways to understand them but\nassessing their qualities might not be straightforward. Different ways to\nassess heatmaps’ quality have their own merits and shortcomings. This paper\nintroduces a synthetic dataset that can be generated adhoc along with the\nground-truth heatmaps for more objective quantitative assessment. Each sample\ndata is an image of a cell with easily recognized features that are\ndistinguished from localization ground-truth mask, hence facilitating a more\ntransparent assessment of different XAI methods. Comparison and recommendations\nare made, shortcomings are clarified along with suggestions for future research\ndirections to handle the finer details of select post-hoc analysis methods.</p>\n", "tags": [] },
{"key": "tomsett2019sanity", "year": "2019", "title":"Sanity Checks for Saliency Metrics", "abstract": "<p>Saliency maps are a popular approach to creating post-hoc explanations of\nimage classifier outputs. These methods produce estimates of the relevance of\neach pixel to the classification output score, which can be displayed as a\nsaliency map that highlights important pixels. Despite a proliferation of such\nmethods, little effort has been made to quantify how good these saliency maps\nare at capturing the true relevance of the pixels to the classifier output\n(i.e. their “fidelity”). We therefore investigate existing metrics for\nevaluating the fidelity of saliency methods (i.e. saliency metrics). We find\nthat there is little consistency in the literature in how such metrics are\ncalculated, and show that such inconsistencies can have a significant effect on\nthe measured fidelity. Further, we apply measures of reliability developed in\nthe psychometric testing literature to assess the consistency of saliency\nmetrics when applied to individual saliency maps. Our results show that\nsaliency metrics can be statistically unreliable and inconsistent, indicating\nthat comparative rankings between saliency methods generated using such metrics\ncan be untrustworthy.</p>\n", "tags": [] },
{"key": "tsai2022faithshap", "year": "2022", "title":"Faith-Shap: The Faithful Shapley Interaction Index", "abstract": "<p>Shapley values, which were originally designed to assign attributions to\nindividual players in coalition games, have become a commonly used approach in\nexplainable machine learning to provide attributions to input features for\nblack-box machine learning models. A key attraction of Shapley values is that\nthey uniquely satisfy a very natural set of axiomatic properties. However,\nextending the Shapley value to assigning attributions to interactions rather\nthan individual players, an interaction index, is non-trivial: as the natural\nset of axioms for the original Shapley values, extended to the context of\ninteractions, no longer specify a unique interaction index. Many proposals thus\nintroduce additional less “natural” axioms, while sacrificing the key axiom of\nefficiency, in order to obtain unique interaction indices. In this work, rather\nthan introduce additional conflicting axioms, we adopt the viewpoint of Shapley\nvalues as coefficients of the most faithful linear approximation to the\npseudo-Boolean coalition game value function. By extending linear to\n$\\ell$-order polynomial approximations, we can then define the general family\nof faithful interaction indices}. We show that by additionally requiring the\nfaithful interaction indices to satisfy interaction-extensions of the standard\nindividual Shapley axioms (dummy, symmetry, linearity, and efficiency), we\nobtain a unique FaithfulShapley Interaction index, which we denote Faith-Shap,\nas a natural generalization of the Shapley value to interactions. We then\nprovide some illustrative contrasts of Faith-Shap with previously proposed\ninteraction indices, and further investigate some of its interesting algebraic\nproperties. We further show the computational efficiency of computing\nFaith-Shap, together with some additional qualitative insights, via some\nillustrative experiments.</p>\n", "tags": [] },
{"key": "tuan2021local", "year": "2021", "title":"Local Explanation of Dialogue Response Generation", "abstract": "<p>In comparison to the interpretation of classification models, the explanation\nof sequence generation models is also an important problem, however it has seen\nlittle attention. In this work, we study model-agnostic explanations of a\nrepresentative text generation task – dialogue response generation. Dialog\nresponse generation is challenging with its open-ended sentences and multiple\nacceptable responses. To gain insights into the reasoning process of a\ngeneration model, we propose a new method, local explanation of response\ngeneration (LERG) that regards the explanations as the mutual interaction of\nsegments in input and output sentences. LERG views the sequence prediction as\nuncertainty estimation of a human response and then creates explanations by\nperturbing the input and calculating the certainty change over the human\nresponse. We show that LERG adheres to desired properties of explanations for\ntext generation including unbiased approximation, consistency and cause\nidentification. Empirically, our results show that our method consistently\nimproves other widely used methods on proposed automatic- and human- evaluation\nmetrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can\nextract both explicit and implicit relations between input and output segments.</p>\n", "tags": [] },
{"key": "unceta2019copying", "year": "2019", "title":"Copying Machine Learning Classifiers", "abstract": "<p>We study model-agnostic copies of machine learning classifiers. We develop\nthe theory behind the problem of copying, highlighting its differences with\nthat of learning, and propose a framework to copy the functionality of any\nclassifier using no prior knowledge of its parameters or training data\ndistribution. We identify the different sources of loss and provide guidelines\non how best to generate synthetic sets for the copying process. We further\nintroduce a set of metrics to evaluate copies in practice. We validate our\nframework through extensive experiments using data from a series of well-known\nproblems. We demonstrate the value of copies in use cases where desiderata such\nas interpretability, fairness or productivization constrains need to be\naddressed. Results show that copies can be exploited to enhance existing\nsolutions and improve them adding new features and characteristics.</p>\n", "tags": [] },
{"key": "utkin2021ensembles", "year": "2021", "title":"Ensembles of Random SHAPs", "abstract": "<p>Ensemble-based modifications of the well-known SHapley Additive exPlanations\n(SHAP) method for the local explanation of a black-box model are proposed. The\nmodifications aim to simplify SHAP which is computationally expensive when\nthere is a large number of features. The main idea behind the proposed\nmodifications is to approximate SHAP by an ensemble of SHAPs with a smaller\nnumber of features. According to the first modification, called ER-SHAP,\nseveral features are randomly selected many times from the feature set, and\nShapley values for the features are computed by means of “small” SHAPs. The\nexplanation results are averaged to get the final Shapley values. According to\nthe second modification, called ERW-SHAP, several points are generated around\nthe explained instance for diversity purposes, and results of their explanation\nare combined with weights depending on distances between points and the\nexplained instance. The third modification, called ER-SHAP-RF, uses the random\nforest for preliminary explanation of instances and determining a feature\nprobability distribution which is applied to selection of features in the\nensemble-based procedure of ER-SHAP. Many numerical experiments illustrating\nthe proposed modifications demonstrate their efficiency and properties for\nlocal explanation.</p>\n", "tags": [] },
{"key": "vaishnav2021understanding", "year": "2021", "title":"Understanding the computational demands underlying visual reasoning", "abstract": "<p>Visual understanding requires comprehending complex visual relations between\nobjects within a scene. Here, we seek to characterize the computational demands\nfor abstract visual reasoning. We do this by systematically assessing the\nability of modern deep convolutional neural networks (CNNs) to learn to solve\nthe “Synthetic Visual Reasoning Test” (SVRT) challenge, a collection of\ntwenty-three visual reasoning problems. Our analysis reveals a novel taxonomy\nof visual reasoning tasks, which can be primarily explained by both the type of\nrelations (same-different vs. spatial-relation judgments) and the number of\nrelations used to compose the underlying rules. Prior cognitive neuroscience\nwork suggests that attention plays a key role in humans’ visual reasoning\nability. To test this hypothesis, we extended the CNNs with spatial and\nfeature-based attention mechanisms. In a second series of experiments, we\nevaluated the ability of these attention networks to learn to solve the SVRT\nchallenge and found the resulting architectures to be much more efficient at\nsolving the hardest of these visual reasoning tasks. Most importantly, the\ncorresponding improvements on individual tasks partially explained our novel\ntaxonomy. Overall, this work provides an granular computational account of\nvisual reasoning and yields testable neuroscience predictions regarding the\ndifferential need for feature-based vs. spatial attention depending on the type\nof visual reasoning problem.</p>\n", "tags": [] },
{"key": "vashishth2019attention", "year": "2019", "title":"Attention Interpretability Across NLP Tasks", "abstract": "<p>The attention layer in a neural network model provides insights into the\nmodel’s reasoning behind its prediction, which are usually criticized for being\nopaque. Recently, seemingly contradictory viewpoints have emerged about the\ninterpretability of attention weights (Jain &amp; Wallace, 2019; Vig &amp; Belinkov,\n2019). Amid such confusion arises the need to understand attention mechanism\nmore systematically. In this work, we attempt to fill this gap by giving a\ncomprehensive explanation which justifies both kinds of observations (i.e.,\nwhen is attention interpretable and when it is not). Through a series of\nexperiments on diverse NLP tasks, we validate our observations and reinforce\nour claim of interpretability of attention through manual evaluation.</p>\n", "tags": [] },
{"key": "vaughan2018explainable", "year": "2018", "title":"Explainable Neural Networks based on Additive Index Models", "abstract": "<p>Machine Learning algorithms are increasingly being used in recent years due\nto their flexibility in model fitting and increased predictive performance.\nHowever, the complexity of the models makes them hard for the data analyst to\ninterpret the results and explain them without additional tools. This has led\nto much research in developing various approaches to understand the model\nbehavior. In this paper, we present the Explainable Neural Network (xNN), a\nstructured neural network designed especially to learn interpretable features.\nUnlike fully connected neural networks, the features engineered by the xNN can\nbe extracted from the network in a relatively straightforward manner and the\nresults displayed. With appropriate regularization, the xNN provides a\nparsimonious explanation of the relationship between the features and the\noutput. We illustrate this interpretable feature–engineering property on\nsimulated examples.</p>\n", "tags": [] },
{"key": "vergari2016visualizing", "year": "2016", "title":"Visualizing and Understanding Sum-Product Networks", "abstract": "<p>Sum-Product Networks (SPNs) are recently introduced deep tractable\nprobabilistic models by which several kinds of inference queries can be\nanswered exactly and in a tractable time. Up to now, they have been largely\nused as black box density estimators, assessed only by comparing their\nlikelihood scores only. In this paper we explore and exploit the inner\nrepresentations learned by SPNs. We do this with a threefold aim: first we want\nto get a better understanding of the inner workings of SPNs; secondly, we seek\nadditional ways to evaluate one SPN model and compare it against other\nprobabilistic models, providing diagnostic tools to practitioners; lastly, we\nwant to empirically evaluate how good and meaningful the extracted\nrepresentations are, as in a classic Representation Learning framework. In\norder to do so we revise their interpretation as deep neural networks and we\npropose to exploit several visualization techniques on their node activations\nand network outputs under different types of inference queries. To investigate\nthese models as feature extractors, we plug some SPNs, learned in a greedy\nunsupervised fashion on image datasets, in supervised classification learning\ntasks. We extract several embedding types from node activations by filtering\nnodes by their type, by their associated feature abstraction level and by their\nscope. In a thorough empirical comparison we prove them to be competitive\nagainst those generated from popular feature extractors as Restricted Boltzmann\nMachines. Finally, we investigate embeddings generated from random\nprobabilistic marginal queries as means to compare other tractable\nprobabilistic models on a common ground, extending our experiments to Mixtures\nof Trees.</p>\n", "tags": [] },
{"key": "verma2020counterfactual", "year": "2020", "title":"Counterfactual Explanations for Machine Learning: A Review", "abstract": "<p>Machine learning plays a role in many deployed decision systems, often in\nways that are difficult or impossible to understand by human stakeholders.\nExplaining, in a human-understandable way, the relationship between the input\nand output of machine learning models is essential to the development of\ntrustworthy machine-learning-based systems. A burgeoning body of research seeks\nto define the goals and methods of explainability in machine learning. In this\npaper, we seek to review and categorize research on counterfactual\nexplanations, a specific class of explanation that provides a link between what\ncould have happened had input to a model been changed in a particular way.\nModern approaches to counterfactual explainability in machine learning draw\nconnections to the established legal doctrine in many countries, making them\nappealing to fielded systems in high-impact areas such as finance and\nhealthcare. Thus, we design a rubric with desirable properties of\ncounterfactual explanation algorithms and comprehensively evaluate all\ncurrently-proposed algorithms against that rubric. Our rubric provides easy\ncomparison and comprehension of the advantages and disadvantages of different\napproaches and serves as an introduction to major research themes in this\nfield. We also identify gaps and discuss promising research directions in the\nspace of counterfactual explainability.</p>\n", "tags": [] },
{"key": "verma2021pitfalls", "year": "2021", "title":"Pitfalls of Explainable ML: An Industry Perspective", "abstract": "<p>As machine learning (ML) systems take a more prominent and central role in\ncontributing to life-impacting decisions, ensuring their trustworthiness and\naccountability is of utmost importance. Explanations sit at the core of these\ndesirable attributes of a ML system. The emerging field is frequently called\n<code class=\"language-plaintext highlighter-rouge\">Explainable AI (XAI)'' or</code>Explainable ML.’’ The goal of explainable ML is\nto intuitively explain the predictions of a ML system, while adhering to the\nneeds to various stakeholders. Many explanation techniques were developed with\ncontributions from both academia and industry. However, there are several\nexisting challenges that have not garnered enough interest and serve as\nroadblocks to widespread adoption of explainable ML. In this short paper, we\nenumerate challenges in explainable ML from an industry perspective. We hope\nthese challenges will serve as promising future research directions, and would\ncontribute to democratizing explainable ML.</p>\n", "tags": [] },
{"key": "vijayaraghavan2021interpretable", "year": "2021", "title":"Interpretable Multi-Modal Hate Speech Detection", "abstract": "<p>With growing role of social media in shaping public opinions and beliefs\nacross the world, there has been an increased attention to identify and counter\nthe problem of hate speech on social media. Hate speech on online spaces has\nserious manifestations, including social polarization and hate crimes. While\nprior works have proposed automated techniques to detect hate speech online,\nthese techniques primarily fail to look beyond the textual content. Moreover,\nfew attempts have been made to focus on the aspects of interpretability of such\nmodels given the social and legal implications of incorrect predictions. In\nthis work, we propose a deep neural multi-modal model that can: (a) detect hate\nspeech by effectively capturing the semantics of the text along with\nsocio-cultural context in which a particular hate expression is made, and (b)\nprovide interpretable insights into decisions of our model. By performing a\nthorough evaluation of different modeling techniques, we demonstrate that our\nmodel is able to outperform the existing state-of-the-art hate speech\nclassification approaches. Finally, we show the importance of social and\ncultural context features towards unearthing clusters associated with different\ncategories of hate.</p>\n", "tags": [] },
{"key": "vivek2018graybox", "year": "2018", "title":"Gray-box Adversarial Training", "abstract": "<p>Adversarial samples are perturbed inputs crafted to mislead the machine\nlearning systems. A training mechanism, called adversarial training, which\npresents adversarial samples along with clean samples has been introduced to\nlearn robust models. In order to scale adversarial training for large datasets,\nthese perturbations can only be crafted using fast and simple methods (e.g.,\ngradient ascent). However, it is shown that adversarial training converges to a\ndegenerate minimum, where the model appears to be robust by generating weaker\nadversaries. As a result, the models are vulnerable to simple black-box\nattacks. In this paper we, (i) demonstrate the shortcomings of existing\nevaluation policy, (ii) introduce novel variants of white-box and black-box\nattacks, dubbed gray-box adversarial attacks” based on which we propose novel\nevaluation method to assess the robustness of the learned models, and (iii)\npropose a novel variant of adversarial training, named Graybox Adversarial\nTraining” that uses intermediate versions of the models to seed the\nadversaries. Experimental evaluation demonstrates that the models trained using\nour method exhibit better robustness compared to both undefended and\nadversarially trained model</p>\n", "tags": [] },
{"key": "voita2019analyzing", "year": "2019", "title":"Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned", "abstract": "<p>Multi-head self-attention is a key component of the Transformer, a\nstate-of-the-art architecture for neural machine translation. In this work we\nevaluate the contribution made by individual attention heads in the encoder to\nthe overall performance of the model and analyze the roles played by them. We\nfind that the most important and confident heads play consistent and often\nlinguistically-interpretable roles. When pruning heads using a method based on\nstochastic gates and a differentiable relaxation of the L0 penalty, we observe\nthat specialized heads are last to be pruned. Our novel pruning method removes\nthe vast majority of heads without seriously affecting performance. For\nexample, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads\nresults in a drop of only 0.15 BLEU.</p>\n", "tags": [] },
{"key": "wallace2018interpreting", "year": "2018", "title":"Interpreting Neural Networks With Nearest Neighbors", "abstract": "<p>Local model interpretation methods explain individual predictions by\nassigning an importance value to each input feature. This value is often\ndetermined by measuring the change in confidence when a feature is removed.\nHowever, the confidence of neural networks is not a robust measure of model\nuncertainty. This issue makes reliably judging the importance of the input\nfeatures difficult. We address this by changing the test-time behavior of\nneural networks using Deep k-Nearest Neighbors. Without harming text\nclassification accuracy, this algorithm provides a more robust uncertainty\nmetric which we use to generate feature importance values. The resulting\ninterpretations better align with human perception than baseline methods.\nFinally, we use our interpretation method to analyze model predictions on\ndataset annotation artifacts.</p>\n", "tags": [] },
{"key": "wang2016time", "year": "2016", "title":"Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline", "abstract": "<p>We propose a simple but strong baseline for time series classification from\nscratch with deep neural networks. Our proposed baseline models are pure\nend-to-end without any heavy preprocessing on the raw data or feature crafting.\nThe proposed Fully Convolutional Network (FCN) achieves premium performance to\nother state-of-the-art approaches and our exploration of the very deep neural\nnetworks with the ResNet structure is also competitive. The global average\npooling in our convolutional model enables the exploitation of the Class\nActivation Map (CAM) to find out the contributing region in the raw data for\nthe specific labels. Our models provides a simple choice for the real world\napplication and a good starting point for the future research. An overall\nanalysis is provided to discuss the generalization capability of our models,\nlearned features, network structures and the classification semantics.</p>\n", "tags": [] },
{"key": "wang2017deep", "year": "2017", "title":"Deep Visual Attention Prediction", "abstract": "<p>In this work, we aim to predict human eye fixation with view-free scenes\nbased on an end-to-end deep learning architecture. Although Convolutional\nNeural Networks (CNNs) have made substantial improvement on human attention\nprediction, it is still needed to improve CNN based attention models by\nefficiently leveraging multi-scale features. Our visual attention network is\nproposed to capture hierarchical saliency information from deep, coarse layers\nwith global saliency information to shallow, fine layers with local saliency\nresponse. Our model is based on a skip-layer network structure, which predicts\nhuman attention from multiple convolutional layers with various reception\nfields. Final saliency prediction is achieved via the cooperation of those\nglobal and local predictions. Our model is learned in a deep supervision\nmanner, where supervision is directly fed into multi-level layers, instead of\nprevious approaches of providing supervision only at the output layer and\npropagating this supervision back to earlier layers. Our model thus\nincorporates multi-level saliency predictions within a single network, which\nsignificantly decreases the redundancy of previous approaches of learning\nmultiple network streams with different input scales. Extensive experimental\nanalysis on various challenging benchmark datasets demonstrate our method\nyields state-of-the-art performance with competitive inference time.</p>\n", "tags": [] },
{"key": "wang2018comparative", "year": "2018", "title":"A Comparative Study of Rule Extraction for Recurrent Neural Networks", "abstract": "<p>Understanding recurrent networks through rule extraction has a long history.\nThis has taken on new interests due to the need for interpreting or verifying\nneural networks. One basic form for representing stateful rules is\ndeterministic finite automata (DFA). Previous research shows that extracting\nDFAs from trained second-order recurrent networks is not only possible but also\nrelatively stable. Recently, several new types of recurrent networks with more\ncomplicated architectures have been introduced. These handle challenging\nlearning tasks usually involving sequential data. However, it remains an open\nproblem whether DFAs can be adequately extracted from these models.\nSpecifically, it is not clear how DFA extraction will be affected when applied\nto different recurrent networks trained on data sets with different levels of\ncomplexity. Here, we investigate DFA extraction on several widely adopted\nrecurrent networks that are trained to learn a set of seven regular Tomita\ngrammars. We first formally analyze the complexity of Tomita grammars and\ncategorize these grammars according to that complexity. Then we empirically\nevaluate different recurrent networks for their performance of DFA extraction\non all Tomita grammars. Our experiments show that for most recurrent networks,\ntheir extraction performance decreases as the complexity of the underlying\ngrammar increases. On grammars of lower complexity, most recurrent networks\nobtain desirable extraction performance. As for grammars with the highest level\nof complexity, while several complicated models fail with only certain\nrecurrent networks having satisfactory extraction performance.</p>\n", "tags": [] },
{"key": "wang2018multilevel", "year": "2018", "title":"Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis", "abstract": "<p>Recent years have witnessed the unprecedented rising of time series from\nalmost all kindes of academic and industrial fields. Various types of deep\nneural network models have been introduced to time series analysis, but the\nimportant frequency information is yet lack of effective modeling. In light of\nthis, in this paper we propose a wavelet-based neural network structure called\nmultilevel Wavelet Decomposition Network (mWDN) for building frequency-aware\ndeep learning models for time series analysis. mWDN preserves the advantage of\nmultilevel discrete wavelet decomposition in frequency learning while enables\nthe fine-tuning of all parameters under a deep neural network framework. Based\non mWDN, we further propose two deep learning models called Residual\nClassification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for\ntime series classification and forecasting, respectively. The two models take\nall or partial mWDN decomposed sub-series in different frequencies as input,\nand resort to the back propagation algorithm to learn all the parameters\nglobally, which enables seamless embedding of wavelet-based frequency analysis\ninto deep learning frameworks. Extensive experiments on 40 UCR datasets and a\nreal-world user volume dataset demonstrate the excellent performance of our\ntime series models based on mWDN. In particular, we propose an importance\nanalysis method to mWDN based models, which successfully identifies those\ntime-series elements and mWDN layers that are crucially important to time\nseries analysis. This indeed indicates the interpretability advantage of mWDN,\nand can be viewed as an indepth exploration to interpretable deep learning.</p>\n", "tags": [] },
{"key": "wang2019learning", "year": "2019", "title":"Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization", "abstract": "<p>Times series classification can be successfully tackled by jointly learning a\nshapelet-based representation of the series in the dataset and classifying the\nseries according to this representation. However, although the learned\nshapelets are discriminative, they are not always similar to pieces of a real\nseries in the dataset. This makes it difficult to interpret the decision, i.e.\ndifficult to analyze if there are particular behaviors in a series that\ntriggered the decision. In this paper, we make use of a simple convolutional\nnetwork to tackle the time series classification task and we introduce an\nadversarial regularization to constrain the model to learn more interpretable\nshapelets. Our classification results on all the usual time series benchmarks\nare comparable with the results obtained by similar state-of-the-art algorithms\nbut our adversarially regularized method learns shapelets that are, by design,\ninterpretable.</p>\n", "tags": [] },
{"key": "wang2020interpreting", "year": "2020", "title":"Interpreting Interpretations: Organizing Attribution Methods by Criteria", "abstract": "<p>Motivated by distinct, though related, criteria, a growing number of\nattribution methods have been developed tointerprete deep learning. While each\nrelies on the interpretability of the concept of “importance” and our ability\nto visualize patterns, explanations produced by the methods often differ. As a\nresult, input attribution for vision models fail to provide any level of human\nunderstanding of model behaviour. In this work we expand the foundationsof\nhuman-understandable concepts with which attributionscan be interpreted beyond\n“importance” and its visualization; we incorporate the logical concepts of\nnecessity andsufficiency, and the concept of proportionality. We definemetrics\nto represent these concepts as quantitative aspectsof an attribution. This\nallows us to compare attributionsproduced by different methods and interpret\nthem in novelways: to what extent does this attribution (or this\nmethod)represent the necessity or sufficiency of the highlighted inputs, and to\nwhat extent is it proportional? We evaluate our measures on a collection of\nmethods explaining convolutional neural networks (CNN) for image\nclassification. We conclude that some attribution methods are more appropriate\nfor interpretation in terms of necessity while others are in terms of\nsufficiency, while no method is always the most appropriate in terms of both.</p>\n", "tags": [] },
{"key": "wang2021fast", "year": "2021", "title":"On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning", "abstract": "<p>Model-agnostic meta-learning (MAML) has emerged as one of the most successful\nmeta-learning techniques in few-shot learning. It enables us to learn a\nmeta-initialization} of model parameters (that we call meta-model) to rapidly\nadapt to new tasks using a small amount of labeled training data. Despite the\ngeneralization power of the meta-model, it remains elusive that how adversarial\nrobustness can be maintained by MAML in few-shot learning. In addition to\ngeneralization, robustness is also desired for a meta-model to defend\nadversarial examples (attacks). Toward promoting adversarial robustness in\nMAML, we first study WHEN a robustness-promoting regularization should be\nincorporated, given the fact that MAML adopts a bi-level (fine-tuning vs.\nmeta-update) learning procedure. We show that robustifying the meta-update\nstage is sufficient to make robustness adapted to the task-specific fine-tuning\nstage even if the latter uses a standard training protocol. We also make\nadditional justification on the acquired robustness adaptation by peering into\nthe interpretability of neurons’ activation maps. Furthermore, we investigate\nHOW robust regularization can efficiently be designed in MAML. We propose a\ngeneral but easily-optimized robustness-regularized meta-learning framework,\nwhich allows the use of unlabeled data augmentation, fast adversarial attack\ngeneration, and computationally-light fine-tuning. In particular, we for the\nfirst time show that the auxiliary contrastive learning task can enhance the\nadversarial robustness of MAML. Finally, extensive experiments are conducted to\ndemonstrate the effectiveness of our proposed methods in robust few-shot\nlearning.</p>\n", "tags": [] },
{"key": "warnecke2019evaluating", "year": "2019", "title":"Evaluating Explanation Methods for Deep Learning in Security", "abstract": "<p>Deep learning is increasingly used as a building block of security systems.\nUnfortunately, neural networks are hard to interpret and typically opaque to\nthe practitioner. The machine learning community has started to address this\nproblem by developing methods for explaining the predictions of neural\nnetworks. While several of these approaches have been successfully applied in\nthe area of computer vision, their application in security has received little\nattention so far. It is an open question which explanation methods are\nappropriate for computer security and what requirements they need to satisfy.\nIn this paper, we introduce criteria for comparing and evaluating explanation\nmethods in the context of computer security. These cover general properties,\nsuch as the accuracy of explanations, as well as security-focused aspects, such\nas the completeness, efficiency, and robustness. Based on our criteria, we\ninvestigate six popular explanation methods and assess their utility in\nsecurity systems for malware detection and vulnerability discovery. We observe\nsignificant differences between the methods and build on these to derive\ngeneral recommendations for selecting and applying explanation methods in\ncomputer security.</p>\n", "tags": [] },
{"key": "weller2019transparency", "year": "2019", "title":"Transparency: Motivations and Challenges", "abstract": "<p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>\n", "tags": ["evaluation"] },
{"key": "wickstrøm2021relax", "year": "2021", "title":"RELAX: Representation Learning Explainability", "abstract": "<p>Despite the significant improvements that representation learning via\nself-supervision has led to when learning from unlabeled data, no methods exist\nthat explain what influences the learned representation. We address this need\nthrough our proposed approach, RELAX, which is the first approach for\nattribution-based explanations of representations. Our approach can also model\nthe uncertainty in its explanations, which is essential to produce trustworthy\nexplanations. RELAX explains representations by measuring similarities in the\nrepresentation space between an input and masked out versions of itself,\nproviding intuitive explanations and significantly outperforming the\ngradient-based baseline. We provide theoretical interpretations of RELAX and\nconduct a novel analysis of feature extractors trained using supervised and\nunsupervised learning, providing insights into different learning strategies.\nFinally, we illustrate the usability of RELAX in multi-view clustering and\nhighlight that incorporating uncertainty can be essential for providing\nlow-complexity explanations, taking a crucial step towards explaining\nrepresentations.</p>\n", "tags": [] },
{"key": "wiegreffe2019attention", "year": "2019", "title":"Attention is not not Explanation", "abstract": "<p>Attention mechanisms play a central role in NLP systems, especially within\nrecurrent neural network (RNN) models. Recently, there has been increasing\ninterest in whether or not the intermediate representations offered by these\nmodules may be used to explain the reasoning for a model’s prediction, and\nconsequently reach insights regarding the model’s decision-making process. A\nrecent paper claims that `Attention is not Explanation’ (Jain and Wallace,\n2019). We challenge many of the assumptions underlying this work, arguing that\nsuch a claim depends on one’s definition of explanation, and that testing it\nneeds to take into account all elements of the model, using a rigorous\nexperimental design. We propose four alternative tests to determine\nwhen/whether attention can be used as explanation: a simple uniform-weights\nbaseline; a variance calibration based on multiple random seed runs; a\ndiagnostic framework using frozen weights from pretrained models; and an\nend-to-end adversarial attention training protocol. Each allows for meaningful\ninterpretation of attention mechanisms in RNN models. We show that even when\nreliable adversarial distributions can be found, they don’t perform well on the\nsimple diagnostic, indicating that prior work does not disprove the usefulness\nof attention mechanisms for explainability.</p>\n", "tags": [] },
{"key": "wiegreffe2021teach", "year": "2021", "title":"Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing", "abstract": "<p>Explainable NLP (ExNLP) has increasingly focused on collecting\nhuman-annotated textual explanations. These explanations are used downstream in\nthree ways: as data augmentation to improve performance on a predictive task,\nas supervision to train models to produce explanations for their predictions,\nand as a ground-truth to evaluate model-generated explanations. In this review,\nwe identify 65 datasets with three predominant classes of textual explanations\n(highlights, free-text, and structured), organize the literature on annotating\neach type, identify strengths and shortcomings of existing collection\nmethodologies, and give recommendations for collecting ExNLP datasets in the\nfuture.</p>\n", "tags": [] },
{"key": "wisdom2016interpretable", "year": "2016", "title":"Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery", "abstract": "<p>Recurrent neural networks (RNNs) are powerful and effective for processing\nsequential data. However, RNNs are usually considered “black box” models whose\ninternal structure and learned parameters are not interpretable. In this paper,\nwe propose an interpretable RNN based on the sequential iterative\nsoft-thresholding algorithm (SISTA) for solving the sequential sparse recovery\nproblem, which models a sequence of correlated observations with a sequence of\nsparse latent vectors. The architecture of the resulting SISTA-RNN is\nimplicitly defined by the computational structure of SISTA, which results in a\nnovel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are\nperfectly interpretable as the parameters of a principled statistical model,\nwhich in this case include a sparsifying dictionary, iterative step size, and\nregularization parameters. In addition, on a particular sequential compressive\nsensing task, the SISTA-RNN trains faster and achieves better performance than\nconventional state-of-the-art black box RNNs, including long-short term memory\n(LSTM) RNNs.</p>\n", "tags": [] },
{"key": "woodcock2022impact", "year": "2022", "title":"The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study", "abstract": "<p>To achieve the promoted benefits of an AI symptom checker, laypeople must\ntrust and subsequently follow its instructions. In AI, explanations are seen as\na tool to communicate the rationale behind black-box decisions to encourage\ntrust and adoption. However, the effectiveness of the types of explanations\nused in AI-driven symptom checkers has not yet been studied. Social theories\nsuggest that why-explanations are better at communicating knowledge and\ncultivating trust among laypeople. This study ascertains whether explanations\nprovided by a symptom checker affect explanatory trust among laypeople (N=750)\nand whether this trust is impacted by their existing knowledge of disease.\n  Results suggest system builders developing explanations for symptom-checking\napps should consider the recipient’s knowledge of a disease and tailor\nexplanations to each user’s specific need. Effort should be placed on\ngenerating explanations that are personalized to each user of a symptom checker\nto fully discount the diseases that they may be aware of and to close their\ninformation gap.</p>\n", "tags": [] },
{"key": "wu2017beyond", "year": "2017", "title":"Beyond Sparsity: Tree Regularization of Deep Models for Interpretability", "abstract": "<p>The lack of interpretability remains a key barrier to the adoption of deep\nmodels in many applications. In this work, we explicitly regularize deep models\nso human users might step through the process behind their predictions in\nlittle time. Specifically, we train deep time-series models so their\nclass-probability predictions have high accuracy while being closely modeled by\ndecision trees with few nodes. Using intuitive toy examples as well as medical\ntasks for treating sepsis and HIV, we demonstrate that this new tree\nregularization yields models that are easier for humans to simulate than\nsimpler L1 or L2 penalties without sacrificing predictive power.</p>\n", "tags": [] },
{"key": "wu2017towards", "year": "2017", "title":"Towards Interpretable R-CNN by Unfolding Latent Structures", "abstract": "<p>This paper first proposes a method of formulating model interpretability in\nvisual understanding tasks based on the idea of unfolding latent structures. It\nthen presents a case study in object detection using popular two-stage\nregion-based convolutional network (i.e., R-CNN) detection systems. We focus on\nweakly-supervised extractive rationale generation, that is learning to unfold\nlatent discriminative part configurations of object instances automatically and\nsimultaneously in detection without using any supervision for part\nconfigurations. We utilize a top-down hierarchical and compositional grammar\nmodel embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold\nthe space of latent part configurations of regions of interest (RoIs). We\npropose an AOGParsing operator to substitute the RoIPooling operator widely\nused in R-CNN. In detection, a bounding box is interpreted by the best parse\ntree derived from the AOG on-the-fly, which is treated as the qualitatively\nextractive rationale generated for interpreting detection. We propose a\nfolding-unfolding method to train the AOG and convolutional networks\nend-to-end. In experiments, we build on R-FCN and test our method on the PASCAL\nVOC 2007 and 2012 datasets. We show that the method can unfold promising latent\nstructures without hurting the performance.</p>\n", "tags": [] },
{"key": "xie2022towards", "year": "2022", "title":"Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning", "abstract": "<p>Artificial intelligence, particularly through recent advancements in deep\nlearning, has achieved exceptional performances in many tasks in fields such as\nnatural language processing and computer vision. In addition to desirable\nevaluation metrics, a high level of interpretability is often required for\nthese models to be reliably utilized. Therefore, explanations that offer\ninsight into the process by which a model maps its inputs onto its outputs are\nmuch sought-after. Unfortunately, current black box nature of machine learning\nmodels is still an unresolved issue and this very nature prevents researchers\nfrom learning and providing explicative descriptions for a model’s behavior and\nfinal predictions. In this work, we propose a novel framework utilizing\nAdversarial Inverse Reinforcement Learning that can provide global explanations\nfor decisions made by a Reinforcement Learning model and capture intuitive\ntendencies that the model follows by summarizing the model’s decision-making\nprocess.</p>\n", "tags": [] },
{"key": "yadav2018sanity", "year": "2018", "title":"Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering", "abstract": "<p>While increasingly complex approaches to question answering (QA) have been\nproposed, the true gain of these systems, particularly with respect to their\nexpensive training requirements, can be inflated when they are not compared to\nadequate baselines. Here we propose an unsupervised, simple, and fast alignment\nand information retrieval baseline that incorporates two novel contributions: a\n\\textit{one-to-many alignment} between query and document terms and\n\\textit{negative alignment} as a proxy for discriminative information. Our\napproach not only outperforms all conventional baselines as well as many\nsupervised recurrent neural networks, but also approaches the state of the art\nfor supervised systems on three QA datasets. With only three hyperparameters,\nwe achieve 47\\% P@1 on an 8th grade Science QA dataset, 32.9\\% P@1 on a Yahoo!\nanswers QA dataset and 64\\% MAP on WikiQA. We also achieve 26.56\\% and 58.36\\%\non ARC challenge and easy dataset respectively. In addition to including the\nadditional ARC results in this version of the paper, for the ARC easy set only\nwe also experimented with one additional parameter – number of justifications\nretrieved.</p>\n", "tags": [] },
{"key": "yadav2021behavior", "year": "2021", "title":"Behavior of k-NN as an Instance-Based Explanation Method", "abstract": "<p>Adoption of DL models in critical areas has led to an escalating demand for\nsound explanation methods. Instance-based explanation methods are a popular\ntype that return selective instances from the training set to explain the\npredictions for a test sample. One way to connect these explanations with\nprediction is to ask the following counterfactual question - how does the loss\nand prediction for a test sample change when explanations are removed from the\ntraining set? Our paper answers this question for k-NNs which are natural\ncontenders for an instance-based explanation method. We first demonstrate\nempirically that the representation space induced by last layer of a neural\nnetwork is the best to perform k-NN in. Using this layer, we conduct our\nexperiments and compare them to influence functions (IFs)\n~\\cite{koh2017understanding} which try to answer a similar question. Our\nevaluations do indicate change in loss and predictions when explanations are\nremoved but we do not find a trend between $k$ and loss or prediction change.\nWe find significant stability in the predictions and loss of MNIST vs.\nCIFAR-10. Surprisingly, we do not observe much difference in the behavior of\nk-NNs vs. IFs on this question. We attribute this to training set subsampling\nfor IFs.</p>\n", "tags": [] },
{"key": "yadav2022learningtheoretic", "year": "2022", "title":"A Learning-Theoretic Framework for Certified Auditing of Machine Learning Models", "abstract": "<p>Responsible use of machine learning requires that models be audited for\nundesirable properties. However, how to do principled auditing in a general\nsetting has remained ill-understood. In this paper, we propose a formal\nlearning-theoretic framework for auditing. We propose algorithms for auditing\nlinear classifiers for feature sensitivity using label queries as well as\ndifferent kinds of explanations, and provide performance guarantees. Our\nresults illustrate that while counterfactual explanations can be extremely\nhelpful for auditing, anchor explanations may not be as beneficial in the worst\ncase.</p>\n", "tags": [] },
{"key": "yang2019benchmarking", "year": "2019", "title":"Benchmarking Attribution Methods with Relative Feature Importance", "abstract": "<p>Interpretability is an important area of research for safe deployment of\nmachine learning systems. One particular type of interpretability method\nattributes model decisions to input features. Despite active development,\nquantitative evaluation of feature attribution methods remains difficult due to\nthe lack of ground truth: we do not know which input features are in fact\nimportant to a model. In this work, we propose a framework for Benchmarking\nAttribution Methods (BAM) with a priori knowledge of relative feature\nimportance. BAM includes 1) a carefully crafted dataset and models trained with\nknown relative feature importance and 2) three complementary metrics to\nquantitatively evaluate attribution methods by comparing feature attributions\nbetween pairs of models and pairs of inputs. Our evaluation on several\nwidely-used attribution methods suggests that certain methods are more likely\nto produce false positive explanations—features that are incorrectly\nattributed as more important to model prediction. We open source our dataset,\nmodels, and metrics.</p>\n", "tags": [] },
{"key": "yang2021exploring", "year": "2021", "title":"Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis", "abstract": "<p>While state-of-the-art NLP models have been achieving the excellent\nperformance of a wide range of tasks in recent years, important questions are\nbeing raised about their robustness and their underlying sensitivity to\nsystematic biases that may exist in their training and test data. Such issues\ncome to be manifest in performance problems when faced with out-of-distribution\ndata in the field. One recent solution has been to use counterfactually\naugmented datasets in order to reduce any reliance on spurious patterns that\nmay exist in the original data. Producing high-quality augmented data can be\ncostly and time-consuming as it usually needs to involve human feedback and\ncrowdsourcing efforts. In this work, we propose an alternative by describing\nand evaluating an approach to automatically generating counterfactual data for\ndata augmentation and explanation. A comprehensive evaluation on several\ndifferent datasets and using a variety of state-of-the-art benchmarks\ndemonstrate how our approach can achieve significant improvements in model\nperformance when compared to models training on the original data and even when\ncompared to models trained with the benefit of human-generated augmented data.</p>\n", "tags": [] },
{"key": "yang2022psychological", "year": "2022", "title":"A Psychological Theory of Explainability", "abstract": "<p>The goal of explainable Artificial Intelligence (XAI) is to generate\nhuman-interpretable explanations, but there are no computationally precise\ntheories of how humans interpret AI generated explanations. The lack of theory\nmeans that validation of XAI must be done empirically, on a case-by-case basis,\nwhich prevents systematic theory-building in XAI. We propose a psychological\ntheory of how humans draw conclusions from saliency maps, the most common form\nof XAI explanation, which for the first time allows for precise prediction of\nexplainee inference conditioned on explanation. Our theory posits that absent\nexplanation humans expect the AI to make similar decisions to themselves, and\nthat they interpret an explanation by comparison to the explanations they\nthemselves would give. Comparison is formalized via Shepard’s universal law of\ngeneralization in a similarity space, a classic theory from cognitive science.\nA pre-registered user study on AI image classifications with saliency map\nexplanations demonstrate that our theory quantitatively matches participants’\npredictions of the AI.</p>\n", "tags": [] },
{"key": "yang2022robust", "year": "2022", "title":"On Robust Prefix-Tuning for Text Classification", "abstract": "<p>Recently, prefix-tuning has gained increasing attention as a\nparameter-efficient finetuning method for large-scale pretrained language\nmodels. The method keeps the pretrained models fixed and only updates the\nprefix token parameters for each downstream task. Despite being lightweight and\nmodular, prefix-tuning still lacks robustness to textual adversarial attacks.\nHowever, most currently developed defense techniques necessitate auxiliary\nmodel update and storage, which inevitably hamper the modularity and low\nstorage of prefix-tuning. In this work, we propose a robust prefix-tuning\nframework that preserves the efficiency and modularity of prefix-tuning. The\ncore idea of our framework is leveraging the layerwise activations of the\nlanguage model by correctly-classified training data as the standard for\nadditional prefix finetuning. During the test phase, an extra batch-level\nprefix is tuned for each batch and added to the original prefix for robustness\nenhancement. Extensive experiments on three text classification benchmarks show\nthat our framework substantially improves robustness over several strong\nbaselines against five textual attacks of different types while maintaining\ncomparable accuracy on clean texts. We also interpret our robust prefix-tuning\nframework from the optimal control perspective and pose several directions for\nfuture research.</p>\n", "tags": [] },
{"key": "yao2021explanatory", "year": "2021", "title":"Explanatory Pluralism in Explainable AI", "abstract": "<p>The increasingly widespread application of AI models motivates increased\ndemand for explanations from a variety of stakeholders. However, this demand is\nambiguous because there are many types of ‘explanation’ with different\nevaluative criteria. In the spirit of pluralism, I chart a taxonomy of types of\nexplanation and the associated XAI methods that can address them. When we look\nto expose the inner mechanisms of AI models, we develop\nDiagnostic-explanations. When we seek to render model output understandable, we\nproduce Explication-explanations. When we wish to form stable generalizations\nof our models, we produce Expectation-explanations. Finally, when we want to\njustify the usage of a model, we produce Role-explanations that situate models\nwithin their social context. The motivation for such a pluralistic view stems\nfrom a consideration of causes as manipulable relationships and the different\ntypes of explanations as identifying the relevant points in AI systems we can\nintervene upon to affect our desired changes. This paper reduces the ambiguity\nin use of the word ‘explanation’ in the field of XAI, allowing practitioners\nand stakeholders a useful template for avoiding equivocation and evaluating XAI\nmethods and putative explanations.</p>\n", "tags": [] },
{"key": "ye2021explanations", "year": "2021", "title":"Can Explanations Be Useful for Calibrating Black Box Models?", "abstract": "<p>NLP practitioners often want to take existing trained models and apply them\nto data from new domains. While fine-tuning or few-shot learning can be used to\nadapt a base model, there is no single recipe for making these techniques work;\nmoreover, one may not have access to the original model weights if it is\ndeployed as a black box. We study how to improve a black box model’s\nperformance on a new domain by leveraging explanations of the model’s behavior.\nOur approach first extracts a set of features combining human intuition about\nthe task with model attributions generated by black box interpretation\ntechniques, then uses a simple calibrator, in the form of a classifier, to\npredict whether the base model was correct or not. We experiment with our\nmethod on two tasks, extractive question answering and natural language\ninference, covering adaptation from several pairs of domains with limited\ntarget-domain data. The experimental results across all the domain pairs show\nthat explanations are useful for calibrating these models, boosting accuracy\nwhen predictions do not have to be returned on every example. We further show\nthat the calibration model transfers to some extent between tasks.</p>\n", "tags": [] },
{"key": "yeh2018representer", "year": "2018", "title":"Representer Point Selection for Explaining Deep Neural Networks", "abstract": "<p>We propose to explain the predictions of a deep neural network, by pointing\nto the set of what we call representer points in the training set, for a given\ntest point prediction. Specifically, we show that we can decompose the\npre-activation prediction of a neural network into a linear combination of\nactivations of training points, with the weights corresponding to what we call\nrepresenter values, which thus capture the importance of that training point on\nthe learned parameters of the network. But it provides a deeper understanding\nof the network than simply training point influence: with positive representer\nvalues corresponding to excitatory training points, and negative values\ncorresponding to inhibitory points, which as we show provides considerably more\ninsight. Our method is also much more scalable, allowing for real-time feedback\nin a manner not feasible with influence functions.</p>\n", "tags": [] },
{"key": "yeh2019infidelity", "year": "2019", "title":"On the (In)fidelity and Sensitivity for Explanations", "abstract": "<p>We consider objective evaluation measures of saliency explanations for\ncomplex black-box machine learning models. We propose simple robust variants of\ntwo notions that have been considered in recent literature: (in)fidelity, and\nsensitivity. We analyze optimal explanations with respect to both these\nmeasures, and while the optimal explanation for sensitivity is a vacuous\nconstant explanation, the optimal explanation for infidelity is a novel\ncombination of two popular explanation methods. By varying the perturbation\ndistribution that defines infidelity, we obtain novel explanations by\noptimizing infidelity, which we show to out-perform existing explanations in\nboth quantitative and qualitative measurements. Another salient question given\nthese measures is how to modify any given explanation to have better values\nwith respect to these measures. We propose a simple modification based on\nlowering sensitivity, and moreover show that when done appropriately, we could\nsimultaneously improve both sensitivity as well as fidelity.</p>\n", "tags": [] },
{"key": "yin2021sensitivity", "year": "2021", "title":"On the Sensitivity and Stability of Model Interpretations in NLP", "abstract": "<p>Recent years have witnessed the emergence of a variety of post-hoc\ninterpretations that aim to uncover how natural language processing (NLP)\nmodels make predictions. Despite the surge of new interpretation methods, it\nremains an open problem how to define and quantitatively measure the\nfaithfulness of interpretations, i.e., to what extent interpretations reflect\nthe reasoning process by a model. We propose two new criteria, sensitivity and\nstability, that provide complementary notions of faithfulness to the existed\nremoval-based criteria. Our results show that the conclusion for how faithful\ninterpretations are could vary substantially based on different notions.\nMotivated by the desiderata of sensitivity and stability, we introduce a new\nclass of interpretation methods that adopt techniques from adversarial\nrobustness. Empirical results show that our proposed methods are effective\nunder the new criteria and overcome limitations of gradient-based methods on\nremoval-based criteria. Besides text classification, we also apply\ninterpretation methods and metrics to dependency parsing. Our results shed\nlight on understanding the diverse set of interpretations.</p>\n", "tags": [] },
{"key": "yona2021revisiting", "year": "2021", "title":"Revisiting Sanity Checks for Saliency Maps", "abstract": "<p>Saliency methods are a popular approach for model debugging and\nexplainability. However, in the absence of ground-truth data for what the\ncorrect maps should be, evaluating and comparing different approaches remains a\nlong-standing challenge. The sanity checks methodology of Adebayo et al\n[Neurips 2018] has sought to address this challenge. They argue that some\npopular saliency methods should not be used for explainability purposes since\nthe maps they produce are not sensitive to the underlying model that is to be\nexplained. Through a causal re-framing of their objective, we argue that their\nempirical evaluation does not fully establish these conclusions, due to a form\nof confounding introduced by the tasks they evaluate on. Through various\nexperiments on simple custom tasks we demonstrate that some of their\nconclusions may indeed be artifacts of the tasks more than a criticism of the\nsaliency methods themselves. More broadly, our work challenges the utility of\nthe sanity check methodology, and further highlights that saliency map\nevaluation beyond ad-hoc visual examination remains a fundamental challenge.</p>\n", "tags": [] },
{"key": "yosinski2014transferable", "year": "2014", "title":"How transferable are features in deep neural networks?", "abstract": "<p>Many deep neural networks trained on natural images exhibit a curious\nphenomenon in common: on the first layer they learn features similar to Gabor\nfilters and color blobs. Such first-layer features appear not to be specific to\na particular dataset or task, but general in that they are applicable to many\ndatasets and tasks. Features must eventually transition from general to\nspecific by the last layer of the network, but this transition has not been\nstudied extensively. In this paper we experimentally quantify the generality\nversus specificity of neurons in each layer of a deep convolutional neural\nnetwork and report a few surprising results. Transferability is negatively\naffected by two distinct issues: (1) the specialization of higher layer neurons\nto their original task at the expense of performance on the target task, which\nwas expected, and (2) optimization difficulties related to splitting networks\nbetween co-adapted neurons, which was not expected. In an example network\ntrained on ImageNet, we demonstrate that either of these two issues may\ndominate, depending on whether features are transferred from the bottom,\nmiddle, or top of the network. We also document that the transferability of\nfeatures decreases as the distance between the base task and target task\nincreases, but that transferring features even from distant tasks can be better\nthan using random features. A final surprising result is that initializing a\nnetwork with transferred features from almost any number of layers can produce\na boost to generalization that lingers even after fine-tuning to the target\ndataset.</p>\n", "tags": [] },
{"key": "yosinski2015understanding", "year": "2015", "title":"Understanding Neural Networks Through Deep Visualization", "abstract": "<p>Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.</p>\n", "tags": [] },
{"key": "you2020large-scale", "year": "2020", "title":"Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study", "abstract": "<p>The quality and accuracy of power system models is critical for\nsimulation-based studies, especially for studying actual stability issues in\nlarge-scale systems. With the deployment of wide-area monitoring systems\n(WAMSs), the high-reporting-rate frequency measurement provides a trustworthy\nground truth for validating models in frequency response studies. This paper\ndocumented an effort to check, tune, and validate the U.S. power system model\nbased on a WAMS called FNET/GridEye. Four metrics are used to quantitatively\ncompare the simulation results and the actual measurement, including frequency\nnadir, RoCoF, settling frequency and settling time. After tuning governor\ndeadband and the governor ratio, the model frequency response shows significant\nimprovement and matches well with the event measurement data. This work serves\nas an example for tuning and validating large-scale power system models.</p>\n", "tags": [] },
{"key": "yousefzadeh2019interpreting", "year": "2019", "title":"Interpreting Neural Networks Using Flip Points", "abstract": "<p>Neural networks have been criticized for their lack of easy interpretation,\nwhich undermines confidence in their use for important applications. Here, we\nintroduce a novel technique, interpreting a trained neural network by\ninvestigating its flip points. A flip point is any point that lies on the\nboundary between two output classes: e.g. for a neural network with a binary\nyes/no output, a flip point is any input that generates equal scores for “yes”\nand “no”. The flip point closest to a given input is of particular importance,\nand this point is the solution to a well-posed optimization problem. This paper\ngives an overview of the uses of flip points and how they are computed. Through\nresults on standard datasets, we demonstrate how flip points can be used to\nprovide detailed interpretation of the output produced by a neural network.\nMoreover, for a given input, flip points enable us to measure confidence in the\ncorrectness of outputs much more effectively than softmax score. They also\nidentify influential features of the inputs, identify bias, and find changes in\nthe input that change the output of the model. We show that distance between an\ninput and the closest flip point identifies the most influential points in the\ntraining data. Using principal component analysis (PCA) and rank-revealing QR\nfactorization (RR-QR), the set of directions from each training input to its\nclosest flip point provides explanations of how a trained neural network\nprocesses an entire dataset: what features are most important for\nclassification into a given class, which features are most responsible for\nparticular misclassifications, how an adversary might fool the network, etc.\nAlthough we investigate flip points for neural networks, their usefulness is\nactually model-agnostic.</p>\n", "tags": [] },
{"key": "yu2019interpreting", "year": "2019", "title":"Interpreting and Evaluating Neural Network Robustness", "abstract": "<p>Recently, adversarial deception becomes one of the most considerable threats\nto deep neural networks. However, compared to extensive research in new designs\nof various adversarial attacks and defenses, the neural networks’ intrinsic\nrobustness property is still lack of thorough investigation. This work aims to\nqualitatively interpret the adversarial attack and defense mechanism through\nloss visualization, and establish a quantitative metric to evaluate the neural\nnetwork model’s intrinsic robustness. The proposed robustness metric identifies\nthe upper bound of a model’s prediction divergence in the given domain and thus\nindicates whether the model can maintain a stable prediction. With extensive\nexperiments, our metric demonstrates several advantages over conventional\nadversarial testing accuracy based robustness estimation: (1) it provides a\nuniformed evaluation to models with different structures and parameter scales;\n(2) it over-performs conventional accuracy based robustness estimation and\nprovides a more reliable evaluation that is invariant to different test\nsettings; (3) it can be fast generated without considerable testing cost.</p>\n", "tags": [] },
{"key": "zafar2021lack", "year": "2021", "title":"On the Lack of Robust Interpretability of Neural Text Classifiers", "abstract": "<p>With the ever-increasing complexity of neural language models, practitioners\nhave turned to methods for understanding the predictions of these models. One\nof the most well-adopted approaches for model interpretability is feature-based\ninterpretability, i.e., ranking the features in terms of their impact on model\npredictions. Several prior studies have focused on assessing the fidelity of\nfeature-based interpretability methods, i.e., measuring the impact of dropping\nthe top-ranked features on the model output. However, relatively little work\nhas been conducted on quantifying the robustness of interpretations. In this\nwork, we assess the robustness of interpretations of neural text classifiers,\nspecifically, those based on pretrained Transformer encoders, using two\nrandomization tests. The first compares the interpretations of two models that\nare identical except for their initializations. The second measures whether the\ninterpretations differ between a model with trained parameters and a model with\nrandom parameters. Both tests show surprising deviations from expected\nbehavior, raising questions about the extent of insights that practitioners may\ndraw from interpretations.</p>\n", "tags": [] },
{"key": "zahavy2016graying", "year": "2016", "title":"Graying the black box: Understanding DQNs", "abstract": "<p>In recent years there is a growing interest in using deep representations for\nreinforcement learning. In this paper, we present a methodology and tools to\nanalyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a\nnew model, the Semi Aggregated Markov Decision Process (SAMDP), and an\nalgorithm that learns it automatically. The SAMDP model allows us to identify\nspatio-temporal abstractions directly from features and may be used as a\nsub-goal detector in future work. Using our tools we reveal that the features\nlearned by DQNs aggregate the state space in a hierarchical fashion, explaining\nits success. Moreover, we are able to understand and describe the policies\nlearned by DQNs for three different Atari2600 games and suggest ways to\ninterpret, debug and optimize deep neural networks in reinforcement learning.</p>\n", "tags": [] },
{"key": "zarlenga2021efficient", "year": "2021", "title":"Efficient Decompositional Rule Extraction for Deep Neural Networks", "abstract": "<p>In recent years, there has been significant work on increasing both\ninterpretability and debuggability of a Deep Neural Network (DNN) by extracting\na rule-based model that approximates its decision boundary. Nevertheless,\ncurrent DNN rule extraction methods that consider a DNN’s latent space when\nextracting rules, known as decompositional algorithms, are either restricted to\nsingle-layer DNNs or intractable as the size of the DNN or data grows. In this\npaper, we address these limitations by introducing ECLAIRE, a novel\npolynomial-time rule extraction algorithm capable of scaling to both large DNN\narchitectures and large training datasets. We evaluate ECLAIRE on a wide\nvariety of tasks, ranging from breast cancer prognosis to particle detection,\nand show that it consistently extracts more accurate and comprehensible rule\nsets than the current state-of-the-art methods while using orders of magnitude\nless computational resources. We make all of our methods available, including a\nrule set visualisation interface, through the open-source REMIX library\n(https://github.com/mateoespinosa/remix).</p>\n", "tags": [] },
{"key": "zeiler2013visualizing", "year": "2013", "title":"Visualizing and Understanding Convolutional Networks", "abstract": "<p>Large Convolutional Network models have recently demonstrated impressive\nclassification performance on the ImageNet benchmark. However there is no clear\nunderstanding of why they perform so well, or how they might be improved. In\nthis paper we address both issues. We introduce a novel visualization technique\nthat gives insight into the function of intermediate feature layers and the\noperation of the classifier. We also perform an ablation study to discover the\nperformance contribution from different model layers. This enables us to find\nmodel architectures that outperform Krizhevsky \\etal on the ImageNet\nclassification benchmark. We show our ImageNet model generalizes well to other\ndatasets: when the softmax classifier is retrained, it convincingly beats the\ncurrent state-of-the-art results on Caltech-101 and Caltech-256 datasets.</p>\n", "tags": [] },
{"key": "zha2021invertible", "year": "2021", "title":"Invertible Attention", "abstract": "<p>Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention</p>\n", "tags": [] },
{"key": "zhang2016growing", "year": "2016", "title":"Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning", "abstract": "<p>This paper proposes a learning strategy that extracts object-part concepts\nfrom a pre-trained convolutional neural network (CNN), in an attempt to 1)\nexplore explicit semantics hidden in CNN units and 2) gradually grow a\nsemantically interpretable graphical model on the pre-trained CNN for\nhierarchical object understanding. Given part annotations on very few (e.g.,\n3-12) objects, our method mines certain latent patterns from the pre-trained\nCNN and associates them with different semantic parts. We use a four-layer\nAnd-Or graph to organize the mined latent patterns, so as to clarify their\ninternal semantic hierarchy. Our method is guided by a small number of part\nannotations, and it achieves superior performance (about 13%-107% improvement)\nin part center prediction on the PASCAL VOC and ImageNet datasets.</p>\n", "tags": [] },
{"key": "zhang2017interpretable", "year": "2017", "title":"Interpretable Convolutional Neural Networks", "abstract": "<p>This paper proposes a method to modify traditional convolutional neural\nnetworks (CNNs) into interpretable CNNs, in order to clarify knowledge\nrepresentations in high conv-layers of CNNs. In an interpretable CNN, each\nfilter in a high conv-layer represents a certain object part. We do not need\nany annotations of object parts or textures to supervise the learning process.\nInstead, the interpretable CNN automatically assigns each filter in a high\nconv-layer with an object part during the learning process. Our method can be\napplied to different types of CNNs with different structures. The clear\nknowledge representation in an interpretable CNN can help people understand the\nlogics inside a CNN, i.e., based on which patterns the CNN makes the decision.\nExperiments showed that filters in an interpretable CNN were more semantically\nmeaningful than those in traditional CNNs.</p>\n", "tags": [] },
{"key": "zhang2017interpreting", "year": "2017", "title":"Interpreting CNN Knowledge via an Explanatory Graph", "abstract": "<p>This paper learns a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering\nthat each filter in a conv-layer of a pre-trained CNN usually represents a\nmixture of object parts, we propose a simple yet efficient method to\nautomatically disentangles different part patterns from each filter, and\nconstruct an explanatory graph. In the explanatory graph, each node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More importantly, we learn the explanatory\ngraph for a pre-trained CNN in an unsupervised manner, i.e., without a need of\nannotating object parts. Experiments show that each graph node consistently\nrepresents the same object part through different images. We transfer part\npatterns in the explanatory graph to the task of part localization, and our\nmethod significantly outperforms other approaches.</p>\n", "tags": [] },
{"key": "zhang2018explanatory", "year": "2018", "title":"Explanatory Graphs for CNNs", "abstract": "<p>This paper introduces a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside conv-layers of a pre-trained CNN.\nEach filter in a conv-layer of a CNN for object classification usually\nrepresents a mixture of object parts. We develop a simple yet effective method\nto disentangle object-part pattern components from each filter. We construct an\nexplanatory graph to organize the mined part patterns, where a node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More crucially, given a pre-trained CNN, the\nexplanatory graph is learned without a need of annotating object parts.\nExperiments show that each graph node consistently represented the same object\npart through different images, which boosted the transferability of CNN\nfeatures. We transferred part patterns in the explanatory graph to the task of\npart localization, and our method significantly outperformed other approaches.</p>\n", "tags": [] },
{"key": "zhang2018interpretable", "year": "2018", "title":"Interpretable Deep Learning under Fire", "abstract": "<p>Providing explanations for deep neural network (DNN) models is crucial for\ntheir use in security-sensitive domains. A plethora of interpretation models\nhave been proposed to help users understand the inner workings of DNNs: how\ndoes a DNN arrive at a specific decision for a given input? The improved\ninterpretability is believed to offer a sense of security by involving human in\nthe decision-making process. Yet, due to its data-driven nature, the\ninterpretability itself is potentially susceptible to malicious manipulations,\nabout which little is known thus far.\n  Here we bridge this gap by conducting the first systematic study on the\nsecurity of interpretable deep learning systems (IDLSes). We show that existing\n\\imlses are highly vulnerable to adversarial manipulations. Specifically, we\npresent ADV^2, a new class of attacks that generate adversarial inputs not only\nmisleading target DNNs but also deceiving their coupled interpretation models.\nThrough empirical evaluation against four major types of IDLSes on benchmark\ndatasets and in security-critical applications (e.g., skin cancer diagnosis),\nwe demonstrate that with ADV^2 the adversary is able to arbitrarily designate\nan input’s prediction and interpretation. Further, with both analytical and\nempirical evidence, we identify the prediction-interpretation gap as one root\ncause of this vulnerability – a DNN and its interpretation model are often\nmisaligned, resulting in the possibility of exploiting both models\nsimultaneously. Finally, we explore potential countermeasures against ADV^2,\nincluding leveraging its low transferability and incorporating it in an\nadversarial training framework. Our findings shed light on designing and\noperating IDLSes in a more secure and informative fashion, leading to several\npromising research directions.</p>\n", "tags": [] },
{"key": "zhang2018interpreting", "year": "2018", "title":"Interpreting CNNs via Decision Trees", "abstract": "<p>This paper aims to quantitatively explain rationales of each prediction that\nis made by a pre-trained convolutional neural network (CNN). We propose to\nlearn a decision tree, which clarifies the specific reason for each prediction\nmade by the CNN at the semantic level. I.e., the decision tree decomposes\nfeature representations in high conv-layers of the CNN into elementary concepts\nof object parts. In this way, the decision tree tells people which object parts\nactivate which filters for the prediction and how much they contribute to the\nprediction score. Such semantic and quantitative explanations for CNN\npredictions have specific values beyond the traditional pixel-level analysis of\nCNNs. More specifically, our method mines all potential decision modes of the\nCNN, where each mode represents a common case of how the CNN uses object parts\nfor prediction. The decision tree organizes all potential decision modes in a\ncoarse-to-fine manner to explain CNN predictions at different fine-grained\nlevels. Experiments have demonstrated the effectiveness of the proposed method.</p>\n", "tags": [] },
{"key": "zhang2018non-rigid", "year": "2018", "title":"Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps", "abstract": "<p>In this paper, we propose a novel effective non-rigid object tracking\nframework based on the spatial-temporal consistent saliency detection. In\ncontrast to most existing trackers that utilize a bounding box to specify the\ntracked target, the proposed framework can extract accurate regions of the\ntarget as tracking outputs. It achieves a better description of the non-rigid\nobjects and reduces the background pollution for the tracking model.\nFurthermore, our model has several unique features. First, a tailored fully\nconvolutional neural network (TFCN) is developed to model the local saliency\nprior for a given image region, which not only provides the pixel-wise outputs\nbut also integrates the semantic information. Second, a novel multi-scale\nmulti-region mechanism is proposed to generate local saliency maps that\neffectively consider visual perceptions with different spatial layouts and\nscale variations. Subsequently, local saliency maps are fused via a weighted\nentropy method, resulting in a final discriminative saliency map. Finally, we\npresent a non-rigid object tracking algorithm based on the predicted saliency\nmaps. By utilizing a spatial-temporal consistent saliency map (STCSM), we\nconduct target-background classification and use a simple fine-tuning scheme\nfor online updating. Extensive experiments demonstrate that the proposed\nalgorithm achieves competitive performance in both saliency detection and\nvisual tracking, especially outperforming other related trackers on the\nnon-rigid object tracking datasets.</p>\n", "tags": [] },
{"key": "zhang2018visual", "year": "2018", "title":"Visual Interpretability for Deep Learning: a Survey", "abstract": "<p>This paper reviews recent studies in understanding neural-network\nrepresentations and learning neural networks with interpretable/disentangled\nmiddle-layer representations. Although deep neural networks have exhibited\nsuperior performance in various tasks, the interpretability is always the\nAchilles’ heel of deep neural networks. At present, deep neural networks obtain\nhigh discrimination power at the cost of low interpretability of their\nblack-box representations. We believe that high model interpretability may help\npeople to break several bottlenecks of deep learning, e.g., learning from very\nfew annotations, learning via human-computer communications at the semantic\nlevel, and semantically debugging network representations. We focus on\nconvolutional neural networks (CNNs), and we revisit the visualization of CNN\nrepresentations, methods of diagnosing representations of pre-trained CNNs,\napproaches for disentangling pre-trained CNN representations, learning of CNNs\nwith disentangled representations, and middle-to-end learning based on model\ninterpretability. Finally, we discuss prospective trends in explainable\nartificial intelligence.</p>\n", "tags": [] },
{"key": "zhang2019towards", "year": "2019", "title":"Towards a Unified Evaluation of Explanation Methods without Ground Truth", "abstract": "<p>This paper proposes a set of criteria to evaluate the objectiveness of\nexplanation methods of neural networks, which is crucial for the development of\nexplainable AI, but it also presents significant challenges. The core challenge\nis that people usually cannot obtain ground-truth explanations of the neural\nnetwork. To this end, we design four metrics to evaluate explanation results\nwithout ground-truth explanations. Our metrics can be broadly applied to nine\nbenchmark methods of interpreting neural networks, which provides new insights\nof explanation methods.</p>\n", "tags": [] },
{"key": "zhang2020explainable", "year": "2020", "title":"Explainable Empirical Risk Minimization", "abstract": "<p>The successful application of machine learning (ML) methods becomes\nincreasingly dependent on their interpretability or explainability. Designing\nexplainable ML systems is instrumental to ensuring transparency of automated\ndecision-making that targets humans. The explainability of ML methods is also\nan essential ingredient for trustworthy artificial intelligence. A key\nchallenge in ensuring explainability is its dependence on the specific human\nuser (“explainee”). The users of machine learning methods might have vastly\ndifferent background knowledge about machine learning principles. One user\nmight have a university degree in machine learning or related fields, while\nanother user might have never received formal training in high-school\nmathematics. This paper applies information-theoretic concepts to develop a\nnovel measure for the subjective explainability of the predictions delivered by\na ML method. We construct this measure via the conditional entropy of\npredictions, given a user feedback. The user feedback might be obtained from\nuser surveys or biophysical measurements. Our main contribution is the\nexplainable empirical risk minimization (EERM) principle of learning a\nhypothesis that optimally balances between the subjective explainability and\nrisk. The EERM principle is flexible and can be combined with arbitrary machine\nlearning models. We present several practical implementations of EERM for\nlinear models and decision trees. Numerical experiments demonstrate the\napplication of EERM to detecting the use of inappropriate language on social\nmedia.</p>\n", "tags": [] },
{"key": "zhang2020survey", "year": "2020", "title":"A Survey on Neural Network Interpretability", "abstract": "<p>Along with the great success of deep neural networks, there is also growing\nconcern about their black-box nature. The interpretability issue affects\npeople’s trust on deep learning systems. It is also related to many ethical\nproblems, e.g., algorithmic discrimination. Moreover, interpretability is a\ndesired property for deep networks to become powerful tools in other research\nfields, e.g., drug discovery and genomics. In this survey, we conduct a\ncomprehensive review of the neural network interpretability research. We first\nclarify the definition of interpretability as it has been used in many\ndifferent contexts. Then we elaborate on the importance of interpretability and\npropose a novel taxonomy organized along three dimensions: type of engagement\n(passive vs. active interpretation approaches), the type of explanation, and\nthe focus (from local to global interpretability). This taxonomy provides a\nmeaningful 3D view of distribution of papers from the relevant literature as\ntwo of the dimensions are not simply categorical but allow ordinal\nsubcategories. Finally, we summarize the existing interpretability evaluation\nmethods and suggest possible research directions inspired by our new taxonomy.</p>\n", "tags": [] },
{"key": "zhang2021evaluating", "year": "2021", "title":"Evaluating Deep Graph Neural Networks", "abstract": "<p>Graph Neural Networks (GNNs) have already been widely applied in various\ngraph mining tasks. However, they suffer from the shallow architecture issue,\nwhich is the key impediment that hinders the model performance improvement.\nAlthough several relevant approaches have been proposed, none of the existing\nstudies provides an in-depth understanding of the root causes of performance\ndegradation in deep GNNs. In this paper, we conduct the first systematic\nexperimental evaluation to present the fundamental limitations of shallow\narchitectures. Based on the experimental results, we answer the following two\nessential questions: (1) what actually leads to the compromised performance of\ndeep GNNs; (2) when we need and how to build deep GNNs. The answers to the\nabove questions provide empirical insights and guidelines for researchers to\ndesign deep and well-performed GNNs. To show the effectiveness of our proposed\nguidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful\napproach (a paradigm in its own right) that helps guide deep GNN designs.\nExperimental results demonstrate three advantages of DGMLP: 1) high accuracy –\nit achieves state-of-the-art node classification performance on various\ndatasets; 2) high flexibility – it can flexibly choose different propagation\nand transformation depths according to graph size and sparsity; 3) high\nscalability and efficiency – it supports fast training on large-scale graphs.\nOur code is available in https://github.com/zwt233/DGMLP.</p>\n", "tags": [] },
{"key": "zhang2021head", "year": "2021", "title":"From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust", "abstract": "<p>Automated vehicles (AVs) are social robots that can potentially benefit our\nsociety. According to the existing literature, AV explanations can promote\npassengers’ trust by reducing the uncertainty associated with the AV’s\nreasoning and actions. However, the literature on AV explanations and trust has\nfailed to consider how the type of trust</p>\n<ul>\n  <li>cognitive versus affective - might alter this relationship. Yet, the\nexisting literature has shown that the implications associated with trust vary\nwidely depending on whether it is cognitive or affective. To address this\nshortcoming and better understand the impacts of explanations on trust in AVs,\nwe designed a study to investigate the effectiveness of explanations on both\ncognitive and affective trust. We expect these results to be of great\nsignificance in designing AV explanations to promote AV trust.</li>\n</ul>\n", "tags": [] },
{"key": "zhao2014person", "year": "2014", "title":"Person Re-identification by Saliency Learning", "abstract": "<p>Human eyes can recognize person identities based on small salient regions,\ni.e. human saliency is distinctive and reliable in pedestrian matching across\ndisjoint camera views. However, such valuable information is often hidden when\ncomputing similarities of pedestrian images with existing approaches. Inspired\nby our user study result of human perception on human saliency, we propose a\nnovel perspective for person re-identification based on learning human saliency\nand matching saliency distribution. The proposed saliency learning and matching\nframework consists of four steps: (1) To handle misalignment caused by drastic\nviewpoint change and pose variations, we apply adjacency constrained patch\nmatching to build dense correspondence between image pairs. (2) We propose two\nalternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a\nsaliency score for each image patch, through which distinctive features stand\nout without using identity labels in the training procedure. (3) saliency\nmatching is proposed based on patch matching. Matching patches with\ninconsistent saliency brings penalty, and images of the same identity are\nrecognized by minimizing the saliency matching cost. (4) Furthermore, saliency\nmatching is tightly integrated with patch matching in a unified structural\nRankSVM learning framework. The effectiveness of our approach is validated on\nthe VIPeR dataset and the CUHK01 dataset. Our approach outperforms the\nstate-of-the-art person re-identification methods on both datasets.</p>\n", "tags": [] },
{"key": "zhao2019confounder", "year": "2019", "title":"Confounder-Aware Visualization of ConvNets", "abstract": "<p>With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.</p>\n", "tags": [] },
{"key": "zhao2020baylime", "year": "2020", "title":"BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations", "abstract": "<p>Given the pressing need for assuring algorithmic transparency, Explainable AI\n(XAI) has emerged as one of the key areas of AI research. In this paper, we\ndevelop a novel Bayesian extension to the LIME framework, one of the most\nwidely used approaches in XAI – which we call BayLIME. Compared to LIME,\nBayLIME exploits prior knowledge and Bayesian reasoning to improve both the\nconsistency in repeated explanations of a single prediction and the robustness\nto kernel settings. BayLIME also exhibits better explanation fidelity than the\nstate-of-the-art (LIME, SHAP and GradCAM) by its ability to integrate prior\nknowledge from, e.g., a variety of other XAI techniques, as well as\nverification and validation (V&amp;V) methods. We demonstrate the desirable\nproperties of BayLIME through both theoretical analysis and extensive\nexperiments.</p>\n", "tags": [] },
{"key": "zhao2021towards", "year": "2021", "title":"Towards Interpretable Deep Metric Learning with Structural Matching", "abstract": "<p>How do the neural networks distinguish two images? It is of critical\nimportance to understand the matching mechanism of deep models for developing\nreliable intelligent systems for many risky visual applications such as\nsurveillance and access control. However, most existing deep metric learning\nmethods match the images by comparing feature vectors, which ignores the\nspatial structure of images and thus lacks interpretability. In this paper, we\npresent a deep interpretable metric learning (DIML) method for more transparent\nembedding learning. Unlike conventional metric learning methods based on\nfeature vector comparison, we propose a structural matching strategy that\nexplicitly aligns the spatial embeddings by computing an optimal matching flow\nbetween feature maps of the two images. Our method enables deep models to learn\nmetrics in a more human-friendly way, where the similarity of two images can be\ndecomposed to several part-wise similarities and their contributions to the\noverall similarity. Our method is model-agnostic, which can be applied to\noff-the-shelf backbone networks and metric learning methods. We evaluate our\nmethod on three major benchmarks of deep metric learning including CUB200-2011,\nCars196, and Stanford Online Products, and achieve substantial improvements\nover popular metric learning methods with better interpretability. Code is\navailable at https://github.com/wl-zhao/DIML</p>\n", "tags": [] },
{"key": "zhou2014object", "year": "2014", "title":"Object Detectors Emerge in Deep Scene CNNs", "abstract": "<p>With the success of new computational architectures for visual processing,\nsuch as convolutional neural networks (CNN) and access to image databases with\nmillions of labeled examples (e.g., ImageNet, Places), the state of the art in\ncomputer vision is advancing rapidly. One important factor for continued\nprogress is to understand the representations that are learned by the inner\nlayers of these deep architectures. Here we show that object detectors emerge\nfrom training CNNs to perform scene classification. As scenes are composed of\nobjects, the CNN for scene classification automatically discovers meaningful\nobjects detectors, representative of the learned scene categories. With object\ndetectors emerging as a result of learning to recognize scenes, our work\ndemonstrates that the same network can perform both scene recognition and\nobject localization in a single forward-pass, without ever having been\nexplicitly taught the notion of objects.</p>\n", "tags": [] },
{"key": "zhou2015learning", "year": "2015", "title":"Learning Deep Features for Discriminative Localization", "abstract": "<p>In this work, we revisit the global average pooling layer proposed in [13],\nand shed light on how it explicitly enables the convolutional neural network to\nhave remarkable localization ability despite being trained on image-level\nlabels. While this technique was previously proposed as a means for\nregularizing training, we find that it actually builds a generic localizable\ndeep representation that can be applied to a variety of tasks. Despite the\napparent simplicity of global average pooling, we are able to achieve 37.1%\ntop-5 error for object localization on ILSVRC 2014, which is remarkably close\nto the 34.2% top-5 error achieved by a fully supervised CNN approach. We\ndemonstrate that our network is able to localize the discriminative image\nregions on a variety of tasks despite not being trained for them</p>\n", "tags": [] },
{"key": "zhou2017interpreting", "year": "2017", "title":"Interpreting Deep Visual Representations via Network Dissection", "abstract": "<p>The success of recent deep convolutional neural networks (CNNs) depends on\nlearning hidden representations that can summarize the important factors of\nvariation behind the data. However, CNNs often criticized as being black boxes\nthat lack interpretability, since they have millions of unexplained model\nparameters. In this work, we describe Network Dissection, a method that\ninterprets networks by providing labels for the units of their deep visual\nrepresentations. The proposed method quantifies the interpretability of CNN\nrepresentations by evaluating the alignment between individual hidden units and\na set of visual semantic concepts. By identifying the best alignments, units\nare given human interpretable labels across a range of objects, parts, scenes,\ntextures, materials, and colors. The method reveals that deep representations\nare more transparent and interpretable than expected: we find that\nrepresentations are significantly more interpretable than they would be under a\nrandom equivalently powerful basis. We apply the method to interpret and\ncompare the latent representations of various network architectures trained to\nsolve different supervised and self-supervised training tasks. We then examine\nfactors affecting the network interpretability such as the number of the\ntraining iterations, regularizations, different initializations, and the\nnetwork depth and width. Finally we show that the interpreted units can be used\nto provide explicit explanations of a prediction given by a CNN for an image.\nOur results highlight that interpretability is an important property of deep\nneural networks that provides new insights into their hierarchical structure.</p>\n", "tags": [] },
{"key": "zhou2022solvability", "year": "2022", "title":"The Solvability of Interpretability Evaluation Metrics", "abstract": "<p>Feature attribution methods are popular for explaining neural network\npredictions, and they are often evaluated on metrics such as comprehensiveness\nand sufficiency, which are motivated by the principle that more important\nfeatures – as judged by the explanation – should have larger impacts on model\nprediction. In this paper, we highlight an intriguing property of these\nmetrics: their solvability. Concretely, we can define the problem of optimizing\nan explanation for a metric and solve it using beam search. This brings up the\nobvious question: given such solvability, why do we still develop other\nexplainers and then evaluate them on the metric? We present a series of\ninvestigations showing that this beam search explainer is generally comparable\nor favorable to current choices such as LIME and SHAP, suggest rethinking the\ngoals of model interpretability, and identify several directions towards better\nevaluations of new method proposals.</p>\n", "tags": [] },
{"key": "zhu2018exploiting", "year": "2018", "title":"Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection", "abstract": "<p>Saliency detection aims to detect the most attractive objects in images and\nis widely used as a foundation for various applications. In this paper, we\npropose a novel salient object detection algorithm for RGB-D images using\ncenter-dark channel priors. First, we generate an initial saliency map based on\na color saliency map and a depth saliency map of a given RGB-D image. Then, we\ngenerate a center-dark channel map based on center saliency and dark channel\npriors. Finally, we fuse the initial saliency map with the center dark channel\nmap to generate the final saliency map. Extensive evaluations over four\nbenchmark datasets demonstrate that our proposed method performs favorably\nagainst most of the state-of-the-art approaches. Besides, we further discuss\nthe application of the proposed algorithm in small target detection and\ndemonstrate the universal value of center-dark channel priors in the field of\nobject detection.</p>\n", "tags": [] },
{"key": "zhu2021going", "year": "2021", "title":"Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective", "abstract": "<p>Convolutional neural network (CNN) is one of the most widely-used successful\narchitectures in the era of deep learning. However, the high-computational cost\nof CNN still hampers more universal uses to light devices. Fortunately, the\nFourier transform on convolution gives an elegant and promising solution to\ndramatically reduce the computation cost. Recently, some studies devote to such\na challenging problem and pursue the complete frequency computation without any\nswitching between spatial domain and frequent domain. In this work, we revisit\nthe Fourier transform theory to derive feed-forward and back-propagation\nfrequency operations of typical network modules such as convolution, activation\nand pooling. Due to the calculation limitation of complex numbers on most\ncomputation tools, we especially extend the Fourier transform to the Laplace\ntransform for CNN, which can run in the real domain with more relaxed\nconstraints. This work more focus on a theoretical extension and discussion\nabout frequency CNN, and lay some theoretical ground for real application.</p>\n", "tags": [] },
{"key": "zimmermann2021well", "year": "2021", "title":"How Well do Feature Visualizations Support Causal Understanding of CNN Activations?", "abstract": "<p>A precise understanding of why units in an artificial network respond to\ncertain stimuli would constitute a big step towards explainable artificial\nintelligence. One widely used approach towards this goal is to visualize unit\nresponses via activation maximization. These synthetic feature visualizations\nare purported to provide humans with precise information about the image\nfeatures that cause a unit to be activated - an advantage over other\nalternatives like strongly activating natural dataset samples. If humans indeed\ngain causal insight from visualizations, this should enable them to predict the\neffect of an intervention, such as how occluding a certain patch of the image\n(say, a dog’s head) changes a unit’s activation. Here, we test this hypothesis\nby asking humans to decide which of two square occlusions causes a larger\nchange to a unit’s activation. Both a large-scale crowdsourced experiment and\nmeasurements with experts show that on average the extremely activating feature\nvisualizations by Olah et al. (2017) indeed help humans on this task ($68 \\pm\n4$% accuracy; baseline performance without any visualizations is $60 \\pm 3$%).\nHowever, they do not provide any substantial advantage over other\nvisualizations (such as e.g. dataset samples), which yield similar performance\n($66\\pm3$% to $67 \\pm3$% accuracy). Taken together, we propose an objective\npsychophysical task to quantify the benefit of unit-level interpretability\nmethods for humans, and find no evidence that a widely-used feature\nvisualization method provides humans with better “causal understanding” of unit\nactivations than simple alternative visualizations.</p>\n", "tags": [] },
{"key": "zunino2020explainable", "year": "2020", "title":"Explainable Deep Classification Models for Domain Generalization", "abstract": "<p>Conventionally, AI models are thought to trade off explainability for lower\naccuracy. We develop a training strategy that not only leads to a more\nexplainable AI system for object classification, but as a consequence, suffers\nno perceptible accuracy degradation. Explanations are defined as regions of\nvisual evidence upon which a deep classification network makes a decision. This\nis represented in the form of a saliency map conveying how much each pixel\ncontributed to the network’s decision. Our training strategy enforces a\nperiodic saliency-based feedback to encourage the model to focus on the image\nregions that directly correspond to the ground-truth object. We quantify\nexplainability using an automated metric, and using human judgement. We propose\nexplainability as a means for bridging the visual-semantic gap between\ndifferent domains where model explanations are used as a means of disentagling\ndomain specific information from otherwise relevant features. We demonstrate\nthat this leads to improved generalization to new domains without hindering\nperformance on the original domain.</p>\n", "tags": [] }

]

