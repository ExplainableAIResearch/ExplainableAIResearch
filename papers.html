<!DOCTYPE html>
<html lang="en-us">

  <head>
  <!-- Global Site Tag (gtag.js) - Google Analytics -->
  <!-- TODO - Enable this when we have the Google Analytics page set up -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-107339008-1"></script> -->
  <!-- <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments)};
    gtag('js', new Date());
    gtag('config', 'UA-107339008-1');
  </script> -->

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <!-- TODO - Add more keywords for SEO -->
  <meta name="keywords" content="xai, explainable ai, explainability, interpretablity">

  <title>
    
      Search all Publications on Machine Learning for Source Code &middot; Explainable AI
    
  </title>

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Lato:ital,wght@0,100;0,300;0,400;0,700;0,900;1,100;1,300;1,400;1,700;1,900&display=swap" rel="stylesheet">

  <!-- Icons -->
  <link rel="shortcut icon" href="/public/favicon.svg">
  <!-- TODO - Write a better description -->
  <!-- <link rel="search" href="/public/opensearchdescription.xml" 
      type="application/opensearchdescription+xml" 
      title="ML4Code" /> -->

  <script src="https://code.jquery.com/jquery-3.2.1.min.js"
  integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
  crossorigin="anonymous"></script>
  
  <link rel="stylesheet" type="text/css" href="//cdn.datatables.net/1.12.1/css/dataTables.bootstrap5.min.css">
  <script type="text/javascript" charset="utf8" src="//cdn.datatables.net/1.12.1/js/jquery.dataTables.min.js"></script>
</head>


  <body class="theme-base-0f layout">

    <!-- <a href='/contributing.html' class='ribbon'>Contribute!</a> -->
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Explainable AI
        </a>
      </h1>
      <p class="lead">Current research on explainability and interpretability of machine learning algorithms</p>      
    </div>

  <nav class="sidebar-nav">
   <div class="sidebar-item">
    <p style="font-size: 12px">
      <input type='text' id='searchTarget' placeholder="Search Repository"/> 
      <button class="button-23 draw" onClick="search();"><i class="fa fa-search"></i></button>
    </p>
  </div>
   <a class="sidebar-nav-item active" href="/papers.html">List of Papers</a>
   <a class="sidebar-nav-item" href="/tags.html">Papers by Tag</a>
   <!-- <a class="sidebar-nav-item" href="/tsne-viz.html">2D Map of Papers</a> -->
   <!-- <a class="sidebar-nav-item" href="/topic-viz.html">Topic-based Explorer</a> -->
  <a class="sidebar-nav-item" href="/resources.html">Resources</a>
  <a class="sidebar-nav-item" href="/contributing.html">Contributing</a>
  </nav>

  <div class="sidebar-item">
    <p style="font-size: 12px">This site is a community effort by the <a href="explainableaiworld.slack.com">Explainable AI</a> members. Please join the group and reach out to the administrators if you have any questions.</p>
    <p style="font-size: 12px"><span style="font-size: 12px">
      Made with <a href="https://jekyllrb.com">Jekyll</a> and <a href="https://github.com/poole/hyde">Hyde</a>. Idea and base code for the website adapted from <a href="https://ml4code.github.io/">ml4code</a>.
    </span></p>
  </div>
</div></div>

<script>
$("#searchTarget").keydown(function (e) {	
  if (e.keyCode == 13) {
    search();
  }
});

function search() {
  try {
    ga('send', 'event', 'search', 'search', $("#searchTarget").val());
  } finally {
    window.location = "/papers.html#" + $("#searchTarget").val();
  }
}
</script>


    <div class="content container">
      Search across all paper titles, abstracts, authors by using the search field.
Please consider <a href="/contributing.html">contributing</a> by updating
the information of existing papers or adding new work.

<table id="allPapers">
<thead><th>Year</th><th>Title</th><th>Authors</th><th>Venue</th><th>Abstract</th></thead><tbody>



<tr>
	<td>2022</td>
	<td><a href="/publications/park2022vision/">How Do Vision Transformers Work?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Do Vision Transformers Work?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Do Vision Transformers Work?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Namuk Park, Songkuk Kim</td>
	<td></td>
	<td><p>The success of multi-head self-attentions (MSAs) for computer vision is now
indisputable. However, little is known about how MSAs work. We present
fundamental explanations to help better understand the nature of MSAs. In
particular, we demonstrate the following properties of MSAs and Vision
Transformers (ViTs): (1) MSAs improve not only accuracy but also generalization
by flattening the loss landscapes. Such improvement is primarily attributable
to their data specificity, not long-range dependency. On the other hand, ViTs
suffer from non-convex losses. Large datasets and loss landscape smoothing
methods alleviate this problem; (2) MSAs and Convs exhibit opposite behaviors.
For example, MSAs are low-pass filters, but Convs are high-pass filters.
Therefore, MSAs and Convs are complementary; (3) Multi-stage neural networks
behave like a series connection of small individual models. In addition, MSAs
at the end of a stage play a key role in prediction. Based on these insights,
we propose AlterNet, a model in which Conv blocks at the end of a stage are
replaced with MSA blocks. AlterNet outperforms CNNs not only in large data
regimes but also in small data regimes. The code is available at
https://github.com/xxxnell/how-do-vits-work.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/nguyen2022visual/">Visual correspondence-based explanations improve AI robustness and human-AI team accuracy</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual correspondence-based explanations improve AI robustness and human-AI team accuracy' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual correspondence-based explanations improve AI robustness and human-AI team accuracy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Giang Nguyen, Mohammad Reza Taesiri, Anh Nguyen</td>
	<td></td>
	<td><p>Explaining artificial intelligence (AI) predictions is increasingly important
and even imperative in many high-stakes applications where humans are the
ultimate decision-makers. In this work, we propose two novel architectures of
self-interpretable image classifiers that first explain, and then predict (as
opposed to post-hoc explanations) by harnessing the visual correspondences
between a query image and exemplars. Our models consistently improve (by 1 to 4
points) on out-of-distribution (OOD) datasets while performing marginally worse
(by 1 to 2 points) on in-distribution tests than ResNet-50 and a $k$-nearest
neighbor classifier (kNN). Via a large-scale, human study on ImageNet and CUB,
our correspondence-based explanations are found to be more useful to users than
kNN explanations. Our explanations help users more accurately reject AI’s wrong
decisions than all other tested methods. Interestingly, for the first time, we
show that it is possible to achieve complementary human-AI team accuracy (i.e.,
that is higher than either AI-alone or human-alone), in ImageNet and CUB image
classification tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/norelli2022explanatory/">Explanatory Learning: Beyond Empiricism in Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Learning: Beyond Empiricism in Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Learning: Beyond Empiricism in Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Antonio Norelli, Giorgio Mariani, Luca Moschella, Andrea Santilli, Giambattista Parascandolo, Simone Melzi, Emanuele Rodolà</td>
	<td></td>
	<td><p>We introduce Explanatory Learning (EL), a framework to let machines use
existing knowledge buried in symbolic sequences – e.g. explanations written in
hieroglyphic – by autonomously learning to interpret them. In EL, the burden
of interpreting symbols is not left to humans or rigid human-coded compilers,
as done in Program Synthesis. Rather, EL calls for a learned interpreter, built
upon a limited collection of symbolic sequences paired with observations of
several phenomena. This interpreter can be used to make predictions on a novel
phenomenon given its explanation, and even to find that explanation using only
a handful of observations, like human scientists do. We formulate the EL
problem as a simple binary classification task, so that common end-to-end
approaches aligned with the dominant empiricist view of machine learning could,
in principle, solve it. To these models, we oppose Critical Rationalist
Networks (CRNs), which instead embrace a rationalist view on the acquisition of
knowledge. CRNs express several desired properties by construction, they are
truly explainable, can adjust their processing at test-time for harder
inferences, and can offer strong confidence guarantees on their predictions. As
a final contribution, we introduce Odeen, a basic EL environment that simulates
a small flatland-style universe full of phenomena to explain. Using Odeen as a
testbed, we show how CRNs outperform empiricist end-to-end approaches of
similar size and architecture (Transformers) in discovering explanations for
novel phenomena.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/nauta2022anecdotal/">From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Meike Nauta, Jan Trienes, Shreyasi Pathak, Elisa Nguyen, Michelle Peters, Yasmin Schmitt, Jörg Schlötterer, Maurice van Keulen, Christin Seifert</td>
	<td></td>
	<td><p>The rising popularity of explainable artificial intelligence (XAI) to
understand high-performing black boxes, also raised the question of how to
evaluate explanations of machine learning (ML) models. While interpretability
and explainability are often presented as a subjectively validated binary
property, we consider it a multi-faceted concept. We identify 12 conceptual
properties, such as Compactness and Correctness, that should be evaluated for
comprehensively assessing the quality of an explanation. Our so-called Co-12
properties serve as categorization scheme for systematically reviewing the
evaluation practice of more than 300 papers published in the last 7 years at
major AI and ML conferences that introduce an XAI method. We find that 1 in 3
papers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate
with users. We also contribute to the call for objective, quantifiable
evaluation methods by presenting an extensive overview of quantitative XAI
evaluation methods. This systematic collection of evaluation methods provides
researchers and practitioners with concrete tools to thoroughly validate,
benchmark and compare new and existing XAI methods. This also opens up
opportunities to include quantitative metrics as optimization criteria during
model training in order to optimize for accuracy and interpretability
simultaneously.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/m%C3%BCller2022interactive/">An Interactive Explanatory AI System for Industrial Quality Control</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Interactive Explanatory AI System for Industrial Quality Control' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Interactive Explanatory AI System for Industrial Quality Control' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dennis Müller, Michael März, Stephan Scheele, Ute Schmid</td>
	<td></td>
	<td><p>Machine learning based image classification algorithms, such as deep neural
network approaches, will be increasingly employed in critical settings such as
quality control in industry, where transparency and comprehensibility of
decisions are crucial. Therefore, we aim to extend the defect detection task
towards an interactive human-in-the-loop approach that allows us to integrate
rich background knowledge and the inference of complex relationships going
beyond traditional purely data-driven approaches. We propose an approach for an
interactive support system for classifications in an industrial quality control
setting that combines the advantages of both (explainable) knowledge-driven and
data-driven machine learning methods, in particular inductive logic programming
and convolutional neural networks, with human expertise and control. The
resulting system can assist domain experts with decisions, provide transparent
explanations for results, and integrate feedback from users; thus reducing
workload for humans while both respecting their expertise and without removing
their agency or accountability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/moorman2022people/">Do People Trust Robots that Learn in the Home?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Do People Trust Robots that Learn in the Home?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Do People Trust Robots that Learn in the Home?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nina Moorman, Matthew Gombolay</td>
	<td></td>
	<td><p>It is not scalable for assistive robotics to have all functionalities
pre-programmed prior to user introduction. Instead, it is more realistic for
agents to perform supplemental on site learning. This opportunity to learn user
and environment particularities is especially helpful for care robots that
assist with individualized caregiver activities in residential or nursing home
environments. Many assistive robots, ranging in complexity from Roomba to
Pepper, already conduct some of their learning in the home, observable to the
user. We lack an understanding of how witnessing this learning impacts the
user. Thus, we propose to assess end-user attitudes towards the concept of
embodied robots that conduct some learning in the home as compared to robots
that are delivered fully-capable. In this virtual, between-subjects study, we
recruit end users (care-givers and care-takers) from nursing homes, and
investigate user trust in three different domains: navigation, manipulation,
and preparation. Informed by the first study where we identify agent learning
as a key factor in determining trust, we propose a second study to explore how
to modulate that trust. This second, in-person study investigates the
effectiveness of apologies, explanations of robot failure, and transparency of
learning at improving trust in embodied learning robots.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/manica2022gt4sd/">GT4SD: Generative Toolkit for Scientific Discovery</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=GT4SD: Generative Toolkit for Scientific Discovery' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=GT4SD: Generative Toolkit for Scientific Discovery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matteo Manica, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Jannis Born, Dean Clarke, Yves Gaetan Nana Teukam, Samuel C. Hoffman, Matthew Buchan, Vijil Chenthamarakshan, Timothy Donovan, Hsiang Han Hsu, Federico Zipoli, Oliver Schilter, Giorgio Giannone, Akihiro Kishimoto, Lisa Hamada, Inkit Padhi, Karl Wehden, Lauren McHugh, Alexy Khrabrov, Payel Das, Seiji Takeda, John R. Smith</td>
	<td></td>
	<td><p>With the growing availability of data within various scientific domains,
generative models hold enormous potential to accelerate scientific discovery at
every step of the scientific method. Perhaps their most valuable application
lies in the speeding up of what has traditionally been the slowest and most
challenging step of coming up with a hypothesis. Powerful representations are
now being learned from large volumes of data to generate novel hypotheses,
which is making a big impact on scientific discovery applications ranging from
material design to drug discovery. The GT4SD
(https://github.com/GT4SD/gt4sd-core) is an extensible open-source library that
enables scientists, developers and researchers to train and use
state-of-the-art generative models for hypothesis generation in scientific
discovery. GT4SD supports a variety of uses of generative models across
material science and drug discovery, including molecule discovery and design
based on properties related to target proteins, omic profiles, scaffold
distances, binding energies and more.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/liu2022generating/">Generating and Visualizing Trace Link Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating and Visualizing Trace Link Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating and Visualizing Trace Link Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yalin Liu, Jinfeng Lin, Oghenemaro Anuyah, Ronald Metoyer, Jane Cleland-Huang</td>
	<td></td>
	<td><p>Recent breakthroughs in deep-learning (DL) approaches have resulted in the
dynamic generation of trace links that are far more accurate than was
previously possible. However, DL-generated links lack clear explanations, and
therefore non-experts in the domain can find it difficult to understand the
underlying semantics of the link, making it hard for them to evaluate the
link’s correctness or suitability for a specific software engineering task. In
this paper we present a novel NLP pipeline for generating and visualizing trace
link explanations. Our approach identifies domain-specific concepts, retrieves
a corpus of concept-related sentences, mines concept definitions and usage
examples, and identifies relations between cross-artifact concepts in order to
explain the links. It applies a post-processing step to prioritize the most
likely acronyms and definitions and to eliminate non-relevant ones. We evaluate
our approach using project artifacts from three different domains of
interstellar telescopes, positive train control, and electronic health-care
systems, and then report coverage, correctness, and potential utility of the
generated definitions. We design and utilize an explanation interface which
leverages concept definitions and relations to visualize and explain trace link
rationales, and we report results from a user study that was conducted to
evaluate the effectiveness of the explanation interface. Results show that the
explanations presented in the interface helped non-experts to understand the
underlying semantics of a trace link and improved their ability to vet the
correctness of the link.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/lertvittayakumjorn2022argumentative/">Argumentative Explanations for Pattern-Based Text Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Argumentative Explanations for Pattern-Based Text Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Argumentative Explanations for Pattern-Based Text Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Piyawat Lertvittayakumjorn, Francesca Toni</td>
	<td></td>
	<td><p>Recent works in Explainable AI mostly address the transparency issue of
black-box models or create explanations for any kind of models (i.e., they are
model-agnostic), while leaving explanations of interpretable models largely
underexplored. In this paper, we fill this gap by focusing on explanations for
a specific interpretable model, namely pattern-based logistic regression (PLR)
for binary text classification. We do so because, albeit interpretable, PLR is
challenging when it comes to explanations. In particular, we found that a
standard way to extract explanations from this model does not consider
relations among the features, making the explanations hardly plausible to
humans. Hence, we propose AXPLR, a novel explanation method using (forms of)
computational argumentation to generate explanations (for outputs computed by
PLR) which unearth model agreements and disagreements among the features.
Specifically, we use computational argumentation as follows: we see features
(patterns) in PLR as arguments in a form of quantified bipolar argumentation
frameworks (QBAFs) and extract attacks and supports between arguments based on
specificity of the arguments; we understand logistic regression as a gradual
semantics for these QBAFs, used to determine the arguments’ dialectic strength;
and we study standard properties of gradual semantics for QBAFs in the context
of our argumentative re-interpretation of PLR, sanctioning its suitability for
explanatory purposes. We then show how to extract intuitive explanations (for
outputs computed by PLR) from the constructed QBAFs. Finally, we conduct an
empirical evaluation and two experiments in the context of human-AI
collaboration to demonstrate the advantages of our resulting AXPLR method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/krishna2022disagreement/">The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Disagreement Problem in Explainable Machine Learning: A Practitioner's Perspective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Satyapriya Krishna, Tessa Han, Alex Gu, Javin Pombra, Shahin Jabbari, Steven Wu, Himabindu Lakkaraju</td>
	<td></td>
	<td><p>As various post hoc explanation methods are increasingly being leveraged to
explain complex models in high-stakes settings, it becomes critical to develop
a deeper understanding of if and when the explanations output by these methods
disagree with each other, and how such disagreements are resolved in practice.
However, there is little to no research that provides answers to these critical
questions. In this work, we introduce and study the disagreement problem in
explainable machine learning. More specifically, we formalize the notion of
disagreement between explanations, analyze how often such disagreements occur
in practice, and how do practitioners resolve these disagreements. To this end,
we first conduct interviews with data scientists to understand what constitutes
disagreement between explanations generated by different methods for the same
model prediction, and introduce a novel quantitative framework to formalize
this understanding. We then leverage this framework to carry out a rigorous
empirical analysis with four real-world datasets, six state-of-the-art post hoc
explanation methods, and eight different predictive models, to measure the
extent of disagreement between the explanations generated by various popular
explanation methods. In addition, we carry out an online user study with data
scientists to understand how they resolve the aforementioned disagreements. Our
results indicate that state-of-the-art explanation methods often disagree in
terms of the explanations they output. Our findings also underscore the
importance of developing principled evaluation metrics that enable
practitioners to effectively compare explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/khakzar2022explanations/">Do Explanations Explain? Model Knows Best</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Do Explanations Explain? Model Knows Best' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Do Explanations Explain? Model Knows Best' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ashkan Khakzar, Pedram Khorsandi, Rozhin Nobahari, Nassir Navab</td>
	<td></td>
	<td><p>It is a mystery which input features contribute to a neural network’s output.
Various explanation (feature attribution) methods are proposed in the
literature to shed light on the problem. One peculiar observation is that these
explanations (attributions) point to different features as being important. The
phenomenon raises the question, which explanation to trust? We propose a
framework for evaluating the explanations using the neural network model
itself. The framework leverages the network to generate input features that
impose a particular behavior on the output. Using the generated features, we
devise controlled experimental setups to evaluate whether an explanation method
conforms to an axiom. Thus we propose an empirical framework for axiomatic
evaluation of explanation methods. We evaluate well-known and promising
explanation solutions using the proposed framework. The framework provides a
toolset to reveal properties and drawbacks within existing and future
explanation solutions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/kaur2022sensible/">Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sensible AI: Re-imagining Interpretability and Explainability using Sensemaking Theory' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Harmanpreet Kaur, Eytan Adar, Eric Gilbert, Cliff Lampe</td>
	<td></td>
	<td><p>Understanding how ML models work is a prerequisite for responsibly designing,
deploying, and using ML-based systems. With interpretability approaches, ML can
now offer explanations for its outputs to aid human understanding. Though these
approaches rely on guidelines for how humans explain things to each other, they
ultimately solve for improving the artifact – an explanation. In this paper,
we propose an alternate framework for interpretability grounded in Weick’s
sensemaking theory, which focuses on who the explanation is intended for.
Recent work has advocated for the importance of understanding stakeholders’
needs – we build on this by providing concrete properties (e.g., identity,
social context, environmental cues, etc.) that shape human understanding. We
use an application of sensemaking in organizations as a template for discussing
design guidelines for Sensible AI, AI that factors in the nuances of human
cognition when trying to explain itself.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/h%C3%B6llig2022tsinterpret/">TSInterpret: A unified framework for time series interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TSInterpret: A unified framework for time series interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TSInterpret: A unified framework for time series interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jacqueline Höllig, Cedric Kulbach, Steffen Thoma</td>
	<td></td>
	<td><p>With the increasing application of deep learning algorithms to time series
classification, especially in high-stake scenarios, the relevance of
interpreting those algorithms becomes key. Although research in time series
interpretability has grown, accessibility for practitioners is still an
obstacle. Interpretability approaches and their visualizations are diverse in
use without a unified API or framework. To close this gap, we introduce
TSInterpret an easily extensible open-source Python library for interpreting
predictions of time series classifiers that combines existing interpretation
approaches into one unified framework. The library features (i)
state-of-the-art interpretability algorithms, (ii) exposes a unified API
enabling users to work with explanations consistently and provides (iii)
suitable visualizations for each explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ivankay2022fooling/">Fooling Explanations in Text Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Fooling Explanations in Text Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Fooling Explanations in Text Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adam Ivankay, Ivan Girardi, Chiara Marchiori, Pascal Frossard</td>
	<td>International Conference on Learning Representations, 2022</td>
	<td><p>State-of-the-art text classification models are becoming increasingly reliant
on deep neural networks (DNNs). Due to their black-box nature, faithful and
robust explanation methods need to accompany classifiers for deployment in
real-life scenarios. However, it has been shown in vision applications that
explanation methods are susceptible to local, imperceptible perturbations that
can significantly alter the explanations without changing the predicted
classes. We show here that the existence of such perturbations extends to text
classifiers as well. Specifically, we introduceTextExplanationFooler (TEF), a
novel explanation attack algorithm that alters text input samples imperceptibly
so that the outcome of widely-used explanation methods changes considerably
while leaving classifier predictions unchanged. We evaluate the performance of
the attribution robustness estimation performance in TEF on five sequence
classification datasets, utilizing three DNN architectures and three
transformer architectures for each dataset. TEF can significantly decrease the
correlation between unchanged and perturbed input attributions, which shows
that all models and explanation methods are susceptible to TEF perturbations.
Moreover, we evaluate how the perturbations transfer to other model
architectures and attribution methods, and show that TEF perturbations are also
effective in scenarios where the target model and explanation method are
unknown. Finally, we introduce a semi-universal attack that is able to compute
fast, computationally light perturbations with no knowledge of the attacked
classifier nor explanation method. Overall, our work shows that explanations in
text classifiers are very fragile and users need to carefully address their
robustness before relying on them in critical applications.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/herm2022stop/">Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lukas-Valentin Herm, Kai Heinrich, Jonas Wanner, Christian Janiesch</td>
	<td>International Journal of Information Management, 2002, 102538</td>
	<td><p>Machine learning algorithms enable advanced decision making in contemporary
intelligent systems. Research indicates that there is a tradeoff between their
model performance and explainability. Machine learning models with higher
performance are often based on more complex algorithms and therefore lack
explainability and vice versa. However, there is little to no empirical
evidence of this tradeoff from an end user perspective. We aim to provide
empirical evidence by conducting two user experiments. Using two distinct
datasets, we first measure the tradeoff for five common classes of machine
learning algorithms. Second, we address the problem of end user perceptions of
explainable artificial intelligence augmentations aimed at increasing the
understanding of the decision logic of high-performing complex models. Our
results diverge from the widespread assumption of a tradeoff curve and indicate
that the tradeoff between model performance and explainability is much less
gradual in the end user’s perception. This is a stark contrast to assumed
inherent model interpretability. Further, we found the tradeoff to be
situational for example due to data complexity. Results of our second
experiment show that while explainable artificial intelligence augmentations
can be used to increase explainability, the type of explanation plays an
essential role in end user perception.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/hernandez2022natural/">Natural Language Descriptions of Deep Visual Features</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Natural Language Descriptions of Deep Visual Features' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Natural Language Descriptions of Deep Visual Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, Jacob Andreas</td>
	<td></td>
	<td><p>Some neurons in deep networks specialize in recognizing highly specific
perceptual, structural, or semantic features of inputs. In computer vision,
techniques exist for identifying neurons that respond to individual concept
categories like colors, textures, and object classes. But these techniques are
limited in scope, labeling only a small subset of neurons and behaviors in any
network. Is a richer characterization of neuron-level computation possible? We
introduce a procedure (called MILAN, for mutual-information-guided linguistic
annotation of neurons) that automatically labels neurons with open-ended,
compositional, natural language descriptions. Given a neuron, MILAN generates a
description by searching for a natural language string that maximizes pointwise
mutual information with the image regions in which the neuron is active. MILAN
produces fine-grained descriptions that capture categorical, relational, and
logical structure in learned features. These descriptions obtain high agreement
with human-generated feature descriptions across a diverse set of model
architectures and tasks, and can aid in understanding and controlling learned
models. We highlight three applications of natural language neuron
descriptions. First, we use MILAN for analysis, characterizing the distribution
and importance of neurons selective for attribute, category, and relational
information in vision models. Second, we use MILAN for auditing, surfacing
neurons sensitive to human faces in datasets designed to obscure them. Finally,
we use MILAN for editing, improving robustness in an image classifier by
deleting neurons sensitive to text features spuriously correlated with class
labels.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ge2022explainable/">Explainable Fairness in Recommendation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Fairness in Recommendation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Fairness in Recommendation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yingqiang Ge, Juntao Tan, Yan Zhu, Yinglong Xia, Jiebo Luo, Shuchang Liu, Zuohui Fu, Shijie Geng, Zelong Li, Yongfeng Zhang</td>
	<td></td>
	<td><p>Existing research on fairness-aware recommendation has mainly focused on the
quantification of fairness and the development of fair recommendation models,
neither of which studies a more substantial problem–identifying the underlying
reason of model disparity in recommendation. This information is critical for
recommender system designers to understand the intrinsic recommendation
mechanism and provides insights on how to improve model fairness to decision
makers. Fortunately, with the rapid development of Explainable AI, we can use
model explainability to gain insights into model (un)fairness. In this paper,
we study the problem of explainable fairness, which helps to gain insights
about why a system is fair or unfair, and guides the design of fair recommender
systems with a more informed and unified methodology. Particularly, we focus on
a common setting with feature-aware recommendation and exposure unfairness, but
the proposed explainable fairness framework is general and can be applied to
other recommendation settings and fairness definitions. We propose a
Counterfactual Explainable Fairness framework, called CEF, which generates
explanations about model fairness that can improve the fairness without
significantly hurting the performance.The CEF framework formulates an
optimization problem to learn the “minimal” change of the input features that
changes the recommendation results to a certain level of fairness. Based on the
counterfactual recommendation result of each feature, we calculate an
explainability score in terms of the fairness-utility trade-off to rank all the
feature-based explanations, and select the top ones as fairness explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/dalvi2022towards/">Towards Teachable Reasoning Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Teachable Reasoning Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Teachable Reasoning Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bhavana Dalvi, Oyvind Tafjord, Peter Clark</td>
	<td></td>
	<td><p>Our goal is a teachable reasoning system for question-answering (QA), where a
user can interact with faithful answer explanations, and correct errors so that
the system improves over time. Our approach is three-fold: First, generated
chains of reasoning show how answers are implied by the system’s own internal
beliefs. Second, users can interact with the explanations to identify erroneous
model beliefs and provide corrections. Third, we augment the model with a
dynamic memory of such corrections. Retrievals from memory are used as
additional context for QA, to help avoid previous mistakes in similar new
situations - a novel type of memory-based continuous learning. To our
knowledge, this is the first system to generate chains that are both faithful
(the answer follows from the reasoning) and truthful (the chain reflects the
system’s own beliefs, as ascertained by self-querying). In evaluation, users
judge that a majority (65%+) of generated chains clearly show how an answer
follows from a set of facts - substantially better than a high-performance
baseline. We also find that using simulated feedback, our system (called
EntailmentWriter) continually improves with time, requiring feedback on only
25% of training examples to reach within 1% of the upper-bound (feedback on all
examples). We observe a similar trend with real users. This suggests new
opportunities for using language models in an interactive setting where users
can inspect, debug, correct, and improve a system’s performance over time.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/chen2022use-case-grounded/">Use-Case-Grounded Simulations for Explanation Evaluation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Use-Case-Grounded Simulations for Explanation Evaluation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Use-Case-Grounded Simulations for Explanation Evaluation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Valerie Chen, Nari Johnson, Nicholay Topin, Gregory Plumb, Ameet Talwalkar</td>
	<td></td>
	<td><p>A growing body of research runs human subject evaluations to study whether
providing users with explanations of machine learning models can help them with
practical real-world use cases. However, running user studies is challenging
and costly, and consequently each study typically only evaluates a limited
number of different settings, e.g., studies often only evaluate a few
arbitrarily selected explanation methods. To address these challenges and aid
user study design, we introduce Use-Case-Grounded Simulated Evaluations
(SimEvals). SimEvals involve training algorithmic agents that take as input the
information content (such as model explanations) that would be presented to
each participant in a human subject study, to predict answers to the use case
of interest. The algorithmic agent’s test set accuracy provides a measure of
the predictiveness of the information content for the downstream use case. We
run a comprehensive evaluation on three real-world use cases (forward
simulation, model debugging, and counterfactual reasoning) to demonstrate that
Simevals can effectively identify which explanation methods will help humans
for each use case. These results provide evidence that SimEvals can be used to
efficiently screen an important set of user study design decisions, e.g.
selecting which explanations should be presented to the user, before running a
potentially costly user study.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/b%C3%B6hle2022b-cos/">B-cos Networks: Alignment is All We Need for Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=B-cos Networks: Alignment is All We Need for Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=B-cos Networks: Alignment is All We Need for Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Moritz Böhle, Mario Fritz, Bernt Schiele</td>
	<td></td>
	<td><p>We present a new direction for increasing the interpretability of deep neural
networks (DNNs) by promoting weight-input alignment during training. For this,
we propose to replace the linear transforms in DNNs by our B-cos transform. As
we show, a sequence (network) of such transforms induces a single linear
transform that faithfully summarises the full model computations. Moreover, the
B-cos transform introduces alignment pressure on the weights during
optimisation. As a result, those induced linear transforms become highly
interpretable and align with task-relevant features. Importantly, the B-cos
transform is designed to be compatible with existing architectures and we show
that it can easily be integrated into common models such as VGGs, ResNets,
InceptionNets, and DenseNets, whilst maintaining similar performance on
ImageNet. The resulting explanations are of high visual quality and perform
well under quantitative metrics for interpretability. Code available at
https://www.github.com/moboehle/B-cos.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/bra%C5%9Foveanu2022visualizing/">Visualizing and Explaining Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Explaining Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Explaining Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adrian M. P. Braşoveanu, Răzvan Andonie</td>
	<td></td>
	<td><p>During the last decade, Natural Language Processing has become, after
Computer Vision, the second field of Artificial Intelligence that was massively
changed by the advent of Deep Learning. Regardless of the architecture, the
language models of the day need to be able to process or generate text, as well
as predict missing words, sentences or relations depending on the task. Due to
their black-box nature, such models are difficult to interpret and explain to
third parties. Visualization is often the bridge that language model designers
use to explain their work, as the coloring of the salient words and phrases,
clustering or neuron activations can be used to quickly understand the
underlying models. This paper showcases the techniques used in some of the most
popular Deep Learning for NLP visualizations, with a special focus on
interpretability and explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/buchholz2022means-end/">A Means-End Account of Explainable Artificial Intelligence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Means-End Account of Explainable Artificial Intelligence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Means-End Account of Explainable Artificial Intelligence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Oliver Buchholz</td>
	<td></td>
	<td><p>Explainable artificial intelligence (XAI) seeks to produce explanations for
those machine learning methods which are deemed opaque. However, there is
considerable disagreement about what this means and how to achieve it. Authors
disagree on what should be explained (topic), to whom something should be
explained (stakeholder), how something should be explained (instrument), and
why something should be explained (goal). In this paper, I employ insights from
means-end epistemology to structure the field. According to means-end
epistemology, different means ought to be rationally adopted to achieve
different epistemic ends. Applied to XAI, different topics, stakeholders, and
goals thus require different instruments. I call this the means-end account of
XAI. The means-end account has a descriptive and a normative component: on the
one hand, I show how the specific means-end relations give rise to a taxonomy
of existing contributions to the field of XAI; on the other hand, I argue that
the suitability of XAI methods can be assessed by analyzing whether they are
prescribed by a given topic, stakeholder, and goal.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/benshmuel2022meet/">Meet You Halfway: Explaining Deep Learning Mysteries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Meet You Halfway: Explaining Deep Learning Mysteries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Meet You Halfway: Explaining Deep Learning Mysteries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Oriel BenShmuel</td>
	<td></td>
	<td><p>Deep neural networks perform exceptionally well on various learning tasks
with state-of-the-art results. While these models are highly expressive and
achieve impressively accurate solutions with excellent generalization
abilities, they are susceptible to minor perturbations. Samples that suffer
such perturbations are known as “adversarial examples”. Even though deep
learning is an extensively researched field, many questions about the nature of
deep learning models remain unanswered. In this paper, we introduce a new
conceptual framework attached with a formal description that aims to shed light
on the network’s behavior and interpret the behind-the-scenes of the learning
process. Our framework provides an explanation for inherent questions
concerning deep learning. Particularly, we clarify: (1) Why do neural networks
acquire generalization abilities? (2) Why do adversarial examples transfer
between different models?. We provide a comprehensive set of experiments that
support this new framework, as well as its underlying theory.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/alregib2022explanatory/">Explanatory Paradigms in Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Paradigms in Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Paradigms in Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ghassan AlRegib, Mohit Prabhushankar</td>
	<td></td>
	<td><p>In this article, we present a leap-forward expansion to the study of
explainability in neural networks by considering explanations as answers to
abstract reasoning-based questions. With $P$ as the prediction from a neural
network, these questions are <code class="language-plaintext highlighter-rouge">Why P?', </code>What if not P?’, and `Why P, rather
than Q?’ for a given contrast prediction $Q$. The answers to these questions
are observed correlations, observed counterfactuals, and observed contrastive
explanations respectively. Together, these explanations constitute the
abductive reasoning scheme. We term the three explanatory schemes as observed
explanatory paradigms. The term observed refers to the specific case of
post-hoc explainability, when an explanatory technique explains the decision
$P$ after a trained neural network has made the decision $P$. The primary
advantage of viewing explanations through the lens of abductive reasoning-based
questions is that explanations can be used as reasons while making decisions.
The post-hoc field of explainability, that previously only justified decisions,
becomes active by being involved in the decision making process and providing
limited, but relevant and contextual interventions. The contributions of this
article are: ($i$) realizing explanations as reasoning paradigms, ($ii$)
providing a probabilistic definition of observed explanations and their
completeness, ($iii$) creating a taxonomy for evaluation of explanations, and
($iv$) positioning gradient-based complete explanainability’s replicability and
reproducibility across multiple applications and data modalities, ($v$) code
repositories, publicly available at
https://github.com/olivesgatech/Explanatory-Paradigms.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/achtibat2022where/">From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From "Where" to "What": Towards Human-Understandable Explanations through Concept Relevance Propagation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Reduan Achtibat, Maximilian Dreyer, Ilona Eisenbraun, Sebastian Bosse, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin</td>
	<td></td>
	<td><p>The emerging field of eXplainable Artificial Intelligence (XAI) aims to bring
transparency to today’s powerful but opaque deep learning models. While local
XAI methods explain individual predictions in form of attribution maps, thereby
identifying where important features occur (but not providing information about
what they represent), global explanation techniques visualize what concepts a
model has generally learned to encode. Both types of methods thus only provide
partial insights and leave the burden of interpreting the model’s reasoning to
the user. Only few contemporary techniques aim at combining the principles
behind both local and global XAI for obtaining more informative explanations.
Those methods, however, are often limited to specific model architectures or
impose additional requirements on training regimes or data and label
availability, which renders the post-hoc application to arbitrarily pre-trained
models practically impossible. In this work we introduce the Concept Relevance
Propagation (CRP) approach, which combines the local and global perspectives of
XAI and thus allows answering both the “where” and “what” questions for
individual predictions, without additional constraints imposed. We further
introduce the principle of Relevance Maximization for finding representative
examples of encoded concepts based on their usefulness to the model. We thereby
lift the dependency on the common practice of Activation Maximization and its
limitations. We demonstrate the capabilities of our methods in various
settings, showcasing that Concept Relevance Propagation and Relevance
Maximization lead to more human interpretable explanations and provide deep
insights into the model’s representations and reasoning through concept
atlases, concept composition analyses, and quantitative investigations of
concept subspaces and their role in fine-grained decision making.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ai2022explanatory/">Explanatory machine learning for sequential human teaching</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory machine learning for sequential human teaching' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory machine learning for sequential human teaching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lun Ai, Johannes Langer, Stephen H. Muggleton, Ute Schmid</td>
	<td></td>
	<td><p>The topic of comprehensibility of machine-learned theories has recently drawn
increasing attention. Inductive Logic Programming (ILP) uses logic programming
to derive logic theories from small data based on abduction and induction
techniques. Learned theories are represented in the form of rules as
declarative descriptions of obtained knowledge. In earlier work, the authors
provided the first evidence of a measurable increase in human comprehension
based on machine-learned logic rules for simple classification tasks. In a
later study, it was found that the presentation of machine-learned explanations
to humans can produce both beneficial and harmful effects in the context of
game learning. We continue our investigation of comprehensibility by examining
the effects of the ordering of concept presentations on human comprehension. In
this work, we examine the explanatory effects of curriculum order and the
presence of machine-learned explanations for sequential problem-solving. We
show that 1) there exist tasks A and B such that learning A before B has a
better human comprehension with respect to learning B before A and 2) there
exist tasks A and B such that the presence of explanations when learning A
contributes to improved human comprehension when subsequently learning B. We
propose a framework for the effects of sequential teaching on comprehension
based on an existing definition of comprehensibility and provide evidence for
support from data collected in human trials. Empirical results show that
sequential teaching of concepts with increasing complexity a) has a beneficial
effect on human comprehension and b) leads to human re-discovery of
divide-and-conquer problem-solving strategies, and c) studying machine-learned
explanations allows adaptations of human problem-solving strategy with better
performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yadav2022learningtheoretic/">A Learning-Theoretic Framework for Certified Auditing of Machine Learning Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Learning-Theoretic Framework for Certified Auditing of Machine Learning Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Learning-Theoretic Framework for Certified Auditing of Machine Learning Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chhavi Yadav, Michal Moshkovitz, Kamalika Chaudhuri</td>
	<td></td>
	<td><p>Responsible use of machine learning requires that models be audited for
undesirable properties. However, how to do principled auditing in a general
setting has remained ill-understood. In this paper, we propose a formal
learning-theoretic framework for auditing. We propose algorithms for auditing
linear classifiers for feature sensitivity using label queries as well as
different kinds of explanations, and provide performance guarantees. Our
results illustrate that while counterfactual explanations can be extremely
helpful for auditing, anchor explanations may not be as beneficial in the worst
case.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/xie2022towards/">Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Reinforcement Learning Models via Inverse Reinforcement Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yuansheng Xie, Soroush Vosoughi, Saeed Hassanpour</td>
	<td></td>
	<td><p>Artificial intelligence, particularly through recent advancements in deep
learning, has achieved exceptional performances in many tasks in fields such as
natural language processing and computer vision. In addition to desirable
evaluation metrics, a high level of interpretability is often required for
these models to be reliably utilized. Therefore, explanations that offer
insight into the process by which a model maps its inputs onto its outputs are
much sought-after. Unfortunately, current black box nature of machine learning
models is still an unresolved issue and this very nature prevents researchers
from learning and providing explicative descriptions for a model’s behavior and
final predictions. In this work, we propose a novel framework utilizing
Adversarial Inverse Reinforcement Learning that can provide global explanations
for decisions made by a Reinforcement Learning model and capture intuitive
tendencies that the model follows by summarizing the model’s decision-making
process.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/woodcock2022impact/">The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Impact of Explanations on Layperson Trust in Artificial Intelligence-Driven Symptom Checker Apps: Experimental Study' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Claire Woodcock, Brent Mittelstadt, Dan Busbridge, Grant Blank</td>
	<td>J Med Internet Res 2021;23(11):e29386</td>
	<td><p>To achieve the promoted benefits of an AI symptom checker, laypeople must
trust and subsequently follow its instructions. In AI, explanations are seen as
a tool to communicate the rationale behind black-box decisions to encourage
trust and adoption. However, the effectiveness of the types of explanations
used in AI-driven symptom checkers has not yet been studied. Social theories
suggest that why-explanations are better at communicating knowledge and
cultivating trust among laypeople. This study ascertains whether explanations
provided by a symptom checker affect explanatory trust among laypeople (N=750)
and whether this trust is impacted by their existing knowledge of disease.
  Results suggest system builders developing explanations for symptom-checking
apps should consider the recipient’s knowledge of a disease and tailor
explanations to each user’s specific need. Effort should be placed on
generating explanations that are personalized to each user of a symptom checker
to fully discount the diseases that they may be aware of and to close their
information gap.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/tsai2022faithshap/">Faith-Shap: The Faithful Shapley Interaction Index</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Faith-Shap: The Faithful Shapley Interaction Index' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Faith-Shap: The Faithful Shapley Interaction Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Che-Ping Tsai, Chih-Kuan Yeh, Pradeep Ravikumar</td>
	<td></td>
	<td><p>Shapley values, which were originally designed to assign attributions to
individual players in coalition games, have become a commonly used approach in
explainable machine learning to provide attributions to input features for
black-box machine learning models. A key attraction of Shapley values is that
they uniquely satisfy a very natural set of axiomatic properties. However,
extending the Shapley value to assigning attributions to interactions rather
than individual players, an interaction index, is non-trivial: as the natural
set of axioms for the original Shapley values, extended to the context of
interactions, no longer specify a unique interaction index. Many proposals thus
introduce additional less “natural” axioms, while sacrificing the key axiom of
efficiency, in order to obtain unique interaction indices. In this work, rather
than introduce additional conflicting axioms, we adopt the viewpoint of Shapley
values as coefficients of the most faithful linear approximation to the
pseudo-Boolean coalition game value function. By extending linear to
$\ell$-order polynomial approximations, we can then define the general family
of faithful interaction indices}. We show that by additionally requiring the
faithful interaction indices to satisfy interaction-extensions of the standard
individual Shapley axioms (dummy, symmetry, linearity, and efficiency), we
obtain a unique FaithfulShapley Interaction index, which we denote Faith-Shap,
as a natural generalization of the Shapley value to interactions. We then
provide some illustrative contrasts of Faith-Shap with previously proposed
interaction indices, and further investigate some of its interesting algebraic
properties. We further show the computational efficiency of computing
Faith-Shap, together with some additional qualitative insights, via some
illustrative experiments.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yang2022psychological/">A Psychological Theory of Explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Psychological Theory of Explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Psychological Theory of Explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Scott Cheng-Hsin Yang, Tomas Folke, Patrick Shafto</td>
	<td></td>
	<td><p>The goal of explainable Artificial Intelligence (XAI) is to generate
human-interpretable explanations, but there are no computationally precise
theories of how humans interpret AI generated explanations. The lack of theory
means that validation of XAI must be done empirically, on a case-by-case basis,
which prevents systematic theory-building in XAI. We propose a psychological
theory of how humans draw conclusions from saliency maps, the most common form
of XAI explanation, which for the first time allows for precise prediction of
explainee inference conditioned on explanation. Our theory posits that absent
explanation humans expect the AI to make similar decisions to themselves, and
that they interpret an explanation by comparison to the explanations they
themselves would give. Comparison is formalized via Shepard’s universal law of
generalization in a similarity space, a classic theory from cognitive science.
A pre-registered user study on AI image classifications with saliency map
explanations demonstrate that our theory quantitatively matches participants’
predictions of the AI.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/stalder2022see/">What You See is What You Classify: Black Box Attributions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What You See is What You Classify: Black Box Attributions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What You See is What You Classify: Black Box Attributions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Steven Stalder, Nathanaël Perraudin, Radhakrishna Achanta, Fernando Perez-Cruz, Michele Volpi</td>
	<td></td>
	<td><p>An important step towards explaining deep image classifiers lies in the
identification of image regions that contribute to individual class scores in
the model’s output. However, doing this accurately is a difficult task due to
the black-box nature of such networks. Most existing approaches find such
attributions either using activations and gradients or by repeatedly perturbing
the input. We instead address this challenge by training a second deep network,
the Explainer, to predict attributions for a pre-trained black-box classifier,
the Explanandum. These attributions are in the form of masks that only show the
classifier-relevant parts of an image, masking out the rest. Our approach
produces sharper and more boundary-precise masks when compared to the saliency
maps generated by other methods. Moreover, unlike most existing approaches,
ours is capable of directly generating very distinct class-specific masks.
Finally, the proposed method is very efficient for inference since it only
takes a single forward pass through the Explainer to generate all
class-specific masks. We show that our attributions are superior to established
methods both visually and quantitatively, by evaluating them on the PASCAL
VOC-2007 and Microsoft COCO-2014 datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/swamy2022evaluating/">Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vinitra Swamy, Bahar Radmehr, Natasa Krco, Mirko Marras, Tanja Käser</td>
	<td></td>
	<td><p>Neural networks are ubiquitous in applied machine learning for education.
Their pervasive success in predictive performance comes alongside a severe
weakness, the lack of explainability of their decisions, especially relevant in
human-centric fields. We implement five state-of-the-art methodologies for
explaining black-box machine learning models (LIME, PermutationSHAP,
KernelSHAP, DiCE, CEM) and examine the strengths of each approach on the
downstream task of student performance prediction for five massive open online
courses. Our experiments demonstrate that the families of explainers do not
agree with each other on feature importance for the same Bidirectional LSTM
models with the same representative set of students. We use Principal Component
Analysis, Jensen-Shannon distance, and Spearman’s rank-order correlation to
quantitatively cross-examine explanations across methods and courses.
Furthermore, we validate explainer performance across curriculum-based
prerequisite relationships. Our results come to the concerning conclusion that
the choice of explainer is an important decision and is in fact paramount to
the interpretation of the predictive results, even more so than the course the
model is trained on. Source code and models are released at
http://github.com/epfl-ml4ed/evaluating-explainers.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/scafarto2022calibrate/">Calibrate to Interpret</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Calibrate to Interpret' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Calibrate to Interpret' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gregory Scafarto, Nicolas Posocco, Antoine Bonnefoy</td>
	<td></td>
	<td><p>Trustworthy machine learning is driving a large number of ML community works
in order to improve ML acceptance and adoption. The main aspect of trustworthy
machine learning are the followings: fairness, uncertainty, robustness,
explainability and formal guaranties. Each of these individual domains gains
the ML community interest, visible by the number of related publications.
However few works tackle the interconnection between these fields. In this
paper we show a first link between uncertainty and explainability, by studying
the relation between calibration and interpretation. As the calibration of a
given model changes the way it scores samples, and interpretation approaches
often rely on these scores, it seems safe to assume that the
confidence-calibration of a model interacts with our ability to interpret such
model. In this paper, we show, in the context of networks trained on image
classification tasks, to what extent interpretations are sensitive to
confidence-calibration. It leads us to suggest a simple practice to improve the
interpretation outcomes: Calibrate to Interpret.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/r%C3%A4uker2022toward/">Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tilman Räuker, Anson Ho, Stephen Casper, Dylan Hadfield-Menell</td>
	<td></td>
	<td><p>The last decade of machine learning has seen drastic increases in scale and
capabilities, and deep neural networks (DNNs) are increasingly being deployed
across a wide range of domains. However, the inner workings of DNNs are
generally difficult to understand, raising concerns about the safety of using
these systems without a rigorous understanding of how they function. In this
survey, we review literature on techniques for interpreting the inner
components of DNNs, which we call “inner” interpretability methods.
Specifically, we review methods for interpreting weights, neurons, subnetworks,
and latent representations with a focus on how these techniques relate to the
goal of designing safer, more trustworthy AI systems. We also highlight
connections between interpretability and work in modularity, adversarial
robustness, continual learning, network compression, and studying the human
visual system. Finally, we discuss key challenges and argue for future work in
interpretability for AI safety that focuses on diagnostics, benchmarking, and
robustness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/rao2022towards/">Towards Better Understanding Attribution Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Better Understanding Attribution Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Better Understanding Attribution Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sukrut Rao, Moritz Böhle, Bernt Schiele</td>
	<td></td>
	<td><p>Deep neural networks are very successful on many vision tasks, but hard to
interpret due to their black box nature. To overcome this, various post-hoc
attribution methods have been proposed to identify image regions most
influential to the models’ decisions. Evaluating such methods is challenging
since no ground truth attributions exist. We thus propose three novel
evaluation schemes to more reliably measure the faithfulness of those methods,
to make comparisons between them more fair, and to make visual inspection more
systematic. To address faithfulness, we propose a novel evaluation setting
(DiFull) in which we carefully control which parts of the input can influence
the output in order to distinguish possible from impossible attributions. To
address fairness, we note that different methods are applied at different
layers, which skews any comparison, and so evaluate all methods on the same
layers (ML-Att) and discuss how this impacts their performance on quantitative
metrics. For more systematic visualizations, we propose a scheme (AggAtt) to
qualitatively evaluate the methods on complete datasets. We use these
evaluation schemes to study strengths and shortcomings of some widely used
attribution methods. Finally, we propose a post-processing smoothing step that
significantly improves the performance of some attribution methods, and discuss
its applicability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/ramamurthy2022analogies/">Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Analogies and Feature Attributions for Model Agnostic Explanation of Similarity Learners' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Karthikeyan Natesan Ramamurthy, Amit Dhurandhar, Dennis Wei, Zaid Bin Tariq</td>
	<td></td>
	<td><p>Post-hoc explanations for black box models have been studied extensively in
classification and regression settings. However, explanations for models that
output similarity between two inputs have received comparatively lesser
attention. In this paper, we provide model agnostic local explanations for
similarity learners applicable to tabular and text data. We first propose a
method that provides feature attributions to explain the similarity between a
pair of inputs as determined by a black box similarity learner. We then propose
analogies as a new form of explanation in machine learning. Here the goal is to
identify diverse analogous pairs of examples that share the same level of
similarity as the input pair and provide insight into (latent) factors
underlying the model’s prediction. The selection of analogies can optionally
leverage feature attributions, thus connecting the two forms of explanation
while still maintaining complementarity. We prove that our analogy objective
function is submodular, making the search for good-quality analogies efficient.
We apply the proposed approaches to explain similarities between sentences as
predicted by a state-of-the-art sentence encoder, and between patients in a
healthcare utilization application. Efficacy is measured through quantitative
evaluations, a careful user study, and examples of explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/yang2022robust/">On Robust Prefix-Tuning for Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Robust Prefix-Tuning for Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Robust Prefix-Tuning for Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zonghan Yang, Yang Liu</td>
	<td></td>
	<td><p>Recently, prefix-tuning has gained increasing attention as a
parameter-efficient finetuning method for large-scale pretrained language
models. The method keeps the pretrained models fixed and only updates the
prefix token parameters for each downstream task. Despite being lightweight and
modular, prefix-tuning still lacks robustness to textual adversarial attacks.
However, most currently developed defense techniques necessitate auxiliary
model update and storage, which inevitably hamper the modularity and low
storage of prefix-tuning. In this work, we propose a robust prefix-tuning
framework that preserves the efficiency and modularity of prefix-tuning. The
core idea of our framework is leveraging the layerwise activations of the
language model by correctly-classified training data as the standard for
additional prefix finetuning. During the test phase, an extra batch-level
prefix is tuned for each batch and added to the original prefix for robustness
enhancement. Extensive experiments on three text classification benchmarks show
that our framework substantially improves robustness over several strong
baselines against five textual attacks of different types while maintaining
comparable accuracy on clean texts. We also interpret our robust prefix-tuning
framework from the optimal control perspective and pose several directions for
future research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2022</td>
	<td><a href="/publications/zhou2022solvability/">The Solvability of Interpretability Evaluation Metrics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Solvability of Interpretability Evaluation Metrics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Solvability of Interpretability Evaluation Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yilun Zhou, Julie Shah</td>
	<td></td>
	<td><p>Feature attribution methods are popular for explaining neural network
predictions, and they are often evaluated on metrics such as comprehensiveness
and sufficiency, which are motivated by the principle that more important
features – as judged by the explanation – should have larger impacts on model
prediction. In this paper, we highlight an intriguing property of these
metrics: their solvability. Concretely, we can define the problem of optimizing
an explanation for a metric and solve it using beam search. This brings up the
obvious question: given such solvability, why do we still develop other
explainers and then evaluate them on the metric? We present a series of
investigations showing that this beam search explainer is generally comparable
or favorable to current choices such as LIME and SHAP, suggest rethinking the
goals of model interpretability, and identify several directions towards better
evaluations of new method proposals.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2021</td>
	<td><a href="/publications/pfitzinger2021interpretable/">An Interpretable Neural Network for Parameter Inference</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Interpretable Neural Network for Parameter Inference' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Interpretable Neural Network for Parameter Inference' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Johann Pfitzinger</td>
	<td></td>
	<td><p>Adoption of deep neural networks in fields such as economics or finance has
been constrained by the lack of interpretability of model outcomes. This paper
proposes a generative neural network architecture - the parameter encoder
neural network (PENN) - capable of estimating local posterior distributions for
the parameters of a regression model. The parameters fully explain predictions
in terms of the inputs and permit visualization, interpretation and inference
in the presence of complex heterogeneous effects and feature dependencies. The
use of Bayesian inference techniques offers an intuitive mechanism to
regularize local parameter estimates towards a stable solution, and to reduce
noise-fitting in settings of limited data availability. The proposed neural
network is particularly well-suited to applications in economics and finance,
where parameter inference plays an important role. An application to an asset
pricing problem demonstrates how the PENN can be used to explore nonlinear risk
dynamics in financial markets, and to compare empirical nonlinear effects to
behavior posited by financial theory.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/pan2021definitions/">The Definitions of Interpretability and Learning of Interpretable Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Definitions of Interpretability and Learning of Interpretable Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Definitions of Interpretability and Learning of Interpretable Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Weishen Pan, Changshui Zhang</td>
	<td></td>
	<td><p>As machine learning algorithms getting adopted in an ever-increasing number
of applications, interpretation has emerged as a crucial desideratum. In this
paper, we propose a mathematical definition for the human-interpretable model.
In particular, we define interpretability between two information process
systems. If a prediction model is interpretable by a human recognition system
based on the above interpretability definition, the prediction model is defined
as a completely human-interpretable model. We further design a practical
framework to train a completely human-interpretable model by user interactions.
Experiments on image datasets show the advantages of our proposed model in two
aspects: 1) The completely human-interpretable model can provide an entire
decision-making process that is human-understandable; 2) The completely
human-interpretable model is more robust against adversarial attacks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/niu2021explainable/">Explainable Student Performance Prediction With Personalized Attention for Explaining Why A Student Fails</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Student Performance Prediction With Personalized Attention for Explaining Why A Student Fails' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Student Performance Prediction With Personalized Attention for Explaining Why A Student Fails' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kun Niu, Xipeng Cao, Yicong Yu</td>
	<td></td>
	<td><p>As student failure rates continue to increase in higher education, predicting
student performance in the following semester has become a significant demand.
Personalized student performance prediction helps educators gain a
comprehensive view of student status and effectively intervene in advance.
However, existing works scarcely consider the explainability of student
performance prediction, which educators are most concerned about. In this
paper, we propose a novel Explainable Student performance prediction method
with Personalized Attention (ESPA) by utilizing relationships in student
profiles and prior knowledge of related courses. The designed Bidirectional
Long Short-Term Memory (BiLSTM) architecture extracts the semantic information
in the paths with specific patterns. As for leveraging similar paths’ internal
relations, a local and global-level attention mechanism is proposed to
distinguish the influence of different students or courses for making
predictions. Hence, valid reasoning on paths can be applied to predict the
performance of students. The ESPA consistently outperforms the other
state-of-the-art models for student performance prediction, and the results are
intuitively explainable. This work can help educators better understand the
different impacts of behavior on students’ studies.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/phuong2021towards/">Towards Understanding Knowledge Distillation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Understanding Knowledge Distillation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Understanding Knowledge Distillation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mary Phuong, Christoph H. Lampert</td>
	<td></td>
	<td><p>Knowledge distillation, i.e., one classifier being trained on the outputs of
another classifier, is an empirically very successful technique for knowledge
transfer between classifiers. It has even been observed that classifiers learn
much faster and more reliably if trained with the outputs of another classifier
as soft labels, instead of from ground truth data. So far, however, there is no
satisfactory theoretical explanation of this phenomenon. In this work, we
provide the first insights into the working mechanisms of distillation by
studying the special case of linear and deep linear classifiers. Specifically,
we prove a generalization bound that establishes fast convergence of the
expected risk of a distillation-trained linear classifier. From the bound and
its proof we extract three key factors that determine the success of
distillation: * data geometry – geometric properties of the data distribution,
in particular class separation, has a direct influence on the convergence speed
of the risk; * optimization bias – gradient descent optimization finds a very
favorable minimum of the distillation objective; and * strong monotonicity –
the expected risk of the student classifier always decreases when the size of
the training set grows.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yona2021revisiting/">Revisiting Sanity Checks for Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Revisiting Sanity Checks for Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Revisiting Sanity Checks for Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gal Yona, Daniel Greenfeld</td>
	<td></td>
	<td><p>Saliency methods are a popular approach for model debugging and
explainability. However, in the absence of ground-truth data for what the
correct maps should be, evaluating and comparing different approaches remains a
long-standing challenge. The sanity checks methodology of Adebayo et al
[Neurips 2018] has sought to address this challenge. They argue that some
popular saliency methods should not be used for explainability purposes since
the maps they produce are not sensitive to the underlying model that is to be
explained. Through a causal re-framing of their objective, we argue that their
empirical evaluation does not fully establish these conclusions, due to a form
of confounding introduced by the tasks they evaluate on. Through various
experiments on simple custom tasks we demonstrate that some of their
conclusions may indeed be artifacts of the tasks more than a criticism of the
saliency methods themselves. More broadly, our work challenges the utility of
the sanity check methodology, and further highlights that saliency map
evaluation beyond ad-hoc visual examination remains a fundamental challenge.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/nallbani2021resvgae/">ResVGAE: Going Deeper with Residual Modules for Link Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=ResVGAE: Going Deeper with Residual Modules for Link Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=ResVGAE: Going Deeper with Residual Modules for Link Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Indrit Nallbani, Reyhan Kevser Keser, Aydin Ayanzadeh, Nurullah Çalık, Behçet Uğur Töreyin</td>
	<td></td>
	<td><p>Graph autoencoders are efficient at embedding graph-based data sets. Most
graph autoencoder architectures have shallow depths which limits their ability
to capture meaningful relations between nodes separated by multi-hops. In this
paper, we propose Residual Variational Graph Autoencoder, ResVGAE, a deep
variational graph autoencoder model with multiple residual modules. We show
that our multiple residual modules, a convolutional layer with residual
connection, improve the average precision of the graph autoencoders.
Experimental results suggest that our proposed model with residual modules
outperforms the models without residual modules and achieves similar results
when compared with other state-of-the-art methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/mougan2021desiderata/">Desiderata for Explainable AI in statistical production systems of the European Central Bank</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Desiderata for Explainable AI in statistical production systems of the European Central Bank' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Desiderata for Explainable AI in statistical production systems of the European Central Bank' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Carlos Mougan, Georgios Kanellos, Thomas Gottron</td>
	<td></td>
	<td><p>Explainable AI constitutes a fundamental step towards establishing fairness
and addressing bias in algorithmic decision-making. Despite the large body of
work on the topic, the benefit of solutions is mostly evaluated from a
conceptual or theoretical point of view and the usefulness for real-world use
cases remains uncertain. In this work, we aim to state clear user-centric
desiderata for explainable AI reflecting common explainability needs
experienced in statistical production systems of the European Central Bank. We
link the desiderata to archetypical user roles and give examples of techniques
and methods which can be used to address the user’s needs. To this end, we
provide two concrete use cases from the domain of statistical data production
in central banks: the detection of outliers in the Centralised Securities
Database and the data-driven identification of data quality checks for the
Supervisory Banking data system.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/nguyen2021effectiveness/">The effectiveness of feature attribution methods and its correlation with automatic evaluation scores</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The effectiveness of feature attribution methods and its correlation with automatic evaluation scores' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The effectiveness of feature attribution methods and its correlation with automatic evaluation scores' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Giang Nguyen, Daeyoung Kim, Anh Nguyen</td>
	<td></td>
	<td><p>Explaining the decisions of an Artificial Intelligence (AI) model is
increasingly critical in many real-world, high-stake applications. Hundreds of
papers have either proposed new feature attribution methods, discussed or
harnessed these tools in their work. However, despite humans being the target
end-users, most attribution methods were only evaluated on proxy
automatic-evaluation metrics (Zhang et al. 2018; Zhou et al. 2016; Petsiuk et
al. 2018). In this paper, we conduct the first user study to measure
attribution map effectiveness in assisting humans in ImageNet classification
and Stanford Dogs fine-grained classification, and when an image is natural or
adversarial (i.e., contains adversarial perturbations). Overall, feature
attribution is surprisingly not more effective than showing humans nearest
training-set examples. On a harder task of fine-grained dog categorization,
presenting attribution maps to humans does not help, but instead hurts the
performance of human-AI teams compared to AI alone. Importantly, we found
automatic attribution-map evaluation measures to correlate poorly with the
actual human-AI team performance. Our findings encourage the community to
rigorously test their methods on the downstream human-in-the-loop applications
and to rethink the existing evaluation metrics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/mayor-torres2021interpretable/">Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable SincNet-based Deep Learning for Emotion Recognition from EEG brain activity' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Juan Manuel Mayor-Torres, Mirco Ravanelli, Sara E. Medina-DeVilliers, Matthew D. Lerner, Giuseppe Riccardi</td>
	<td></td>
	<td><p>Machine learning methods, such as deep learning, show promising results in
the medical domain. However, the lack of interpretability of these algorithms
may hinder their applicability to medical decision support systems. This paper
studies an interpretable deep learning technique, called SincNet. SincNet is a
convolutional neural network that efficiently learns customized band-pass
filters through trainable sinc-functions. In this study, we use SincNet to
analyze the neural activity of individuals with Autism Spectrum Disorder (ASD),
who experience characteristic differences in neural oscillatory activity. In
particular, we propose a novel SincNet-based neural network for detecting
emotions in ASD patients using EEG signals. The learned filters can be easily
inspected to detect which part of the EEG spectrum is used for predicting
emotions. We found that our system automatically learns the high-$\alpha$ (9-13
Hz) and $\beta$ (13-30 Hz) band suppression often present in individuals with
ASD. This result is consistent with recent neuroscience studies on emotion
recognition, which found an association between these band suppressions and the
behavioral deficits observed in individuals with ASD. The improved
interpretability of SincNet is achieved without sacrificing performance in
emotion recognition.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/makinwa2021detection/">Detection Accuracy for Evaluating Compositional Explanations of Units</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Detection Accuracy for Evaluating Compositional Explanations of Units' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Detection Accuracy for Evaluating Compositional Explanations of Units' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sayo M. Makinwa, Biagio La Rosa, Roberto Capobianco</td>
	<td></td>
	<td><p>The recent success of deep learning models in solving complex problems and in
different domains has increased interest in understanding what they learn.
Therefore, different approaches have been employed to explain these models, one
of which uses human-understandable concepts as explanations. Two examples of
methods that use this approach are Network Dissection and Compositional
explanations. The former explains units using atomic concepts, while the latter
makes explanations more expressive, replacing atomic concepts with logical
forms. While intuitively, logical forms are more informative than atomic
concepts, it is not clear how to quantify this improvement, and their
evaluation is often based on the same metric that is optimized during the
search-process and on the usage of hyper-parameters to be tuned. In this paper,
we propose to use as evaluation metric the Detection Accuracy, which measures
units’ consistency of detection of their assigned explanations. We show that
this metric (1) evaluates explanations of different lengths effectively, (2)
can be used as a stopping criterion for the compositional explanation search,
eliminating the explanation length hyper-parameter, and (3) exposes new
specialized units whose length 1 explanations are the perceptual abstractions
of their longer explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ye2021explanations/">Can Explanations Be Useful for Calibrating Black Box Models?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Can Explanations Be Useful for Calibrating Black Box Models?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Can Explanations Be Useful for Calibrating Black Box Models?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xi Ye, Greg Durrett</td>
	<td></td>
	<td><p>NLP practitioners often want to take existing trained models and apply them
to data from new domains. While fine-tuning or few-shot learning can be used to
adapt a base model, there is no single recipe for making these techniques work;
moreover, one may not have access to the original model weights if it is
deployed as a black box. We study how to improve a black box model’s
performance on a new domain by leveraging explanations of the model’s behavior.
Our approach first extracts a set of features combining human intuition about
the task with model attributions generated by black box interpretation
techniques, then uses a simple calibrator, in the form of a classifier, to
predict whether the base model was correct or not. We experiment with our
method on two tasks, extractive question answering and natural language
inference, covering adaptation from several pairs of domains with limited
target-domain data. The experimental results across all the domain pairs show
that explanations are useful for calibrating these models, boosting accuracy
when predictions do not have to be returned on every example. We further show
that the calibration model transfers to some extent between tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/lu2021crowdsourcing/">Crowdsourcing Evaluation of Saliency-based XAI Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Crowdsourcing Evaluation of Saliency-based XAI Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Crowdsourcing Evaluation of Saliency-based XAI Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaotian Lu, Arseny Tolmachev, Tatsuya Yamamoto, Koh Takeuchi, Seiji Okajima, Tomoyoshi Takebayashi, Koji Maruhashi, Hisashi Kashima</td>
	<td></td>
	<td><p>Understanding the reasons behind the predictions made by deep neural networks
is critical for gaining human trust in many important applications, which is
reflected in the increasing demand for explainability in AI (XAI) in recent
years. Saliency-based feature attribution methods, which highlight important
parts of images that contribute to decisions by classifiers, are often used as
XAI methods, especially in the field of computer vision. In order to compare
various saliency-based XAI methods quantitatively, several approaches for
automated evaluation schemes have been proposed; however, there is no guarantee
that such automated evaluation metrics correctly evaluate explainability, and a
high rating by an automated evaluation scheme does not necessarily mean a high
explainability for humans. In this study, instead of the automated evaluation,
we propose a new human-based evaluation scheme using crowdsourcing to evaluate
XAI methods. Our method is inspired by a human computation game, “Peek-a-boom”,
and can efficiently compare different XAI methods by exploiting the power of
crowds. We evaluate the saliency maps of various XAI methods on two datasets
with automated and crowd-based evaluation schemes. Our experiments show that
the result of our crowd-based evaluation scheme is different from those of
automated evaluation schemes. In addition, we regard the crowd-based evaluation
results as ground truths and provide a quantitative performance measure to
compare different automated evaluation schemes. We also discuss the impact of
crowd workers on the results and show that the varying ability of crowd workers
does not significantly impact the results.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/looveren2021conditional/">Conditional Generative Models for Counterfactual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Conditional Generative Models for Counterfactual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Conditional Generative Models for Counterfactual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Arnaud Van Looveren, Janis Klaise, Giovanni Vacanti, Oliver Cobb</td>
	<td></td>
	<td><p>Counterfactual instances offer human-interpretable insight into the local
behaviour of machine learning models. We propose a general framework to
generate sparse, in-distribution counterfactual model explanations which match
a desired target prediction with a conditional generative model, allowing
batches of counterfactual instances to be generated with a single forward pass.
The method is flexible with respect to the type of generative model used as
well as the task of the underlying predictive model. This allows
straightforward application of the framework to different modalities such as
images, time series or tabular data as well as generative model paradigms such
as GANs or autoencoders and predictive tasks like classification or regression.
We illustrate the effectiveness of our method on image (CelebA), time series
(ECG) and mixed-type tabular (Adult Census) data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/liu2021synthetic/">Synthetic Benchmarks for Scientific Research in Explainable Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Synthetic Benchmarks for Scientific Research in Explainable Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Synthetic Benchmarks for Scientific Research in Explainable Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yang Liu, Sujay Khandagale, Colin White, Willie Neiswanger</td>
	<td></td>
	<td><p>As machine learning models grow more complex and their applications become
more high-stakes, tools for explaining model predictions have become
increasingly important. This has spurred a flurry of research in model
explainability and has given rise to feature attribution methods such as LIME
and SHAP. Despite their widespread use, evaluating and comparing different
feature attribution methods remains challenging: evaluations ideally require
human studies, and empirical evaluation metrics are often data-intensive or
computationally prohibitive on real-world datasets. In this work, we address
this issue by releasing XAI-Bench: a suite of synthetic datasets along with a
library for benchmarking feature attribution algorithms. Unlike real-world
datasets, synthetic datasets allow the efficient computation of conditional
expected values that are needed to evaluate ground-truth Shapley values and
other metrics. The synthetic datasets we release offer a wide variety of
parameters that can be configured to simulate real-world data. We demonstrate
the power of our library by benchmarking popular explainability techniques
across several evaluation metrics and across a variety of settings. The
versatility and efficiency of our library will help researchers bring their
explainability methods from development to deployment. Our code is available at
https://github.com/abacusai/xai-bench.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/liu2021going/">Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zixuan Liu, Ehsan Adeli, Kilian M. Pohl, Qingyu Zhao</td>
	<td></td>
	<td><p>Interpretability is a critical factor in applying complex deep learning
models to advance the understanding of brain disorders in neuroimaging studies.
To interpret the decision process of a trained classifier, existing techniques
typically rely on saliency maps to quantify the voxel-wise or feature-level
importance for classification through partial derivatives. Despite providing
some level of localization, these maps are not human-understandable from the
neuroscience perspective as they do not inform the specific meaning of the
alteration linked to the brain disorder. Inspired by the image-to-image
translation scheme, we propose to train simulator networks that can warp a
given image to inject or remove patterns of the disease. These networks are
trained such that the classifier produces consistently increased or decreased
prediction logits for the simulated images. Moreover, we propose to couple all
the simulators into a unified model based on conditional convolution. We
applied our approach to interpreting classifiers trained on a synthetic dataset
and two neuroimaging datasets to visualize the effect of the Alzheimer’s
disease and alcohol use disorder. Compared to the saliency maps generated by
baseline approaches, our simulations and visualizations based on the Jacobian
determinants of the warping field reveal meaningful and understandable patterns
related to the diseases.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zarlenga2021efficient/">Efficient Decompositional Rule Extraction for Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Decompositional Rule Extraction for Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Decompositional Rule Extraction for Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mateo Espinosa Zarlenga, Zohreh Shams, Mateja Jamnik</td>
	<td></td>
	<td><p>In recent years, there has been significant work on increasing both
interpretability and debuggability of a Deep Neural Network (DNN) by extracting
a rule-based model that approximates its decision boundary. Nevertheless,
current DNN rule extraction methods that consider a DNN’s latent space when
extracting rules, known as decompositional algorithms, are either restricted to
single-layer DNNs or intractable as the size of the DNN or data grows. In this
paper, we address these limitations by introducing ECLAIRE, a novel
polynomial-time rule extraction algorithm capable of scaling to both large DNN
architectures and large training datasets. We evaluate ECLAIRE on a wide
variety of tasks, ranging from breast cancer prognosis to particle detection,
and show that it consistently extracts more accurate and comprehensible rule
sets than the current state-of-the-art methods while using orders of magnitude
less computational resources. We make all of our methods available, including a
rule set visualisation interface, through the open-source REMIX library
(https://github.com/mateoespinosa/remix).</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/langer2021want/">What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What Do We Want From Explainable Artificial Intelligence (XAI)? -- A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Markus Langer, Daniel Oster, Timo Speith, Holger Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, Kevin Baum</td>
	<td></td>
	<td><p>Previous research in Explainable Artificial Intelligence (XAI) suggests that
a main aim of explainability approaches is to satisfy specific interests,
goals, expectations, needs, and demands regarding artificial systems (we call
these stakeholders’ desiderata) in a variety of contexts. However, the
literature on XAI is vast, spreads out across multiple largely disconnected
disciplines, and it often remains unclear how explainability approaches are
supposed to achieve the goal of satisfying stakeholders’ desiderata. This paper
discusses the main classes of stakeholders calling for explainability of
artificial systems and reviews their desiderata. We provide a model that
explicitly spells out the main concepts and relations necessary to consider and
investigate when evaluating, adjusting, choosing, and developing explainability
approaches that aim to satisfy stakeholders’ desiderata. This model can serve
researchers from the variety of different disciplines involved in XAI as a
common ground. It emphasizes where there is interdisciplinary potential in the
evaluation and the development of explainability approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/lampinen2021tell/">Tell me why! Explanations support learning relational and causal structure</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Tell me why! Explanations support learning relational and causal structure' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Tell me why! Explanations support learning relational and causal structure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrew K. Lampinen, Nicholas A. Roy, Ishita Dasgupta, Stephanie C. Y. Chan, Allison C. Tam, James L. McClelland, Chen Yan, Adam Santoro, Neil C. Rabinowitz, Jane X. Wang, Felix Hill</td>
	<td></td>
	<td><p>Inferring the abstract relational and causal structure of the world is a
major challenge for reinforcement-learning (RL) agents. For humans,
language–particularly in the form of explanations–plays a considerable role
in overcoming this challenge. Here, we show that language can play a similar
role for deep RL agents in complex environments. While agents typically
struggle to acquire relational and causal knowledge, augmenting their
experience by training them to predict language descriptions and explanations
can overcome these limitations. We show that language can help agents learn
challenging relational tasks, and examine which aspects of language contribute
to its benefits. We then show that explanations can help agents to infer not
only relational but also causal structure. Language can shape the way that
agents to generalize out-of-distribution from ambiguous, causally-confounded
training, and explanations even allow agents to learn to perform experimental
interventions to identify causal relationships. Our results suggest that
language description and explanation may be powerful tools for improving agent
learning and generalization.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/konstantinov2021attention/">Attention-like feature explanation for tabular data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention-like feature explanation for tabular data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention-like feature explanation for tabular data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrei V. Konstantinov, Lev V. Utkin</td>
	<td></td>
	<td><p>A new method for local and global explanation of the machine learning
black-box model predictions by tabular data is proposed. It is implemented as a
system called AFEX (Attention-like Feature EXplanation) and consisting of two
main parts. The first part is a set of the one-feature neural subnetworks which
aim to get a specific representation for every feature in the form of a basis
of shape functions. The subnetworks use shortcut connections with trainable
parameters to improve the network performance. The second part of AFEX produces
shape functions of features as the weighted sum of the basis shape functions
where weights are computed by using an attention-like mechanism. AFEX
identifies pairwise interactions between features based on pairwise
multiplications of shape functions corresponding to different features. A
modification of AFEX with incorporating an additional surrogate model which
approximates the black-box model is proposed. AFEX is trained end-to-end on a
whole dataset only once such that it does not require to train neural networks
again in the explanation stage. Numerical experiments with synthetic and real
data illustrate AFEX.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kim2021sanity/">Sanity Simulations for Saliency Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Simulations for Saliency Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Simulations for Saliency Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Joon Sik Kim, Gregory Plumb, Ameet Talwalkar</td>
	<td></td>
	<td><p>Saliency methods are a popular class of feature attribution explanation
methods that aim to capture a model’s predictive reasoning by identifying
“important” pixels in an input image. However, the development and adoption of
these methods are hindered by the lack of access to ground-truth model
reasoning, which prevents accurate evaluation. In this work, we design a
synthetic benchmarking framework, SMERF, that allows us to perform
ground-truth-based evaluation while controlling the complexity of the model’s
reasoning. Experimentally, SMERF reveals significant limitations in existing
saliency methods and, as a result, represents a useful tool for the development
of new saliency methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kokhlikyan2021investigating/">Investigating sanity checks for saliency maps with image and text classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Investigating sanity checks for saliency maps with image and text classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Investigating sanity checks for saliency maps with image and text classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Narine Kokhlikyan, Vivek Miglani, Bilal Alsallakh, Miguel Martin, Orion Reblitz-Richardson</td>
	<td></td>
	<td><p>Saliency maps have shown to be both useful and misleading for explaining
model predictions especially in the context of images. In this paper, we
perform sanity checks for text modality and show that the conclusions made for
image do not directly transfer to text. We also analyze the effects of the
input multiplier in certain saliency maps using similarity scores,
max-sensitivity and infidelity evaluation metrics. Our observations reveal that
the input multiplier carries input’s structural patterns in explanation maps,
thus leading to similar results regardless of the choice of model parameters.
We also show that the smoothness of a Neural Network (NN) function can affect
the quality of saliency-based explanations. Our investigations reveal that
replacing ReLUs with Softplus and MaxPool with smoother variants such as
LogSumExp (LSE) can lead to explanations that are more reliable based on the
infidelity evaluation metric.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kong2021deeprare/">DeepRare: Generic Unsupervised Visual Attention Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepRare: Generic Unsupervised Visual Attention Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepRare: Generic Unsupervised Visual Attention Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Phutphalla Kong, Matei Mancas, Bernard Gosselin, Kimtho Po</td>
	<td></td>
	<td><p>Human visual system is modeled in engineering field providing
feature-engineered methods which detect contrasted/surprising/unusual data into
images. This data is “interesting” for humans and leads to numerous
applications. Deep learning (DNNs) drastically improved the algorithms
efficiency on the main benchmark datasets. However, DNN-based models are
counter-intuitive: surprising or unusual data is by definition difficult to
learn because of its low occurrence probability. In reality, DNN-based models
mainly learn top-down features such as faces, text, people, or animals which
usually attract human attention, but they have low efficiency in extracting
surprising or unusual data in the images. In this paper, we propose a new
visual attention model called DeepRare2021 (DR21) which uses the power of DNNs
feature extraction and the genericity of feature-engineered algorithms. This
algorithm is an evolution of a previous version called DeepRare2019 (DR19)
based on a common framework. DR21 1) does not need any training and uses the
default ImageNet training, 2) is fast even on CPU, 3) is tested on four very
different eye-tracking datasets showing that the DR21 is generic and is always
in the within the top models on all datasets and metrics while no other model
exhibits such a regularity and genericity. Finally DR21 4) is tested with
several network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2
(MN2) and 5) it provides explanation and transparency on which parts of the
image are the most surprising at different levels despite the use of a
DNN-based feature extractor. DeepRare2021 code can be found at
https://github.com/numediart/VisualAttention-RareFamil}.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/keane2021twin/">Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based Reasoning Pairings for Explanation and Data Augmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mark T Keane, Eoin M Kenny, Mohammed Temraz, Derek Greene, Barry Smyth</td>
	<td>IJCAI-21 Workshop on DL-CBR-AML, July 2021</td>
	<td><p>Recently, it has been proposed that fruitful synergies may exist between Deep
Learning (DL) and Case Based Reasoning (CBR); that there are insights to be
gained by applying CBR ideas to problems in DL (what could be called DeepCBR).
In this paper, we report on a program of research that applies CBR solutions to
the problem of Explainable AI (XAI) in the DL. We describe a series of
twin-systems pairings of opaque DL models with transparent CBR models that
allow the latter to explain the former using factual, counterfactual and
semi-factual explanation strategies. This twinning shows that functional
abstractions of DL (e.g., feature weights, feature importance and decision
boundaries) can be used to drive these explanatory solutions. We also raise the
prospect that this research also applies to the problem of Data Augmentation in
DL, underscoring the fecundity of these DeepCBR ideas.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/lam2021finding/">Finding Representative Interpretations on Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Finding Representative Interpretations on Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Finding Representative Interpretations on Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Peter Cho-Ho Lam, Lingyang Chu, Maxim Torgonskiy, Jian Pei, Yong Zhang, Lanjun Wang</td>
	<td></td>
	<td><p>Interpreting the decision logic behind effective deep convolutional neural
networks (CNN) on images complements the success of deep learning models.
However, the existing methods can only interpret some specific decision logic
on individual or a small number of images. To facilitate human
understandability and generalization ability, it is important to develop
representative interpretations that interpret common decision logics of a CNN
on a large group of similar images, which reveal the common semantics data
contributes to many closely related predictions. In this paper, we develop a
novel unsupervised approach to produce a highly representative interpretation
for a large number of similar images. We formulate the problem of finding
representative interpretations as a co-clustering problem, and convert it into
a submodular cost submodular cover problem based on a sample of the linear
decision boundaries of a CNN. We also present a visualization and similarity
ranking method. Our extensive experiments demonstrate the excellent performance
of our method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/li2021interpretable/">Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, Dejing Dou</td>
	<td></td>
	<td><p>Deep neural networks have been well-known for their superb handling of
various machine learning and artificial intelligence tasks. However, due to
their over-parameterized black-box nature, it is often difficult to understand
the prediction results of deep models. In recent years, many interpretation
tools have been proposed to explain or reveal how deep models make decisions.
In this paper, we review this line of research and try to make a comprehensive
survey. Specifically, we first introduce and clarify two basic concepts –
interpretations and interpretability – that people usually get confused about.
To address the research efforts in interpretations, we elaborate the designs of
a number of interpretation algorithms, from different perspectives, by
proposing a new taxonomy. Then, to understand the interpretation results, we
also survey the performance metrics for evaluating interpretation algorithms.
Further, we summarize the current works in evaluating models’ interpretability
using “trustworthy” interpretation algorithms. Finally, we review and discuss
the connections between deep models’ interpretations and other factors, such as
adversarial robustness and learning from interpretations, and we introduce
several open-source libraries for interpretation algorithms and evaluation
approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/poursabzisangdeh2021manipulating/">Manipulating and Measuring Model Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Manipulating and Measuring Model Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Manipulating and Measuring Model Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Forough Poursabzi-Sangdeh, Daniel G Goldstein, Jake M Hofman, Jennifer Wortman Wortman Vaughan, Hanna Wallach</td>
	<td>CHI</td>
	<td><p>With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model’s predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N=3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model’s predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model’s sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.</p>
</td>
	<td>measuring </td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zafar2021lack/">On the Lack of Robust Interpretability of Neural Text Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Lack of Robust Interpretability of Neural Text Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Lack of Robust Interpretability of Neural Text Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Muhammad Bilal Zafar, Michele Donini, Dylan Slack, Cédric Archambeau, Sanjiv Das, Krishnaram Kenthapadi</td>
	<td></td>
	<td><p>With the ever-increasing complexity of neural language models, practitioners
have turned to methods for understanding the predictions of these models. One
of the most well-adopted approaches for model interpretability is feature-based
interpretability, i.e., ranking the features in terms of their impact on model
predictions. Several prior studies have focused on assessing the fidelity of
feature-based interpretability methods, i.e., measuring the impact of dropping
the top-ranked features on the model output. However, relatively little work
has been conducted on quantifying the robustness of interpretations. In this
work, we assess the robustness of interpretations of neural text classifiers,
specifically, those based on pretrained Transformer encoders, using two
randomization tests. The first compares the interpretations of two models that
are identical except for their initializations. The second measures whether the
interpretations differ between a model with trained parameters and a model with
random parameters. Both tests show surprising deviations from expected
behavior, raising questions about the extent of insights that practitioners may
draw from interpretations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/joshi2021review/">A Review on Explainability in Multimodal Deep Neural Nets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Review on Explainability in Multimodal Deep Neural Nets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Review on Explainability in Multimodal Deep Neural Nets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gargi Joshi, Rahee Walambe, Ketan Kotecha</td>
	<td>in IEEE Access, vol. 9, pp. 59800-59821, 2021</td>
	<td><p>Artificial Intelligence techniques powered by deep neural nets have achieved
much success in several application domains, most significantly and notably in
the Computer Vision applications and Natural Language Processing tasks.
Surpassing human-level performance propelled the research in the applications
where different modalities amongst language, vision, sensory, text play an
important role in accurate predictions and identification. Several multimodal
fusion methods employing deep learning models are proposed in the literature.
Despite their outstanding performance, the complex, opaque and black-box nature
of the deep neural nets limits their social acceptance and usability. This has
given rise to the quest for model interpretability and explainability, more so
in the complex tasks involving multimodal AI methods. This paper extensively
reviews the present literature to present a comprehensive survey and commentary
on the explainability in multimodal deep neural nets, especially for the vision
and language tasks. Several topics on multimodal AI and its applications for
generic domains have been covered in this paper, including the significance,
datasets, fundamental building blocks of the methods and techniques,
challenges, applications, and future trends in this domain</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/jalwana2021cameras/">CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CAMERAS: Enhanced Resolution And Sanity preserving Class Activation Mapping for image saliency' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mohammad A. A. K. Jalwana, Naveed Akhtar, Mohammed Bennamoun, Ajmal Mian</td>
	<td></td>
	<td><p>Backpropagation image saliency aims at explaining model predictions by
estimating model-centric importance of individual pixels in the input. However,
class-insensitivity of the earlier layers in a network only allows saliency
computation with low resolution activation maps of the deeper layers, resulting
in compromised image saliency. Remedifying this can lead to sanity failures. We
propose CAMERAS, a technique to compute high-fidelity backpropagation saliency
maps without requiring any external priors and preserving the map sanity. Our
method systematically performs multi-scale accumulation and fusion of the
activation maps and backpropagated gradients to compute precise saliency maps.
From accurate image saliency to articulation of relative importance of input
features for different models, and precise discrimination between model
perception of visually similar objects, our high-resolution mapping offers
multiple novel insights into the black-box deep visual models, which are
presented in the paper. We also demonstrate the utility of our saliency maps in
adversarial setup by drastically reducing the norm of attack signals by
focusing them on the precise regions identified by our maps. Our method also
inspires new evaluation metrics and a sanity check for this developing research
direction. Code is available here https://github.com/VisMIL/CAMERAS</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/kakogeorgiou2021evaluating/">Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating explainable artificial intelligence methods for multi-label deep learning classification tasks in remote sensing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ioannis Kakogeorgiou, Konstantinos Karantzalos</td>
	<td>International Journal of Applied Earth Observation and Geoinformation 103 (2021) 102520</td>
	<td><p>Although deep neural networks hold the state-of-the-art in several remote
sensing tasks, their black-box operation hinders the understanding of their
decisions, concealing any bias and other shortcomings in datasets and model
performance. To this end, we have applied explainable artificial intelligence
(XAI) methods in remote sensing multi-label classification tasks towards
producing human-interpretable explanations and improve transparency. In
particular, we utilized and trained deep learning models with state-of-the-art
performance in the benchmark BigEarthNet and SEN12MS datasets. Ten XAI methods
were employed towards understanding and interpreting models’ predictions, along
with quantitative metrics to assess and compare their performance. Numerous
experiments were performed to assess the overall performance of XAI methods for
straightforward prediction cases, competing multiple labels, as well as
misclassification cases. According to our findings, Occlusion, Grad-CAM and
Lime were the most interpretable and reliable XAI methods. However, none
delivers high-resolution outputs, while apart from Grad-CAM, both Lime and
Occlusion are computationally expensive. We also highlight different aspects of
XAI performance and elaborate with insights on black-box decisions in order to
improve transparency, understand their behavior and reveal, as well, datasets’
particularities.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/jacovi2021contrastive/">Contrastive Explanations for Model Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Explanations for Model Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contrastive Explanations for Model Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin Choi, Yoav Goldberg</td>
	<td></td>
	<td><p>Contrastive explanations clarify why an event occurred in contrast to
another. They are more inherently intuitive to humans to both produce and
comprehend. We propose a methodology to produce contrastive explanations for
classification models by modifying the representation to disregard
non-contrastive information, and modifying model behavior to only be based on
contrastive reasoning. Our method is based on projecting model representation
to a latent space that captures only the features that are useful (to the
model) to differentiate two potential decisions. We demonstrate the value of
contrastive explanations by analyzing two different scenarios, using both
high-level abstract concept attribution and low-level input token/span
attribution, on two widely used text classification tasks. Specifically, we
produce explanations for answering: for which label, and against which
alternative label, is some aspect of the input useful? And which aspects of the
input are useful for and against particular decisions? Overall, our findings
shed light on the ability of label-contrastive explanations to provide a more
accurate and finer-grained interpretability of a model’s decision.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/islam2021explainable/">Explainable Artificial Intelligence Approaches: A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Artificial Intelligence Approaches: A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Artificial Intelligence Approaches: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sheikh Rabiul Islam, William Eberle, Sheikh Khaled Ghafoor, Mohiuddin Ahmed</td>
	<td></td>
	<td><p>The lack of explainability of a decision from an Artificial Intelligence (AI)
based “black box” system/model, despite its superiority in many real-world
applications, is a key stumbling block for adopting AI in many high stakes
applications of different domain or industry. While many popular Explainable
Artificial Intelligence (XAI) methods or approaches are available to facilitate
a human-friendly explanation of the decision, each has its own merits and
demerits, with a plethora of open challenges. We demonstrate popular XAI
methods with a mutual case study/task (i.e., credit default prediction),
analyze for competitive advantages from multiple perspectives (e.g., local,
global), provide meaningful insight on quantifying explainability, and
recommend paths towards responsible or human-centered AI using XAI as a medium.
Practitioners can use this work as a catalog to understand, compare, and
correlate competitive advantages of popular XAI methods. In addition, this
survey elicits future research directions towards responsible or human-centric
AI systems, which is crucial to adopt AI in high stakes applications.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hvilsh%C3%B8j2021quantitative/">On Quantitative Evaluations of Counterfactuals</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Quantitative Evaluations of Counterfactuals' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Quantitative Evaluations of Counterfactuals' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Frederik Hvilshøj, Alexandros Iosifidis, Ira Assent</td>
	<td></td>
	<td><p>As counterfactual examples become increasingly popular for explaining
decisions of deep learning models, it is essential to understand what
properties quantitative evaluation metrics do capture and equally important
what they do not capture. Currently, such understanding is lacking, potentially
slowing down scientific progress. In this paper, we consolidate the work on
evaluating visual counterfactual examples through an analysis and experiments.
We find that while most metrics behave as intended for sufficiently simple
datasets, some fail to tell the difference between good and bad counterfactuals
when the complexity increases. We observe experimentally that metrics give good
scores to tiny adversarial-like changes, wrongly identifying such changes as
superior counterfactual examples. To mitigate this issue, we propose two new
metrics, the Label Variation Score and the Oracle score, which are both less
vulnerable to such tiny changes. We conclude that a proper quantitative
evaluation of visual counterfactual examples should combine metrics to ensure
that all aspects of good counterfactuals are quantified.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/huang2021physically/">Physically Explainable CNN for SAR Image Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Physically Explainable CNN for SAR Image Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Physically Explainable CNN for SAR Image Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhongling Huang, Xiwen Yao, Ying Liu, Corneliu Octavian Dumitru, Mihai Datcu, Junwei Han</td>
	<td></td>
	<td><p>Integrating the special electromagnetic characteristics of Synthetic Aperture
Radar (SAR) in deep neural networks is essential in order to enhance the
explainability and physics awareness of deep learning. In this paper, we first
propose a novel physically explainable convolutional neural network for SAR
image classification, namely physics guided and injected learning (PGIL). It
comprises three parts: (1) explainable models (XM) to provide prior physics
knowledge, (2) physics guided network (PGN) to encode the knowledge into
physics-aware features, and (3) physics injected network (PIN) to adaptively
introduce the physics-aware features into classification pipeline for label
prediction. A hybrid Image-Physics SAR dataset format is proposed for
evaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The
results show that the proposed PGIL substantially improve the classification
performance in case of limited labeled data compared with the counterpart
data-driven CNN and other pre-training methods. Additionally, the physics
explanations are discussed to indicate the interpretability and the physical
consistency preserved in the predictions. We deem the proposed method would
promote the development of physically explainable deep learning in SAR image
interpretation field.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hoffmann2021looks/">This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adrian Hoffmann, Claudio Fanconi, Rahul Rade, Jonas Kohler</td>
	<td>ICML 2021 Workshop on Theoretic Foundation, Criticism, and Application Trend of Explainable AI</td>
	<td><p>Deep neural networks that yield human interpretable decisions by
architectural design have lately become an increasingly popular alternative to
post hoc interpretation of traditional black-box models. Among these networks,
the arguably most widespread approach is so-called prototype learning, where
similarities to learned latent prototypes serve as the basis of classifying an
unseen data point. In this work, we point to an important shortcoming of such
approaches. Namely, there is a semantic gap between similarity in latent space
and similarity in input space, which can corrupt interpretability. We design
two experiments that exemplify this issue on the so-called ProtoPNet.
Specifically, we find that this network’s interpretability mechanism can be led
astray by intentionally crafted or even JPEG compression artefacts, which can
produce incomprehensible decisions. We argue that practitioners ought to have
this shortcoming in mind when deploying prototype-based models in practice.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/geada2021trustyai/">TrustyAI Explainability Toolkit</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TrustyAI Explainability Toolkit' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TrustyAI Explainability Toolkit' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Rob Geada, Tommaso Teofili, Rui Vieira, Rebecca Whitworth, Daniele Zonca</td>
	<td></td>
	<td><p>Artificial intelligence (AI) is becoming increasingly more popular and can be
found in workplaces and homes around the world. The decisions made by such
“black box” systems are often opaque; that is, so complex as to be functionally
impossible to understand. How do we ensure that these systems are behaving as
desired? TrustyAI is an initiative which looks into explainable artificial
intelligence (XAI) solutions to address this issue of explainability in the
context of both AI models and decision services. This paper presents the
TrustyAI Explainability Toolkit, a Java and Python library that provides XAI
explanations of decision services and predictive models for both enterprise and
data science use-cases. We describe the TrustyAI implementations and extensions
to techniques such as LIME, SHAP and counterfactuals, which are benchmarked
against existing implementations in a variety of experiments.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/gerlings2021explainable/">Explainable AI, but explainable to whom?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable AI, but explainable to whom?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable AI, but explainable to whom?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Julie Gerlings, Millie Søndergaard Jensen, Arisa Shollo</td>
	<td></td>
	<td><p>Advances in AI technologies have resulted in superior levels of AI-based
model performance. However, this has also led to a greater degree of model
complexity, resulting in ‘black box’ models. In response to the AI black box
problem, the field of explainable AI (xAI) has emerged with the aim of
providing explanations catered to human understanding, trust, and transparency.
Yet, we still have a limited understanding of how xAI addresses the need for
explainable AI in the context of healthcare. Our research explores the
differing explanation needs amongst stakeholders during the development of an
AI-system for classifying COVID-19 patients for the ICU. We demonstrate that
there is a constellation of stakeholders who have different explanation needs,
not just the ‘user’. Further, the findings demonstrate how the need for xAI
emerges through concerns associated with specific stakeholder groups i.e., the
development team, subject matter experts, decision makers, and the audience.
Our findings contribute to the expansion of xAI by highlighting that different
stakeholders have different explanation needs. From a practical perspective,
the study provides insights on how AI systems can be adjusted to support
different stakeholders needs, ensuring better implementation and operation in a
healthcare context.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/garreau2021lime/">What does LIME really see in images?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What does LIME really see in images?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What does LIME really see in images?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Damien Garreau, Dina Mardaoui</td>
	<td></td>
	<td><p>The performance of modern algorithms on certain computer vision tasks such as
object recognition is now close to that of humans. This success was achieved at
the price of complicated architectures depending on millions of parameters and
it has become quite challenging to understand how particular predictions are
made. Interpretability methods propose to give us this understanding. In this
paper, we study LIME, perhaps one of the most popular. On the theoretical side,
we show that when the number of generated examples is large, LIME explanations
are concentrated around a limit explanation for which we give an explicit
expression. We further this study for elementary shape detectors and linear
models. As a consequence of this analysis, we uncover a connection between LIME
and integrated gradients, another explanation method. More precisely, the LIME
explanations are similar to the sum of integrated gradients over the
superpixels used in the preprocessing step of LIME.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/gautam2021looks/">This looks more like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=This looks more like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=This looks more like that: Enhancing Self-Explaining Models by Prototypical Relevance Propagation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Srishti Gautam, Marina M. -C. Höhne, Stine Hansen, Robert Jenssen, Michael Kampffmeyer</td>
	<td></td>
	<td><p>Current machine learning models have shown high efficiency in solving a wide
variety of real-world problems. However, their black box character poses a
major challenge for the understanding and traceability of the underlying
decision-making strategies. As a remedy, many post-hoc explanation and
self-explanatory methods have been developed to interpret the models’ behavior.
These methods, in addition, enable the identification of artifacts that can be
learned by the model as class-relevant features. In this work, we provide a
detailed case study of the self-explaining network, ProtoPNet, in the presence
of a spectrum of artifacts. Accordingly, we identify the main drawbacks of
ProtoPNet, especially, its coarse and spatially imprecise explanations. We
address these limitations by introducing Prototypical Relevance Propagation
(PRP), a novel method for generating more precise model-aware explanations.
Furthermore, in order to obtain a clean dataset, we propose to use multi-view
clustering strategies for segregating the artifact images using the PRP
explanations, thereby suppressing the potential artifact learning in the
models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/hase2021models/">When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=When Can Models Learn From Explanations? A Formal Framework for Understanding the Roles of Explanation Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Peter Hase, Mohit Bansal</td>
	<td></td>
	<td><p>Many methods now exist for conditioning model outputs on task instructions,
retrieved documents, and user-provided explanations and feedback. Rather than
relying solely on examples of task inputs and outputs, these approaches use
valuable additional data for improving model correctness and aligning learned
models with human priors. Meanwhile, a growing body of evidence suggests that
some language models can (1) store a large amount of knowledge in their
parameters, and (2) perform inference over tasks in textual inputs at test
time. These results raise the possibility that, for some tasks, humans cannot
explain to a model any more about the task than it already knows or could infer
on its own. In this paper, we study the circumstances under which explanations
of individual data points can (or cannot) improve modeling performance. In
order to carefully control important properties of the data and explanations,
we introduce a synthetic dataset for experiments, and we also make use of three
existing datasets with explanations: e-SNLI, TACRED, and SemEval. We first give
a formal framework for the available modeling approaches, in which explanation
data can be used as model inputs, as targets, or as a prior. After arguing that
the most promising role for explanation data is as model inputs, we propose to
use a retrieval-based method and show that it solves our synthetic task with
accuracies upwards of 95%, while baselines without explanation data achieve
below 65% accuracy. We then identify properties of datasets for which
retrieval-based modeling fails. With the three existing datasets, we find no
improvements from explanation retrieval. Drawing on findings from our synthetic
task, we suggest that at least one of six preconditions for successful modeling
fails to hold with these datasets. Our code is publicly available at
https://github.com/peterbhase/ExplanationRoles</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/folke2021explainable/">Explainable AI for Natural Adversarial Images</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable AI for Natural Adversarial Images' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable AI for Natural Adversarial Images' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tomas Folke, ZhaoBin Li, Ravi B. Sojitra, Scott Cheng-Hsin Yang, Patrick Shafto</td>
	<td></td>
	<td><p>Adversarial images highlight how vulnerable modern image classifiers are to
perturbations outside of their training set. Human oversight might mitigate
this weakness, but depends on humans understanding the AI well enough to
predict when it is likely to make a mistake. In previous work we have found
that humans tend to assume that the AI’s decision process mirrors their own.
Here we evaluate if methods from explainable AI can disrupt this assumption to
help participants predict AI classifications for adversarial and standard
images. We find that both saliency maps and examples facilitate catching AI
errors, but their effects are not additive, and saliency maps are more
effective than examples.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/eckstein2021discriminative/">Discriminative Attribution from Counterfactuals</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Discriminative Attribution from Counterfactuals' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Discriminative Attribution from Counterfactuals' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nils Eckstein, Alexander S. Bates, Gregory S. X. E. Jefferis, Jan Funke</td>
	<td></td>
	<td><p>We present a method for neural network interpretability by combining feature
attribution with counterfactual explanations to generate attribution maps that
highlight the most discriminative features between pairs of classes. We show
that this method can be used to quantitatively evaluate the performance of
feature attribution methods in an objective manner, thus preventing potential
observer bias. We evaluate the proposed method on three diverse datasets,
including a challenging artificial dataset and real-world biological data. We
show quantitatively and qualitatively that the highlighted features are
substantially more discriminative than those extracted using conventional
attribution methods and argue that this type of explanation is better suited
for understanding fine grained class differences as learned by a deep neural
network.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/donnelly2021deformable/">Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deformable ProtoPNet: An Interpretable Image Classifier Using Deformable Prototypes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jon Donnelly, Alina Jade Barnett, Chaofan Chen</td>
	<td>2022 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</td>
	<td><p>We present a deformable prototypical part network (Deformable ProtoPNet), an
interpretable image classifier that integrates the power of deep learning and
the interpretability of case-based reasoning. This model classifies input
images by comparing them with prototypes learned during training, yielding
explanations in the form of “this looks like that.” However, while previous
methods use spatially rigid prototypes, we address this shortcoming by
proposing spatially flexible prototypes. Each prototype is made up of several
prototypical parts that adaptively change their relative spatial positions
depending on the input image. Consequently, a Deformable ProtoPNet can
explicitly capture pose variations and context, improving both model accuracy
and the richness of explanations provided. Compared to other case-based
interpretable models using prototypes, our approach achieves state-of-the-art
accuracy and gives an explanation with greater context. The code is available
at https://github.com/jdonnelly36/Deformable-ProtoPNet.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ehsan2021explainability/">Explainability Pitfalls: Beyond Dark Patterns in Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainability Pitfalls: Beyond Dark Patterns in Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainability Pitfalls: Beyond Dark Patterns in Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Upol Ehsan, Mark O. Riedl</td>
	<td></td>
	<td><p>To make Explainable AI (XAI) systems trustworthy, understanding harmful
effects is just as important as producing well-designed explanations. In this
paper, we address an important yet unarticulated type of negative effect in
XAI. We introduce explainability pitfalls(EPs), unanticipated negative
downstream effects from AI explanations manifesting even when there is no
intention to manipulate users. EPs are different from, yet related to, dark
patterns, which are intentionally deceptive practices. We articulate the
concept of EPs by demarcating it from dark patterns and highlighting the
challenges arising from uncertainties around pitfalls. We situate and
operationalize the concept using a case study that showcases how, despite best
intentions, unsuspecting negative effects such as unwarranted trust in
numerical explanations can emerge. We propose proactive and preventative
strategies to address EPs at three interconnected levels: research, design, and
organizational.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ding2021evaluating/">Evaluating Saliency Methods for Neural Language Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Saliency Methods for Neural Language Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Saliency Methods for Neural Language Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shuoyang Ding, Philipp Koehn</td>
	<td></td>
	<td><p>Saliency methods are widely used to interpret neural network predictions, but
different variants of saliency methods often disagree even on the
interpretations of the same prediction made by the same model. In these cases,
how do we identify when are these interpretations trustworthy enough to be used
in analyses? To address this question, we conduct a comprehensive and
quantitative evaluation of saliency methods on a fundamental category of NLP
models: neural language models. We evaluate the quality of prediction
interpretations from two perspectives that each represents a desirable property
of these interpretations: plausibility and faithfulness. Our evaluation is
conducted on four different datasets constructed from the existing human
annotation of syntactic and semantic agreements, on both sentence-level and
document-level. Through our evaluation, we identified various ways saliency
methods could yield interpretations of low quality. We recommend that future
work deploying such methods to neural language models should carefully validate
their interpretations before drawing insights.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/colin2021cannot/">What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What I Cannot Predict, I Do Not Understand: A Human-Centered Evaluation Framework for Explainability Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Julien Colin, Thomas Fel, Remi Cadene, Thomas Serre</td>
	<td></td>
	<td><p>A multitude of explainability methods and associated fidelity performance
metrics have been proposed to help better understand how modern AI systems make
decisions. However, much of the current work has remained theoretical –
without much consideration for the human end-user. In particular, it is not yet
known (1) how useful current explainability methods are in practice for more
real-world scenarios and (2) how well associated performance metrics accurately
predict how much knowledge individual explanations contribute to a human
end-user trying to understand the inner-workings of the system. To fill this
gap, we conducted psychophysics experiments at scale to evaluate the ability of
human participants to leverage representative attribution methods for
understanding the behavior of different image classifiers representing three
real-world scenarios: identifying bias in an AI system, characterizing the
visual strategy it uses for tasks that are too difficult for an untrained
non-expert human observer as well as understanding its failure cases. Our
results demonstrate that the degree to which individual attribution methods
help human participants better understand an AI system varied widely across
these scenarios. This suggests a critical need for the field to move past
quantitative improvements of current attribution methods towards the
development of complementary approaches that provide qualitatively different
sources of information to human end-users.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/chou2021counterfactuals/">Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactuals and Causability in Explainable Artificial Intelligence: Theory, Algorithms, and Applications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yu-Liang Chou, Catarina Moreira, Peter Bruza, Chun Ouyang, Joaquim Jorge</td>
	<td></td>
	<td><p>There has been a growing interest in model-agnostic methods that can make
deep learning models more transparent and explainable to a user. Some
researchers recently argued that for a machine to achieve a certain degree of
human-level explainability, this machine needs to provide human causally
understandable explanations, also known as causability. A specific class of
algorithms that have the potential to provide causability are counterfactuals.
This paper presents an in-depth systematic review of the diverse existing body
of literature on counterfactuals and causability for explainable artificial
intelligence. We performed an LDA topic modelling analysis under a PRISMA
framework to find the most relevant literature articles. This analysis resulted
in a novel taxonomy that considers the grounding theories of the surveyed
algorithms, together with their underlying properties and applications in
real-world data. This research suggests that current model-agnostic
counterfactual algorithms for explainable AI are not grounded on a causal
theoretical formalism and, consequently, cannot promote causability to a human
decision-maker. Our findings suggest that the explanations derived from major
algorithms in the literature provide spurious correlations rather than
cause/effects relationships, leading to sub-optimal, erroneous or even biased
explanations. This paper also advances the literature with new directions and
challenges on promoting causability in model-agnostic approaches for
explainable artificial intelligence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/cheng2021tsgb/">TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=TSGB: Target-Selective Gradient Backprop for Probing CNN Visual Saliency' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lin Cheng, Pengfei Fang, Yanjie Liang, Liao Zhang, Chunhua Shen, Hanzi Wang</td>
	<td></td>
	<td><p>The explanation for deep neural networks has drawn extensive attention in the
deep learning community over the past few years. In this work, we study the
visual saliency, a.k.a. visual explanation, to interpret convolutional neural
networks. Compared to iteration based saliency methods, single backward pass
based saliency methods benefit from faster speed, and they are widely used in
downstream visual tasks. Thus, we focus on single backward pass based methods.
However, existing methods in this category struggle to uccessfully produce
fine-grained saliency maps concentrating on specific target classes. That said,
producing faithful saliency maps satisfying both target-selectiveness and
fine-grainedness using a single backward pass is a challenging problem in the
field. To mitigate this problem, we revisit the gradient flow inside the
network, and find that the entangled semantics and original weights may disturb
the propagation of target-relevant saliency. Inspired by those observations, we
propose a novel visual saliency method, termed Target-Selective Gradient
Backprop (TSGB), which leverages rectification operations to effectively
emphasize target classes and further efficiently propagate the saliency to the
image space, thereby generating target-selective and fine-grained saliency
maps. The proposed TSGB consists of two components, namely, TSGB-Conv and
TSGB-FC, which rectify the gradients for convolutional layers and
fully-connected layers, respectively. Extensive qualitative and quantitative
experiments on the ImageNet and Pascal VOC datasets show that the proposed
method achieves more accurate and reliable results than the other competitive
methods. Code is available at https://github.com/123fxdx/CNNvisualizationTSGB.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/chrysostomou2021enjoy/">Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>George Chrysostomou, Nikolaos Aletras</td>
	<td></td>
	<td><p>Pretrained transformer-based models such as BERT have demonstrated
state-of-the-art predictive performance when adapted into a range of natural
language processing tasks. An open problem is how to improve the faithfulness
of explanations (rationales) for the predictions of these models. In this
paper, we hypothesize that salient information extracted a priori from the
training data can complement the task-specific information learned by the model
during fine-tuning on a downstream task. In this way, we aim to help BERT not
to forget assigning importance to informative input tokens when making
predictions by proposing SaLoss; an auxiliary loss function for guiding the
multi-head attention mechanism during training to be close to salient
information extracted a priori using TextRank. Experiments for explanation
faithfulness across five datasets, show that models trained with SaLoss
consistently provide more faithful explanations across four different feature
attribution methods compared to vanilla BERT. Using the rationales extracted
from vanilla BERT and SaLoss models to train inherently faithful classifiers,
we further show that the latter result in higher predictive performance in
downstream tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/crabb%C3%A92021explaining/">Explaining Latent Representations with a Corpus of Examples</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Latent Representations with a Corpus of Examples' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Latent Representations with a Corpus of Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jonathan Crabbé, Zhaozhi Qian, Fergus Imrie, Mihaela van der Schaar</td>
	<td></td>
	<td><p>Modern machine learning models are complicated. Most of them rely on
convoluted latent representations of their input to issue a prediction. To
achieve greater transparency than a black-box that connects inputs to
predictions, it is necessary to gain a deeper understanding of these latent
representations. To that aim, we propose SimplEx: a user-centred method that
provides example-based explanations with reference to a freely selected set of
examples, called the corpus. SimplEx uses the corpus to improve the user’s
understanding of the latent space with post-hoc explanations answering two
questions: (1) Which corpus examples explain the prediction issued for a given
test example? (2) What features of these corpus examples are relevant for the
model to relate them to the test example? SimplEx provides an answer by
reconstructing the test latent representation as a mixture of corpus latent
representations. Further, we propose a novel approach, the Integrated Jacobian,
that allows SimplEx to make explicit the contribution of each corpus feature in
the mixture. Through experiments on tasks ranging from mortality prediction to
image classification, we demonstrate that these decompositions are robust and
accurate. With illustrative use cases in medicine, we show that SimplEx
empowers the user by highlighting relevant patterns in the corpus that explain
model representations. Moreover, we demonstrate how the freedom in choosing the
corpus allows the user to have personalized explanations in terms of examples
that are meaningful for them.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yao2021explanatory/">Explanatory Pluralism in Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Pluralism in Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Pluralism in Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yiheng Yao</td>
	<td></td>
	<td><p>The increasingly widespread application of AI models motivates increased
demand for explanations from a variety of stakeholders. However, this demand is
ambiguous because there are many types of ‘explanation’ with different
evaluative criteria. In the spirit of pluralism, I chart a taxonomy of types of
explanation and the associated XAI methods that can address them. When we look
to expose the inner mechanisms of AI models, we develop
Diagnostic-explanations. When we seek to render model output understandable, we
produce Explication-explanations. When we wish to form stable generalizations
of our models, we produce Expectation-explanations. Finally, when we want to
justify the usage of a model, we produce Role-explanations that situate models
within their social context. The motivation for such a pluralistic view stems
from a consideration of causes as manipulable relationships and the different
types of explanations as identifying the relevant points in AI systems we can
intervene upon to affect our desired changes. This paper reduces the ambiguity
in use of the word ‘explanation’ in the field of XAI, allowing practitioners
and stakeholders a useful template for avoiding equivocation and evaluating XAI
methods and putative explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/b%C3%B6hle2021optimising/">Optimising for Interpretability: Convolutional Dynamic Alignment Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Optimising for Interpretability: Convolutional Dynamic Alignment Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Optimising for Interpretability: Convolutional Dynamic Alignment Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Moritz Böhle, Mario Fritz, Bernt Schiele</td>
	<td></td>
	<td><p>We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which are optimised to transform their inputs
with dynamically computed weight vectors that align with task-relevant
patterns. As a result, CoDA Nets model the classification prediction through a
series of input-dependent linear transformations, allowing for linear
decomposition of the output into individual input contributions. Given the
alignment of the DAUs, the resulting contribution maps align with
discriminative input patterns. These model-inherent decompositions are of high
visual quality and outperform existing attribution methods under quantitative
metrics. Further, CoDA Nets constitute performant classifiers, achieving on par
results to ResNet and VGG models on e.g. CIFAR-10 and TinyImagenet. Lastly,
CoDA Nets can be combined with conventional neural network models to yield
powerful classifiers that more easily scale to complex datasets such as
Imagenet whilst exhibiting an increased interpretable depth, i.e., the output
can be explained well in terms of contributions from intermediate layers within
the network.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/b%C3%B6hle2021convolutional/">Convolutional Dynamic Alignment Networks for Interpretable Classifications</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Convolutional Dynamic Alignment Networks for Interpretable Classifications' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Convolutional Dynamic Alignment Networks for Interpretable Classifications' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Moritz Böhle, Mario Fritz, Bernt Schiele</td>
	<td></td>
	<td><p>We introduce a new family of neural network models called Convolutional
Dynamic Alignment Networks (CoDA-Nets), which are performant classifiers with a
high degree of inherent interpretability. Their core building blocks are
Dynamic Alignment Units (DAUs), which linearly transform their input with
weight vectors that dynamically align with task-relevant patterns. As a result,
CoDA-Nets model the classification prediction through a series of
input-dependent linear transformations, allowing for linear decomposition of
the output into individual input contributions. Given the alignment of the
DAUs, the resulting contribution maps align with discriminative input patterns.
These model-inherent decompositions are of high visual quality and outperform
existing attribution methods under quantitative metrics. Further, CoDA-Nets
constitute performant classifiers, achieving on par results to ResNet and VGG
models on e.g. CIFAR-10 and TinyImagenet.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/bodria2021benchmarking/">Benchmarking and Survey of Explanation Methods for Black Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking and Survey of Explanation Methods for Black Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Benchmarking and Survey of Explanation Methods for Black Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Francesco Bodria, Fosca Giannotti, Riccardo Guidotti, Francesca Naretto, Dino Pedreschi, Salvatore Rinzivillo</td>
	<td></td>
	<td><p>The widespread adoption of black-box models in Artificial Intelligence has
enhanced the need for explanation methods to reveal how these obscure models
reach specific decisions. Retrieving explanations is fundamental to unveil
possible biases and to resolve practical or ethical issues. Nowadays, the
literature is full of methods with different explanations. We provide a
categorization of explanation methods based on the type of explanation
returned. We present the most recent and widely used explainers, and we show a
visual comparison among explanations and a quantitative benchmarking.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/cai2021xproax-local/">XPROAX-Local explanations for text classification with progressive neighborhood approximation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=XPROAX-Local explanations for text classification with progressive neighborhood approximation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=XPROAX-Local explanations for text classification with progressive neighborhood approximation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yi Cai, Arthur Zimek, Eirini Ntoutsi</td>
	<td></td>
	<td><p>The importance of the neighborhood for training a local surrogate model to
approximate the local decision boundary of a black box classifier has been
already highlighted in the literature. Several attempts have been made to
construct a better neighborhood for high dimensional data, like texts, by using
generative autoencoders. However, existing approaches mainly generate neighbors
by selecting purely at random from the latent space and struggle under the
curse of dimensionality to learn a good local decision boundary. To overcome
this problem, we propose a progressive approximation of the neighborhood using
counterfactual instances as initial landmarks and a careful 2-stage sampling
approach to refine counterfactuals and generate factuals in the neighborhood of
the input instance to be explained. Our work focuses on textual data and our
explanations consist of both word-level explanations from the original instance
(intrinsic) and the neighborhood (extrinsic) and factual- and
counterfactual-instances discovered during the neighborhood generation process
that further reveal the effect of altering certain parts in the input text. Our
experiments on real-world datasets demonstrate that our method outperforms the
competitors in terms of usefulness and stability (for the qualitative part) and
completeness, compactness and correctness (for the quantitative part).</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/biessmann2021quality/">Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Felix Biessmann, Dionysius Refiano</td>
	<td></td>
	<td><p>The field explainable artificial intelligence (XAI) has brought about an
arsenal of methods to render Machine Learning (ML) predictions more
interpretable. But how useful explanations provided by transparent ML methods
are for humans remains difficult to assess. Here we investigate the quality of
interpretable computer vision algorithms using techniques from psychophysics.
In crowdsourced annotation tasks we study the impact of different
interpretability approaches on annotation accuracy and task time. We compare
these quality metrics with classical XAI, automated quality metrics. Our
results demonstrate that psychophysical experiments allow for robust quality
assessment of transparency in machine learning. Interestingly the quality
metrics computed without humans in the loop did not provide a consistent
ranking of interpretability methods nor were they representative for how useful
an explanation was for humans. These findings highlight the potential of
methods from classical psychophysics for modern machine learning applications.
We hope that our results provide convincing arguments for evaluating
interpretability in its natural habitat, human-ML interaction, if the goal is
to obtain an authentic assessment of interpretability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/bastings2021will/">"Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jasmijn Bastings, Sebastian Ebert, Polina Zablotskaia, Anders Sandholm, Katja Filippova</td>
	<td></td>
	<td><p>Feature attribution a.k.a. input salience methods which assign an importance
score to a feature are abundant but may produce surprisingly different results
for the same model on the same input. While differences are expected if
disparate definitions of importance are assumed, most methods claim to provide
faithful attributions and point at the features most relevant for a model’s
prediction. Existing work on faithfulness evaluation is not conclusive and does
not provide a clear answer as to how different methods are to be compared.
Focusing on text classification and the model debugging scenario, our main
contribution is a protocol for faithfulness evaluation that makes use of
partially synthetic data to obtain ground truth for feature importance ranking.
Following the protocol, we do an in-depth analysis of four standard salience
method classes on a range of datasets and shortcuts for BERT and LSTM models
and demonstrate that some of the most popular method configurations provide
poor results even for simplest shortcuts. We recommend following the protocol
for each new task and model combination to find the best method for identifying
shortcuts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/augenstein2021towards/">Towards Explainable Fact Checking</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explainable Fact Checking' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explainable Fact Checking' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Isabelle Augenstein</td>
	<td></td>
	<td><p>The past decade has seen a substantial rise in the amount of mis- and
disinformation online, from targeted disinformation campaigns to influence
politics, to the unintentional spreading of misinformation about public health.
This development has spurred research in the area of automatic fact checking,
from approaches to detect check-worthy claims and determining the stance of
tweets towards claims, to methods to determine the veracity of claims given
evidence documents. These automatic methods are often content-based, using
natural language processing methods, which in turn utilise deep neural networks
to learn higher-order features from text in order to make predictions. As deep
neural networks are black-box models, their inner workings cannot be easily
explained. At the same time, it is desirable to explain how they arrive at
certain decisions, especially if they are to be used for decision making. While
this has been known for some time, the issues this raises have been exacerbated
by models increasing in size, and by EU legislation requiring models to be used
for decision making to provide explanations, and, very recently, by legislation
requiring online platforms operating in the EU to provide transparent reporting
on their services. Despite this, current solutions for explainability are still
lacking in the area of fact checking. This thesis presents my research on
automatic fact checking, including claim check-worthiness detection, stance
detection and veracity prediction. Its contributions go beyond fact checking,
with the thesis proposing more general machine learning solutions for natural
language processing in the area of learning with limited labelled data.
Finally, the thesis presents some first solutions for explainable fact
checking.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/barr2021counterfactual/">Counterfactual Explanations via Latent Space Projection and Interpolation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactual Explanations via Latent Space Projection and Interpolation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactual Explanations via Latent Space Projection and Interpolation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Brian Barr, Matthew R. Harrington, Samuel Sharpe, C. Bayan Bruss</td>
	<td></td>
	<td><p>Counterfactual explanations represent the minimal change to a data sample
that alters its predicted classification, typically from an unfavorable initial
class to a desired target class. Counterfactuals help answer questions such as
“what needs to change for this application to get accepted for a loan?”. A
number of recently proposed approaches to counterfactual generation give
varying definitions of “plausible” counterfactuals and methods to generate
them. However, many of these methods are computationally intensive and provide
unconvincing explanations. Here we introduce SharpShooter, a method for binary
classification that starts by creating a projected version of the input that
classifies as the target class. Counterfactual candidates are then generated in
latent space on the interpolation line between the input and its projection. We
then demonstrate that our framework translates core characteristics of a sample
to its counterfactual through the use of learned representations. Furthermore,
we show that SharpShooter is competitive across common quality metrics on
tabular and image datasets while being orders of magnitude faster than two
comparable methods and excels at measures of realism, making it well-suited for
high velocity machine learning applications which require timely explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/arya2021ai/">AI Explainability 360: Impact and Design</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=AI Explainability 360: Impact and Design' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=AI Explainability 360: Impact and Design' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilovic, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, Yunfeng Zhang</td>
	<td>IAAI 2022</td>
	<td><p>As artificial intelligence and machine learning algorithms become
increasingly prevalent in society, multiple stakeholders are calling for these
algorithms to provide explanations. At the same time, these stakeholders,
whether they be affected citizens, government regulators, domain experts, or
system developers, have different explanation needs. To address these needs, in
2019, we created AI Explainability 360 (Arya et al. 2020), an open source
software toolkit featuring ten diverse and state-of-the-art explainability
methods and two evaluation metrics. This paper examines the impact of the
toolkit with several case studies, statistics, and community feedback. The
different ways in which users have experienced AI Explainability 360 have
resulted in multiple types of impact and improvements in multiple metrics,
highlighted by the adoption of the toolkit by the independent LF AI &amp; Data
Foundation. The paper also describes the flexible design of the toolkit,
examples of its use, and the significant educational material and documentation
available to its users.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/artelt2021evaluating/">Evaluating Robustness of Counterfactual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Robustness of Counterfactual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Robustness of Counterfactual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>André Artelt, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, Barbara Hammer</td>
	<td></td>
	<td><p>Transparency is a fundamental requirement for decision making systems when
these should be deployed in the real world. It is usually achieved by providing
explanations of the system’s behavior. A prominent and intuitive type of
explanations are counterfactual explanations. Counterfactual explanations
explain a behavior to the user by proposing actions – as changes to the input
– that would cause a different (specified) behavior of the system. However,
such explanation methods can be unstable with respect to small changes to the
input – i.e. even a small change in the input can lead to huge or arbitrary
changes in the output and of the explanation. This could be problematic for
counterfactual explanations, as two similar individuals might get very
different explanations. Even worse, if the recommended actions differ
considerably in their complexity, one would consider such unstable
(counterfactual) explanations as individually unfair.
  In this work, we formally and empirically study the robustness of
counterfactual explanations in general, as well as under different models and
different kinds of perturbations. Furthermore, we propose that plausible
counterfactual explanations can be used instead of closest counterfactual
explanations to improve the robustness and consequently the individual fairness
of counterfactual explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/akhtar2021attack/">Attack to Fool and Explain Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attack to Fool and Explain Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attack to Fool and Explain Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Naveed Akhtar, Muhammad A. A. K. Jalwana, Mohammed Bennamoun, Ajmal Mian</td>
	<td></td>
	<td><p>Deep visual models are susceptible to adversarial perturbations to inputs.
Although these signals are carefully crafted, they still appear noise-like
patterns to humans. This observation has led to the argument that deep visual
representation is misaligned with human perception. We counter-argue by
providing evidence of human-meaningful patterns in adversarial perturbations.
We first propose an attack that fools a network to confuse a whole category of
objects (source class) with a target label. Our attack also limits the
unintended fooling by samples from non-sources classes, thereby circumscribing
human-defined semantic notions for network fooling. We show that the proposed
attack not only leads to the emergence of regular geometric patterns in the
perturbations, but also reveals insightful information about the decision
boundaries of deep models. Exploring this phenomenon further, we alter the
<code class="language-plaintext highlighter-rouge">adversarial' objective of our attack to use it as a tool to </code>explain’ deep
visual representation. We show that by careful channeling and projection of the
perturbations computed by our method, we can visualize a model’s understanding
of human-defined semantic notions. Finally, we exploit the explanability
properties of our perturbations to perform image generation, inpainting and
interactive image manipulation by attacking adversarialy robust
`classifiers’.In all, our major contribution is a novel pragmatic adversarial
attack that is subsequently transformed into a tool to interpret the visual
models. The article also makes secondary contributions in terms of establishing
the utility of our attack beyond the adversarial objective with multiple
interesting applications.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/albini2021counterfactual/">Counterfactual Shapley Additive Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactual Shapley Additive Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactual Shapley Additive Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Emanuele Albini, Jason Long, Danial Dervovic, Daniele Magazzeni</td>
	<td></td>
	<td><p>Feature attributions are a common paradigm for model explanations due to
their simplicity in assigning a single numeric score for each input feature to
a model. In the actionable recourse setting, wherein the goal of the
explanations is to improve outcomes for model consumers, it is often unclear
how feature attributions should be correctly used. With this work, we aim to
strengthen and clarify the link between actionable recourse and feature
attributions. Concretely, we propose a variant of SHAP, Counterfactual SHAP
(CF-SHAP), that incorporates counterfactual information to produce a background
dataset for use within the marginal (a.k.a. interventional) Shapley value
framework. We motivate the need within the actionable recourse setting for
careful consideration of background datasets when using Shapley values for
feature attributions with numerous synthetic examples. Moreover, we demonstrate
the efficacy of CF-SHAP by proposing and justifying a quantitative score for
feature attributions, counterfactual-ability, showing that as measured by this
metric, CF-SHAP is superior to existing methods when evaluated on public
datasets using tree ensembles.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/agarwal2021towards/">Towards the Unification and Robustness of Perturbation and Gradient Based Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards the Unification and Robustness of Perturbation and Gradient Based Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards the Unification and Robustness of Perturbation and Gradient Based Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sushant Agarwal, Shahin Jabbari, Chirag Agarwal, Sohini Upadhyay, Zhiwei Steven Wu, Himabindu Lakkaraju</td>
	<td></td>
	<td><p>As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a post
hoc manner. In this work, we analyze two popular post hoc interpretation
techniques: SmoothGrad which is a gradient based method, and a variant of LIME
which is a perturbation based method. More specifically, we derive explicit
closed form expressions for the explanations output by these two methods and
show that they both converge to the same explanation in expectation, i.e., when
the number of perturbed samples used by these methods is large. We then
leverage this connection to establish other desirable properties, such as
robustness, for these techniques. We also derive finite sample complexity
bounds for the number of perturbations required for these methods to converge
to their expected explanation. Finally, we empirically validate our theory
using extensive experimentation on both synthetic and real world datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/abid2021meaningfully/">Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Meaningfully Debugging Model Mistakes using Conceptual Counterfactual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abubakar Abid, Mert Yuksekgonul, James Zou</td>
	<td></td>
	<td><p>Understanding and explaining the mistakes made by trained models is critical
to many machine learning objectives, such as improving robustness, addressing
concept drift, and mitigating biases. However, this is often an ad hoc process
that involves manually looking at the model’s mistakes on many test samples and
guessing at the underlying reasons for those incorrect predictions. In this
paper, we propose a systematic approach, conceptual counterfactual explanations
(CCE), that explains why a classifier makes a mistake on a particular test
sample(s) in terms of human-understandable concepts (e.g. this zebra is
misclassified as a dog because of faint stripes). We base CCE on two prior
ideas: counterfactual explanations and concept activation vectors, and validate
our approach on well-known pretrained models, showing that it explains the
models’ mistakes meaningfully. In addition, for new models trained on data with
spurious correlations, CCE accurately identifies the spurious correlation as
the cause of model mistakes from a single misclassified test sample. On two
challenging medical applications, CCE generated useful insights, confirmed by
clinicians, into biases and mistakes the model makes in real-world settings.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zha2021invertible/">Invertible Attention</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Invertible Attention' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Invertible Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiajun Zha, Yiran Zhong, Jing Zhang, Richard Hartley, Liang Zheng</td>
	<td></td>
	<td><p>Attention has been proved to be an efficient mechanism to capture long-range
dependencies. However, so far it has not been deployed in invertible networks.
This is due to the fact that in order to make a network invertible, every
component within the network needs to be a bijective transformation, but a
normal attention block is not. In this paper, we propose invertible attention
that can be plugged into existing invertible models. We mathematically and
experimentally prove that the invertibility of an attention model can be
achieved by carefully constraining its Lipschitz constant. We validate the
invertibility of our invertible attention on image reconstruction task with 3
popular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible
attention achieves similar performance in comparison with normal non-invertible
attention on dense prediction tasks. The code is available at
https://github.com/Schwartz-Zha/InvertibleAttention</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/sudhakar2021ada/">Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Ada-SISE: Adaptive Semantic Input Sampling for Efficient Explanation of Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mahesh Sudhakar, Sam Sattarzadeh, Konstantinos N. Plataniotis, Jongseong Jang, Yeonjeong Jeong, Hyunwoo Kim</td>
	<td></td>
	<td><p>Explainable AI (XAI) is an active research area to interpret a neural
network’s decision by ensuring transparency and trust in the task-specified
learned models. Recently, perturbation-based model analysis has shown better
interpretation, but backpropagation techniques are still prevailing because of
their computational efficiency. In this work, we combine both approaches as a
hybrid visual explanation algorithm and propose an efficient interpretation
method for convolutional neural networks. Our method adaptively selects the
most critical features that mainly contribute towards a prediction to probe the
model by finding the activated features. Experimental results show that the
proposed method can reduce the execution time up to 30% while enhancing
competitive interpretability without compromising the quality of explanation
generated.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/suresh2021beyond/">Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Harini Suresh, Steven R. Gomez, Kevin K. Nam, Arvind Satyanarayan</td>
	<td></td>
	<td><p>To ensure accountability and mitigate harm, it is critical that diverse
stakeholders can interrogate black-box automated systems and find information
that is understandable, relevant, and useful to them. In this paper, we eschew
prior expertise- and role-based categorizations of interpretability
stakeholders in favor of a more granular framework that decouples stakeholders’
knowledge from their interpretability needs. We characterize stakeholders by
their formal, instrumental, and personal knowledge and how it manifests in the
contexts of machine learning, the data domain, and the general milieu. We
additionally distill a hierarchical typology of stakeholder needs that
distinguishes higher-level domain goals from lower-level interpretability
tasks. In assessing the descriptive, evaluative, and generative powers of our
framework, we find our more nuanced treatment of stakeholders reveals gaps and
opportunities in the interpretability literature, adds precision to the design
and comparison of user studies, and facilitates a more reflexive approach to
conducting this research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/swamy2021interpreting/">Interpreting Language Models Through Knowledge Graph Extraction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Language Models Through Knowledge Graph Extraction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Language Models Through Knowledge Graph Extraction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vinitra Swamy, Angelika Romanou, Martin Jaggi</td>
	<td></td>
	<td><p>Transformer-based language models trained on large text corpora have enjoyed
immense popularity in the natural language processing community and are
commonly used as a starting point for downstream tasks. While these models are
undeniably useful, it is a challenge to quantify their performance beyond
traditional accuracy metrics. In this paper, we compare BERT-based language
models through snapshots of acquired knowledge at sequential stages of the
training process. Structured relationships from training corpora may be
uncovered through querying a masked language model with probing tasks. We
present a methodology to unveil a knowledge acquisition timeline by generating
knowledge graph extracts from cloze “fill-in-the-blank” statements at various
stages of RoBERTa’s early training. We extend this analysis to a comparison of
pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This
work proposes a quantitative framework to compare language models through
knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech
analysis (POSOR) to identify the linguistic strengths of each model variant.
Using these metrics, machine learning practitioners can compare models,
diagnose their models’ behavioral strengths and weaknesses, and identify new
targeted datasets to improve model performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/slack2021counterfactual/">Counterfactual Explanations Can Be Manipulated</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactual Explanations Can Be Manipulated' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactual Explanations Can Be Manipulated' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dylan Slack, Sophie Hilgard, Himabindu Lakkaraju, Sameer Singh</td>
	<td></td>
	<td><p>Counterfactual explanations are emerging as an attractive option for
providing recourse to individuals adversely impacted by algorithmic decisions.
As they are deployed in critical applications (e.g. law enforcement, financial
lending), it becomes important to ensure that we clearly understand the
vulnerabilities of these methods and find ways to address them. However, there
is little understanding of the vulnerabilities and shortcomings of
counterfactual explanations. In this work, we introduce the first framework
that describes the vulnerabilities of counterfactual explanations and shows how
they can be manipulated. More specifically, we show counterfactual explanations
may converge to drastically different counterfactuals under a small
perturbation indicating they are not robust. Leveraging this insight, we
introduce a novel objective to train seemingly fair models where counterfactual
explanations find much lower cost recourse under a slight perturbation. We
describe how these models can unfairly provide low-cost recourse for specific
subgroups in the data while appearing fair to auditors. We perform experiments
on loan and violent crime prediction data sets where certain subgroups achieve
up to 20x lower cost recourse under the perturbation. These results raise
concerns regarding the dependability of current counterfactual explanation
techniques, which we hope will inspire investigations in robust counterfactual
explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zimmermann2021well/">How Well do Feature Visualizations Support Causal Understanding of CNN Activations?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Well do Feature Visualizations Support Causal Understanding of CNN Activations?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Well do Feature Visualizations Support Causal Understanding of CNN Activations?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Roland S. Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge, Thomas S. A. Wallis, Wieland Brendel</td>
	<td></td>
	<td><p>A precise understanding of why units in an artificial network respond to
certain stimuli would constitute a big step towards explainable artificial
intelligence. One widely used approach towards this goal is to visualize unit
responses via activation maximization. These synthetic feature visualizations
are purported to provide humans with precise information about the image
features that cause a unit to be activated - an advantage over other
alternatives like strongly activating natural dataset samples. If humans indeed
gain causal insight from visualizations, this should enable them to predict the
effect of an intervention, such as how occluding a certain patch of the image
(say, a dog’s head) changes a unit’s activation. Here, we test this hypothesis
by asking humans to decide which of two square occlusions causes a larger
change to a unit’s activation. Both a large-scale crowdsourced experiment and
measurements with experts show that on average the extremely activating feature
visualizations by Olah et al. (2017) indeed help humans on this task ($68 \pm
4$% accuracy; baseline performance without any visualizations is $60 \pm 3$%).
However, they do not provide any substantial advantage over other
visualizations (such as e.g. dataset samples), which yield similar performance
($66\pm3$% to $67 \pm3$% accuracy). Taken together, we propose an objective
psychophysical task to quantify the benefit of unit-level interpretability
methods for humans, and find no evidence that a widely-used feature
visualization method provides humans with better “causal understanding” of unit
activations than simple alternative visualizations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhu2021going/">Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Going Deeper in Frequency Convolutional Neural Network: A Theoretical Perspective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaohan Zhu, Zhen Cui, Tong Zhang, Yong Li, Jian Yang</td>
	<td></td>
	<td><p>Convolutional neural network (CNN) is one of the most widely-used successful
architectures in the era of deep learning. However, the high-computational cost
of CNN still hampers more universal uses to light devices. Fortunately, the
Fourier transform on convolution gives an elegant and promising solution to
dramatically reduce the computation cost. Recently, some studies devote to such
a challenging problem and pursue the complete frequency computation without any
switching between spatial domain and frequent domain. In this work, we revisit
the Fourier transform theory to derive feed-forward and back-propagation
frequency operations of typical network modules such as convolution, activation
and pooling. Due to the calculation limitation of complex numbers on most
computation tools, we especially extend the Fourier transform to the Laplace
transform for CNN, which can run in the real domain with more relaxed
constraints. This work more focus on a theoretical extension and discussion
about frequency CNN, and lay some theoretical ground for real application.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/samuel2021evaluation/">Evaluation of Saliency-based Explainability Method</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluation of Saliency-based Explainability Method' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluation of Saliency-based Explainability Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sam Zabdiel Sunder Samuel, Vidhya Kamakshi, Namrata Lodhi, Narayanan C Krishnan</td>
	<td></td>
	<td><p>A particular class of Explainable AI (XAI) methods provide saliency maps to
highlight part of the image a Convolutional Neural Network (CNN) model looks at
to classify the image as a way to explain its working. These methods provide an
intuitive way for users to understand predictions made by CNNs. Other than
quantitative computational tests, the vast majority of evidence to highlight
that the methods are valuable is anecdotal. Given that humans would be the
end-users of such methods, we devise three human subject experiments through
which we gauge the effectiveness of these saliency-based explainability
methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/santos2021impact/">On the Impact of Interpretability Methods in Active Image Augmentation Method</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Impact of Interpretability Methods in Active Image Augmentation Method' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Impact of Interpretability Methods in Active Image Augmentation Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Flavio Santos, Cleber Zanchettin, Leonardo Matos, Paulo Novais</td>
	<td>Logic Journal of the IGPL, 2021, jzab006</td>
	<td><p>Robustness is a significant constraint in machine learning models. The
performance of the algorithms must not deteriorate when training and testing
with slightly different data. Deep neural network models achieve awe-inspiring
results in a wide range of applications of computer vision. Still, in the
presence of noise or region occlusion, some models exhibit inaccurate
performance even with data handled in training. Besides, some experiments
suggest deep learning models sometimes use incorrect parts of the input
information to perform inference. Activate Image Augmentation (ADA) is an
augmentation method that uses interpretability methods to augment the training
data and improve its robustness to face the described problems. Although ADA
presented interesting results, its original version only used the Vanilla
Backpropagation interpretability to train the U-Net model. In this work, we
propose an extensive experimental analysis of the interpretability method’s
impact on ADA. We use five interpretability methods: Vanilla Backpropagation,
Guided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The
results show that all methods achieve similar performance at the ending of
training, but when combining ADA with GradCam, the U-Net model presented an
impressive fast convergence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/schwalbe2021comprehensive/">A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Comprehensive Taxonomy for Explainable Artificial Intelligence: A Systematic Survey of Surveys on Methods and Concepts' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gesina Schwalbe, Bettina Finzel</td>
	<td></td>
	<td><p>In the meantime, a wide variety of terminologies, motivations, approaches,
and evaluation criteria have been developed within the research field of
explainable artificial intelligence (XAI). With the amount of XAI methods
vastly growing, a taxonomy of methods is needed by researchers as well as
practitioners: To grasp the breadth of the topic, compare methods, and to
select the right XAI method based on traits required by a specific use-case
context. Many taxonomies for XAI methods of varying level of detail and depth
can be found in the literature. While they often have a different focus, they
also exhibit many points of overlap. This paper unifies these efforts and
provides a complete taxonomy of XAI methods with respect to notions present in
the current state of research. In a structured literature analysis and
meta-study, we identified and reviewed more than 50 of the most cited and
current surveys on XAI methods, metrics, and method traits. After summarizing
them in a survey of surveys, we merge terminologies and concepts of the
articles into a unified structured taxonomy. Single concepts therein are
illustrated by more than 50 diverse selected example methods in total, which we
categorize accordingly. The taxonomy may serve both beginners, researchers, and
practitioners as a reference and wide-ranging overview of XAI method traits and
aspects. Hence, it provides foundations for targeted, use-case-oriented, and
context-sensitive future research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhang2021evaluating/">Evaluating Deep Graph Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Deep Graph Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Deep Graph Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wentao Zhang, Zeang Sheng, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, Bin Cui</td>
	<td></td>
	<td><p>Graph Neural Networks (GNNs) have already been widely applied in various
graph mining tasks. However, they suffer from the shallow architecture issue,
which is the key impediment that hinders the model performance improvement.
Although several relevant approaches have been proposed, none of the existing
studies provides an in-depth understanding of the root causes of performance
degradation in deep GNNs. In this paper, we conduct the first systematic
experimental evaluation to present the fundamental limitations of shallow
architectures. Based on the experimental results, we answer the following two
essential questions: (1) what actually leads to the compromised performance of
deep GNNs; (2) when we need and how to build deep GNNs. The answers to the
above questions provide empirical insights and guidelines for researchers to
design deep and well-performed GNNs. To show the effectiveness of our proposed
guidelines, we present Deep Graph Multi-Layer Perceptron (DGMLP), a powerful
approach (a paradigm in its own right) that helps guide deep GNN designs.
Experimental results demonstrate three advantages of DGMLP: 1) high accuracy –
it achieves state-of-the-art node classification performance on various
datasets; 2) high flexibility – it can flexibly choose different propagation
and transformation depths according to graph size and sparsity; 3) high
scalability and efficiency – it supports fast training on large-scale graphs.
Our code is available in https://github.com/zwt233/DGMLP.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/rudin2021interpretable/">Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Cynthia Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, Chudi Zhong</td>
	<td>Statistics Surveys, 2021</td>
	<td><p>Interpretability in machine learning (ML) is crucial for high stakes
decisions and troubleshooting. In this work, we provide fundamental principles
for interpretable ML, and dispel common misunderstandings that dilute the
importance of this crucial topic. We also identify 10 technical challenge areas
in interpretable machine learning and provide history and background on each
problem. Some of these problems are classically important, and some are recent
problems that have arisen in the last few years. These problems are: (1)
Optimizing sparse logical models such as decision trees; (2) Optimization of
scoring systems; (3) Placing constraints into generalized additive models to
encourage sparsity and better interpretability; (4) Modern case-based
reasoning, including neural networks and matching for causal inference; (5)
Complete supervised disentanglement of neural networks; (6) Complete or even
partial unsupervised disentanglement of neural networks; (7) Dimensionality
reduction for data visualization; (8) Machine learning models that can
incorporate physics and other generative or causal constraints; (9)
Characterization of the “Rashomon set” of good models; and (10) Interpretable
reinforcement learning. This survey is suitable as a starting point for
statisticians and computer scientists interested in working in interpretable
machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ravfogel2021counterfactual/">Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shauli Ravfogel, Grusha Prasad, Tal Linzen, Yoav Goldberg</td>
	<td></td>
	<td><p>When language models process syntactically complex sentences, do they use
their representations of syntax in a manner that is consistent with the grammar
of the language? We propose AlterRep, an intervention-based method to address
this question. For any linguistic feature of a given sentence, AlterRep
generates counterfactual representations by altering how the feature is
encoded, while leaving intact all other aspects of the original representation.
By measuring the change in a model’s word prediction behavior when these
counterfactual representations are substituted for the original ones, we can
draw conclusions about the causal effect of the linguistic feature in question
on the model’s behavior. We apply this method to study how BERT models of
different sizes process relative clauses (RCs). We find that BERT variants use
RC boundary information during word prediction in a manner that is consistent
with the rules of English grammar; this RC boundary information generalizes to
a considerable extent across different RC types, suggesting that BERT
represents RCs as an abstract linguistic category.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/repetto2021multicriteria/">Multicriteria interpretability driven Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multicriteria interpretability driven Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multicriteria interpretability driven Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Marco Repetto</td>
	<td></td>
	<td><p>Deep Learning methods are renowned for their performances, yet their lack of
interpretability prevents them from high-stakes contexts. Recent model agnostic
methods address this problem by providing post-hoc interpretability methods by
reverse-engineering the model’s inner workings. However, in many regulated
fields, interpretability should be kept in mind from the start, which means
that post-hoc methods are valid only as a sanity check after model training.
Interpretability from the start, in an abstract setting, means posing a set of
soft constraints on the model’s behavior by injecting knowledge and
annihilating possible biases. We propose a Multicriteria technique that allows
to control the feature effects on the model’s outcome by injecting knowledge in
the objective function. We then extend the technique by including a non-linear
knowledge function to account for more complex effects and local lack of
knowledge. The result is a Deep Learning model that embodies interpretability
from the start and aligns with the recent regulations. A practical empirical
example based on credit risk, suggests that our approach creates performant yet
robust models capable of overcoming biases derived from data scarcity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/rao2021first/">A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Varun Nagaraj Rao, Xingjian Zhen, Karen Hovsepian, Mingwei Shen</td>
	<td></td>
	<td><p>Explainable deep learning models are advantageous in many situations. Prior
work mostly provide unimodal explanations through post-hoc approaches not part
of the original system design. Explanation mechanisms also ignore useful
textual information present in images. In this paper, we propose MTXNet, an
end-to-end trainable multimodal architecture to generate multimodal
explanations, which focuses on the text in the image. We curate a novel dataset
TextVQA-X, containing ground truth visual and multi-reference textual
explanations that can be leveraged during both training and evaluation. We then
quantitatively show that training with multimodal explanations complements
model performance and surpasses unimodal baselines by up to 7% in CIDEr scores
and 2% in IoU. More importantly, we demonstrate that the multimodal
explanations are consistent with human interpretations, help justify the
models’ decision, and provide useful insights to help diagnose an incorrect
prediction. Finally, we describe a real-world e-commerce application for using
the generated multimodal explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/ralekar2021understanding/">Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Character Recognition using Visual Explanations Derived from the Human Visual System and Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chetan Ralekar, Shubham Choudhary, Tapan Kumar Gandhi, Santanu Chaudhury</td>
	<td></td>
	<td><p>Human observers engage in selective information uptake when classifying
visual patterns. The same is true of deep neural networks, which currently
constitute the best performing artificial vision systems. Our goal is to
examine the congruence, or lack thereof, in the information-gathering
strategies of the two systems. We have operationalized our investigation as a
character recognition task. We have used eye-tracking to assay the spatial
distribution of information hotspots for humans via fixation maps and an
activation mapping technique for obtaining analogous distributions for deep
networks through visualization maps. Qualitative comparison between
visualization maps and fixation maps reveals an interesting correlate of
congruence. The deep learning model considered similar regions in character,
which humans have fixated in the case of correctly classified characters. On
the other hand, when the focused regions are different for humans and deep
nets, the characters are typically misclassified by the latter. Hence, we
propose to use the visual fixation maps obtained from the eye-tracking
experiment as a supervisory input to align the model’s focus on relevant
character regions. We find that such supervision improves the model’s
performance significantly and does not require any additional parameters. This
approach has the potential to find applications in diverse domains such as
medical analysis and surveillance in which explainability helps to determine
system fidelity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/verma2021pitfalls/">Pitfalls of Explainable ML: An Industry Perspective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Pitfalls of Explainable ML: An Industry Perspective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Pitfalls of Explainable ML: An Industry Perspective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sahil Verma, Aditya Lahiri, John P. Dickerson, Su-In Lee</td>
	<td></td>
	<td><p>As machine learning (ML) systems take a more prominent and central role in
contributing to life-impacting decisions, ensuring their trustworthiness and
accountability is of utmost importance. Explanations sit at the core of these
desirable attributes of a ML system. The emerging field is frequently called
<code class="language-plaintext highlighter-rouge">Explainable AI (XAI)'' or</code>Explainable ML.’’ The goal of explainable ML is
to intuitively explain the predictions of a ML system, while adhering to the
needs to various stakeholders. Many explanation techniques were developed with
contributions from both academia and industry. However, there are several
existing challenges that have not garnered enough interest and serve as
roadblocks to widespread adoption of explainable ML. In this short paper, we
enumerate challenges in explainable ML from an industry perspective. We hope
these challenges will serve as promising future research directions, and would
contribute to democratizing explainable ML.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/tuan2021local/">Local Explanation of Dialogue Response Generation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Local Explanation of Dialogue Response Generation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Local Explanation of Dialogue Response Generation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yi-Lin Tuan, Connor Pryor, Wenhu Chen, Lise Getoor, William Yang Wang</td>
	<td></td>
	<td><p>In comparison to the interpretation of classification models, the explanation
of sequence generation models is also an important problem, however it has seen
little attention. In this work, we study model-agnostic explanations of a
representative text generation task – dialogue response generation. Dialog
response generation is challenging with its open-ended sentences and multiple
acceptable responses. To gain insights into the reasoning process of a
generation model, we propose a new method, local explanation of response
generation (LERG) that regards the explanations as the mutual interaction of
segments in input and output sentences. LERG views the sequence prediction as
uncertainty estimation of a human response and then creates explanations by
perturbing the input and calculating the certainty change over the human
response. We show that LERG adheres to desired properties of explanations for
text generation including unbiased approximation, consistency and cause
identification. Empirically, our results show that our method consistently
improves other widely used methods on proposed automatic- and human- evaluation
metrics for this new task by 4.4-12.8%. Our analysis demonstrates that LERG can
extract both explicit and implicit relations between input and output segments.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/utkin2021ensembles/">Ensembles of Random SHAPs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Ensembles of Random SHAPs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Ensembles of Random SHAPs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lev V. Utkin, Andrei V. Konstantinov</td>
	<td></td>
	<td><p>Ensemble-based modifications of the well-known SHapley Additive exPlanations
(SHAP) method for the local explanation of a black-box model are proposed. The
modifications aim to simplify SHAP which is computationally expensive when
there is a large number of features. The main idea behind the proposed
modifications is to approximate SHAP by an ensemble of SHAPs with a smaller
number of features. According to the first modification, called ER-SHAP,
several features are randomly selected many times from the feature set, and
Shapley values for the features are computed by means of “small” SHAPs. The
explanation results are averaged to get the final Shapley values. According to
the second modification, called ERW-SHAP, several points are generated around
the explained instance for diversity purposes, and results of their explanation
are combined with weights depending on distances between points and the
explained instance. The third modification, called ER-SHAP-RF, uses the random
forest for preliminary explanation of instances and determining a feature
probability distribution which is applied to selection of features in the
ensemble-based procedure of ER-SHAP. Many numerical experiments illustrating
the proposed modifications demonstrate their efficiency and properties for
local explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/vaishnav2021understanding/">Understanding the computational demands underlying visual reasoning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding the computational demands underlying visual reasoning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding the computational demands underlying visual reasoning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mohit Vaishnav, Remi Cadene, Andrea Alamia, Drew Linsley, Rufin VanRullen, Thomas Serre</td>
	<td>Neural Computation, 2022</td>
	<td><p>Visual understanding requires comprehending complex visual relations between
objects within a scene. Here, we seek to characterize the computational demands
for abstract visual reasoning. We do this by systematically assessing the
ability of modern deep convolutional neural networks (CNNs) to learn to solve
the “Synthetic Visual Reasoning Test” (SVRT) challenge, a collection of
twenty-three visual reasoning problems. Our analysis reveals a novel taxonomy
of visual reasoning tasks, which can be primarily explained by both the type of
relations (same-different vs. spatial-relation judgments) and the number of
relations used to compose the underlying rules. Prior cognitive neuroscience
work suggests that attention plays a key role in humans’ visual reasoning
ability. To test this hypothesis, we extended the CNNs with spatial and
feature-based attention mechanisms. In a second series of experiments, we
evaluated the ability of these attention networks to learn to solve the SVRT
challenge and found the resulting architectures to be much more efficient at
solving the hardest of these visual reasoning tasks. Most importantly, the
corresponding improvements on individual tasks partially explained our novel
taxonomy. Overall, this work provides an granular computational account of
visual reasoning and yields testable neuroscience predictions regarding the
differential need for feature-based vs. spatial attention depending on the type
of visual reasoning problem.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/vijayaraghavan2021interpretable/">Interpretable Multi-Modal Hate Speech Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Multi-Modal Hate Speech Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Multi-Modal Hate Speech Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Prashanth Vijayaraghavan, Hugo Larochelle, Deb Roy</td>
	<td>ICML Workshop on AI for Social Good, 2019</td>
	<td><p>With growing role of social media in shaping public opinions and beliefs
across the world, there has been an increased attention to identify and counter
the problem of hate speech on social media. Hate speech on online spaces has
serious manifestations, including social polarization and hate crimes. While
prior works have proposed automated techniques to detect hate speech online,
these techniques primarily fail to look beyond the textual content. Moreover,
few attempts have been made to focus on the aspects of interpretability of such
models given the social and legal implications of incorrect predictions. In
this work, we propose a deep neural multi-modal model that can: (a) detect hate
speech by effectively capturing the semantics of the text along with
socio-cultural context in which a particular hate expression is made, and (b)
provide interpretable insights into decisions of our model. By performing a
thorough evaluation of different modeling techniques, we demonstrate that our
model is able to outperform the existing state-of-the-art hate speech
classification approaches. Finally, we show the importance of social and
cultural context features towards unearthing clusters associated with different
categories of hate.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wickstr%C3%B8m2021relax/">RELAX: Representation Learning Explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=RELAX: Representation Learning Explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=RELAX: Representation Learning Explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kristoffer K. Wickstrøm, Daniel J. Trosten, Sigurd Løkse, Ahcène Boubekki, Karl Øyvind Mikalsen, Michael C. Kampffmeyer, Robert Jenssen</td>
	<td></td>
	<td><p>Despite the significant improvements that representation learning via
self-supervision has led to when learning from unlabeled data, no methods exist
that explain what influences the learned representation. We address this need
through our proposed approach, RELAX, which is the first approach for
attribution-based explanations of representations. Our approach can also model
the uncertainty in its explanations, which is essential to produce trustworthy
explanations. RELAX explains representations by measuring similarities in the
representation space between an input and masked out versions of itself,
providing intuitive explanations and significantly outperforming the
gradient-based baseline. We provide theoretical interpretations of RELAX and
conduct a novel analysis of feature extractors trained using supervised and
unsupervised learning, providing insights into different learning strategies.
Finally, we illustrate the usability of RELAX in multi-view clustering and
highlight that incorporating uncertainty can be essential for providing
low-complexity explanations, taking a crucial step towards explaining
representations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wiegreffe2021teach/">Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarah Wiegreffe, Ana Marasović</td>
	<td></td>
	<td><p>Explainable NLP (ExNLP) has increasingly focused on collecting
human-annotated textual explanations. These explanations are used downstream in
three ways: as data augmentation to improve performance on a predictive task,
as supervision to train models to produce explanations for their predictions,
and as a ground-truth to evaluate model-generated explanations. In this review,
we identify 65 datasets with three predominant classes of textual explanations
(highlights, free-text, and structured), organize the literature on annotating
each type, identify strengths and shortcomings of existing collection
methodologies, and give recommendations for collecting ExNLP datasets in the
future.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yang2021exploring/">Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Linyi Yang, Jiazheng Li, Pádraig Cunningham, Yue Zhang, Barry Smyth, Ruihai Dong</td>
	<td></td>
	<td><p>While state-of-the-art NLP models have been achieving the excellent
performance of a wide range of tasks in recent years, important questions are
being raised about their robustness and their underlying sensitivity to
systematic biases that may exist in their training and test data. Such issues
come to be manifest in performance problems when faced with out-of-distribution
data in the field. One recent solution has been to use counterfactually
augmented datasets in order to reduce any reliance on spurious patterns that
may exist in the original data. Producing high-quality augmented data can be
costly and time-consuming as it usually needs to involve human feedback and
crowdsourcing efforts. In this work, we propose an alternative by describing
and evaluating an approach to automatically generating counterfactual data for
data augmentation and explanation. A comprehensive evaluation on several
different datasets and using a variety of state-of-the-art benchmarks
demonstrate how our approach can achieve significant improvements in model
performance when compared to models training on the original data and even when
compared to models trained with the benefit of human-generated augmented data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yadav2021behavior/">Behavior of k-NN as an Instance-Based Explanation Method</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Behavior of k-NN as an Instance-Based Explanation Method' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Behavior of k-NN as an Instance-Based Explanation Method' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chhavi Yadav, Kamalika Chaudhuri</td>
	<td></td>
	<td><p>Adoption of DL models in critical areas has led to an escalating demand for
sound explanation methods. Instance-based explanation methods are a popular
type that return selective instances from the training set to explain the
predictions for a test sample. One way to connect these explanations with
prediction is to ask the following counterfactual question - how does the loss
and prediction for a test sample change when explanations are removed from the
training set? Our paper answers this question for k-NNs which are natural
contenders for an instance-based explanation method. We first demonstrate
empirically that the representation space induced by last layer of a neural
network is the best to perform k-NN in. Using this layer, we conduct our
experiments and compare them to influence functions (IFs)
~\cite{koh2017understanding} which try to answer a similar question. Our
evaluations do indicate change in loss and predictions when explanations are
removed but we do not find a trend between $k$ and loss or prediction change.
We find significant stability in the predictions and loss of MNIST vs.
CIFAR-10. Surprisingly, we do not observe much difference in the behavior of
k-NNs vs. IFs on this question. We attribute this to training set subsampling
for IFs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhao2021towards/">Towards Interpretable Deep Metric Learning with Structural Matching</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Metric Learning with Structural Matching' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Metric Learning with Structural Matching' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wenliang Zhao, Yongming Rao, Ziyi Wang, Jiwen Lu, Jie Zhou</td>
	<td></td>
	<td><p>How do the neural networks distinguish two images? It is of critical
importance to understand the matching mechanism of deep models for developing
reliable intelligent systems for many risky visual applications such as
surveillance and access control. However, most existing deep metric learning
methods match the images by comparing feature vectors, which ignores the
spatial structure of images and thus lacks interpretability. In this paper, we
present a deep interpretable metric learning (DIML) method for more transparent
embedding learning. Unlike conventional metric learning methods based on
feature vector comparison, we propose a structural matching strategy that
explicitly aligns the spatial embeddings by computing an optimal matching flow
between feature maps of the two images. Our method enables deep models to learn
metrics in a more human-friendly way, where the similarity of two images can be
decomposed to several part-wise similarities and their contributions to the
overall similarity. Our method is model-agnostic, which can be applied to
off-the-shelf backbone networks and metric learning methods. We evaluate our
method on three major benchmarks of deep metric learning including CUB200-2011,
Cars196, and Stanford Online Products, and achieve substantial improvements
over popular metric learning methods with better interpretability. Code is
available at https://github.com/wl-zhao/DIML</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/wang2021fast/">On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Fast Adversarial Robustness Adaptation in Model-Agnostic Meta-Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, Meng Wang</td>
	<td></td>
	<td><p>Model-agnostic meta-learning (MAML) has emerged as one of the most successful
meta-learning techniques in few-shot learning. It enables us to learn a
meta-initialization} of model parameters (that we call meta-model) to rapidly
adapt to new tasks using a small amount of labeled training data. Despite the
generalization power of the meta-model, it remains elusive that how adversarial
robustness can be maintained by MAML in few-shot learning. In addition to
generalization, robustness is also desired for a meta-model to defend
adversarial examples (attacks). Toward promoting adversarial robustness in
MAML, we first study WHEN a robustness-promoting regularization should be
incorporated, given the fact that MAML adopts a bi-level (fine-tuning vs.
meta-update) learning procedure. We show that robustifying the meta-update
stage is sufficient to make robustness adapted to the task-specific fine-tuning
stage even if the latter uses a standard training protocol. We also make
additional justification on the acquired robustness adaptation by peering into
the interpretability of neurons’ activation maps. Furthermore, we investigate
HOW robust regularization can efficiently be designed in MAML. We propose a
general but easily-optimized robustness-regularized meta-learning framework,
which allows the use of unlabeled data augmentation, fast adversarial attack
generation, and computationally-light fine-tuning. In particular, we for the
first time show that the auxiliary contrastive learning task can enhance the
adversarial robustness of MAML. Finally, extensive experiments are conducted to
demonstrate the effectiveness of our proposed methods in robust few-shot
learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/shen2021interpretable/">Interpretable Compositional Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Compositional Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Compositional Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wen Shen, Zhihua Wei, Shikun Huang, Binbin Zhang, Jiaqi Fan, Ping Zhao, Quanshi Zhang</td>
	<td></td>
	<td><p>The reasonable definition of semantic interpretability presents the core
challenge in explainable AI. This paper proposes a method to modify a
traditional convolutional neural network (CNN) into an interpretable
compositional CNN, in order to learn filters that encode meaningful visual
patterns in intermediate convolutional layers. In a compositional CNN, each
filter is supposed to consistently represent a specific compositional object
part or image region with a clear meaning. The compositional CNN learns from
image labels for classification without any annotations of parts or regions for
supervision. Our method can be broadly applied to different types of CNNs.
Experiments have demonstrated the effectiveness of our method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/yin2021sensitivity/">On the Sensitivity and Stability of Model Interpretations in NLP</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Sensitivity and Stability of Model Interpretations in NLP' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Sensitivity and Stability of Model Interpretations in NLP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Fan Yin, Zhouxing Shi, Cho-Jui Hsieh, Kai-Wei Chang</td>
	<td></td>
	<td><p>Recent years have witnessed the emergence of a variety of post-hoc
interpretations that aim to uncover how natural language processing (NLP)
models make predictions. Despite the surge of new interpretation methods, it
remains an open problem how to define and quantitatively measure the
faithfulness of interpretations, i.e., to what extent interpretations reflect
the reasoning process by a model. We propose two new criteria, sensitivity and
stability, that provide complementary notions of faithfulness to the existed
removal-based criteria. Our results show that the conclusion for how faithful
interpretations are could vary substantially based on different notions.
Motivated by the desiderata of sensitivity and stability, we introduce a new
class of interpretation methods that adopt techniques from adversarial
robustness. Empirical results show that our proposed methods are effective
under the new criteria and overcome limitations of gradient-based methods on
removal-based criteria. Besides text classification, we also apply
interpretation methods and metrics to dependency parsing. Our results shed
light on understanding the diverse set of interpretations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2021</td>
	<td><a href="/publications/zhang2021head/">From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Qiaoning Zhang, X. Jessie Yang, Lionel P. Robert Jr</td>
	<td></td>
	<td><p>Automated vehicles (AVs) are social robots that can potentially benefit our
society. According to the existing literature, AV explanations can promote
passengers’ trust by reducing the uncertainty associated with the AV’s
reasoning and actions. However, the literature on AV explanations and trust has
failed to consider how the type of trust</p>
<ul>
  <li>cognitive versus affective - might alter this relationship. Yet, the
existing literature has shown that the implications associated with trust vary
widely depending on whether it is cognitive or affective. To address this
shortcoming and better understand the impacts of explanations on trust in AVs,
we designed a study to investigate the effectiveness of explanations on both
cognitive and affective trust. We expect these results to be of great
significance in designing AV explanations to promote AV trust.</li>
</ul>
</td>
	<td></td>
</tr>



<tr>
	<td>2020</td>
	<td><a href="/publications/kang2020multivariate/">Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multivariate Regression of Mixed Responses for Evaluation of Visualization Designs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaoning Kang, Xiaoyu Chen, Ran Jin, Hao Wu, Xinwei Deng</td>
	<td></td>
	<td><p>Information visualization significantly enhances human perception by
graphically representing complex data sets. The variety of visualization
designs makes it challenging to efficiently evaluate all possible designs
catering to users’ preferences and characteristics. Most of existing evaluation
methods perform user studies to obtain multivariate qualitative responses from
users via questionnaires and interviews. However, these methods cannot support
online evaluation of designs as they are often time-consuming. A statistical
model is desired to predict users’ preferences on visualization designs based
on non-interference measurements (i.e., wearable sensor signals). In this work,
we propose a multivariate regression of mixed responses (MRMR) to facilitate
quantitative evaluation of visualization designs. The proposed MRMR method is
able to provide accurate model prediction with meaningful variable selection. A
simulation study and a user study of evaluating visualization designs with 14
effective participants are conducted to illustrate the merits of the proposed
model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/joshi2020explainable/">Explainable Disease Classification via weakly-supervised segmentation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Disease Classification via weakly-supervised segmentation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Disease Classification via weakly-supervised segmentation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aniket Joshi, Gaurav Mishra, Jayanthi Sivaswamy</td>
	<td>Interpretable and Annotation-Efficient Learning for Medical Image Computing. IMIMIC 2020, MIL3ID 2020, LABELS 2020. Lecture Notes in Computer Science, vol 12446. Springer, Cham</td>
	<td><p>Deep learning based approaches to Computer Aided Diagnosis (CAD) typically
pose the problem as an image classification (Normal or Abnormal) problem. These
systems achieve high to very high accuracy in specific disease detection for
which they are trained but lack in terms of an explanation for the provided
decision/classification result. The activation maps which correspond to
decisions do not correlate well with regions of interest for specific diseases.
This paper examines this problem and proposes an approach which mimics the
clinical practice of looking for an evidence prior to diagnosis. A CAD model is
learnt using a mixed set of information: class labels for the entire training
set of images plus a rough localisation of suspect regions as an extra input
for a smaller subset of training images for guiding the learning. The proposed
approach is illustrated with detection of diabetic macular edema (DME) from OCT
slices. Results of testing on on a large public dataset show that with just a
third of images with roughly segmented fluid filled regions, the classification
accuracy is on par with state of the art methods while providing a good
explanation in the form of anatomically accurate heatmap /region of interest.
The proposed solution is then adapted to Breast Cancer detection from
mammographic images. Good evaluation results on public datasets underscores the
generalisability of the proposed solution.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/jacovi2020aligning/">Aligning Faithful Interpretations with their Social Attribution</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Aligning Faithful Interpretations with their Social Attribution' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Aligning Faithful Interpretations with their Social Attribution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alon Jacovi, Yoav Goldberg</td>
	<td></td>
	<td><p>We find that the requirement of model interpretations to be faithful is vague
and incomplete. With interpretation by textual highlights as a case-study, we
present several failure cases. Borrowing concepts from social science, we
identify that the problem is a misalignment between the causal chain of
decisions (causal attribution) and the attribution of human behavior to the
interpretation (social attribution). We re-formulate faithfulness as an
accurate attribution of causality to the model, and introduce the concept of
aligned faithfulness: faithful causal chains that are aligned with their
expected social behavior. The two steps of causal attribution and social
attribution together complete the process of explaining behavior. With this
formalization, we characterize various failures of misaligned faithful
highlight interpretations, and propose an alternative causal chain to remedy
the issues. Finally, we implement highlight explanations of the proposed causal
format using contrastive explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/izza2020explaining/">On Explaining Decision Trees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Explaining Decision Trees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Explaining Decision Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yacine Izza, Alexey Ignatiev, Joao Marques-Silva</td>
	<td></td>
	<td><p>Decision trees (DTs) epitomize what have become to be known as interpretable
machine learning (ML) models. This is informally motivated by paths in DTs
being often much smaller than the total number of features. This paper shows
that in some settings DTs can hardly be deemed interpretable, with paths in a
DT being arbitrarily larger than a PI-explanation, i.e. a subset-minimal set of
feature values that entails the prediction. As a result, the paper proposes a
novel model for computing PI-explanations of DTs, which enables computing one
PI-explanation in polynomial time. Moreover, it is shown that enumeration of
PI-explanations can be reduced to the enumeration of minimal hitting sets.
Experimental results were obtained on a wide range of publicly available
datasets with well-known DT-learning tools, and confirm that in most cases DTs
have paths that are proper supersets of PI-explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/jacovi2020towards/">Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alon Jacovi, Yoav Goldberg</td>
	<td></td>
	<td><p>With the growing popularity of deep-learning based NLP models, comes a need
for interpretable systems. But what is interpretability, and what constitutes a
high-quality interpretation? In this opinion piece we reflect on the current
state of interpretability evaluation research. We call for more clearly
differentiating between different desired criteria an interpretation should
satisfy, and focus on the faithfulness criteria. We survey the literature with
respect to faithfulness evaluation, and arrange the current approaches around
three assumptions, providing an explicit form to how faithfulness is “defined”
by the community. We provide concrete guidelines on how evaluation of
interpretation methods should and should not be conducted. Finally, we claim
that the current binary definition for faithfulness sets a potentially
unrealistic bar for being considered faithful. We call for discarding the
binary notion of faithfulness in favor of a more graded one, which we believe
will be of greater practical utility.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ismail2020benchmarking/">Benchmarking Deep Learning Interpretability in Time Series Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking Deep Learning Interpretability in Time Series Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Benchmarking Deep Learning Interpretability in Time Series Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aya Abdelsalam Ismail, Mohamed Gunady, Héctor Corrada Bravo, Soheil Feizi</td>
	<td>NeurIPS 2020</td>
	<td><p>Saliency methods are used extensively to highlight the importance of input
features in model predictions. These methods are mostly used in vision and
language tasks, and their applications to time series data is relatively
unexplored. In this paper, we set out to extensively compare the performance of
various saliency-based interpretability methods across diverse neural
architectures, including Recurrent Neural Network, Temporal Convolutional
Networks, and Transformers in a new benchmark of synthetic time series data. We
propose and report multiple metrics to empirically evaluate the performance of
saliency methods for detecting feature importance over time using both
precision (i.e., whether identified features contain meaningful signals) and
recall (i.e., the number of features with signal identified as important).
Through several experiments, we show that (i) in general, network architectures
and saliency methods fail to reliably and accurately identify feature
importance over time in time series data, (ii) this failure is mainly due to
the conflation of time and feature domains, and (iii) the quality of saliency
maps can be improved substantially by using our proposed two-step temporal
saliency rescaling (TSR) approach that first calculates the importance of each
time step before calculating the importance of each feature at a time step.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ignatiev2020relating/">On Relating 'Why?' and 'Why Not?' Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Relating 'Why?' and 'Why Not?' Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Relating 'Why?' and 'Why Not?' Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alexey Ignatiev, Nina Narodytska, Nicholas Asher, Joao Marques-Silva</td>
	<td></td>
	<td><p>Explanations of Machine Learning (ML) models often address a ‘Why?’ question.
Such explanations can be related with selecting feature-value pairs which are
sufficient for the prediction. Recent work has investigated explanations that
address a ‘Why Not?’ question, i.e. finding a change of feature values that
guarantee a change of prediction. Given their goals, these two forms of
explaining predictions of ML models appear to be mostly unrelated. However,
this paper demonstrates otherwise, and establishes a rigorous formal
relationship between ‘Why?’ and ‘Why Not?’ explanations. Concretely, the paper
proves that, for any given instance, ‘Why?’ explanations are minimal hitting
sets of ‘Why Not?’ explanations and vice-versa. Furthermore, the paper devises
novel algorithms for extracting and enumerating both forms of explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/jain2020learning/">Learning to Faithfully Rationalize by Construction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Faithfully Rationalize by Construction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Faithfully Rationalize by Construction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarthak Jain, Sarah Wiegreffe, Yuval Pinter, Byron C. Wallace</td>
	<td></td>
	<td><p>In many settings it is important for one to be able to understand why a model
made a particular prediction. In NLP this often entails extracting snippets of
an input text <code class="language-plaintext highlighter-rouge">responsible for' corresponding model output; when such a snippet
comprises tokens that indeed informed the model's prediction, it is a faithful
explanation. In some settings, faithfulness may be critical to ensure
transparency. Lei et al. (2016) proposed a model to produce faithful rationales
for neural text classification by defining independent snippet extraction and
prediction modules. However, the discrete selection over input tokens performed
by this method complicates training, leading to high variance and requiring
careful hyperparameter tuning. We propose a simpler variant of this approach
that provides faithful explanations by construction. In our scheme, named
FRESH, arbitrary feature importance scores (e.g., gradients from a trained
model) are used to induce binary labels over token inputs, which an extractor
can be trained to predict. An independent classifier module is then trained
exclusively on snippets provided by the extractor; these snippets thus
constitute faithful explanations, even if the classifier is arbitrarily
complex. In both automatic and manual evaluations we find that variants of this
simple framework yield predictive performance superior to </code>end-to-end’
approaches, while being more general and easier to train. Code is available at
https://github.com/successar/FRESH</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/karimi2020survey/">A survey of algorithmic recourse: definitions, formulations, solutions, and prospects</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A survey of algorithmic recourse: definitions, formulations, solutions, and prospects' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A survey of algorithmic recourse: definitions, formulations, solutions, and prospects' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Amir-Hossein Karimi, Gilles Barthe, Bernhard Schölkopf, Isabel Valera</td>
	<td></td>
	<td><p>Machine learning is increasingly used to inform decision-making in sensitive
situations where decisions have consequential effects on individuals’ lives. In
these settings, in addition to requiring models to be accurate and robust,
socially relevant values such as fairness, privacy, accountability, and
explainability play an important role for the adoption and impact of said
technologies. In this work, we focus on algorithmic recourse, which is
concerned with providing explanations and recommendations to individuals who
are unfavourably treated by automated decision-making systems. We first perform
an extensive literature review, and align the efforts of many authors by
presenting unified definitions, formulations, and solutions to recourse. Then,
we provide an overview of the prospective research directions towards which the
community may engage, challenging existing assumptions and making explicit
connections to other ethical challenges such as security, privacy, and
fairness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/sokol2020towards/">Towards Faithful and Meaningful Interpretable Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Faithful and Meaningful Interpretable Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Faithful and Meaningful Interpretable Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kacper Sokol, Peter Flach</td>
	<td></td>
	<td><p>Interpretable representations are the backbone of many black-box explainers.
They translate the low-level data representation necessary for good predictive
performance into high-level human-intelligible concepts used to convey the
explanation. Notably, the explanation type and its cognitive complexity are
directly controlled by the interpretable representation, allowing to target a
particular audience and use case. However, many explainers that rely on
interpretable representations overlook their merit and fall back on default
solutions, which may introduce implicit assumptions, thereby degrading the
explanatory power of such techniques. To address this problem, we study
properties of interpretable representations that encode presence and absence of
human-comprehensible concepts. We show how they are operationalised for
tabular, image and text data, discussing their strengths and weaknesses.
Finally, we analyse their explanatory properties in the context of tabular
data, where a linear model is used to quantify the importance of interpretable
concepts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hsieh2020evaluations/">Evaluations and Methods for Explanation through Robustness Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluations and Methods for Explanation through Robustness Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluations and Methods for Explanation through Robustness Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh</td>
	<td></td>
	<td><p>Feature based explanations, that provide importance of each feature towards
the model prediction, is arguably one of the most intuitive ways to explain a
model. In this paper, we establish a novel set of evaluation criteria for such
feature based explanations by robustness analysis. In contrast to existing
evaluations which require us to specify some way to “remove” features that
could inevitably introduces biases and artifacts, we make use of the subtler
notion of smaller adversarial perturbations. By optimizing towards our proposed
evaluation criteria, we obtain new explanations that are loosely necessary and
sufficient for a prediction. We further extend the explanation to extract the
set of features that would move the current prediction to a target class by
adopting targeted adversarial attack for the robustness analysis. Through
experiments across multiple domains and a user study, we validate the
usefulness of our evaluation criteria and our derived explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hong2020interpretable/">Interpretable Sequence Classification Via Prototype Trajectory</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Sequence Classification Via Prototype Trajectory' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Sequence Classification Via Prototype Trajectory' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dat Hong, Stephen S. Baek, Tong Wang</td>
	<td></td>
	<td><p>We propose a novel interpretable deep neural network for text classification,
called ProtoryNet, based on a new concept of prototype trajectories. Motivated
by the prototype theory in modern linguistics, ProtoryNet makes a prediction by
finding the most similar prototype for each sentence in a text sequence and
feeding an RNN backbone with the proximity of each sentence to the
corresponding active prototype. The RNN backbone then captures the temporal
pattern of the prototypes, which we refer to as prototype trajectories.
Prototype trajectories enable intuitive and fine-grained interpretation of the
reasoning process of the RNN model, in resemblance to how humans analyze texts.
We also design a prototype pruning procedure to reduce the total number of
prototypes used by the model for better interpretability. Experiments on
multiple public data sets show that ProtoryNet is more accurate than the
baseline prototype-based deep neural net and reduces the performance gap
compared to state-of-the-art black-box models. In addition, after prototype
pruning, the resulting ProtoryNet models only need less than or around 20
prototypes for all datasets, which significantly benefits interpretability.
Furthermore, we report a survey result indicating that human users find
ProtoryNet more intuitive and easier to understand than other prototype-based
methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/honeycutt2020soliciting/">Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Soliciting Human-in-the-Loop User Feedback for Interactive Machine Learning Reduces User Trust and Impressions of Model Accuracy' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Donald R. Honeycutt, Mahsan Nourani, Eric D. Ragan</td>
	<td></td>
	<td><p>Mixed-initiative systems allow users to interactively provide feedback to
potentially improve system performance. Human feedback can correct model errors
and update model parameters to dynamically adapt to changing data.
Additionally, many users desire the ability to have a greater level of control
and fix perceived flaws in systems they rely on. However, how the ability to
provide feedback to autonomous systems influences user trust is a largely
unexplored area of research. Our research investigates how the act of providing
feedback can affect user understanding of an intelligent system and its
accuracy. We present a controlled experiment using a simulated object detection
system with image data to study the effects of interactive feedback collection
on user impressions. The results show that providing human-in-the-loop feedback
lowered both participants’ trust in the system and their perception of system
accuracy, regardless of whether the system accuracy improved in response to
their feedback. These results highlight the importance of considering the
effects of allowing end-user feedback on user trust when designing intelligent
systems.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/homdee2020actionable/">Actionable Interpretation of Machine Learning Models for Sequential Data: Dementia-related Agitation Use Case</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Actionable Interpretation of Machine Learning Models for Sequential Data: Dementia-related Agitation Use Case' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Actionable Interpretation of Machine Learning Models for Sequential Data: Dementia-related Agitation Use Case' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nutta Homdee, John Lach</td>
	<td></td>
	<td><p>Machine learning has shown successes for complex learning problems in which
data/parameters can be multidimensional and too complex for a first-principles
based analysis. Some applications that utilize machine learning require human
interpretability, not just to understand a particular result (classification,
detection, etc.) but also for humans to take action based on that result.
Black-box machine learning model interpretation has been studied, but recent
work has focused on validation and improving model performance. In this work,
an actionable interpretation of black-box machine learning models is presented.
The proposed technique focuses on the extraction of actionable measures to help
users make a decision or take an action. Actionable interpretation can be
implemented in most traditional black-box machine learning models. It uses the
already trained model, used training data, and data processing techniques to
extract actionable items from the model outcome and its time-series inputs. An
implementation of the actionable interpretation is shown with a use case:
dementia-related agitation prediction and the ambient environment. It is shown
that actionable items can be extracted, such as the decreasing of in-home light
level, which is triggering an agitation episode. This use case of actionable
interpretation can help dementia caregivers take action to intervene and
prevent agitation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/han2020explaining/">Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaochuang Han, Byron C. Wallace, Yulia Tsvetkov</td>
	<td></td>
	<td><p>Modern deep learning models for NLP are notoriously opaque. This has
motivated the development of methods for interpreting such models, e.g., via
gradient-based saliency maps or the visualization of attention weights. Such
approaches aim to provide explanations for a particular model prediction by
highlighting important words in the corresponding input text. While this might
be useful for tasks where decisions are explicitly influenced by individual
tokens in the input, we suspect that such highlighting is not suitable for
tasks where model decisions should be driven by more complex reasoning. In this
work, we investigate the use of influence functions for NLP, providing an
alternative approach to interpreting neural text classifiers. Influence
functions explain the decisions of a model by identifying influential training
examples. Despite the promise of this approach, influence functions have not
yet been extensively evaluated in the context of NLP, a gap addressed by this
work. We conduct a comparison between influence functions and common
word-saliency methods on representative tasks. As suspected, we find that
influence functions are particularly useful for natural language inference, a
task in which ‘saliency maps’ may not have clear interpretation. Furthermore,
we develop a new quantitative measure based on influence functions that can
reveal artifacts in training data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/gu2020interpretable/">Interpretable Graph Capsule Networks for Object Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Graph Capsule Networks for Object Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Graph Capsule Networks for Object Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jindong Gu, Volker Tresp</td>
	<td>The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI), 2021</td>
	<td><p>Capsule Networks, as alternatives to Convolutional Neural Networks, have been
proposed to recognize objects from images. The current literature demonstrates
many advantages of CapsNets over CNNs. However, how to create explanations for
individual classifications of CapsNets has not been well explored. The widely
used saliency methods are mainly proposed for explaining CNN-based
classifications; they create saliency map explanations by combining activation
values and the corresponding gradients, e.g., Grad-CAM. These saliency methods
require a specific architecture of the underlying classifiers and cannot be
trivially applied to CapsNets due to the iterative routing mechanism therein.
To overcome the lack of interpretability, we can either propose new post-hoc
interpretation methods for CapsNets or modifying the model to have build-in
explanations. In this work, we explore the latter. Specifically, we propose
interpretable Graph Capsule Networks (GraCapsNets), where we replace the
routing part with a multi-head attention-based Graph Pooling approach. In the
proposed model, individual classification explanations can be created
effectively and efficiently. Our model also demonstrates some unexpected
benefits, even though it replaces the fundamental part of CapsNets. Our
GraCapsNets achieve better classification performance with fewer parameters and
better adversarial robustness, when compared to CapsNets. Besides, GraCapsNets
also keep other advantages of CapsNets, namely, disentangled representations
and affine transformation robustness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/hase2020evaluating/">Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Peter Hase, Mohit Bansal</td>
	<td></td>
	<td><p>Algorithmic approaches to interpreting machine learning models have
proliferated in recent years. We carry out human subject tests that are the
first of their kind to isolate the effect of algorithmic explanations on a key
aspect of model interpretability, simulatability, while avoiding important
confounding experimental factors. A model is simulatable when a person can
predict its behavior on new inputs. Through two kinds of simulation tests
involving text and tabular data, we evaluate five explanations methods: (1)
LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a
Composite approach that combines explanations from each method. Clear evidence
of method effectiveness is found in very few cases: LIME improves
simulatability in tabular classification, and our Prototype method is effective
in counterfactual simulation tests. We also collect subjective ratings of
explanations, but we do not find that ratings are predictive of how helpful
explanations are. Our results provide the first reliable and comprehensive
estimates of how explanations influence simulatability across a variety of
explanation methods and data domains. We show that (1) we need to be careful
about the metrics we use to evaluate explanation methods, and (2) there is
significant room for improvement in current methods. All our supporting code,
data, and models are publicly available at:
https://github.com/peterbhase/InterpretableNLP-ACL2020</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fu2020interactive/">Interactive Knowledge Distillation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interactive Knowledge Distillation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interactive Knowledge Distillation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shipeng Fu, Zhen Li, Jun Xu, Ming-Ming Cheng, Zitao Liu, Xiaomin Yang</td>
	<td></td>
	<td><p>Knowledge distillation is a standard teacher-student learning framework to
train a light-weight student network under the guidance of a well-trained large
teacher network. As an effective teaching strategy, interactive teaching has
been widely employed at school to motivate students, in which teachers not only
provide knowledge but also give constructive feedback to students upon their
responses, to improve their learning performance. In this work, we propose an
InterActive Knowledge Distillation (IAKD) scheme to leverage the interactive
teaching strategy for efficient knowledge distillation. In the distillation
process, the interaction between teacher and student networks is implemented by
a swapping-in operation: randomly replacing the blocks in the student network
with the corresponding blocks in the teacher network. In the way, we directly
involve the teacher’s powerful feature transformation ability to largely boost
the student’s performance. Experiments with typical settings of teacher-student
networks demonstrate that the student networks trained by our IAKD achieve
better performance than those trained by conventional knowledge distillation
methods on diverse image classification datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/garreau2020looking/">Looking Deeper into Tabular LIME</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Looking Deeper into Tabular LIME' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Looking Deeper into Tabular LIME' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Damien Garreau, Ulrike von Luxburg</td>
	<td></td>
	<td><p>In this paper, we present a thorough theoretical analysis of the default
implementation of LIME in the case of tabular data. We prove that in the large
sample limit, the interpretable coefficients provided by Tabular LIME can be
computed in an explicit way as a function of the algorithm parameters and some
expectation computations related to the black-box model. When the function to
explain has some nice algebraic structure (linear, multiplicative, or sparsely
depending on a subset of the coordinates), our analysis provides interesting
insights into the explanations provided by LIME. These can be applied to a
range of machine learning models including Gaussian kernels or CART random
forests. As an example, for linear functions we show that LIME has the
desirable property to provide explanations that are proportional to the
coefficients of the function to explain and to ignore coordinates that are not
used by the function to explain. For partition-based regressors, on the other
side, we show that LIME produces undesired artifacts that may provide
misleading explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shvo2020towards/">Towards the Role of Theory of Mind in Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards the Role of Theory of Mind in Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards the Role of Theory of Mind in Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Maayan Shvo, Toryn Q. Klassen, Sheila A. McIlraith</td>
	<td></td>
	<td><p>Theory of Mind is commonly defined as the ability to attribute mental states
(e.g., beliefs, goals) to oneself, and to others. A large body of previous work</p>
<ul>
  <li>from the social sciences to artificial intelligence - has observed that
Theory of Mind capabilities are central to providing an explanation to another
agent or when explaining that agent’s behaviour. In this paper, we build and
expand upon previous work by providing an account of explanation in terms of
the beliefs of agents and the mechanism by which agents revise their beliefs
given possible explanations. We further identify a set of desiderata for
explanations that utilize Theory of Mind. These desiderata inform our
belief-based account of explanation.</li>
</ul>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fel2020good/">How Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Good is your Explanation? Algorithmic Stability Measures to Assess the Quality of Explanations for Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Thomas Fel, David Vigouroux, Rémi Cadène, Thomas Serre</td>
	<td>2022 CVF Winter Conference on Applications of Computer Vision (WACV), Jan 2022, Hawaii, United States</td>
	<td><p>A plethora of methods have been proposed to explain how deep neural networks
reach their decisions but comparatively, little effort has been made to ensure
that the explanations produced by these methods are objectively relevant. While
several desirable properties for trustworthy explanations have been formulated,
objective measures have been harder to derive. Here, we propose two new
measures to evaluate explanations borrowed from the field of algorithmic
stability: mean generalizability MeGe and relative consistency ReCo. We conduct
extensive experiments on different network architectures, common explainability
methods, and several image datasets to demonstrate the benefits of the proposed
measures.In comparison to ours, popular fidelity measures are not sufficient to
guarantee trustworthy explanations.Finally, we found that 1-Lipschitz networks
produce explanations with higher MeGe and ReCo than common neural networks
while reaching similar accuracy. This suggests that 1-Lipschitz networks are a
relevant direction towards predictors that are more explainable and
trustworthy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/etheredge2020decontextualized/">Decontextualized learning for interpretable hierarchical representations of visual patterns</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Decontextualized learning for interpretable hierarchical representations of visual patterns' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Decontextualized learning for interpretable hierarchical representations of visual patterns' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>R. Ian Etheredge, Manfred Schartl, Alex Jordan</td>
	<td></td>
	<td><p>Apart from discriminative models for classification and object detection
tasks, the application of deep convolutional neural networks to basic research
utilizing natural imaging data has been somewhat limited; particularly in cases
where a set of interpretable features for downstream analysis is needed, a key
requirement for many scientific investigations. We present an algorithm and
training paradigm designed specifically to address this: decontextualized
hierarchical representation learning (DHRL). By combining a generative model
chaining procedure with a ladder network architecture and latent space
regularization for inference, DHRL address the limitations of small datasets
and encourages a disentangled set of hierarchically organized features. In
addition to providing a tractable path for analyzing complex hierarchal
patterns using variation inference, this approach is generative and can be
directly combined with empirical and theoretical approaches. To highlight the
extensibility and usefulness of DHRL, we demonstrate this method in application
to a question from evolutionary biology.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fan2020trust/">Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Can We Trust Your Explanations? Sanity Checks for Interpreters in Android Malware Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ming Fan, Wenying Wei, Xiaofei Xie, Yang Liu, Xiaohong Guan, Ting Liu</td>
	<td></td>
	<td><p>With the rapid growth of Android malware, many machine learning-based malware
analysis approaches are proposed to mitigate the severe phenomenon. However,
such classifiers are opaque, non-intuitive, and difficult for analysts to
understand the inner decision reason. For this reason, a variety of explanation
approaches are proposed to interpret predictions by providing important
features. Unfortunately, the explanation results obtained in the malware
analysis domain cannot achieve a consensus in general, which makes the analysts
confused about whether they can trust such results. In this work, we propose
principled guidelines to assess the quality of five explanation approaches by
designing three critical quantitative metrics to measure their stability,
robustness, and effectiveness. Furthermore, we collect five widely-used malware
datasets and apply the explanation approaches on them in two tasks, including
malware detection and familial identification. Based on the generated
explanation results, we conduct a sanity check of such explanation approaches
in terms of the three metrics. The results demonstrate that our metrics can
assess the explanation approaches and help us obtain the knowledge of most
typical malicious behaviors for malware analysis.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/su2020sanity/">Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity-Checking Pruning Methods: Random Tickets can Win the Jackpot' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jingtong Su, Yihang Chen, Tianle Cai, Tianhao Wu, Ruiqi Gao, Liwei Wang, Jason D. Lee</td>
	<td></td>
	<td><p>Network pruning is a method for reducing test-time computational resource
requirements with minimal performance degradation. Conventional wisdom of
pruning algorithms suggests that: (1) Pruning methods exploit information from
training data to find good subnetworks; (2) The architecture of the pruned
network is crucial for good performance. In this paper, we conduct sanity
checks for the above beliefs on several recent unstructured pruning methods and
surprisingly find that: (1) A set of methods which aims to find good
subnetworks of the randomly-initialized network (which we call “initial
tickets”), hardly exploits any information from the training data; (2) For the
pruned networks obtained by these methods, randomly changing the preserved
weights in each layer, while keeping the total number of preserved weights
unchanged per layer, does not affect the final performance. These findings
inspire us to choose a series of simple \emph{data-independent} prune ratios
for each layer, and randomly prune each layer accordingly to get a subnetwork
(which we call “random tickets”). Experimental results show that our zero-shot
random tickets outperform or attain a similar performance compared to existing
“initial tickets”. In addition, we identify one existing pruning method that
passes our sanity checks. We hybridize the ratios in our random ticket with
this method and propose a new method called “hybrid tickets”, which achieves
further improvement. (Our code is publicly available at
https://github.com/JingtongSu/sanity-checking-pruning)</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shickel2020sequential/">Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sequential Interpretability: Methods, Applications, and Future Direction for Understanding Deep Learning Models in the Context of Sequential Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Benjamin Shickel, Parisa Rashidi</td>
	<td></td>
	<td><p>Deep learning continues to revolutionize an ever-growing number of critical
application areas including healthcare, transportation, finance, and basic
sciences. Despite their increased predictive power, model transparency and
human explainability remain a significant challenge due to the “black box”
nature of modern deep learning models. In many cases the desired balance
between interpretability and performance is predominately task specific.
Human-centric domains such as healthcare necessitate a renewed focus on
understanding how and why these frameworks are arriving at critical and
potentially life-or-death decisions. Given the quantity of research and
empirical successes of deep learning for computer vision, most of the existing
interpretability research has focused on image processing techniques.
Comparatively, less attention has been paid to interpreting deep learning
frameworks using sequential data. Given recent deep learning advancements in
highly sequential domains such as natural language processing and physiological
signal processing, the need for deep sequential explanations is at an all-time
high. In this paper, we review current techniques for interpreting deep
learning techniques involving sequential data, identify similarities to
non-sequential methods, and discuss current limitations and future avenues of
sequential interpretability research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/subramanian2020obtaining/">Obtaining Faithful Interpretations from Compositional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Obtaining Faithful Interpretations from Compositional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Obtaining Faithful Interpretations from Compositional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner</td>
	<td></td>
	<td><p>Neural module networks (NMNs) are a popular approach for modeling
compositionality: they achieve high accuracy when applied to problems in
language and vision, while reflecting the compositional structure of the
problem in the network architecture. However, prior work implicitly assumed
that the structure of the network modules, describing the abstract reasoning
process, provides a faithful explanation of the model’s reasoning; that is,
that all modules perform their intended behaviour. In this work, we propose and
conduct a systematic evaluation of the intermediate outputs of NMNs on NLVR2
and DROP, two datasets which require composing multiple reasoning steps. We
find that the intermediate outputs differ from the expected output,
illustrating that the network structure does not provide a faithful explanation
of model behaviour. To remedy that, we train the model with auxiliary
supervision and propose particular choices for module architecture that yield
much better faithfulness, at a minimal cost to accuracy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/davis2020measure/">Measure Utility, Gain Trust: Practical Advice for XAI Researcher</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Measure Utility, Gain Trust: Practical Advice for XAI Researcher' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Measure Utility, Gain Trust: Practical Advice for XAI Researcher' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Brittany Davis, Maria Glenski, William Sealy, Dustin Arendt</td>
	<td></td>
	<td><p>Research into the explanation of machine learning models, i.e., explainable
AI (XAI), has seen a commensurate exponential growth alongside deep artificial
neural networks throughout the past decade. For historical reasons, explanation
and trust have been intertwined. However, the focus on trust is too narrow, and
has led the research community astray from tried and true empirical methods
that produced more defensible scientific knowledge about people and
explanations. To address this, we contribute a practical path forward for
researchers in the XAI field. We recommend researchers focus on the utility of
machine learning explanations instead of trust. We outline five broad use cases
where explanations are useful and, for each, we describe pseudo-experiments
that rely on objective empirical measurements and falsifiable hypotheses. We
believe that this experimental rigor is necessary to contribute to scientific
knowledge in the field of XAI.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/das2020leveraging/">Leveraging Rationales to Improve Human Task Performance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Leveraging Rationales to Improve Human Task Performance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Leveraging Rationales to Improve Human Task Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Devleena Das, Sonia Chernova</td>
	<td></td>
	<td><p>Machine learning (ML) systems across many application areas are increasingly
demonstrating performance that is beyond that of humans. In response to the
proliferation of such models, the field of Explainable AI (XAI) has sought to
develop techniques that enhance the transparency and interpretability of
machine learning methods. In this work, we consider a question not previously
explored within the XAI and ML communities: Given a computational system whose
performance exceeds that of its human user, can explainable AI capabilities be
leveraged to improve the performance of the human? We study this question in
the context of the game of Chess, for which computational game engines that
surpass the performance of the average player are widely available. We
introduce the Rationale-Generating Algorithm, an automated technique for
generating rationales for utility-based computational methods, which we
evaluate with a multi-day user study against two baselines. The results show
that our approach produces rationales that lead to statistically significant
improvement in human task performance, demonstrating that rationales
automatically generated from an AI’s internal task model can be used not only
to explain what the system is doing, but also to instruct the user and
ultimately improve their task performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/delaney2020instance-based/">Instance-based Counterfactual Explanations for Time Series Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Instance-based Counterfactual Explanations for Time Series Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Instance-based Counterfactual Explanations for Time Series Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Eoin Delaney, Derek Greene, Mark T. Keane</td>
	<td></td>
	<td><p>In recent years, there has been a rapidly expanding focus on explaining the
predictions made by black-box AI systems that handle image and tabular data.
However, considerably less attention has been paid to explaining the
predictions of opaque AI systems handling time series data. In this paper, we
advance a novel model-agnostic, case-based technique – Native Guide – that
generates counterfactual explanations for time series classifiers. Given a
query time series, $T_{q}$, for which a black-box classification system
predicts class, $c$, a counterfactual time series explanation shows how $T_{q}$
could change, such that the system predicts an alternative class, $c’$. The
proposed instance-based technique adapts existing counterfactual instances in
the case-base by highlighting and modifying discriminative areas of the time
series that underlie the classification. Quantitative and qualitative results
from two comparative experiments indicate that Native Guide generates
plausible, proximal, sparse and diverse explanations that are better than those
produced by key benchmark counterfactual methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/dieber2020model/">Why model why? Assessing the strengths and limitations of LIME</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Why model why? Assessing the strengths and limitations of LIME' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Why model why? Assessing the strengths and limitations of LIME' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jürgen Dieber, Sabrina Kirrane</td>
	<td></td>
	<td><p>When it comes to complex machine learning models, commonly referred to as
black boxes, understanding the underlying decision making process is crucial
for domains such as healthcare and financial services, and also when it is used
in connection with safety critical systems such as autonomous vehicles. As such
interest in explainable artificial intelligence (xAI) tools and techniques has
increased in recent years. However, the effectiveness of existing xAI
frameworks, especially concerning algorithms that work with data as opposed to
images, is still an open research question. In order to address this gap, in
this paper we examine the effectiveness of the Local Interpretable
Model-Agnostic Explanations (LIME) xAI framework, one of the most popular model
agnostic frameworks found in the literature, with a specific focus on its
performance in terms of making tabular models more interpretable. In
particular, we apply several state of the art machine learning algorithms on a
tabular dataset, and demonstrate how LIME can be used to supplement
conventional performance assessment methods. In addition, we evaluate the
understandability of the output produced by LIME both via a usability study,
involving participants who are not familiar with LIME, and its overall
usability via an assessment framework, which is derived from the International
Organisation for Standardisation 9241-11:1998 standard.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/wang2020interpreting/">Interpreting Interpretations: Organizing Attribution Methods by Criteria</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Interpretations: Organizing Attribution Methods by Criteria' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Interpretations: Organizing Attribution Methods by Criteria' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zifan Wang, Piotr Mardziel, Anupam Datta, Matt Fredrikson</td>
	<td></td>
	<td><p>Motivated by distinct, though related, criteria, a growing number of
attribution methods have been developed tointerprete deep learning. While each
relies on the interpretability of the concept of “importance” and our ability
to visualize patterns, explanations produced by the methods often differ. As a
result, input attribution for vision models fail to provide any level of human
understanding of model behaviour. In this work we expand the foundationsof
human-understandable concepts with which attributionscan be interpreted beyond
“importance” and its visualization; we incorporate the logical concepts of
necessity andsufficiency, and the concept of proportionality. We definemetrics
to represent these concepts as quantitative aspectsof an attribution. This
allows us to compare attributionsproduced by different methods and interpret
them in novelways: to what extent does this attribution (or this
method)represent the necessity or sufficiency of the highlighted inputs, and to
what extent is it proportional? We evaluate our measures on a collection of
methods explaining convolutional neural networks (CNN) for image
classification. We conclude that some attribution methods are more appropriate
for interpretation in terms of necessity while others are in terms of
sufficiency, while no method is always the most appropriate in terms of both.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/danilevsky2020survey/">A Survey of the State of Explainable AI for Natural Language Processing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey of the State of Explainable AI for Natural Language Processing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey of the State of Explainable AI for Natural Language Processing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, Prithviraj Sen</td>
	<td>Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing 2020</td>
	<td><p>Recent years have seen important advances in the quality of state-of-the-art
models, but this has come at the expense of models becoming less interpretable.
This survey presents an overview of the current state of Explainable AI (XAI),
considered within the domain of Natural Language Processing (NLP). We discuss
the main categorization of explanations, as well as the various ways
explanations can be arrived at and visualized. We detail the operations and
explainability techniques currently available for generating explanations for
NLP model predictions, to serve as a resource for model developers in the
community. Finally, we point out the current gaps and encourage directions for
future work in this important research area.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/custode2020evolutionary/">Evolutionary learning of interpretable decision trees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evolutionary learning of interpretable decision trees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evolutionary learning of interpretable decision trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leonardo Lucio Custode, Giovanni Iacca</td>
	<td></td>
	<td><p>Reinforcement learning techniques achieved human-level performance in several
tasks in the last decade. However, in recent years, the need for
interpretability emerged: we want to be able to understand how a system works
and the reasons behind its decisions. Not only we need interpretability to
assess the safety of the produced systems, we also need it to extract knowledge
about unknown problems. While some techniques that optimize decision trees for
reinforcement learning do exist, they usually employ greedy algorithms or they
do not exploit the rewards given by the environment. This means that these
techniques may easily get stuck in local optima. In this work, we propose a
novel approach to interpretable reinforcement learning that uses decision
trees. We present a two-level optimization scheme that combines the advantages
of evolutionary algorithms with the advantages of Q-learning. This way we
decompose the problem into two sub-problems: the problem of finding a
meaningful and useful decomposition of the state space, and the problem of
associating an action to each state. We test the proposed method on three
well-known reinforcement learning benchmarks, on which it results competitive
with respect to the state-of-the-art in both performance and interpretability.
Finally, we perform an ablation study that confirms that using the two-level
optimization scheme gives a boost in performance in non-trivial environments
with respect to a one-layer optimization technique.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhang2020survey/">A Survey on Neural Network Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey on Neural Network Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey on Neural Network Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yu Zhang, Peter Tiňo, Aleš Leonardis, Ke Tang</td>
	<td>IEEE Transactions on Emerging Topics in Computational Intelligence, vol. 5, no.5, pp. 726-742, Oct. 2021</td>
	<td><p>Along with the great success of deep neural networks, there is also growing
concern about their black-box nature. The interpretability issue affects
people’s trust on deep learning systems. It is also related to many ethical
problems, e.g., algorithmic discrimination. Moreover, interpretability is a
desired property for deep networks to become powerful tools in other research
fields, e.g., drug discovery and genomics. In this survey, we conduct a
comprehensive review of the neural network interpretability research. We first
clarify the definition of interpretability as it has been used in many
different contexts. Then we elaborate on the importance of interpretability and
propose a novel taxonomy organized along three dimensions: type of engagement
(passive vs. active interpretation approaches), the type of explanation, and
the focus (from local to global interpretability). This taxonomy provides a
meaningful 3D view of distribution of papers from the relevant literature as
two of the dimensions are not simply categorical but allow ordinal
subcategories. Finally, we summarize the existing interpretability evaluation
methods and suggest possible research directions inspired by our new taxonomy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/chen2020generating/">Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hanjie Chen, Guangtao Zheng, Yangfeng Ji</td>
	<td></td>
	<td><p>Generating explanations for neural networks has become crucial for their
applications in real-world with respect to reliability and trustworthiness. In
natural language processing, existing methods usually provide important
features which are words or phrases selected from an input text as an
explanation, but ignore the interactions between them. It poses challenges for
humans to interpret an explanation and connect it to model prediction. In this
work, we build hierarchical explanations by detecting feature interactions.
Such explanations visualize how words and phrases are combined at different
levels of the hierarchy, which can help users understand the decision-making of
black-box models. The proposed method is evaluated with three neural text
classifiers (LSTM, CNN, and BERT) on two benchmark datasets, via both automatic
and human evaluations. Experiments show the effectiveness of the proposed
method in providing explanations that are both faithful to models and
interpretable to humans.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/freiesleben2020intriguing/">The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Intriguing Relation Between Counterfactual Explanations and Adversarial Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Timo Freiesleben</td>
	<td></td>
	<td><p>The same method that creates adversarial examples (AEs) to fool
image-classifiers can be used to generate counterfactual explanations (CEs)
that explain algorithmic decisions. This observation has led researchers to
consider CEs as AEs by another name. We argue that the relationship to the true
label and the tolerance with respect to proximity are two properties that
formally distinguish CEs and AEs. Based on these arguments, we introduce CEs,
AEs, and related concepts mathematically in a common framework. Furthermore, we
show connections between current methods for generating CEs and AEs, and
estimate that the fields will merge more and more as the number of common
use-cases grows.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shvo2020interpretable/">Interpretable Sequence Classification via Discrete Optimization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Sequence Classification via Discrete Optimization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Sequence Classification via Discrete Optimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Maayan Shvo, Andrew C. Li, Rodrigo Toro Icarte, Sheila A. McIlraith</td>
	<td></td>
	<td><p>Sequence classification is the task of predicting a class label given a
sequence of observations. In many applications such as healthcare monitoring or
intrusion detection, early classification is crucial to prompt intervention. In
this work, we learn sequence classifiers that favour early classification from
an evolving observation trace. While many state-of-the-art sequence classifiers
are neural networks, and in particular LSTMs, our classifiers take the form of
finite state automata and are learned via discrete optimization. Our
automata-based classifiers are interpretable—supporting explanation,
counterfactual reasoning, and human-in-the-loop modification—and have strong
empirical performance. Experiments over a suite of goal recognition and
behaviour classification datasets show our learned automata-based classifiers
to have comparable test performance to LSTM-based classifiers, with the added
advantage of being interpretable.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/chefer2020transformer/">Transformer Interpretability Beyond Attention Visualization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transformer Interpretability Beyond Attention Visualization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transformer Interpretability Beyond Attention Visualization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hila Chefer, Shir Gur, Lior Wolf</td>
	<td></td>
	<td><p>Self-attention techniques, and specifically Transformers, are dominating the
field of text processing and are becoming increasingly popular in computer
vision classification tasks. In order to visualize the parts of the image that
led to a certain classification, existing methods either rely on the obtained
attention maps or employ heuristic propagation along the attention graph. In
this work, we propose a novel way to compute relevancy for Transformer
networks. The method assigns local relevance based on the Deep Taylor
Decomposition principle and then propagates these relevancy scores through the
layers. This propagation involves attention layers and skip connections, which
challenge existing methods. Our solution is based on a specific formulation
that is shown to maintain the total relevancy across layers. We benchmark our
method on very recent visual Transformer networks, as well as on a text
classification problem, and demonstrate a clear advantage over the existing
explainability methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/camburu2020explaining/">Explaining Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Oana-Maria Camburu</td>
	<td></td>
	<td><p>Deep neural networks are becoming more and more popular due to their
revolutionary success in diverse areas, such as computer vision, natural
language processing, and speech recognition. However, the decision-making
processes of these models are generally not interpretable to users. In various
domains, such as healthcare, finance, or law, it is critical to know the
reasons behind a decision made by an artificial intelligence system. Therefore,
several directions for explaining neural models have recently been explored. In
this thesis, I investigate two major directions for explaining deep neural
networks. The first direction consists of feature-based post-hoc explanatory
methods, that is, methods that aim to explain an already trained and fixed
model (post-hoc), and that provide explanations in terms of input features,
such as tokens for text and superpixels for images (feature-based). The second
direction consists of self-explanatory neural models that generate natural
language explanations, that is, models that have a built-in module that
generates explanations for the predictions of the model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/chang2020invariant/">Invariant Rationalization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Invariant Rationalization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Invariant Rationalization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shiyu Chang, Yang Zhang, Mo Yu, Tommi S. Jaakkola</td>
	<td></td>
	<td><p>Selective rationalization improves neural network interpretability by
identifying a small subset of input features – the rationale – that best
explains or supports the prediction. A typical rationalization criterion, i.e.
maximum mutual information (MMI), finds the rationale that maximizes the
prediction performance based only on the rationale. However, MMI can be
problematic because it picks up spurious correlations between the input
features and the output. Instead, we introduce a game-theoretic invariant
rationalization criterion where the rationales are constrained to enable the
same predictor to be optimal across different environments. We show both
theoretically and empirically that the proposed rationales can rule out
spurious correlations, generalize better to different test scenarios, and align
better with human judgments. Our data and code are available.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/b%C3%A4uerle2020explornn/">exploRNN: Understanding Recurrent Neural Networks through Visual Exploration</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=exploRNN: Understanding Recurrent Neural Networks through Visual Exploration' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=exploRNN: Understanding Recurrent Neural Networks through Visual Exploration' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alex Bäuerle, Patrick Albus, Raphael Störk, Tina Seufert, Timo Ropinski</td>
	<td></td>
	<td><p>Due to the success of deep learning (DL) and its growing job market, students
and researchers from many areas are interested in learning about DL
technologies. Visualization has proven to be of great help during this learning
process. While most current educational visualizations are targeted towards one
specific architecture or use case, recurrent neural networks (RNNs), which are
capable of processing sequential data, are not covered yet. This is despite the
fact that tasks on sequential data, such as text and function analysis, are at
the forefront of DL research. Therefore, we propose exploRNN, the first
interactively explorable educational visualization for RNNs. On the basis of
making learning easier and more fun, we define educational objectives targeted
towards understanding RNNs. We use these objectives to form guidelines for the
visual design process. By means of exploRNN, which is accessible online, we
provide an overview of the training process of RNNs at a coarse level, while
also allowing a detailed inspection of the data flow within LSTM cells. In an
empirical study, we assessed 37 subjects in a between-subjects design to
investigate the learning outcomes and cognitive load of exploRNN compared to a
classic text-based learning environment. While learners in the text group are
ahead in superficial knowledge acquisition, exploRNN is particularly helpful
for deeper understanding of the learning content. In addition, the complex
content in exploRNN is perceived as significantly easier and causes less
extraneous load than in the text group. The study shows that for difficult
learning material such as recurrent networks, where deep understanding is
important, interactive visualizations such as exploRNN can be helpful.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bykov2020much/">How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Much Can I Trust You? -- Quantifying Uncertainties in Explaining Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kirill Bykov, Marina M. -C. Höhne, Klaus-Robert Müller, Shinichi Nakajima, Marius Kloft</td>
	<td></td>
	<td><p>Explainable AI (XAI) aims to provide interpretations for predictions made by
learning machines, such as deep neural networks, in order to make the machines
more transparent for the user and furthermore trustworthy also for applications
in e.g. safety-critical areas. So far, however, no methods for quantifying
uncertainties of explanations have been conceived, which is problematic in
domains where a high confidence in explanations is a prerequisite. We therefore
contribute by proposing a new framework that allows to convert any arbitrary
explanation method for neural networks into an explanation method for Bayesian
neural networks, with an in-built modeling of uncertainties. Within the
Bayesian framework a network’s weights follow a distribution that extends
standard single explanation scores and heatmaps to distributions thereof, in
this manner translating the intrinsic network model uncertainties into a
quantification of explanation uncertainties. This allows us for the first time
to carve out uncertainties associated with a model explanation and subsequently
gauge the appropriate level of explanation confidence for a user (using
percentiles). We demonstrate the effectiveness and usefulness of our approach
extensively in various experiments, both qualitatively and quantitatively.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhao2020baylime/">BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=BayLIME: Bayesian Local Interpretable Model-Agnostic Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xingyu Zhao, Wei Huang, Xiaowei Huang, Valentin Robu, David Flynn</td>
	<td></td>
	<td><p>Given the pressing need for assuring algorithmic transparency, Explainable AI
(XAI) has emerged as one of the key areas of AI research. In this paper, we
develop a novel Bayesian extension to the LIME framework, one of the most
widely used approaches in XAI – which we call BayLIME. Compared to LIME,
BayLIME exploits prior knowledge and Bayesian reasoning to improve both the
consistency in repeated explanations of a single prediction and the robustness
to kernel settings. BayLIME also exhibits better explanation fidelity than the
state-of-the-art (LIME, SHAP and GradCAM) by its ability to integrate prior
knowledge from, e.g., a variety of other XAI techniques, as well as
verification and validation (V&amp;V) methods. We demonstrate the desirable
properties of BayLIME through both theoretical analysis and extensive
experiments.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zhang2020explainable/">Explainable Empirical Risk Minimization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Empirical Risk Minimization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Empirical Risk Minimization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>L. Zhang, G. Karakasidis, A. Odnoblyudova, L. Dogruel, A. Jung</td>
	<td></td>
	<td><p>The successful application of machine learning (ML) methods becomes
increasingly dependent on their interpretability or explainability. Designing
explainable ML systems is instrumental to ensuring transparency of automated
decision-making that targets humans. The explainability of ML methods is also
an essential ingredient for trustworthy artificial intelligence. A key
challenge in ensuring explainability is its dependence on the specific human
user (“explainee”). The users of machine learning methods might have vastly
different background knowledge about machine learning principles. One user
might have a university degree in machine learning or related fields, while
another user might have never received formal training in high-school
mathematics. This paper applies information-theoretic concepts to develop a
novel measure for the subjective explainability of the predictions delivered by
a ML method. We construct this measure via the conditional entropy of
predictions, given a user feedback. The user feedback might be obtained from
user surveys or biophysical measurements. Our main contribution is the
explainable empirical risk minimization (EERM) principle of learning a
hypothesis that optimally balances between the subjective explainability and
risk. The EERM principle is flexible and can be combined with arbitrary machine
learning models. We present several practical implementations of EERM for
linear models and decision trees. Numerical experiments demonstrate the
application of EERM to detecting the use of inappropriate language on social
media.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bhatt2020evaluating/">Evaluating and Aggregating Feature-based Model Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating and Aggregating Feature-based Model Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating and Aggregating Feature-based Model Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Umang Bhatt, Adrian Weller, José M. F. Moura</td>
	<td></td>
	<td><p>A feature-based model explanation denotes how much each input feature
contributes to a model’s output for a given data point. As the number of
proposed explanation functions grows, we lack quantitative evaluation criteria
to help practitioners know when to use which explanation function. This paper
proposes quantitative evaluation criteria for feature-based explanations: low
sensitivity, high faithfulness, and low complexity. We devise a framework for
aggregating explanation functions. We develop a procedure for learning an
aggregate explanation function with lower complexity and then derive a new
aggregate Shapley value explanation function that minimizes sensitivity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/chen2020adaptive/">Adaptive Explainable Neural Networks (AxNNs)</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adaptive Explainable Neural Networks (AxNNs)' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adaptive Explainable Neural Networks (AxNNs)' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jie Chen, Joel Vaughan, Vijayan N. Nair, Agus Sudjianto</td>
	<td></td>
	<td><p>While machine learning techniques have been successfully applied in several
fields, the black-box nature of the models presents challenges for interpreting
and explaining the results. We develop a new framework called Adaptive
Explainable Neural Networks (AxNN) for achieving the dual goals of good
predictive performance and model interpretability. For predictive performance,
we build a structured neural network made up of ensembles of generalized
additive model networks and additive index models (through explainable neural
networks) using a two-stage process. This can be done using either a boosting
or a stacking ensemble. For interpretability, we show how to decompose the
results of AxNN into main effects and higher-order interaction effects. The
computations are inherited from Google’s open source tool AdaNet and can be
efficiently accelerated by training with distributed computing. The results are
illustrated on simulated and real datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/begley2020explainability/">Explainability for fair machine learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainability for fair machine learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainability for fair machine learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tom Begley, Tobias Schwedes, Christopher Frye, Ilya Feige</td>
	<td></td>
	<td><p>As the decisions made or influenced by machine learning models increasingly
impact our lives, it is crucial to detect, understand, and mitigate unfairness.
But even simply determining what “unfairness” should mean in a given context is
non-trivial: there are many competing definitions, and choosing between them
often requires a deep understanding of the underlying task. It is thus tempting
to use model explainability to gain insights into model fairness, however
existing explainability tools do not reliably indicate whether a model is
indeed fair. In this work we present a new approach to explaining fairness in
machine learning, based on the Shapley value paradigm. Our fairness
explanations attribute a model’s overall unfairness to individual input
features, even in cases where the model does not operate on sensitive
attributes directly. Moreover, motivated by the linearity of Shapley
explainability, we propose a meta algorithm for applying existing training-time
fairness interventions, wherein one trains a perturbation to the original
model, rather than a new model entirely. By explaining the original model, the
perturbation, and the fair-corrected model, we gain insight into the
accuracy-fairness trade-off that is being made by the intervention. We further
show that this meta algorithm enjoys both flexibility and stability benefits
with no loss in performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bastings2020elephant/">The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jasmijn Bastings, Katja Filippova</td>
	<td>Proceedings of the 2020 EMNLP Workshop BlackboxNLP</td>
	<td><p>There is a recent surge of interest in using attention as explanation of
model predictions, with mixed evidence on whether attention can be used as
such. While attention conveniently gives us one weight per input token and is
easily extracted, it is often unclear toward what goal it is used as
explanation. We find that often that goal, whether explicitly stated or not, is
to find out what input tokens are the most relevant to a prediction, and that
the implied user for the explanation is a model developer. For this goal and
user, we argue that input saliency methods are better suited, and that there
are no compelling reasons to use attention, despite the coincidence that it
provides a weight for each input. With this position paper, we hope to shift
some of the recent focus on attention to saliency methods, and for authors to
clearly state the goal and user for their explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/benchekroun2020need/">The Need for Standardized Explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Need for Standardized Explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Need for Standardized Explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Othman Benchekroun, Adel Rahimi, Qini Zhang, Tetiana Kodliuk</td>
	<td></td>
	<td><p>Explainable AI (XAI) is paramount in industry-grade AI; however existing
methods fail to address this necessity, in part due to a lack of
standardisation of explainability methods. The purpose of this paper is to
offer a perspective on the current state of the area of explainability, and to
provide novel definitions for Explainability and Interpretability to begin
standardising this area of research. To do so, we provide an overview of the
literature on explainability, and of the existing methods that are already
implemented. Finally, we offer a tentative taxonomy of the different
explainability methods, opening the door to future research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/zunino2020explainable/">Explainable Deep Classification Models for Domain Generalization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Deep Classification Models for Domain Generalization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Deep Classification Models for Domain Generalization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrea Zunino, Sarah Adel Bargal, Riccardo Volpi, Mehrnoosh Sameki, Jianming Zhang, Stan Sclaroff, Vittorio Murino, Kate Saenko</td>
	<td></td>
	<td><p>Conventionally, AI models are thought to trade off explainability for lower
accuracy. We develop a training strategy that not only leads to a more
explainable AI system for object classification, but as a consequence, suffers
no perceptible accuracy degradation. Explanations are defined as regions of
visual evidence upon which a deep classification network makes a decision. This
is represented in the form of a saliency map conveying how much each pixel
contributed to the network’s decision. Our training strategy enforces a
periodic saliency-based feedback to encourage the model to focus on the image
regions that directly correspond to the ground-truth object. We quantify
explainability using an automated metric, and using human judgement. We propose
explainability as a means for bridging the visual-semantic gap between
different domains where model explanations are used as a means of disentagling
domain specific information from otherwise relevant features. We demonstrate
that this leads to improved generalization to new domains without hindering
performance on the original domain.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/barr2020towards/">Towards Ground Truth Explainability on Tabular Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Ground Truth Explainability on Tabular Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Ground Truth Explainability on Tabular Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Brian Barr, Ke Xu, Claudio Silva, Enrico Bertini, Robert Reilly, C. Bayan Bruss, Jason D. Wittenbach</td>
	<td></td>
	<td><p>In data science, there is a long history of using synthetic data for method
development, feature selection and feature engineering. Our current interest in
synthetic data comes from recent work in explainability. Today’s datasets are
typically larger and more complex - requiring less interpretable models. In the
setting of \textit{post hoc} explainability, there is no ground truth for
explanations. Inspired by recent work in explaining image classifiers that does
provide ground truth, we propose a similar solution for tabular data. Using
copulas, a concise specification of the desired statistical properties of a
dataset, users can build intuition around explainability using controlled data
sets and experimentation. The current capabilities are demonstrated on three
use cases: one dimensional logistic regression, impact of correlation from
informative features, impact of correlation from redundant variables.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/markus2020role/">The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aniek F. Markus, Jan A. Kors, Peter R. Rijnbeek</td>
	<td>Journal of Biomedical Informatics, 113 (2021), 103655</td>
	<td><p>Artificial intelligence (AI) has huge potential to improve the health and
well-being of people, but adoption in clinical practice is still limited. Lack
of transparency is identified as one of the main barriers to implementation, as
clinicians should be confident the AI system can be trusted. Explainable AI has
the potential to overcome this issue and can be a step towards trustworthy AI.
In this paper we review the recent literature to provide guidance to
researchers and practitioners on the design of explainable AI systems for the
health-care domain and contribute to formalization of the field of explainable
AI. We argue the reason to demand explainability determines what should be
explained as this determines the relative importance of the properties of
explainability (i.e. interpretability and fidelity). Based on this, we propose
a framework to guide the choice between classes of explainable AI methods
(explainable modelling versus post-hoc explanation; model-based,
attribution-based, or example-based explanations; global and local
explanations). Furthermore, we find that quantitative evaluation metrics, which
are important for objective standardized evaluation, are still lacking for some
properties (e.g. clarity) and types of explanations (e.g. example-based
methods). We conclude that explainable modelling can contribute to trustworthy
AI, but the benefits of explainability still need to be proven in practice and
complementary measures might be needed to create trustworthy AI in health care
(e.g. reporting data quality, performing extensive (external) validation, and
regulation).</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/marcos2020contextual/">Contextual Semantic Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contextual Semantic Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contextual Semantic Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Diego Marcos, Ruth Fong, Sylvain Lobry, Remi Flamary, Nicolas Courty, Devis Tuia</td>
	<td>ACCV 2020</td>
	<td><p>Convolutional neural networks (CNN) are known to learn an image
representation that captures concepts relevant to the task, but do so in an
implicit way that hampers model interpretability. However, one could argue that
such a representation is hidden in the neurons and can be made explicit by
teaching the model to recognize semantically interpretable attributes that are
present in the scene. We call such an intermediate layer a \emph{semantic
bottleneck}. Once the attributes are learned, they can be re-combined to reach
the final decision and provide both an accurate prediction and an explicit
reasoning behind the CNN decision. In this paper, we look into semantic
bottlenecks that capture context: we want attributes to be in groups of a few
meaningful elements and participate jointly to the final decision. We use a
two-layer semantic bottleneck that gathers attributes into interpretable,
sparse groups, allowing them contribute differently to the final output
depending on the context. We test our contextual semantic interpretable
bottleneck (CSIB) on the task of landscape scenicness estimation and train the
semantic interpretable bottleneck using an auxiliary database (SUN Attributes).
Our model yields in predictions as accurate as a non-interpretable baseline
when applied to a real-world test set of Flickr images, all while providing
clear and interpretable explanations for each prediction.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mangla2020saliency/">On Saliency Maps and Adversarial Robustness</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Saliency Maps and Adversarial Robustness' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Saliency Maps and Adversarial Robustness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Puneet Mangla, Vedant Singh, Vineeth N Balasubramanian</td>
	<td></td>
	<td><p>A Very recent trend has emerged to couple the notion of interpretability and
adversarial robustness, unlike earlier efforts which solely focused on good
interpretations or robustness against adversaries. Works have shown that
adversarially trained models exhibit more interpretable saliency maps than
their non-robust counterparts, and that this behavior can be quantified by
considering the alignment between input image and saliency map. In this work,
we provide a different perspective to this coupling, and provide a method,
Saliency based Adversarial training (SAT), to use saliency maps to improve
adversarial robustness of a model. In particular, we show that using
annotations such as bounding boxes and segmentation masks, already provided
with a dataset, as weak saliency maps, suffices to improve adversarial
robustness with no additional effort to generate the perturbations themselves.
Our empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17
datasets consistently corroborate our claim, by showing improved adversarial
robustness using our method. saliency maps. We also show how using finer and
stronger saliency maps leads to more robust models, and how integrating SAT
with existing adversarial training methods, further boosts performance of these
existing methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mardaoui2020analysis/">An Analysis of LIME for Text Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Analysis of LIME for Text Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Analysis of LIME for Text Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dina Mardaoui, Damien Garreau</td>
	<td></td>
	<td><p>Text data are increasingly handled in an automated fashion by machine
learning algorithms. But the models handling these data are not always
well-understood due to their complexity and are more and more often referred to
as “black-boxes.” Interpretability methods aim to explain how these models
operate. Among them, LIME has become one of the most popular in recent years.
However, it comes without theoretical guarantees: even for simple models, we
are not sure that LIME behaves accurately. In this paper, we provide a first
theoretical analysis of LIME for text data. As a consequence of our theoretical
findings, we show that LIME indeed provides meaningful explanations for simple
models, namely decision trees and linear models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/schramowski2020making/">Making deep neural networks right for the right scientific reasons by interacting with their explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Making deep neural networks right for the right scientific reasons by interacting with their explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Making deep neural networks right for the right scientific reasons by interacting with their explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Patrick Schramowski, Wolfgang Stammer, Stefano Teso, Anna Brugger, Xiaoting Shao, Hans-Georg Luigs, Anne-Katrin Mahlein, Kristian Kersting</td>
	<td></td>
	<td><p>Deep neural networks have shown excellent performances in many real-world
applications. Unfortunately, they may show “Clever Hans”-like behavior—making
use of confounding factors within datasets—to achieve high performance. In
this work, we introduce the novel learning setting of “explanatory interactive
learning” (XIL) and illustrate its benefits on a plant phenotyping research
task. XIL adds the scientist into the training loop such that she interactively
revises the original model via providing feedback on its explanations. Our
experimental results demonstrate that XIL can help avoiding Clever Hans moments
in machine learning and encourages (or discourages, if appropriate) trust into
the underlying model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/schneider2020deceptive/">Deceptive AI Explanations: Creation and Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deceptive AI Explanations: Creation and Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deceptive AI Explanations: Creation and Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Johannes Schneider, Christian Meske, Michalis Vlachos</td>
	<td>International Conference on Agents and Artificial Intelligence (2022)</td>
	<td><p>Artificial intelligence (AI) comes with great opportunities but can also pose
significant risks. Automatically generated explanations for decisions can
increase transparency and foster trust, especially for systems based on
automated predictions by AI models. However, given, e.g., economic incentives
to create dishonest AI, to what extent can we trust explanations? To address
this issue, our work investigates how AI models (i.e., deep learning, and
existing instruments to increase transparency regarding AI decisions) can be
used to create and detect deceptive explanations. As an empirical evaluation,
we focus on text classification and alter the explanations generated by
GradCAM, a well-established explanation technique in neural networks. Then, we
evaluate the effect of deceptive explanations on users in an experiment with
200 participants. Our findings confirm that deceptive explanations can indeed
fool humans. However, one can deploy machine learning (ML) methods to detect
seemingly minor deception attempts with accuracy exceeding 80% given sufficient
domain knowledge. Without domain knowledge, one can still infer inconsistencies
in the explanations in an unsupervised manner, given basic knowledge of the
predictive model under scrutiny.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shen2020explain/">To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=To Explain or Not to Explain: A Study on the Necessity of Explanations for Autonomous Vehicles' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yuan Shen, Shanduojiao Jiang, Yanlin Chen, Eileen Yang, Xilun Jin, Yuliang Fan, Katie Driggs Campbell</td>
	<td></td>
	<td><p>Explainable AI, in the context of autonomous systems, like self driving cars,
has drawn broad interests from researchers. Recent studies have found that
providing explanations for an autonomous vehicle actions has many benefits,
e.g., increase trust and acceptance, but put little emphasis on when an
explanation is needed and how the content of explanation changes with context.
In this work, we investigate which scenarios people need explanations and how
the critical degree of explanation shifts with situations and driver types.
Through a user experiment, we ask participants to evaluate how necessary an
explanation is and measure the impact on their trust in the self driving cars
in different contexts. We also present a self driving explanation dataset with
first person explanations and associated measure of the necessity for 1103
video clips, augmenting the Berkeley Deep Drive Attention dataset.
Additionally, we propose a learning based model that predicts how necessary an
explanation for a given situation in real time, using camera data inputs. Our
research reveals that driver types and context dictates whether or not an
explanation is necessary and what is helpful for improved interaction and
understanding.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/lin2020see/">What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik</td>
	<td></td>
	<td><p>EXplainable AI (XAI) methods have been proposed to interpret how a deep
neural network predicts inputs through model saliency explanations that
highlight the parts of the inputs deemed important to arrive a decision at a
specific target. However, it remains challenging to quantify correctness of
their interpretability as current evaluation approaches either require
subjective input from humans or incur high computation cost with automated
evaluation. In this paper, we propose backdoor trigger patterns–hidden
malicious functionalities that cause misclassification–to automate the
evaluation of saliency explanations. Our key observation is that triggers
provide ground truth for inputs to evaluate whether the regions identified by
an XAI method are truly relevant to its output. Since backdoor triggers are the
most important features that cause deliberate misclassification, a robust XAI
method should reveal their presence at inference time. We introduce three
complementary metrics for systematic evaluation of explanations that an XAI
method generates and evaluate seven state-of-the-art model-free and
model-specific posthoc methods through 36 models trojaned with specifically
crafted triggers using color, shape, texture, location, and size. We discovered
six methods that use local explanation and feature relevance fail to completely
highlight trigger regions, and only a model-free approach can uncover the
entire trigger region.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/lim2020time/">Time Series Forecasting With Deep Learning: A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Time Series Forecasting With Deep Learning: A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Time Series Forecasting With Deep Learning: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bryan Lim, Stefan Zohren</td>
	<td>Philosophical Transactions of the Royal Society A 2020</td>
	<td><p>Numerous deep learning architectures have been developed to accommodate the
diversity of time series datasets across different domains. In this article, we
survey common encoder and decoder designs used in both one-step-ahead and
multi-horizon time series forecasting – describing how temporal information is
incorporated into predictions by each model. Next, we highlight recent
developments in hybrid deep learning models, which combine well-studied
statistical models with neural network components to improve pure methods in
either category. Lastly, we outline some ways in which deep learning can also
facilitate decision support with time series data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/leavitt2020towards/">Towards falsifiable interpretability research</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards falsifiable interpretability research' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards falsifiable interpretability research' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matthew L. Leavitt, Ari Morcos</td>
	<td></td>
	<td><p>Methods for understanding the decisions of and mechanisms underlying deep
neural networks (DNNs) typically rely on building intuition by emphasizing
sensory or semantic features of individual examples. For instance, methods aim
to visualize the components of an input which are “important” to a network’s
decision, or to measure the semantic properties of single neurons. Here, we
argue that interpretability research suffers from an over-reliance on
intuition-based approaches that risk-and in some cases have caused-illusory
progress and misleading conclusions. We identify a set of limitations that we
argue impede meaningful progress in interpretability research, and examine two
popular classes of interpretability methods-saliency and single-neuron-based
approaches-that serve as case studies for how overreliance on intuition and
lack of falsifiability can undermine interpretability research. To address
these concerns, we propose a strategy to address these impediments in the form
of a framework for strongly falsifiable interpretability research. We encourage
researchers to use their intuitions as a starting point to develop and test
clear, falsifiable hypotheses, and hope that our framework yields robust,
evidence-based interpretability methods that generate meaningful advances in
our understanding of DNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shen2020useful/">How Useful Are the Machine-Generated Interpretations to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Useful Are the Machine-Generated Interpretations to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Useful Are the Machine-Generated Interpretations to General Users? A Human Evaluation on Guessing the Incorrectly Predicted Labels' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hua Shen, Ting-Hao Kenneth Huang</td>
	<td></td>
	<td><p>Explaining to users why automated systems make certain mistakes is important
and challenging. Researchers have proposed ways to automatically produce
interpretations for deep neural network models. However, it is unclear how
useful these interpretations are in helping users figure out why they are
getting an error. If an interpretation effectively explains to users how the
underlying deep neural network model works, people who were presented with the
interpretation should be better at predicting the model’s outputs than those
who were not. This paper presents an investigation on whether or not showing
machine-generated visual interpretations helps users understand the incorrectly
predicted labels produced by image classifiers. We showed the images and the
correct labels to 150 online crowd workers and asked them to select the
incorrectly predicted labels with or without showing them the machine-generated
visual interpretations. The results demonstrated that displaying the visual
interpretations did not increase, but rather decreased, the average guessing
accuracy by roughly 10%.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/shen2020interfacegan/">InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=InterFaceGAN: Interpreting the Disentangled Face Representation Learned by GANs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yujun Shen, Ceyuan Yang, Xiaoou Tang, Bolei Zhou</td>
	<td></td>
	<td><p>Although Generative Adversarial Networks (GANs) have made significant
progress in face synthesis, there lacks enough understanding of what GANs have
learned in the latent representation to map a random code to a photo-realistic
image. In this work, we propose a framework called InterFaceGAN to interpret
the disentangled face representation learned by the state-of-the-art GAN models
and study the properties of the facial semantics encoded in the latent space.
We first find that GANs learn various semantics in some linear subspaces of the
latent space. After identifying these subspaces, we can realistically
manipulate the corresponding facial attributes without retraining the model. We
then conduct a detailed study on the correlation between different semantics
and manage to better disentangle them via subspace projection, resulting in
more precise control of the attribute manipulation. Besides manipulating the
gender, age, expression, and presence of eyeglasses, we can even alter the face
pose and fix the artifacts accidentally made by GANs. Furthermore, we perform
an in-depth face identity analysis and a layer-wise analysis to evaluate the
editing results quantitatively. Finally, we apply our approach to real face
editing by employing GAN inversion approaches and explicitly training
feed-forward models based on the synthetic data established by InterFaceGAN.
Extensive experimental results suggest that learning to synthesize faces
spontaneously brings a disentangled and controllable face representation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/rajani2020explaining/">Explaining and Improving Model Behavior with k Nearest Neighbor Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining and Improving Model Behavior with k Nearest Neighbor Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining and Improving Model Behavior with k Nearest Neighbor Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nazneen Fatema Rajani, Ben Krause, Wengpeng Yin, Tong Niu, Richard Socher, Caiming Xiong</td>
	<td></td>
	<td><p>Interpretability techniques in NLP have mainly focused on understanding
individual predictions using attention visualization or gradient-based saliency
maps over tokens. We propose using k nearest neighbor (kNN) representations to
identify training examples responsible for a model’s predictions and obtain a
corpus-level understanding of the model’s behavior. Apart from
interpretability, we show that kNN representations are effective at uncovering
learned spurious associations, identifying mislabeled examples, and improving
the fine-tuned model’s performance. We focus on Natural Language Inference
(NLI) as a case study and experiment with multiple datasets. Our method deploys
backoff to kNN for BERT and RoBERTa on examples with low model confidence
without any update to the model parameters. Our results indicate that the kNN
approach makes the finetuned model more robust to adversarial inputs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/kim2020interpretation/">Interpretation of NLP models through input marginalization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretation of NLP models through input marginalization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretation of NLP models through input marginalization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Siwon Kim, Jihun Yi, Eunji Kim, Sungroh Yoon</td>
	<td></td>
	<td><p>To demystify the “black box” property of deep neural networks for natural
language processing (NLP), several methods have been proposed to interpret
their predictions by measuring the change in prediction probability after
erasing each token of an input. Since existing methods replace each token with
a predefined value (i.e., zero), the resulting sentence lies out of the
training data distribution, yielding misleading interpretations. In this study,
we raise the out-of-distribution problem induced by the existing interpretation
methods and present a remedy; we propose to marginalize each token out. We
interpret various NLP models trained for sentiment analysis and natural
language inference using the proposed method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/nguyen2020interpretable/">Interpretable Time Series Classification using Linear Models and Multi-resolution Multi-domain Symbolic Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Time Series Classification using Linear Models and Multi-resolution Multi-domain Symbolic Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Time Series Classification using Linear Models and Multi-resolution Multi-domain Symbolic Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Thach Le Nguyen, Severin Gsponer, Iulia Ilie, Martin O'Reilly, Georgiana Ifrim</td>
	<td>Data Mining and Knowledge Discovery 33 (2019) 1183-1222</td>
	<td><p>The time series classification literature has expanded rapidly over the last
decade, with many new classification approaches published each year. Prior
research has mostly focused on improving the accuracy and efficiency of
classifiers, with interpretability being somewhat neglected. This aspect of
classifiers has become critical for many application domains and the
introduction of the EU GDPR legislation in 2018 is likely to further emphasize
the importance of interpretable learning algorithms. Currently,
state-of-the-art classification accuracy is achieved with very complex models
based on large ensembles (COTE) or deep neural networks (FCN). These approaches
are not efficient with regard to either time or space, are difficult to
interpret and cannot be applied to variable-length time series, requiring
pre-processing of the original series to a set fixed-length. In this paper we
propose new time series classification algorithms to address these gaps. Our
approach is based on symbolic representations of time series, efficient
sequence mining algorithms and linear classification models. Our linear models
are as accurate as deep learning models but are more efficient regarding
running time and memory, can work with variable-length time series and can be
interpreted by highlighting the discriminative symbolic features on the
original time series. We show that our multi-resolution multi-domain linear
classifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art
COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a
fraction of the time and memory required by either COTE or deep models. To
further analyse the interpretability of our classifier, we present a case study
on a human motion dataset collected by the authors. We release all the results,
source code and data to encourage reproducibility.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/naik2020explanation/">Explanation from Specification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation from Specification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation from Specification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Harish Naik, György Turán</td>
	<td></td>
	<td><p>Explainable components in XAI algorithms often come from a familiar set of
models, such as linear models or decision trees. We formulate an approach where
the type of explanation produced is guided by a specification. Specifications
are elicited from the user, possibly using interaction with the user and
contributions from other areas. Areas where a specification could be obtained
include forensic, medical, and scientific applications. Providing a menu of
possible types of specifications in an area is an exploratory knowledge
representation and reasoning task for the algorithm designer, aiming at
understanding the possibilities and limitations of efficiently computable modes
of explanations. Two examples are discussed: explanations for Bayesian networks
using the theory of argumentation, and explanations for graph neural networks.
The latter case illustrates the possibility of having a representation
formalism available to the user for specifying the type of explanation
requested, for example, a chemical query language for classifying molecules.
The approach is motivated by a theory of explanation in the philosophy of
science, and it is related to current questions in the philosophy of science on
the role of machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/nauta2020looks/">This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=This Looks Like That, Because ... Explaining Prototypes for Interpretable Image Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Meike Nauta, Annemarie Jutte, Jesper Provoost, Christin Seifert</td>
	<td></td>
	<td><p>Image recognition with prototypes is considered an interpretable alternative
for black box deep learning models. Classification depends on the extent to
which a test image “looks like” a prototype. However, perceptual similarity for
humans can be different from the similarity learned by the classification
model. Hence, only visualising prototypes can be insufficient for a user to
understand what a prototype exactly represents, and why the model considers a
prototype and an image to be similar. We address this ambiguity and argue that
prototypes should be explained. We improve interpretability by automatically
enhancing visual prototypes with textual quantitative information about visual
characteristics deemed important by the classification model. Specifically, our
method clarifies the meaning of a prototype by quantifying the influence of
colour hue, shape, texture, contrast and saturation and can generate both
global and local explanations. Because of the generality of our approach, it
can improve the interpretability of any similarity-based method for
prototypical image recognition. In our experiments, we apply our method to the
existing Prototypical Part Network (ProtoPNet). Our analysis confirms that the
global explanations are generalisable, and often correspond to the visually
perceptible properties of a prototype. Our explanations are especially relevant
for prototypes which might have been interpreted incorrectly otherwise. By
explaining such ‘misleading’ prototypes, we improve the interpretability and
simulatability of a prototype-based classification model. We also use our
method to check whether visually similar prototypes have similar explanations,
and are able to discover redundancy. Code is available at
https://github.com/M-Nauta/Explaining_Prototypes .</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/nguyen2020towards/">Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable ANNs: An Exact Transformation to Multi-Class Multivariate Decision Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Duy T. Nguyen, Kathryn E. Kasmarik, Hussein A. Abbass</td>
	<td></td>
	<td><p>On the one hand, artificial neural networks (ANNs) are commonly labelled as
black-boxes, lacking interpretability; an issue that hinders human
understanding of ANNs’ behaviors. A need exists to generate a meaningful
sequential logic of the ANN for interpreting a production process of a specific
output. On the other hand, decision trees exhibit better interpretability and
expressive power due to their representation language and the existence of
efficient algorithms to transform the trees into rules. However, growing a
decision tree based on the available data could produce larger than necessary
trees or trees that do not generalise well. In this paper, we introduce two
novel multivariate decision tree (MDT) algorithms for rule extraction from
ANNs: an Exact-Convertible Decision Tree (EC-DT) and an Extended C-Net
algorithm. They both transform a neural network with Rectified Linear Unit
activation functions into a representative tree, which can further be used to
extract multivariate rules for reasoning. While the EC-DT translates an ANN in
a layer-wise manner to represent exactly the decision boundaries implicitly
learned by the hidden layers of the network, the Extended C-Net combines the
decompositional approach from EC-DT with a C5 tree learning algorithm to form
decision rules. The results suggest that while EC-DT is superior in preserving
the structure and the fidelity of ANN, Extended C-Net generates the most
compact and highly effective trees from ANN. Both proposed MDT algorithms
generate rules including combinations of multiple attributes for precise
interpretations for decision-making.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mukhopadhyay2020decoding/">Decoding CNN based Object Classifier Using Visualization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Decoding CNN based Object Classifier Using Visualization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Decoding CNN based Object Classifier Using Visualization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abhishek Mukhopadhyay, Imon Mukherjee, Pradipta Biswas</td>
	<td></td>
	<td><p>This paper investigates how working of Convolutional Neural Network (CNN) can
be explained through visualization in the context of machine perception of
autonomous vehicles. We visualize what type of features are extracted in
different convolution layers of CNN that helps to understand how CNN gradually
increases spatial information in every layer. Thus, it concentrates on region
of interests in every transformation. Visualizing heat map of activation helps
us to understand how CNN classifies and localizes different objects in image.
This study also helps us to reason behind low accuracy of a model helps to
increase trust on object detection module.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/moradi2020explaining/">Explaining Black-box Models for Biomedical Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Black-box Models for Biomedical Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Black-box Models for Biomedical Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Milad Moradi, Matthias Samwald</td>
	<td></td>
	<td><p>In this paper, we propose a novel method named Biomedical Confident Itemsets
Explanation (BioCIE), aiming at post-hoc explanation of black-box machine
learning models for biomedical text classification. Using sources of domain
knowledge and a confident itemset mining method, BioCIE discretizes the
decision space of a black-box into smaller subspaces and extracts semantic
relationships between the input text and class labels in different subspaces.
Confident itemsets discover how biomedical concepts are related to class labels
in the black-box’s decision space. BioCIE uses the itemsets to approximate the
black-box’s behavior for individual predictions. Optimizing fidelity,
interpretability, and coverage measures, BioCIE produces class-wise
explanations that represent decision boundaries of the black-box. Results of
evaluations on various biomedical text classification tasks and black-box
models demonstrated that BioCIE can outperform perturbation-based and decision
set methods in terms of producing concise, accurate, and interpretable
explanations. BioCIE improved the fidelity of instance-wise and class-wise
explanations by 11.6% and 7.5%, respectively. It also improved the
interpretability of explanations by 8%. BioCIE can be effectively used to
explain how a black-box biomedical text classification model semantically
relates input texts to class labels. The source code and supplementary material
are available at https://github.com/mmoradi-iut/BioCIE.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/mohankumar2020towards/">Towards Transparent and Explainable Attention Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Transparent and Explainable Attention Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Transparent and Explainable Attention Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M. Khapra, Balaji Vasan Srinivasan, Balaraman Ravindran</td>
	<td></td>
	<td><p>Recent studies on interpretability of attention distributions have led to
notions of faithful and plausible explanations for a model’s predictions.
Attention distributions can be considered a faithful explanation if a higher
attention weight implies a greater impact on the model’s prediction. They can
be considered a plausible explanation if they provide a human-understandable
justification for the model’s predictions. In this work, we first explain why
current attention mechanisms in LSTM based encoders can neither provide a
faithful nor a plausible explanation of the model’s predictions. We observe
that in LSTM based encoders the hidden representations at different time-steps
are very similar to each other (high conicity) and attention weights in these
situations do not carry much meaning because even a random permutation of the
attention weights does not affect the model’s predictions. Based on experiments
on a wide variety of tasks and datasets, we observe attention distributions
often attribute the model’s predictions to unimportant words such as
punctuation and fail to offer a plausible explanation for the predictions. To
make attention mechanisms more faithful and plausible, we propose a modified
LSTM cell with a diversity-driven training objective that ensures that the
hidden representations learned at different time steps are diverse. We show
that the resulting attention distributions offer more transparency as they (i)
provide a more precise importance ranking of the hidden states (ii) are better
indicative of words important for the model’s predictions (iii) correlate
better with gradient-based attribution methods. Human evaluations indicate that
the attention distributions learned by our model offer a plausible explanation
of the model’s predictions. Our code has been made publicly available at
https://github.com/akashkm99/Interpretable-Attention</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/you2020large-scale/">Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Large-scale Interconnection Power System Model Sanity Check, Tuning, and Validation for Frequency Response Study' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shutang You, Yilu Liu</td>
	<td></td>
	<td><p>The quality and accuracy of power system models is critical for
simulation-based studies, especially for studying actual stability issues in
large-scale systems. With the deployment of wide-area monitoring systems
(WAMSs), the high-reporting-rate frequency measurement provides a trustworthy
ground truth for validating models in frequency response studies. This paper
documented an effort to check, tune, and validate the U.S. power system model
based on a WAMS called FNET/GridEye. Four metrics are used to quantitatively
compare the simulation results and the actual measurement, including frequency
nadir, RoCoF, settling frequency and settling time. After tuning governor
deadband and the governor ratio, the model frequency response shows significant
improvement and matches well with the event measurement data. This work serves
as an example for tuning and validating large-scale power system models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/pruthi2020evaluating/">Evaluating Explanations: How much do explanations from the teacher aid students?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Explanations: How much do explanations from the teacher aid students?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Explanations: How much do explanations from the teacher aid students?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Danish Pruthi, Rachit Bansal, Bhuwan Dhingra, Livio Baldini Soares, Michael Collins, Zachary C. Lipton, Graham Neubig, William W. Cohen</td>
	<td></td>
	<td><p>While many methods purport to explain predictions by highlighting salient
features, what aims these explanations serve and how they ought to be evaluated
often go unstated. In this work, we introduce a framework to quantify the value
of explanations via the accuracy gains that they confer on a student model
trained to simulate a teacher model. Crucially, the explanations are available
to the student during training, but are not available at test time. Compared to
prior proposals, our approach is less easily gamed, enabling principled,
automatic, model-agnostic evaluation of attributions. Using our framework, we
compare numerous attribution methods for text classification and question
answering, and observe quantitative differences that are consistent (to a
moderate to high degree) across different student model architectures and
learning strategies.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/molnar2020interpretable/">Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Machine Learning -- A Brief History, State-of-the-Art and Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Christoph Molnar, Giuseppe Casalicchio, Bernd Bischl</td>
	<td>Koprinska I. et al. (eds) ECML PKDD 2020 Workshops. ECML PKDD 2020. Communications in Computer and Information Science, vol 1323. Springer, Cham</td>
	<td><p>We present a brief history of the field of interpretable machine learning
(IML), give an overview of state-of-the-art interpretation methods, and discuss
challenges. Research in IML has boomed in recent years. As young as the field
is, it has over 200 years old roots in regression modeling and rule-based
machine learning, starting in the 1960s. Recently, many new IML methods have
been proposed, many of them model-agnostic, but also interpretation techniques
specific to deep learning and tree-based ensembles. IML methods either directly
analyze model components, study sensitivity to input perturbations, or analyze
local or global surrogate approximations of the ML model. The field approaches
a state of readiness and stability, with many methods not only proposed in
research, but also implemented in open-source software. But many important
challenges remain for IML, such as dealing with dependent features, causal
interpretation, and uncertainty estimation, which need to be resolved for its
successful application to scientific problems. A further challenge is a missing
rigorous definition of interpretability, which is accepted by the community. To
address the challenges and advance the field, we urge to recall our roots of
interpretable, data-driven modeling in statistics and (rule-based) ML, but also
to consider other areas such as sensitivity analysis, causal inference, and the
social sciences.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/tjoa2020quantifying/">Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Erico Tjoa, Cuntai Guan</td>
	<td></td>
	<td><p>Post-hoc analysis is a popular category in eXplainable artificial
intelligence (XAI) study. In particular, methods that generate heatmaps have
been used to explain the deep neural network (DNN), a black-box model. Heatmaps
can be appealing due to the intuitive and visual ways to understand them but
assessing their qualities might not be straightforward. Different ways to
assess heatmaps’ quality have their own merits and shortcomings. This paper
introduces a synthetic dataset that can be generated adhoc along with the
ground-truth heatmaps for more objective quantitative assessment. Each sample
data is an image of a cell with easily recognized features that are
distinguished from localization ground-truth mask, hence facilitating a more
transparent assessment of different XAI methods. Comparison and recommendations
are made, shortcomings are clarified along with suggestions for future research
directions to handle the finer details of select post-hoc analysis methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/bai2020attentions/">Why Attentions May Not Be Interpretable?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Why Attentions May Not Be Interpretable?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Why Attentions May Not Be Interpretable?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bing Bai, Jian Liang, Guanhua Zhang, Hao Li, Kun Bai, Fei Wang</td>
	<td></td>
	<td><p>Attention-based methods have played important roles in model interpretations,
where the calculated attention weights are expected to highlight the critical
parts of inputs~(e.g., keywords in sentences). However, recent research found
that attention-as-importance interpretations often do not work as we expected.
For example, learned attention weights sometimes highlight less meaningful
tokens like “[SEP]”, “,”, and “.”, and are frequently uncorrelated with other
feature importance indicators like gradient-based measures. A recent debate
over whether attention is an explanation or not has drawn considerable
interest. In this paper, we demonstrate that one root cause of this phenomenon
is the combinatorial shortcuts, which means that, in addition to the
highlighted parts, the attention weights themselves may carry extra information
that could be utilized by downstream models after attention layers. As a
result, the attention weights are no longer pure importance indicators. We
theoretically analyze combinatorial shortcuts, design one intuitive experiment
to show their existence, and propose two methods to mitigate this issue. We
conduct empirical studies on attention-based interpretation models. The results
show that the proposed methods can effectively improve the interpretability of
attention mechanisms.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/baniecki2020grammar/">The Grammar of Interactive Explanatory Model Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Grammar of Interactive Explanatory Model Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Grammar of Interactive Explanatory Model Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hubert Baniecki, Dariusz Parzych, Przemyslaw Biecek</td>
	<td></td>
	<td><p>The growing need for in-depth analysis of predictive models leads to a series
of new methods for explaining their local and global properties. Which of these
methods is the best? It turns out that this is an ill-posed question. One
cannot sufficiently explain a black-box machine learning model using a single
method that gives only one perspective. Isolated explanations are prone to
misunderstanding, leading to wrong or simplistic reasoning. This problem is
known as the Rashomon effect and refers to diverse, even contradictory,
interpretations of the same phenomenon. Surprisingly, most methods developed
for explainable and responsible machine learning focus on a single-aspect of
the model behavior. In contrast, we showcase the problem of explainability as
an interactive and sequential analysis of a model. This paper proposes how
different Explanatory Model Analysis (EMA) methods complement each other and
discusses why it is essential to juxtapose them. The introduced process of
Interactive EMA (IEMA) derives from the algorithmic side of explainable machine
learning and aims to embrace ideas developed in cognitive sciences. We
formalize the grammar of IEMA to describe potential human-model dialogues. It
is implemented in a widely used human-centered open-source software framework
that adopts interactivity, customizability and automation as its main traits.
We conduct a user study to evaluate the usefulness of IEMA, which indicates
that an interactive sequential analysis of a model increases the performance
and confidence of human decision making.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/tan2020locality/">Locality Guided Neural Networks for Explainable Artificial Intelligence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Locality Guided Neural Networks for Explainable Artificial Intelligence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Locality Guided Neural Networks for Explainable Artificial Intelligence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Randy Tan, Naimul Khan, Ling Guan</td>
	<td></td>
	<td><p>In current deep network architectures, deeper layers in networks tend to
contain hundreds of independent neurons which makes it hard for humans to
understand how they interact with each other. By organizing the neurons by
correlation, humans can observe how clusters of neighbouring neurons interact
with each other. In this paper, we propose a novel algorithm for back
propagation, called Locality Guided Neural Network(LGNN) for training networks
that preserves locality between neighbouring neurons within each layer of a
deep network. Heavily motivated by Self-Organizing Map (SOM), the goal is to
enforce a local topology on each layer of a deep network such that neighbouring
neurons are highly correlated with each other. This method contributes to the
domain of Explainable Artificial Intelligence (XAI), which aims to alleviate
the black-box nature of current AI methods and make them understandable by
humans. Our method aims to achieve XAI in deep learning without changing the
structure of current models nor requiring any post processing. This paper
focuses on Convolutional Neural Networks (CNNs), but can theoretically be
applied to any type of deep learning architecture. In our experiments, we train
various VGG and Wide ResNet (WRN) networks for image classification on
CIFAR100. In depth analyses presenting both qualitative and quantitative
results demonstrate that our method is capable of enforcing a topology on each
layer while achieving a small increase in classification accuracy</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/arun2020assessing/">Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Assessing the (Un)Trustworthiness of Saliency Maps for Localizing Abnormalities in Medical Imaging' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nishanth Arun, Nathan Gaw, Praveer Singh, Ken Chang, Mehak Aggarwal, Bryan Chen, Katharina Hoebel, Sharut Gupta, Jay Patel, Mishka Gidwani, Julius Adebayo, Matthew D. Li, Jayashree Kalpathy-Cramer</td>
	<td></td>
	<td><p>Saliency maps have become a widely used method to make deep learning models
more interpretable by providing post-hoc explanations of classifiers through
identification of the most pertinent areas of the input medical image. They are
increasingly being used in medical imaging to provide clinically plausible
explanations for the decisions the neural network makes. However, the utility
and robustness of these visualization maps has not yet been rigorously examined
in the context of medical imaging. We posit that trustworthiness in this
context requires 1) localization utility, 2) sensitivity to model weight
randomization, 3) repeatability, and 4) reproducibility. Using the localization
information available in two large public radiology datasets, we quantify the
performance of eight commonly used saliency map approaches for the above
criteria using area under the precision-recall curves (AUPRC) and structural
similarity index (SSIM), comparing their performance to various baseline
measures. Using our framework to quantify the trustworthiness of saliency maps,
we show that all eight saliency map techniques fail at least one of the
criteria and are, in most cases, less trustworthy when compared to the
baselines. We suggest that their usage in the high-risk domain of medical
imaging warrants additional scrutiny and recommend that detection or
segmentation models be used if localization is the desired output of the
network. Additionally, to promote reproducibility of our findings, we provide
the code we used for all tests performed in this work at this link:
https://github.com/QTIM-Lab/Assessing-Saliency-Maps.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/arras2020ground/">Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leila Arras, Ahmed Osman, Wojciech Samek</td>
	<td></td>
	<td><p>The rise of deep learning in today’s applications entailed an increasing need
in explaining the model’s decisions beyond prediction performances in order to
foster trust and accountability. Recently, the field of explainable AI (XAI)
has developed methods that provide such explanations for already trained neural
networks. In computer vision tasks such explanations, termed heatmaps,
visualize the contributions of individual pixels to the prediction. So far XAI
methods along with their heatmaps were mainly validated qualitatively via
human-based assessment, or evaluated through auxiliary proxy tasks such as
pixel perturbation, weak object localization or randomization tests. Due to the
lack of an objective and commonly accepted quality measure for heatmaps, it was
debatable which XAI method performs best and whether explanations can be
trusted at all. In the present work, we tackle the problem by proposing a
ground truth based evaluation framework for XAI methods based on the CLEVR
visual question answering task. Our framework provides a (1) selective, (2)
controlled and (3) realistic testbed for the evaluation of neural network
explanations. We compare ten different explanation methods, resulting in new
insights about the quality and properties of XAI methods, sometimes
contradicting with conclusions from previous comparative studies. The CLEVR-XAI
dataset and the benchmarking code can be found at
https://github.com/ahmedmagdiosman/clevr-xai.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/asher2020adequate/">Adequate and fair explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Adequate and fair explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Adequate and fair explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nicholas Asher, Soumya Paul, Chris Russell</td>
	<td>Machine Learning and Knowledge Extraction, eds. Andreas Holzinger, Peter Kieseberg, A Min Tjoa, Edgar Weippl, Lecture Notes in Computer Science 12844, Springer, pp. 79-99, 2021</td>
	<td><p>Explaining sophisticated machine-learning based systems is an important issue
at the foundations of AI. Recent efforts have shown various methods for
providing explanations. These approaches can be broadly divided into two
schools: those that provide a local and human interpreatable approximation of a
machine learning algorithm, and logical approaches that exactly characterise
one aspect of the decision. In this paper we focus upon the second school of
exact explanations with a rigorous logical foundation. There is an
epistemological problem with these exact methods. While they can furnish
complete explanations, such explanations may be too complex for humans to
understand or even to write down in human readable form. Interpretability
requires epistemically accessible explanations, explanations humans can grasp.
Yet what is a sufficiently complete epistemically accessible explanation still
needs clarification. We do this here in terms of counterfactuals, following
[Wachter et al., 2017]. With counterfactual explanations, many of the
assumptions needed to provide a complete explanation are left implicit. To do
so, counterfactual explanations exploit the properties of a particular data
point or sample, and as such are also local as well as partial explanations. We
explore how to move from local partial explanations to what we call complete
local explanations and then to global ones. But to preserve accessibility we
argue for the need for partiality. This partiality makes it possible to hide
explicit biases present in the algorithm that may be injurious or unfair.We
investigate how easy it is to uncover these biases in providing complete and
fair explanations by exploiting the structure of the set of counterfactuals
providing a complete local explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/berger2020visually/">Visually Analyzing Contextualized Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visually Analyzing Contextualized Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visually Analyzing Contextualized Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matthew Berger</td>
	<td></td>
	<td><p>In this paper we introduce a method for visually analyzing contextualized
embeddings produced by deep neural network-based language models. Our approach
is inspired by linguistic probes for natural language processing, where tasks
are designed to probe language models for linguistic structure, such as
parts-of-speech and named entities. These approaches are largely confirmatory,
however, only enabling a user to test for information known a priori. In this
work, we eschew supervised probing tasks, and advocate for unsupervised probes,
coupled with visual exploration techniques, to assess what is learned by
language models. Specifically, we cluster contextualized embeddings produced
from a large text corpus, and introduce a visualization design based on this
clustering and textual structure - cluster co-occurrences, cluster spans, and
cluster-word membership - to help elicit the functionality of, and relationship
between, individual clusters. User feedback highlights the benefits of our
design in discovering different types of linguistic structures.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/agarwal2020neural/">Neural Additive Models: Interpretable Machine Learning with Neural Nets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Additive Models: Interpretable Machine Learning with Neural Nets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Additive Models: Interpretable Machine Learning with Neural Nets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caruana, Geoffrey Hinton</td>
	<td></td>
	<td><p>Deep neural networks (DNNs) are powerful black-box predictors that have
achieved impressive performance on a wide variety of tasks. However, their
accuracy comes at the cost of intelligibility: it is usually unclear how they
make their decisions. This hinders their applicability to high stakes
decision-making domains such as healthcare. We propose Neural Additive Models
(NAMs) which combine some of the expressivity of DNNs with the inherent
intelligibility of generalized additive models. NAMs learn a linear combination
of neural networks that each attend to a single input feature. These networks
are trained jointly and can learn arbitrarily complex relationships between
their input feature and the output. Our experiments on regression and
classification datasets show that NAMs are more accurate than widely used
intelligible models such as logistic regression and shallow decision trees.
They perform similarly to existing state-of-the-art generalized additive models
in accuracy, but are more flexible because they are based on neural nets
instead of boosted trees. To demonstrate this, we show how NAMs can be used for
multitask learning on synthetic data and on the COMPAS recidivism data due to
their composability, and demonstrate that the differentiability of NAMs allows
them to train more complex interpretable models for COVID-19.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/chen2020concept/">Concept Whitening for Interpretable Image Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Concept Whitening for Interpretable Image Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Concept Whitening for Interpretable Image Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhi Chen, Yijie Bei, Cynthia Rudin</td>
	<td>Nature Machine Intelligence, Vol 2, Dec 2020, 772-782</td>
	<td><p>What does a neural network encode about a concept as we traverse through the
layers? Interpretability in machine learning is undoubtedly important, but the
calculations of neural networks are very challenging to understand. Attempts to
see inside their hidden layers can either be misleading, unusable, or rely on
the latent space to possess properties that it may not have. In this work,
rather than attempting to analyze a neural network posthoc, we introduce a
mechanism, called concept whitening (CW), to alter a given layer of the network
to allow us to better understand the computation leading up to that layer. When
a concept whitening module is added to a CNN, the axes of the latent space are
aligned with known concepts of interest. By experiment, we show that CW can
provide us a much clearer understanding for how the network gradually learns
concepts over layers. CW is an alternative to a batch normalization layer in
that it normalizes, and also decorrelates (whitens) the latent space. CW can be
used in any layer of the network without hurting predictive performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/abnar2020quantifying/">Quantifying Attention Flow in Transformers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Quantifying Attention Flow in Transformers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Quantifying Attention Flow in Transformers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Samira Abnar, Willem Zuidema</td>
	<td></td>
	<td><p>In the Transformer model, “self-attention” combines information from attended
embeddings into the representation of the focal embedding in the next layer.
Thus, across layers of the Transformer, information originating from different
tokens gets increasingly mixed. This makes attention weights unreliable as
explanations probes. In this paper, we consider the problem of quantifying this
flow of information through self-attention. We propose two methods for
approximating the attention to input tokens given attention weights, attention
rollout and attention flow, as post hoc methods when we use attention weights
as the relative relevance of the input tokens. We show that these methods give
complementary views on the flow of information, and compared to raw attention,
both yield higher correlations with importance scores of input tokens obtained
using an ablation method and input gradients.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/adebayo2020debugging/">Debugging Tests for Model Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Debugging Tests for Model Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Debugging Tests for Model Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Julius Adebayo, Michael Muelly, Ilaria Liccardi, Been Kim</td>
	<td></td>
	<td><p>We investigate whether post-hoc model explanations are effective for
diagnosing model errors–model debugging. In response to the challenge of
explaining a model’s prediction, a vast array of explanation methods have been
proposed. Despite increasing use, it is unclear if they are effective. To
start, we categorize \textit{bugs}, based on their source, into:~\textit{data,
model, and test-time} contamination bugs. For several explanation methods, we
assess their ability to: detect spurious correlation artifacts (data
contamination), diagnose mislabeled training examples (data contamination),
differentiate between a (partially) re-initialized model and a trained one
(model contamination), and detect out-of-distribution inputs (test-time
contamination). We find that the methods tested are able to diagnose a spurious
background bug, but not conclusively identify mislabeled training examples. In
addition, a class of methods, that modify the back-propagation algorithm are
invariant to the higher layer parameters of a deep network; hence, ineffective
for diagnosing model contamination. We complement our analysis with a human
subject study, and find that subjects fail to identify defective models using
attributions, but instead rely, primarily, on model predictions. Taken
together, our results provide guidance for practitioners and researchers
turning to explanations as tools for model debugging.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/antor%C3%A1n2020getting/">Getting a CLUE: A Method for Explaining Uncertainty Estimates</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Getting a CLUE: A Method for Explaining Uncertainty Estimates' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Getting a CLUE: A Method for Explaining Uncertainty Estimates' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Javier Antorán, Umang Bhatt, Tameem Adel, Adrian Weller, José Miguel Hernández-Lobato</td>
	<td></td>
	<td><p>Both uncertainty estimation and interpretability are important factors for
trustworthy machine learning systems. However, there is little work at the
intersection of these two areas. We address this gap by proposing a novel
method for interpreting uncertainty estimates from differentiable probabilistic
models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent
Uncertainty Explanations (CLUE), indicates how to change an input, while
keeping it on the data manifold, such that a BNN becomes more confident about
the input’s prediction. We validate CLUE through 1) a novel framework for
evaluating counterfactual explanations of uncertainty, 2) a series of ablation
experiments, and 3) a user study. Our experiments show that CLUE outperforms
baselines and enables practitioners to better understand which input patterns
are responsible for predictive uncertainty.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/atanasova2020generating/">Generating Fact Checking Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Fact Checking Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Fact Checking Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein</td>
	<td></td>
	<td><p>Most existing work on automated fact checking is concerned with predicting
the veracity of claims based on metadata, social network spread, language used
in claims, and, more recently, evidence supporting or denying claims. A crucial
piece of the puzzle that is still missing is to understand how to automate the
most elaborate part of the process – generating justifications for verdicts on
claims. This paper provides the first study of how these explanations can be
generated automatically based on available claim context, and how this task can
be modelled jointly with veracity prediction. Our results indicate that
optimising both objectives at the same time, rather than training them
separately, improves the performance of a fact checking system. The results of
a manual evaluation further suggest that the informativeness, coverage and
overall quality of the generated explanations are also improved in the
multi-task model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/fr%C3%A4mling2020explainable/">Explainable AI without Interpretable Model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable AI without Interpretable Model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable AI without Interpretable Model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kary Främling</td>
	<td></td>
	<td><p>Explainability has been a challenge in AI for as long as AI has existed. With
the recently increased use of AI in society, it has become more important than
ever that AI systems would be able to explain the reasoning behind their
results also to end-users in situations such as being eliminated from a
recruitment process or having a bank loan application refused by an AI system.
Especially if the AI system has been trained using Machine Learning, it tends
to contain too many parameters for them to be analysed and understood, which
has caused them to be called `black-box’ systems. Most Explainable AI (XAI)
methods are based on extracting an interpretable model that can be used for
producing explanations. However, the interpretable model does not necessarily
map accurately to the original black-box model. Furthermore, the
understandability of interpretable models for an end-user remains questionable.
The notions of Contextual Importance and Utility (CIU) presented in this paper
make it possible to produce human-like explanations of black-box outcomes
directly, without creating an interpretable model. Therefore, CIU explanations
map accurately to the black-box model itself. CIU is completely model-agnostic
and can be used with any black-box system. In addition to feature importance,
the utility concept that is well-known in Decision Theory provides a new
dimension to explanations compared to most existing XAI methods. Finally, CIU
can produce explanations at any level of abstraction and using different
vocabularies and other means of interaction, which makes it possible to adjust
explanations and interaction according to the context and to the target users.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/verma2020counterfactual/">Counterfactual Explanations for Machine Learning: A Review</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactual Explanations for Machine Learning: A Review' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactual Explanations for Machine Learning: A Review' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sahil Verma, John Dickerson, Keegan Hines</td>
	<td></td>
	<td><p>Machine learning plays a role in many deployed decision systems, often in
ways that are difficult or impossible to understand by human stakeholders.
Explaining, in a human-understandable way, the relationship between the input
and output of machine learning models is essential to the development of
trustworthy machine-learning-based systems. A burgeoning body of research seeks
to define the goals and methods of explainability in machine learning. In this
paper, we seek to review and categorize research on counterfactual
explanations, a specific class of explanation that provides a link between what
could have happened had input to a model been changed in a particular way.
Modern approaches to counterfactual explainability in machine learning draw
connections to the established legal doctrine in many countries, making them
appealing to fielded systems in high-impact areas such as finance and
healthcare. Thus, we design a rubric with desirable properties of
counterfactual explanation algorithms and comprehensively evaluate all
currently-proposed algorithms against that rubric. Our rubric provides easy
comparison and comprehension of the advantages and disadvantages of different
approaches and serves as an introduction to major research themes in this
field. We also identify gaps and discuss promising research directions in the
space of counterfactual explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/ras2020explainable/">Explainable Deep Learning: A Field Guide for the Uninitiated</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Deep Learning: A Field Guide for the Uninitiated' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Deep Learning: A Field Guide for the Uninitiated' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gabrielle Ras, Ning Xie, Marcel van Gerven, Derek Doran</td>
	<td></td>
	<td><p>Deep neural networks (DNNs) have become a proven and indispensable machine
learning tool. As a black-box model, it remains difficult to diagnose what
aspects of the model’s input drive the decisions of a DNN. In countless
real-world domains, from legislation and law enforcement to healthcare, such
diagnosis is essential to ensure that DNN decisions are driven by aspects
appropriate in the context of its use. The development of methods and studies
enabling the explanation of a DNN’s decisions has thus blossomed into an
active, broad area of research. A practitioner wanting to study explainable
deep learning may be intimidated by the plethora of orthogonal directions the
field has taken. This complexity is further exacerbated by competing
definitions of what it means <code class="language-plaintext highlighter-rouge">to explain'' the actions of a DNN and to
evaluate an approach's</code>ability to explain’’. This article offers a field
guide to explore the space of explainable deep learning aimed at those
uninitiated in the field. The field guide: i) Introduces three simple
dimensions defining the space of foundational methods that contribute to
explainable deep learning, ii) discusses the evaluations for model
explanations, iii) places explainability in the context of other related deep
learning research areas, and iv) finally elaborates on user-oriented
explanation designing and potential future directions on explainable deep
learning. We hope the guide is used as an easy-to-digest starting point for
those just embarking on research in this field.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2020</td>
	<td><a href="/publications/puiutta2020explainable/">Explainable Reinforcement Learning: A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Reinforcement Learning: A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Reinforcement Learning: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Erika Puiutta, Eric MSP Veith</td>
	<td></td>
	<td><p>Explainable Artificial Intelligence (XAI), i.e., the development of more
transparent and interpretable AI models, has gained increased traction over the
last few years. This is due to the fact that, in conjunction with their growth
into powerful and ubiquitous tools, AI models exhibit one detrimential
characteristic: a performance-transparency trade-off. This describes the fact
that the more complex a model’s inner workings, the less clear it is how its
predictions or decisions were achieved. But, especially considering Machine
Learning (ML) methods like Reinforcement Learning (RL) where the system learns
autonomously, the necessity to understand the underlying reasoning for their
decisions becomes apparent. Since, to the best of our knowledge, there exists
no single work offering an overview of Explainable Reinforcement Learning (XRL)
methods, this survey attempts to address this gap. We give a short summary of
the problem, a definition of important terms, and offer a classification and
assessment of current XRL methods. We found that a) the majority of XRL methods
function by mimicking and simplifying a complex model instead of designing an
inherently simple one, and b) XRL (and XAI) methods often neglect to consider
the human side of the equation, not taking into account research from related
fields like psychology or philosophy. Thus, an interdisciplinary effort is
needed to adapt the generated explanations to a (non-expert) human user in
order to effectively progress in the field of XRL and XAI in general.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2019</td>
	<td><a href="/publications/marx2019disentangling/">Disentangling Influence: Using Disentangled Representations to Audit Model Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Disentangling Influence: Using Disentangled Representations to Audit Model Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Disentangling Influence: Using Disentangled Representations to Audit Model Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Charles T. Marx, Richard Lanas Phillips, Sorelle A. Friedler, Carlos Scheidegger, Suresh Venkatasubramanian</td>
	<td></td>
	<td><p>Motivated by the need to audit complex and black box models, there has been
extensive research on quantifying how data features influence model
predictions. Feature influence can be direct (a direct influence on model
outcomes) and indirect (model outcomes are influenced via proxy features).
Feature influence can also be expressed in aggregate over the training or test
data or locally with respect to a single point. Current research has typically
focused on one of each of these dimensions. In this paper, we develop
disentangled influence audits, a procedure to audit the indirect influence of
features. Specifically, we show that disentangled representations provide a
mechanism to identify proxy features in the dataset, while allowing an explicit
computation of feature influence on either individual outcomes or
aggregate-level outcomes. We show through both theory and experiments that
disentangled influence audits can both detect proxy features and show, for each
individual or in aggregate, which of these proxy features affects the
classifier being audited the most. In this respect, our method is more powerful
than existing methods for ascertaining feature influence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/schneider2019humantoai/">Human-to-AI Coach: Improving Human Inputs to AI Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human-to-AI Coach: Improving Human Inputs to AI Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human-to-AI Coach: Improving Human Inputs to AI Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Johannes Schneider</td>
	<td>Symposium on Intelligent Data Analysis 2020, Konstanz</td>
	<td><p>Humans increasingly interact with Artificial intelligence(AI) systems. AI
systems are optimized for objectives such as minimum computation or minimum
error rate in recognizing and interpreting inputs from humans. In contrast,
inputs created by humans are often treated as a given. We investigate how
inputs of humans can be altered to reduce misinterpretation by the AI system
and to improve efficiency of input generation for the human while altered
inputs should remain as similar as possible to the original inputs. These
objectives result in trade-offs that are analyzed for a deep learning system
classifying handwritten digits. To create examples that serve as demonstrations
for humans to improve, we develop a model based on a conditional convolutional
autoencoder (CCAE). Our quantitative and qualitative evaluation shows that in
many occasions the generated proposals lead to lower error rates, require less
effort to create and differ only modestly from the original samples.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/du2019attribution/">On Attribution of Recurrent Neural Network Predictions via Additive Decomposition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On Attribution of Recurrent Neural Network Predictions via Additive Decomposition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On Attribution of Recurrent Neural Network Predictions via Additive Decomposition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mengnan Du, Ninghao Liu, Fan Yang, Shuiwang Ji, Xia Hu</td>
	<td></td>
	<td><p>RNN models have achieved the state-of-the-art performance in a wide range of
text mining tasks. However, these models are often regarded as black-boxes and
are criticized due to the lack of interpretability. In this paper, we enhance
the interpretability of RNNs by providing interpretable rationales for RNN
predictions. Nevertheless, interpreting RNNs is a challenging problem. Firstly,
unlike existing methods that rely on local approximation, we aim to provide
rationales that are more faithful to the decision making process of RNN models.
Secondly, a flexible interpretation method should be able to assign
contribution scores to text segments of varying lengths, instead of only to
individual words. To tackle these challenges, we propose a novel attribution
method, called REAT, to provide interpretations to RNN predictions. REAT
decomposes the final prediction of a RNN into additive contribution of each
word in the input text. This additive decomposition enables REAT to further
obtain phrase-level attribution scores. In addition, REAT is generally
applicable to various RNN architectures, including GRU, LSTM and their
bidirectional versions. Experimental results demonstrate the faithfulness and
interpretability of the proposed attribution method. Comprehensive analysis
shows that our attribution method could unveil the useful linguistic knowledge
captured by RNNs. Some analysis further demonstrates our method could be
utilized as a debugging tool to examine the vulnerability and failure reasons
of RNNs, which may lead to several promising future directions to promote
generalization ability of RNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/meade2019exploring/">Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring Conditioning for Generative Music Systems with Human-Interpretable Controls' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nicholas Meade, Nicholas Barreyre, Scott C. Lowe, Sageev Oore</td>
	<td>International Conference on Computational Creativity, 2019</td>
	<td><p>Performance RNN is a machine-learning system designed primarily for the
generation of solo piano performances using an event-based (rather than audio)
representation. More specifically, Performance RNN is a long short-term memory
(LSTM) based recurrent neural network that models polyphonic music with
expressive timing and dynamics (Oore et al., 2018). The neural network uses a
simple language model based on the Musical Instrument Digital Interface (MIDI)
file format. Performance RNN is trained on the e-Piano Junior Competition
Dataset (International Piano e-Competition, 2018), a collection of solo piano
performances by expert pianists. As an artistic tool, one of the limitations of
the original model has been the lack of useable controls. The standard form of
Performance RNN can generate interesting pieces, but little control is provided
over what specifically is generated. This paper explores a set of
conditioning-based controls used to influence the generation process.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/patro2019explanation/">Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation vs Attention: A Two-Player Game to Obtain Attention for VQA' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Badri N. Patro, Anupriy, Vinay P. Namboodiri</td>
	<td></td>
	<td><p>In this paper, we aim to obtain improved attention for a visual question
answering (VQA) task. It is challenging to provide supervision for attention.
An observation we make is that visual explanations as obtained through class
activation mappings (specifically Grad-CAM) that are meant to explain the
performance of various networks could form a means of supervision. However, as
the distributions of attention maps and that of Grad-CAMs differ, it would not
be suitable to directly use these as a form of supervision. Rather, we propose
the use of a discriminator that aims to distinguish samples of visual
explanation and attention maps. The use of adversarial training of the
attention regions as a two-player game between attention and explanation serves
to bring the distributions of attention maps and visual explanations closer.
Significantly, we observe that providing such a means of supervision also
results in attention maps that are more closely related to human attention
resulting in a substantial improvement over baseline stacked attention network
(SAN) models. It also results in a good improvement in rank correlation metric
on the VQA task. This method can also be combined with recent MCB based methods
and results in consistent improvement. We also provide comparisons with other
means for learning distributions such as based on Correlation Alignment
(Coral), Maximum Mean Discrepancy (MMD) and Mean Square Error (MSE) losses and
observe that the adversarial loss outperforms the other forms of learning the
attention maps. Visualization of the results also confirms our hypothesis that
attention maps improve using this form of supervision.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/dhamdhere2019shapley/">The Shapley Taylor Interaction Index</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Shapley Taylor Interaction Index' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Shapley Taylor Interaction Index' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kedar Dhamdhere, Ashish Agarwal, Mukund Sundararajan</td>
	<td></td>
	<td><p>The attribution problem, that is the problem of attributing a model’s
prediction to its base features, is well-studied. We extend the notion of
attribution to also apply to feature interactions.
  The Shapley value is a commonly used method to attribute a model’s prediction
to its base features. We propose a generalization of the Shapley value called
Shapley-Taylor index that attributes the model’s prediction to interactions of
subsets of features up to some size k. The method is analogous to how the
truncated Taylor Series decomposes the function value at a certain point using
its derivatives at a different point. In fact, we show that the Shapley Taylor
index is equal to the Taylor Series of the multilinear extension of the
set-theoretic behavior of the model.
  We axiomatize this method using the standard Shapley axioms – linearity,
dummy, symmetry and efficiency – and an additional axiom that we call the
interaction distribution axiom. This new axiom explicitly characterizes how
interactions are distributed for a class of functions that model pure
interaction.
  We contrast the Shapley-Taylor index against the previously proposed Shapley
Interaction index (cf. [9]) from the cooperative game theory literature. We
also apply the Shapley Taylor index to three models and identify interesting
qualitative insights.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/shankaranarayana2019alime/">ALIME: Autoencoder Based Approach for Local Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=ALIME: Autoencoder Based Approach for Local Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=ALIME: Autoencoder Based Approach for Local Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sharath M. Shankaranarayana, Davor Runje</td>
	<td></td>
	<td><p>Machine learning and especially deep learning have garneredtremendous
popularity in recent years due to their increased performanceover other
methods. The availability of large amount of data has aidedin the progress of
deep learning. Nevertheless, deep learning models areopaque and often seen as
black boxes. Thus, there is an inherent need tomake the models interpretable,
especially so in the medical domain. Inthis work, we propose a locally
interpretable method, which is inspiredby one of the recent tools that has
gained a lot of interest, called localinterpretable model-agnostic explanations
(LIME). LIME generates singleinstance level explanation by artificially
generating a dataset aroundthe instance (by randomly sampling and using
perturbations) and thentraining a local linear interpretable model. One of the
major issues inLIME is the instability in the generated explanation, which is
caused dueto the randomly generated dataset. Another issue in these kind of
localinterpretable models is the local fidelity. We propose novel
modificationsto LIME by employing an autoencoder, which serves as a better
weightingfunction for the local model. We perform extensive comparisons
withdifferent datasets and show that our proposed method results in
bothimproved stability, as well as local fidelity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wiegreffe2019attention/">Attention is not not Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention is not not Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention is not not Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarah Wiegreffe, Yuval Pinter</td>
	<td></td>
	<td><p>Attention mechanisms play a central role in NLP systems, especially within
recurrent neural network (RNN) models. Recently, there has been increasing
interest in whether or not the intermediate representations offered by these
modules may be used to explain the reasoning for a model’s prediction, and
consequently reach insights regarding the model’s decision-making process. A
recent paper claims that `Attention is not Explanation’ (Jain and Wallace,
2019). We challenge many of the assumptions underlying this work, arguing that
such a claim depends on one’s definition of explanation, and that testing it
needs to take into account all elements of the model, using a rigorous
experimental design. We propose four alternative tests to determine
when/whether attention can be used as explanation: a simple uniform-weights
baseline; a variance calibration based on multiple random seed runs; a
diagnostic framework using frozen weights from pretrained models; and an
end-to-end adversarial attention training protocol. Each allows for meaningful
interpretation of attention mechanisms in RNN models. We show that even when
reliable adversarial distributions can be found, they don’t perform well on the
simple diagnostic, indicating that prior work does not disprove the usefulness
of attention mechanisms for explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/weller2019transparency/">Transparency: Motivations and Challenges</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Transparency: Motivations and Challenges' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Transparency: Motivations and Challenges' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Adrian Weller</td>
	<td></td>
	<td><p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>
</td>
	<td>evaluation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/cui2019chip/">CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=CHIP: Channel-wise Disentangled Interpretation of Deep Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xinrui Cui, Dan Wang, Z. Jane Wang</td>
	<td></td>
	<td><p>With the widespread applications of deep convolutional neural networks
(DCNNs), it becomes increasingly important for DCNNs not only to make accurate
predictions but also to explain how they make their decisions. In this work, we
propose a CHannel-wise disentangled InterPretation (CHIP) model to give the
visual interpretation to the predictions of DCNNs. The proposed model distills
the class-discriminative importance of channels in networks by utilizing the
sparse regularization. Here, we first introduce the network perturbation
technique to learn the model. The proposed model is capable to not only distill
the global perspective knowledge from networks but also present the
class-discriminative visual interpretation for specific predictions of
networks. It is noteworthy that the proposed model is able to interpret
different layers of networks without re-training. By combining the distilled
interpretation knowledge in different layers, we further propose the Refined
CHIP visual interpretation that is both high-resolution and
class-discriminative. Experimental results on the standard dataset demonstrate
that the proposed model provides promising visual interpretation for the
predictions of networks in image classification task compared with existing
visual interpretation methods. Besides, the proposed method outperforms related
approaches in the application of ILSVRC 2015 weakly-supervised localization
task.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/dong2019towards/">Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yinpeng Dong, Fan Bao, Hang Su, Jun Zhu</td>
	<td></td>
	<td><p>Sometimes it is not enough for a DNN to produce an outcome. For example, in
applications such as healthcare, users need to understand the rationale of the
decisions. Therefore, it is imperative to develop algorithms to learn models
with good interpretability (Doshi-Velez 2017). An important factor that leads
to the lack of interpretability of DNNs is the ambiguity of neurons, where a
neuron may fire for various unrelated concepts. This work aims to increase the
interpretability of DNNs on the whole image space by reducing the ambiguity of
neurons. In this paper, we make the following contributions:
  1) We propose a metric to evaluate the consistency level of neurons in a
network quantitatively.
  2) We find that the learned features of neurons are ambiguous by leveraging
adversarial examples.
  3) We propose to improve the consistency of neurons on adversarial example
subset by an adversarial training algorithm with a consistent loss.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chyung2019extracting/">Extracting Interpretable Concept-Based Decision Trees from CNNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Extracting Interpretable Concept-Based Decision Trees from CNNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Extracting Interpretable Concept-Based Decision Trees from CNNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Conner Chyung, Michael Tsang, Yan Liu</td>
	<td></td>
	<td><p>In an attempt to gather a deeper understanding of how convolutional neural
networks (CNNs) reason about human-understandable concepts, we present a method
to infer labeled concept data from hidden layer activations and interpret the
concepts through a shallow decision tree. The decision tree can provide
information about which concepts a model deems important, as well as provide an
understanding of how the concepts interact with each other. Experiments
demonstrate that the extracted decision tree is capable of accurately
representing the original CNN’s classifications at low tree depths, thus
encouraging human-in-the-loop understanding of discriminative concepts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/peri2019show,/">Show, Translate and Tell</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Show, Translate and Tell' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Show, Translate and Tell' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dheeraj Peri, Shagan Sah, Raymond Ptucha</td>
	<td></td>
	<td><p>Humans have an incredible ability to process and understand information from
multiple sources such as images, video, text, and speech. Recent success of
deep neural networks has enabled us to develop algorithms which give machines
the ability to understand and interpret this information. There is a need to
both broaden their applicability and develop methods which correlate visual
information along with semantic content. We propose a unified model which
jointly trains on images and captions, and learns to generate new captions
given either an image or a caption query. We evaluate our model on three
different tasks namely cross-modal retrieval, image captioning, and sentence
paraphrasing. Our model gains insight into cross-modal vector embeddings,
generalizes well on multiple tasks and is competitive to state of the art
methods on retrieval.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/chattopadhyay2019neural/">Neural Network Attributions: A Causal Perspective</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Neural Network Attributions: A Causal Perspective' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Neural Network Attributions: A Causal Perspective' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aditya Chattopadhyay, Piyushi Manupriya, Anirban Sarkar, Vineeth N Balasubramanian</td>
	<td>Proceedings of the 36th International Conference on Machine Learning 97 (2019) 981-990</td>
	<td><p>We propose a new attribution method for neural networks developed using first
principles of causality (to the best of our knowledge, the first such). The
neural network architecture is viewed as a Structural Causal Model, and a
methodology to compute the causal effect of each feature on the output is
presented. With reasonable assumptions on the causal structure of the input
data, we propose algorithms to efficiently compute the causal effects, as well
as scale the approach to data with large dimensionality. We also show how this
method can be used for recurrent neural networks. We report experimental
results on both simulated and real datasets showcasing the promise and
usefulness of the proposed algorithm.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/mundhenk2019efficient/">Efficient Saliency Maps for Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Efficient Saliency Maps for Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Efficient Saliency Maps for Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>T. Nathan Mundhenk, Barry Y. Chen, Gerald Friedland</td>
	<td></td>
	<td><p>We describe an explainable AI saliency map method for use with deep
convolutional neural networks (CNN) that is much more efficient than popular
fine-resolution gradient methods. It is also quantitatively similar or better
in accuracy. Our technique works by measuring information at the end of each
network scale which is then combined into a single saliency map. We describe
how saliency measures can be made more efficient by exploiting Saliency Map
Order Equivalence. We visualize individual scale/layer contributions by using a
Layer Ordered Visualization of Information. This provides an interesting
comparison of scale information contributions within the network not provided
by other saliency map methods. Using our method instead of Guided Backprop,
coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem
to yield demonstrably superior results without sacrificing speed. This will
make fine-resolution saliency methods feasible on resource limited platforms
such as robots, cell phones, low-cost industrial devices, astronomy and
satellite imagery.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/tomsett2019sanity/">Sanity Checks for Saliency Metrics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Checks for Saliency Metrics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Checks for Saliency Metrics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, Alun Preece</td>
	<td></td>
	<td><p>Saliency maps are a popular approach to creating post-hoc explanations of
image classifier outputs. These methods produce estimates of the relevance of
each pixel to the classification output score, which can be displayed as a
saliency map that highlights important pixels. Despite a proliferation of such
methods, little effort has been made to quantify how good these saliency maps
are at capturing the true relevance of the pixels to the classifier output
(i.e. their “fidelity”). We therefore investigate existing metrics for
evaluating the fidelity of saliency methods (i.e. saliency metrics). We find
that there is little consistency in the literature in how such metrics are
calculated, and show that such inconsistencies can have a significant effect on
the measured fidelity. Further, we apply measures of reliability developed in
the psychometric testing literature to assess the consistency of saliency
metrics when applied to individual saliency maps. Our results show that
saliency metrics can be statistically unreliable and inconsistent, indicating
that comparative rankings between saliency methods generated using such metrics
can be untrustworthy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yousefzadeh2019interpreting/">Interpreting Neural Networks Using Flip Points</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Neural Networks Using Flip Points' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Neural Networks Using Flip Points' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Roozbeh Yousefzadeh, Dianne P. O'Leary</td>
	<td></td>
	<td><p>Neural networks have been criticized for their lack of easy interpretation,
which undermines confidence in their use for important applications. Here, we
introduce a novel technique, interpreting a trained neural network by
investigating its flip points. A flip point is any point that lies on the
boundary between two output classes: e.g. for a neural network with a binary
yes/no output, a flip point is any input that generates equal scores for “yes”
and “no”. The flip point closest to a given input is of particular importance,
and this point is the solution to a well-posed optimization problem. This paper
gives an overview of the uses of flip points and how they are computed. Through
results on standard datasets, we demonstrate how flip points can be used to
provide detailed interpretation of the output produced by a neural network.
Moreover, for a given input, flip points enable us to measure confidence in the
correctness of outputs much more effectively than softmax score. They also
identify influential features of the inputs, identify bias, and find changes in
the input that change the output of the model. We show that distance between an
input and the closest flip point identifies the most influential points in the
training data. Using principal component analysis (PCA) and rank-revealing QR
factorization (RR-QR), the set of directions from each training input to its
closest flip point provides explanations of how a trained neural network
processes an entire dataset: what features are most important for
classification into a given class, which features are most responsible for
particular misclassifications, how an adversary might fool the network, etc.
Although we investigate flip points for neural networks, their usefulness is
actually model-agnostic.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/he2019understanding/">Understanding and Visualizing Deep Visual Saliency Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding and Visualizing Deep Visual Saliency Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding and Visualizing Deep Visual Saliency Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sen He, Hamed R. Tavakoli, Ali Borji, Yang Mi, Nicolas Pugeault</td>
	<td></td>
	<td><p>Recently, data-driven deep saliency models have achieved high performance and
have outperformed classical saliency models, as demonstrated by results on
datasets such as the MIT300 and SALICON. Yet, there remains a large gap between
the performance of these models and the inter-human baseline. Some outstanding
questions include what have these models learned, how and where they fail, and
how they can be improved. This article attempts to answer these questions by
analyzing the representations learned by individual neurons located at the
intermediate layers of deep saliency models. To this end, we follow the steps
of existing deep saliency models, that is borrowing a pre-trained model of
object recognition to encode the visual features and learning a decoder to
infer the saliency. We consider two cases when the encoder is used as a fixed
feature extractor and when it is fine-tuned, and compare the inner
representations of the network. To study how the learned representations depend
on the task, we fine-tune the same network using the same image set but for two
different tasks: saliency prediction versus scene classification. Our analyses
reveal that: 1) some visual regions (e.g. head, text, symbol, vehicle) are
already encoded within various layers of the network pre-trained for object
recognition, 2) using modern datasets, we find that fine-tuning pre-trained
models for saliency prediction makes them favor some categories (e.g. head)
over some others (e.g. text), 3) although deep models of saliency outperform
classical models on natural images, the converse is true for synthetic stimuli
(e.g. pop-out search arrays), an evidence of significant difference between
human and data-driven saliency models, and 4) we confirm that, after-fine
tuning, the change in inner-representations is mostly due to the task and not
the domain shift in the data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/liu2019towards/">Towards Visually Explaining Variational Autoencoders</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Visually Explaining Variational Autoencoders' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Visually Explaining Variational Autoencoders' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wenqian Liu, Runze Li, Meng Zheng, Srikrishna Karanam, Ziyan Wu, Bir Bhanu, Richard J. Radke, Octavia Camps</td>
	<td></td>
	<td><p>Recent advances in Convolutional Neural Network (CNN) model interpretability
have led to impressive progress in visualizing and understanding model
predictions. In particular, gradient-based visual attention methods have driven
much recent effort in using visual attention maps as a means for visual
explanations. A key problem, however, is these methods are designed for
classification and categorization tasks, and their extension to explaining
generative models, e.g. variational autoencoders (VAE) is not trivial. In this
work, we take a step towards bridging this crucial gap, proposing the first
technique to visually explain VAEs by means of gradient-based attention. We
present methods to generate visual attention from the learned latent space, and
also demonstrate such attention explanations serve more than just explaining
VAE predictions. We show how these attention maps can be used to localize
anomalies in images, demonstrating state-of-the-art performance on the MVTec-AD
dataset. We also show how they can be infused into model training, helping
bootstrap the VAE into learning improved latent space disentanglement,
demonstrated on the Dsprites dataset.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gupta2019simple/">A Simple Saliency Method That Passes the Sanity Checks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Simple Saliency Method That Passes the Sanity Checks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Simple Saliency Method That Passes the Sanity Checks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Arushi Gupta, Sanjeev Arora</td>
	<td></td>
	<td><p>There is great interest in “saliency methods” (also called “attribution
methods”), which give “explanations” for a deep net’s decision, by assigning a
“score” to each feature/pixel in the input. Their design usually involves
credit-assignment via the gradient of the output with respect to input.
Recently Adebayo et al. [arXiv:1810.03292] questioned the validity of many of
these methods since they do not pass simple <em>sanity checks</em> which test whether
the scores shift/vanish when layers of the trained net are randomized, or when
the net is retrained using random labels for inputs.
  We propose a simple fix to existing saliency methods that helps them pass
sanity checks, which we call “competition for pixels”. This involves computing
saliency maps for all possible labels in the classification task, and using a
simple competition among them to identify and remove less relevant pixels from
the map. The simplest variant of this is “Competitive Gradient $\odot$ Input
(CGI)”: it is efficient, requires no additional training, and uses only the
input and gradient. Some theoretical justification is provided for it
(especially for ReLU networks) and its performance is empirically demonstrated.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/guo2019exploring/">Exploring Interpretable LSTM Neural Networks over Multi-Variable Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring Interpretable LSTM Neural Networks over Multi-Variable Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring Interpretable LSTM Neural Networks over Multi-Variable Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tian Guo, Tao Lin, Nino Antulov-Fantulin</td>
	<td></td>
	<td><p>For recurrent neural networks trained on time series with target and
exogenous variables, in addition to accurate prediction, it is also desired to
provide interpretable insights into the data. In this paper, we explore the
structure of LSTM recurrent neural networks to learn variable-wise hidden
states, with the aim to capture different dynamics in multi-variable time
series and distinguish the contribution of variables to the prediction. With
these variable-wise hidden states, a mixture attention mechanism is proposed to
model the generative process of the target. Then we develop associated training
methods to jointly learn network parameters, variable and temporal importance
w.r.t the prediction of the target variable. Extensive experiments on real
datasets demonstrate enhanced prediction performance by capturing the dynamics
of different variables. Meanwhile, we evaluate the interpretation results both
qualitatively and quantitatively. It exhibits the prospect as an end-to-end
framework for both forecasting and knowledge extraction over multi-variable
data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lim2019temporal/">Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bryan Lim, Sercan O. Arik, Nicolas Loeff, Tomas Pfister</td>
	<td></td>
	<td><p>Multi-horizon forecasting problems often contain a complex mix of inputs –
including static (i.e. time-invariant) covariates, known future inputs, and
other exogenous time series that are only observed historically – without any
prior information on how they interact with the target. While several deep
learning models have been proposed for multi-step prediction, they typically
comprise black-box models which do not account for the full range of inputs
present in common scenarios. In this paper, we introduce the Temporal Fusion
Transformer (TFT) – a novel attention-based architecture which combines
high-performance multi-horizon forecasting with interpretable insights into
temporal dynamics. To learn temporal relationships at different scales, the TFT
utilizes recurrent layers for local processing and interpretable self-attention
layers for learning long-term dependencies. The TFT also uses specialized
components for the judicious selection of relevant features and a series of
gating layers to suppress unnecessary components, enabling high performance in
a wide range of regimes. On a variety of real-world datasets, we demonstrate
significant performance improvements over existing benchmarks, and showcase
three practical interpretability use-cases of TFT.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ghosh2019generating/">Generating Natural Language Explanations for Visual Question Answering using Scene Graphs and Visual Attention</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Natural Language Explanations for Visual Question Answering using Scene Graphs and Visual Attention' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Natural Language Explanations for Visual Question Answering using Scene Graphs and Visual Attention' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shalini Ghosh, Giedrius Burachas, Arijit Ray, Avi Ziskind</td>
	<td></td>
	<td><p>In this paper, we present a novel approach for the task of eXplainable
Question Answering (XQA), i.e., generating natural language (NL) explanations
for the Visual Question Answering (VQA) problem. We generate NL explanations
comprising of the evidence to support the answer to a question asked to an
image using two sources of information: (a) annotations of entities in an image
(e.g., object labels, region descriptions, relation phrases) generated from the
scene graph of the image, and (b) the attention map generated by a VQA model
when answering the question. We show how combining the visual attention map
with the NL representation of relevant scene graph entities, carefully selected
using a language model, can give reasonable textual explanations without the
need of any additional collected data (explanation captions, etc). We run our
algorithms on the Visual Genome (VG) dataset and conduct internal user-studies
to demonstrate the efficacy of our approach over a strong baseline. We have
also released a live web demo showcasing our VQA and textual explanation
generation using scene graphs and visual attention.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gilpin2019explaining/">Explaining Explanations to Society</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Explanations to Society' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Explanations to Society' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leilani H. Gilpin, Cecilia Testart, Nathaniel Fruchter, Julius Adebayo</td>
	<td></td>
	<td><p>There is a disconnect between explanatory artificial intelligence (XAI)
methods and the types of explanations that are useful for and demanded by
society (policy makers, government officials, etc.) Questions that experts in
artificial intelligence (AI) ask opaque systems provide inside explanations,
focused on debugging, reliability, and validation. These are different from
those that society will ask of these systems to build trust and confidence in
their decisions. Although explanatory AI systems can answer many questions that
experts desire, they often don’t explain why they made decisions in a way that
is precise (true to the model) and understandable to humans. These outside
explanations can be used to build trust, comply with regulatory and policy
changes, and act as external validation. In this paper, we focus on XAI methods
for deep neural networks (DNNs) because of DNNs’ use in decision-making and
inherent opacity. We explore the types of questions that explanatory DNN
systems can answer and discuss challenges in building explanatory systems that
provide outside explanations for societal requirements and benefit.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/gosiewska2019trust/">Do Not Trust Additive Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Do Not Trust Additive Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Do Not Trust Additive Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alicja Gosiewska, Przemyslaw Biecek</td>
	<td></td>
	<td><p>Explainable Artificial Intelligence (XAI)has received a great deal of
attention recently. Explainability is being presented as a remedy for the
distrust of complex and opaque models. Model agnostic methods such as LIME,
SHAP, or Break Down promise instance-level interpretability for any complex
machine learning model. But how faithful are these additive explanations? Can
we rely on additive explanations for non-additive models?
  In this paper, we (1) examine the behavior of the most popular instance-level
explanations under the presence of interactions, (2) introduce a new method
that detects interactions for instance-level explanations, (3) perform a large
scale benchmark to see how frequently additive explanations may be misleading.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/goyal2019counterfactual/">Counterfactual Visual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Counterfactual Visual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Counterfactual Visual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, Stefan Lee</td>
	<td></td>
	<td><p>In this work, we develop a technique to produce counterfactual visual
explanations. Given a ‘query’ image $I$ for which a vision system predicts
class $c$, a counterfactual visual explanation identifies how $I$ could change
such that the system would output a different specified class $c’$. To do this,
we select a ‘distractor’ image $I’$ that the system predicts as class $c’$ and
identify spatial regions in $I$ and $I’$ such that replacing the identified
region in $I$ with the identified region in $I’$ would push the system towards
classifying $I$ as $c’$. We apply our approach to multiple image classification
datasets generating qualitative results showcasing the interpretability and
discriminativeness of our counterfactual explanations. To explore the
effectiveness of our explanations in teaching humans, we present machine
teaching experiments for the task of fine-grained bird classification. We find
that users trained to distinguish bird species fare better when given access to
counterfactual explanations in addition to training examples.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/huang2019understanding/">Understanding Generalization through Visualizations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Generalization through Visualizations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Generalization through Visualizations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>W. Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, J. K. Terry, Furong Huang, Tom Goldstein</td>
	<td></td>
	<td><p>The power of neural networks lies in their ability to generalize to unseen
data, yet the underlying reasons for this phenomenon remain elusive. Numerous
rigorous attempts have been made to explain generalization, but available
bounds are still quite loose, and analysis does not always lead to true
understanding. The goal of this work is to make generalization more intuitive.
Using visualization methods, we discuss the mystery of generalization, the
geometry of loss landscapes, and how the curse (or, rather, the blessing) of
dimensionality causes optimizers to settle into minima that generalize well.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/jain2019attention/">Attention is not Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention is not Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention is not Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sarthak Jain, Byron C. Wallace</td>
	<td></td>
	<td><p>Attention mechanisms have seen wide adoption in neural NLP models. In
addition to improving predictive performance, these are often touted as
affording transparency: models equipped with attention provide a distribution
over attended-to input units, and this is often presented (at least implicitly)
as communicating the relative importance of inputs. However, it is unclear what
relationship exists between attention weights and model outputs. In this work,
we perform extensive experiments across a variety of NLP tasks that aim to
assess the degree to which attention weights provide meaningful `explanations’
for predictions. We find that they largely do not. For example, learned
attention weights are frequently uncorrelated with gradient-based measures of
feature importance, and one can identify very different attention distributions
that nonetheless yield equivalent predictions. Our findings show that standard
attention modules do not provide meaningful explanations and should not be
treated as though they do. Code for all experiments is available at
https://github.com/successar/AttentionExplanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/voita2019analyzing/">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, Ivan Titov</td>
	<td></td>
	<td><p>Multi-head self-attention is a key component of the Transformer, a
state-of-the-art architecture for neural machine translation. In this work we
evaluate the contribution made by individual attention heads in the encoder to
the overall performance of the model and analyze the roles played by them. We
find that the most important and confident heads play consistent and often
linguistically-interpretable roles. When pruning heads using a method based on
stochastic gates and a differentiable relaxation of the L0 penalty, we observe
that specialized heads are last to be pruned. Our novel pruning method removes
the vast majority of heads without seriously affecting performance. For
example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads
results in a drop of only 0.15 BLEU.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/sindhgatta2019exploring/">Exploring Interpretability for Predictive Process Analytics</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring Interpretability for Predictive Process Analytics' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring Interpretability for Predictive Process Analytics' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Renuka Sindhgatta, Chun Ouyang, Catarina Moreira</td>
	<td></td>
	<td><p>Modern predictive analytics underpinned by machine learning techniques has
become a key enabler to the automation of data-driven decision making. In the
context of business process management, predictive analytics has been applied
to making predictions about the future state of an ongoing business process
instance, for example, when will the process instance complete and what will be
the outcome upon completion. Machine learning models can be trained on event
log data recording historical process execution to build the underlying
predictive models. Multiple techniques have been proposed so far which encode
the information available in an event log and construct input features required
to train a predictive model. While accuracy has been a dominant criterion in
the choice of various techniques, they are often applied as a black-box in
building predictive models. In this paper, we derive explanations using
interpretable machine learning techniques to compare and contrast the
suitability of multiple predictive models of high accuracy. The explanations
allow us to gain an understanding of the underlying reasons for a prediction
and highlight scenarios where accuracy alone may not be sufficient in assessing
the suitability of techniques used to encode event log data to features used by
a predictive model. Findings from this study motivate the need and importance
to incorporate interpretability in predictive process analytics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ismail2019input-cell/">Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aya Abdelsalam Ismail, Mohamed Gunady, Luiz Pessoa, Héctor Corrada Bravo, Soheil Feizi</td>
	<td>Neurips 2019</td>
	<td><p>Recent efforts to improve the interpretability of deep neural networks use
saliency to characterize the importance of input features to predictions made
by models. Work on interpretability using saliency-based methods on Recurrent
Neural Networks (RNNs) has mostly targeted language tasks, and their
applicability to time series data is less understood. In this work we analyze
saliency-based methods for RNNs, both classical and gated cell architectures.
We show that RNN saliency vanishes over time, biasing detection of salient
features only to later time steps and are, therefore, incapable of reliably
detecting important features at arbitrary time intervals. To address this
vanishing saliency problem, we propose a novel RNN cell structure (input-cell
attention), which can extend any RNN cell architecture. At each time step,
instead of only looking at the current input vector, input-cell attention uses
a fixed-size matrix embedding, each row of the matrix attending to different
inputs from current or previous time steps. Using synthetic data, we show that
the saliency map produced by the input-cell attention RNN is able to faithfully
detect important features regardless of their occurrence in time. We also apply
the input-cell attention RNN on a neuroscience task analyzing functional
Magnetic Resonance Imaging (fMRI) data for human subjects performing a variety
of tasks. In this case, we use saliency to characterize brain regions (input
features) for which activity is important to distinguish between tasks. We show
that standard RNN architectures are only capable of detecting important brain
regions in the last few time steps of the fMRI data, while the input-cell
attention model is able to detect important brain region activity across time
without latter time step biases.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/islam2019enabling/">Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Enabling Explainable Fusion in Deep Learning with Fuzzy Integral Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Muhammad Aminul Islam, Derek T. Anderson, Anthony J. Pinar, Timothy C. Havens, Grant Scott, James M. Keller</td>
	<td></td>
	<td><p>Information fusion is an essential part of numerous engineering systems and
biological functions, e.g., human cognition. Fusion occurs at many levels,
ranging from the low-level combination of signals to the high-level aggregation
of heterogeneous decision-making processes. While the last decade has witnessed
an explosion of research in deep learning, fusion in neural networks has not
observed the same revolution. Specifically, most neural fusion approaches are
ad hoc, are not understood, are distributed versus localized, and/or
explainability is low (if present at all). Herein, we prove that the fuzzy
Choquet integral (ChI), a powerful nonlinear aggregation function, can be
represented as a multi-layer network, referred to hereafter as ChIMP. We also
put forth an improved ChIMP (iChIMP) that leads to a stochastic gradient
descent-based optimization in light of the exponential number of ChI inequality
constraints. An additional benefit of ChIMP/iChIMP is that it enables
eXplainable AI (XAI). Synthetic validation experiments are provided and iChIMP
is applied to the fusion of a set of heterogeneous architecture deep models in
remote sensing. We show an improvement in model accuracy and our previously
established XAI indices shed light on the quality of our data, model, and its
decisions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/karimi2019model/">Model-Agnostic Counterfactual Explanations for Consequential Decisions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Model-Agnostic Counterfactual Explanations for Consequential Decisions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Model-Agnostic Counterfactual Explanations for Consequential Decisions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Amir-Hossein Karimi, Gilles Barthe, Borja Balle, Isabel Valera</td>
	<td></td>
	<td><p>Predictive models are being increasingly used to support consequential
decision making at the individual level in contexts such as pretrial bail and
loan approval. As a result, there is increasing social and legal pressure to
provide explanations that help the affected individuals not only to understand
why a prediction was output, but also how to act to obtain a desired outcome.
To this end, several works have proposed optimization-based methods to generate
nearest counterfactual explanations. However, these methods are often
restricted to a particular subset of models (e.g., decision trees or linear
models) and differentiable distance functions. In contrast, we build on
standard theory and tools from formal verification and propose a novel
algorithm that solves a sequence of satisfiability problems, where both the
distance function (objective) and predictive model (constraints) are
represented as logic formulae. As shown by our experiments on real-world data,
our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable,
{non-}convex); ii) data-type-agnostic (heterogeneous features); iii)
distance-agnostic ($\ell_0, \ell_1, \ell_\infty$, and combinations thereof);
iv) able to generate plausible and diverse counterfactuals for any sample
(i.e., 100% coverage); and v) at provably optimal distances.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/sokol2019explainability/">Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainability Fact Sheets: A Framework for Systematic Assessment of Explainable Approaches' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kacper Sokol, Peter Flach</td>
	<td></td>
	<td><p>Explanations in Machine Learning come in many forms, but a consensus
regarding their desired properties is yet to emerge. In this paper we introduce
a taxonomy and a set of descriptors that can be used to characterise and
systematically assess explainable systems along five key dimensions:
functional, operational, usability, safety and validation. In order to design a
comprehensive and representative taxonomy and associated descriptors we
surveyed the eXplainable Artificial Intelligence literature, extracting the
criteria and desiderata that other authors have proposed or implicitly used in
their research. The survey includes papers introducing new explainability
algorithms to see what criteria are used to guide their development and how
these algorithms are evaluated, as well as papers proposing such criteria from
both computer science and social science perspectives. This novel framework
allows to systematically compare and contrast explainability approaches, not
just to better understand their capabilities but also to identify discrepancies
between their theoretical qualities and properties of their implementations. We
developed an operationalisation of the framework in the form of Explainability
Fact Sheets, which enable researchers and practitioners alike to quickly grasp
capabilities and limitations of a particular explainable method. When used as a
Work Sheet, our taxonomy can guide the development of new explainability
approaches by aiding in their critical evaluation along the five proposed
dimensions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/wang2019learning/">Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yichang Wang, Rémi Emonet, Elisa Fromont, Simon Malinowski, Etienne Menager, Loïc Mosser, Romain Tavenard</td>
	<td></td>
	<td><p>Times series classification can be successfully tackled by jointly learning a
shapelet-based representation of the series in the dataset and classifying the
series according to this representation. However, although the learned
shapelets are discriminative, they are not always similar to pieces of a real
series in the dataset. This makes it difficult to interpret the decision, i.e.
difficult to analyze if there are particular behaviors in a series that
triggered the decision. In this paper, we make use of a simple convolutional
network to tackle the time series classification task and we introduce an
adversarial regularization to constrain the model to learn more interpretable
shapelets. Our classification results on all the usual time series benchmarks
are comparable with the results obtained by similar state-of-the-art algorithms
but our adversarially regularized method learns shapelets that are, by design,
interpretable.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kang2019interpreting/">Interpreting Undesirable Pixels for Image Classification on Black-Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Undesirable Pixels for Image Classification on Black-Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Undesirable Pixels for Image Classification on Black-Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sin-Han Kang, Hong-Gyu Jung, Seong-Whan Lee</td>
	<td></td>
	<td><p>In an effort to interpret black-box models, researches for developing
explanation methods have proceeded in recent years. Most studies have tried to
identify input pixels that are crucial to the prediction of a classifier. While
this approach is meaningful to analyse the characteristic of blackbox models,
it is also important to investigate pixels that interfere with the prediction.
To tackle this issue, in this paper, we propose an explanation method that
visualizes undesirable regions to classify an image as a target class. To be
specific, we divide the concept of undesirable regions into two terms: (1)
factors for a target class, which hinder that black-box models identify
intrinsic characteristics of a target class and (2) factors for non-target
classes that are important regions for an image to be classified as other
classes. We visualize such undesirable regions on heatmaps to qualitatively
validate the proposed method. Furthermore, we present an evaluation metric to
provide quantitative results on ImageNet.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kang2019towards/">Towards Interpretable Deep Extreme Multi-label Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Extreme Multi-label Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Extreme Multi-label Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yihuang Kang, I-Ling Cheng, Wenjui Mao, Bowen Kuo, Pei-Ju Lee</td>
	<td></td>
	<td><p>Many Machine Learning algorithms, such as deep neural networks, have long
been criticized for being “black-boxes”-a kind of models unable to provide how
it arrive at a decision without further efforts to interpret. This problem has
raised concerns on model applications’ trust, safety, nondiscrimination, and
other ethical issues. In this paper, we discuss the machine learning
interpretability of a real-world application, eXtreme Multi-label Learning
(XML), which involves learning models from annotated data with many pre-defined
labels. We propose a two-step XML approach that combines deep non-negative
autoencoder with other multi-label classifiers to tackle different data
applications with a large number of labels. Our experimental result shows that
the proposed approach is able to cope with many-label problems as well as to
provide interpretable label hierarchies and dependencies that helps us
understand how the model recognizes the existences of objects in an image.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/li2019interpretable/">Interpretable Neural Network Decoupling</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Neural Network Decoupling' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Neural Network Decoupling' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yuchao Li, Rongrong Ji, Shaohui Lin, Baochang Zhang, Chenqian Yan, Yongjian Wu, Feiyue Huang, Ling Shao</td>
	<td></td>
	<td><p>The remarkable performance of convolutional neural networks (CNNs) is
entangled with their huge number of uninterpretable parameters, which has
become the bottleneck limiting the exploitation of their full potential.
Towards network interpretation, previous endeavors mainly resort to the single
filter analysis, which however ignores the relationship between filters. In
this paper, we propose a novel architecture decoupling method to interpret the
network from a perspective of investigating its calculation paths. More
specifically, we introduce a novel architecture controlling module in each
layer to encode the network architecture by a vector. By maximizing the mutual
information between the vectors and input images, the module is trained to
select specific filters to distill a unique calculation path for each input.
Furthermore, to improve the interpretability and compactness of the decoupled
network, the output of each layer is encoded to align the architecture encoding
vector with the constraint of sparsity regularization. Unlike conventional
pixel-level or filter-level network interpretation methods, we propose a
path-level analysis to explore the relationship between the combination of
filter and semantic concepts, which is more suitable to interpret the working
rationale of the decoupled network. Extensive experiments show that the
decoupled network achieves several applications, i.e., network interpretation,
network acceleration, and adversarial samples detection.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/li2019deepgcns/">DeepGCNs: Can GCNs Go as Deep as CNNs?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=DeepGCNs: Can GCNs Go as Deep as CNNs?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=DeepGCNs: Can GCNs Go as Deep as CNNs?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Guohao Li, Matthias Müller, Ali Thabet, Bernard Ghanem</td>
	<td></td>
	<td><p>Convolutional Neural Networks (CNNs) achieve impressive performance in a wide
variety of fields. Their success benefited from a massive boost when very deep
CNN models were able to be reliably trained. Despite their merits, CNNs fail to
properly address problems with non-Euclidean data. To overcome this challenge,
Graph Convolutional Networks (GCNs) build graphs to represent non-Euclidean
data, borrow concepts from CNNs, and apply them in training. GCNs show
promising results, but they are usually limited to very shallow models due to
the vanishing gradient problem. As a result, most state-of-the-art GCN models
are no deeper than 3 or 4 layers. In this work, we present new ways to
successfully train very deep GCNs. We do this by borrowing concepts from CNNs,
specifically residual/dense connections and dilated convolutions, and adapting
them to GCN architectures. Extensive experiments show the positive effect of
these deep GCN frameworks. Finally, we use these new concepts to build a very
deep 56-layer GCN, and show how it significantly boosts performance (+3.7% mIoU
over state-of-the-art) in the task of point cloud semantic segmentation. We
believe that the community can greatly benefit from this work, as it opens up
many opportunities for advancing GCN-based research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/roscher2019explainable/">Explainable Machine Learning for Scientific Insights and Discoveries</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Machine Learning for Scientific Insights and Discoveries' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Machine Learning for Scientific Insights and Discoveries' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ribana Roscher, Bastian Bohn, Marco F. Duarte, Jochen Garcke</td>
	<td>IEEE Access, vol. 8, pp. 42200-42216, 2020</td>
	<td><p>Machine learning methods have been remarkably successful for a wide range of
application areas in the extraction of essential information from data. An
exciting and relatively recent development is the uptake of machine learning in
the natural sciences, where the major goal is to obtain novel scientific
insights and discoveries from observational or simulated data. A prerequisite
for obtaining a scientific outcome is domain knowledge, which is needed to gain
explainability, but also to enhance scientific consistency. In this article we
review explainable machine learning in view of applications in the natural
sciences and discuss three core elements which we identified as relevant in
this context: transparency, interpretability, and explainability. With respect
to these core elements, we provide a survey of recent scientific works that
incorporate machine learning and the way that explainable machine learning is
used in combination with domain knowledge from the application areas.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/stylianou2019visualizing/">Visualizing Deep Similarity Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing Deep Similarity Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing Deep Similarity Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abby Stylianou, Richard Souvenir, Robert Pless</td>
	<td></td>
	<td><p>For convolutional neural network models that optimize an image embedding, we
propose a method to highlight the regions of images that contribute most to
pairwise similarity. This work is a corollary to the visualization tools
developed for classification networks, but applicable to the problem domains
better suited to similarity learning. The visualization shows how similarity
networks that are fine-tuned learn to focus on different features. We also
generalize our approach to embedding networks that use different pooling
strategies and provide a simple mechanism to support image similarity searches
on objects or sub-regions in the query image.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yu2019interpreting/">Interpreting and Evaluating Neural Network Robustness</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting and Evaluating Neural Network Robustness' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting and Evaluating Neural Network Robustness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen</td>
	<td></td>
	<td><p>Recently, adversarial deception becomes one of the most considerable threats
to deep neural networks. However, compared to extensive research in new designs
of various adversarial attacks and defenses, the neural networks’ intrinsic
robustness property is still lack of thorough investigation. This work aims to
qualitatively interpret the adversarial attack and defense mechanism through
loss visualization, and establish a quantitative metric to evaluate the neural
network model’s intrinsic robustness. The proposed robustness metric identifies
the upper bound of a model’s prediction divergence in the given domain and thus
indicates whether the model can maintain a stable prediction. With extensive
experiments, our metric demonstrates several advantages over conventional
adversarial testing accuracy based robustness estimation: (1) it provides a
uniformed evaluation to models with different structures and parameter scales;
(2) it over-performs conventional accuracy based robustness estimation and
provides a more reliable evaluation that is invariant to different test
settings; (3) it can be fast generated without considerable testing cost.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/buhrmester2019analysis/">Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vanessa Buhrmester, David Münch, Michael Arens</td>
	<td></td>
	<td><p>Deep Learning is a state-of-the-art technique to make inference on extensive
or complex data. As a black box model due to their multilayer nonlinear
structure, Deep Neural Networks are often criticized to be non-transparent and
their predictions not traceable by humans. Furthermore, the models learn from
artificial datasets, often with bias or contaminated discriminating content.
Through their increased distribution, decision-making algorithms can contribute
promoting prejudge and unfairness which is not easy to notice due to lack of
transparency. Hence, scientists developed several so-called explanators or
explainers which try to point out the connection between input and output to
represent in a simplified way the inner structure of machine learning black
boxes. In this survey we differ the mechanisms and properties of explaining
systems for Deep Neural Networks for Computer Vision tasks. We give a
comprehensive overview about taxonomy of related studies and compare several
survey papers that deal with explainability in general. We work out the
drawbacks and gaps and summarize further research ideas.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhao2019confounder/">Confounder-Aware Visualization of ConvNets</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Confounder-Aware Visualization of ConvNets' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Confounder-Aware Visualization of ConvNets' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Qingyu Zhao, Ehsan Adeli, Adolf Pfefferbaum, Edith V. Sullivan, Kilian M. Pohl</td>
	<td></td>
	<td><p>With recent advances in deep learning, neuroimaging studies increasingly rely
on convolutional networks (ConvNets) to predict diagnosis based on MR images.
To gain a better understanding of how a disease impacts the brain, the studies
visualize the salience maps of the ConvNet highlighting voxels within the brain
majorly contributing to the prediction. However, these salience maps are
generally confounded, i.e., some salient regions are more predictive of
confounding variables (such as age) than the diagnosis. To avoid such
misinterpretation, we propose in this paper an approach that aims to visualize
confounder-free saliency maps that only highlight voxels predictive of the
diagnosis. The approach incorporates univariate statistical tests to identify
confounding effects within the intermediate features learned by ConvNet. The
influence from the subset of confounded features is then removed by a novel
partial back-propagation procedure. We use this two-step approach to visualize
confounder-free saliency maps extracted from synthetic and two real datasets.
These experiments reveal the potential of our visualization in producing
unbiased model-interpretation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bodell2019interpretable/">Interpretable Word Embeddings via Informative Priors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Word Embeddings via Informative Priors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Word Embeddings via Informative Priors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Miriam Hurtado Bodell, Martin Arvidsson, Måns Magnusson</td>
	<td></td>
	<td><p>Word embeddings have demonstrated strong performance on NLP tasks. However,
lack of interpretability and the unsupervised nature of word embeddings have
limited their use within computational social science and digital humanities.
We propose the use of informative priors to create interpretable and
domain-informed dimensions for probabilistic word embeddings. Experimental
results show that sensible priors can capture latent semantic concepts better
than or on-par with the current state of the art, while retaining the
simplicity and generalizability of using priors.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bhatt2019explainable/">Explainable Machine Learning in Deployment</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Machine Learning in Deployment' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Machine Learning in Deployment' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, Ruchir Puri, José M. F. Moura, Peter Eckersley</td>
	<td></td>
	<td><p>Explainable machine learning offers the potential to provide stakeholders
with insights into model behavior by using various methods such as feature
importance scores, counterfactual explanations, or influential training data.
Yet there is little understanding of how organizations use these methods in
practice. This study explores how organizations view and use explainability for
stakeholder consumption. We find that, currently, the majority of deployments
are not for end users affected by the model but rather for machine learning
engineers, who use explainability to debug the model itself. There is thus a
gap between explainability in practice and the goal of transparency, since
explanations primarily serve internal stakeholders rather than external ones.
Our study synthesizes the limitations of current explainability techniques that
hamper their use for end users. To facilitate end user interaction, we develop
a framework for establishing clear goals for explainability. We end by
discussing concerns raised regarding explainability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yang2019benchmarking/">Benchmarking Attribution Methods with Relative Feature Importance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Benchmarking Attribution Methods with Relative Feature Importance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Benchmarking Attribution Methods with Relative Feature Importance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mengjiao Yang, Been Kim</td>
	<td></td>
	<td><p>Interpretability is an important area of research for safe deployment of
machine learning systems. One particular type of interpretability method
attributes model decisions to input features. Despite active development,
quantitative evaluation of feature attribution methods remains difficult due to
the lack of ground truth: we do not know which input features are in fact
important to a model. In this work, we propose a framework for Benchmarking
Attribution Methods (BAM) with a priori knowledge of relative feature
importance. BAM includes 1) a carefully crafted dataset and models trained with
known relative feature importance and 2) three complementary metrics to
quantitatively evaluate attribution methods by comparing feature attributions
between pairs of models and pairs of inputs. Our evaluation on several
widely-used attribution methods suggests that certain methods are more likely
to produce false positive explanations—features that are incorrectly
attributed as more important to model prediction. We open source our dataset,
models, and metrics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/mueller2019explanation/">Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shane T. Mueller, Robert R. Hoffman, William Clancey, Abigail Emrey, Gary Klein</td>
	<td></td>
	<td><p>This is an integrative review that address the question, “What makes for a
good explanation?” with reference to AI systems. Pertinent literatures are
vast. Thus, this review is necessarily selective. That said, most of the key
concepts and issues are expressed in this Report. The Report encapsulates the
history of computer science efforts to create systems that explain and instruct
(intelligent tutoring systems and expert systems). The Report expresses the
explainability issues and challenges in modern AI, and presents capsule views
of the leading psychological theories of explanation. Certain articles stand
out by virtue of their particular relevance to XAI, and their methods, results,
and key points are highlighted. It is recommended that AI/XAI researchers be
encouraged to include in their research reports fuller details on their
empirical or experimental methods, in the fashion of experimental psychology
research reports: details on Participants, Instructions, Procedures, Tasks,
Dependent Variables (operational definitions of the measures and metrics),
Independent Variables (conditions), and Control Conditions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ming2019interpretable/">Interpretable and Steerable Sequence Learning via Prototypes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable and Steerable Sequence Learning via Prototypes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable and Steerable Sequence Learning via Prototypes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yao Ming, Panpan Xu, Huamin Qu, Liu Ren</td>
	<td>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019</td>
	<td><p>One of the major challenges in machine learning nowadays is to provide
predictions with not only high accuracy but also user-friendly explanations.
Although in recent years we have witnessed increasingly popular use of deep
neural networks for sequence modeling, it is still challenging to explain the
rationales behind the model outputs, which is essential for building trust and
supporting the domain experts to validate, critique and refine the model. We
propose ProSeNet, an interpretable and steerable deep sequence model with
natural explanations derived from case-based reasoning. The prediction is
obtained by comparing the inputs to a few prototypes, which are exemplar cases
in the problem domain. For better interpretability, we define several criteria
for constructing the prototypes, including simplicity, diversity, and sparsity
and propose the learning objective and the optimization procedure. ProSeNet
also provides a user-friendly approach to model steering: domain experts
without any knowledge on the underlying model or parameters can easily
incorporate their intuition and experience by manually refining the prototypes.
We conduct experiments on a wide range of real-world applications, including
predictive diagnostics for automobiles, ECG, and protein sequence
classification and sentiment analysis on texts. The result shows that ProSeNet
can achieve accuracy on par with state-of-the-art deep learning models. We also
evaluate the interpretability of the results with concrete case studies.
Finally, through user study on Amazon Mechanical Turk (MTurk), we demonstrate
that the model selects high-quality prototypes which align well with human
knowledge and can be interactively refined for better interpretability without
loss of performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/yeh2019infidelity/">On the (In)fidelity and Sensitivity for Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the (In)fidelity and Sensitivity for Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the (In)fidelity and Sensitivity for Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I. Inouye, Pradeep Ravikumar</td>
	<td></td>
	<td><p>We consider objective evaluation measures of saliency explanations for
complex black-box machine learning models. We propose simple robust variants of
two notions that have been considered in recent literature: (in)fidelity, and
sensitivity. We analyze optimal explanations with respect to both these
measures, and while the optimal explanation for sensitivity is a vacuous
constant explanation, the optimal explanation for infidelity is a novel
combination of two popular explanation methods. By varying the perturbation
distribution that defines infidelity, we obtain novel explanations by
optimizing infidelity, which we show to out-perform existing explanations in
both quantitative and qualitative measurements. Another salient question given
these measures is how to modify any given explanation to have better values
with respect to these measures. We propose a simple modification based on
lowering sensitivity, and moreover show that when done appropriately, we could
simultaneously improve both sensitivity as well as fidelity.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/barbado2019rule/">Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alberto Barbado, Óscar Corcho, Richard Benjamins</td>
	<td>Expert Systems with Applications Volume 189, 1 March 2022, 116100</td>
	<td><p>OneClass SVM is a popular method for unsupervised anomaly detection. As many
other methods, it suffers from the black box problem: it is difficult to
justify, in an intuitive and simple manner, why the decision frontier is
identifying data points as anomalous or non anomalous. Such type of problem is
being widely addressed for supervised models. However, it is still an uncharted
area for unsupervised learning. In this paper, we evaluate several rule
extraction techniques over OneClass SVM models, as well as present alternative
designs for some of those algorithms. Together with that, we propose algorithms
to compute metrics related with eXplainable Artificial Intelligence (XAI)
regarding the “comprehensibility”, “representativeness”, “stability” and
“diversity” of the extracted rules. We evaluate our proposals with different
datasets, including real-world data coming from industry. With this, our
proposal contributes to extend XAI techniques to unsupervised machine learning
models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bansal2019case/">A Case for Backward Compatibility for Human-AI Teams</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Case for Backward Compatibility for Human-AI Teams' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Case for Backward Compatibility for Human-AI Teams' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Gagan Bansal, Besmira Nushi, Ece Kamar, Dan Weld, Walter Lasecki, Eric Horvitz</td>
	<td></td>
	<td><p>AI systems are being deployed to support human decision making in high-stakes
domains. In many cases, the human and AI form a team, in which the human makes
decisions after reviewing the AI’s inferences. A successful partnership
requires that the human develops insights into the performance of the AI
system, including its failures. We study the influence of updates to an AI
system in this setting. While updates can increase the AI’s predictive
performance, they may also lead to changes that are at odds with the user’s
prior experiences and confidence in the AI’s inferences, hurting therefore the
overall team performance. We introduce the notion of the compatibility of an AI
update with prior user experience and present methods for studying the role of
compatibility in human-AI teams. Empirical results on three high-stakes domains
show that current machine learning algorithms do not produce compatible
updates. We propose a re-training objective to improve the compatibility of an
update by penalizing new errors. The objective offers full leverage of the
performance/compatibility tradeoff, enabling more compatible yet accurate
updates.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/vashishth2019attention/">Attention Interpretability Across NLP Tasks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attention Interpretability Across NLP Tasks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attention Interpretability Across NLP Tasks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh Tomar, Manaal Faruqui</td>
	<td></td>
	<td><p>The attention layer in a neural network model provides insights into the
model’s reasoning behind its prediction, which are usually criticized for being
opaque. Recently, seemingly contradictory viewpoints have emerged about the
interpretability of attention weights (Jain &amp; Wallace, 2019; Vig &amp; Belinkov,
2019). Amid such confusion arises the need to understand attention mechanism
more systematically. In this work, we attempt to fill this gap by giving a
comprehensive explanation which justifies both kinds of observations (i.e.,
when is attention interpretable and when it is not). Through a series of
experiments on diverse NLP tasks, we validate our observations and reinforce
our claim of interpretability of attention through manual evaluation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/bai2019rectified/">Rectified Decision Trees: Towards Interpretability, Compression and Empirical Soundness</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Rectified Decision Trees: Towards Interpretability, Compression and Empirical Soundness' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Rectified Decision Trees: Towards Interpretability, Compression and Empirical Soundness' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiawang Bai, Yiming Li, Jiawei Li, Yong Jiang, Shutao Xia</td>
	<td></td>
	<td><p>How to obtain a model with good interpretability and performance has always
been an important research topic. In this paper, we propose rectified decision
trees (ReDT), a knowledge distillation based decision trees rectification with
high interpretability, small model size, and empirical soundness. Specifically,
we extend the impurity calculation and the pure ending condition of the
classical decision tree to propose a decision tree extension that allows the
use of soft labels generated by a well-trained teacher model in training and
prediction process. It is worth noting that for the acquisition of soft labels,
we propose a new multiple cross-validation based method to reduce the effects
of randomness and overfitting. These approaches ensure that ReDT retains
excellent interpretability and even achieves fewer nodes than the decision tree
in the aspect of compression while having relatively good performance. Besides,
in contrast to traditional knowledge distillation, back propagation of the
student model is not necessarily required in ReDT, which is an attempt of a new
knowledge distillation approach. Extensive experiments are conducted, which
demonstrates the superiority of ReDT in interpretability, compression, and
empirical soundness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kasirzadeh2019mathematical/">Mathematical decisions and non-causal elements of explainable AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Mathematical decisions and non-causal elements of explainable AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Mathematical decisions and non-causal elements of explainable AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Atoosa Kasirzadeh</td>
	<td></td>
	<td><p>The social implications of algorithmic decision-making in sensitive contexts
have generated lively debates among multiple stakeholders, such as moral and
political philosophers, computer scientists, and the public. Yet, the lack of a
common language and a conceptual framework for an appropriate bridging of the
moral, technical, and political aspects of the debate prevents the discussion
to be as effective as it can be. Social scientists and psychologists are
contributing to this debate by gathering a wealth of empirical data, yet a
philosophical analysis of the social implications of algorithmic
decision-making remains comparatively impoverished. In attempting to address
this lacuna, this paper argues that a hierarchy of different types of
explanations for why and how an algorithmic decision outcome is achieved can
establish the relevant connection between the moral and technical aspects of
algorithmic decision-making. In particular, I offer a multi-faceted conceptual
framework for the explanations and the interpretations of algorithmic
decisions, and I claim that this framework can lay the groundwork for a focused
discussion among multiple stakeholders about the social implications of
algorithmic decision-making, as well as AI governance and ethics more
generally.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/rieger2019interpretations/">Interpretations are useful: penalizing explanations to align neural networks with prior knowledge</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretations are useful: penalizing explanations to align neural networks with prior knowledge' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretations are useful: penalizing explanations to align neural networks with prior knowledge' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Laura Rieger, Chandan Singh, W. James Murdoch, Bin Yu</td>
	<td></td>
	<td><p>For an explanation of a deep learning model to be effective, it must provide
both insight into a model and suggest a corresponding action in order to
achieve some objective. Too often, the litany of proposed explainable deep
learning methods stop at the first step, providing practitioners with insight
into a model, but no way to act on it. In this paper, we propose contextual
decomposition explanation penalization (CDEP), a method which enables
practitioners to leverage existing explanation methods in order to increase the
predictive accuracy of deep learning models. In particular, when shown that a
model has incorrectly assigned importance to some features, CDEP enables
practitioners to correct these errors by directly regularizing the provided
explanations. Using explanations provided by contextual decomposition (CD)
(Murdoch et al., 2018), we demonstrate the ability of our method to increase
performance on an array of toy and real datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/arya2019one/">One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI Explainability Techniques' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar, Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss, Aleksandra Mojsilović, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra, John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh, Kush R. Varshney, Dennis Wei, Yunfeng Zhang</td>
	<td></td>
	<td><p>As artificial intelligence and machine learning algorithms make further
inroads into society, calls are increasing from multiple stakeholders for these
algorithms to explain their outputs. At the same time, these stakeholders,
whether they be affected citizens, government regulators, domain experts, or
system developers, present different requirements for explanations. Toward
addressing these needs, we introduce AI Explainability 360
(http://aix360.mybluemix.net/), an open-source software toolkit featuring eight
diverse and state-of-the-art explainability methods and two evaluation metrics.
Equally important, we provide a taxonomy to help entities requiring
explanations to navigate the space of explanation methods, not only those in
the toolkit but also in the broader literature on explainability. For data
scientists and other users of the toolkit, we have implemented an extensible
software architecture that organizes methods according to their place in the AI
modeling pipeline. We also discuss enhancements to bring research innovations
closer to consumers of explanations, ranging from simplified, more accessible
versions of algorithms, to tutorials and an interactive web demo to introduce
AI explainability to different audiences and application domains. Together, our
toolkit and taxonomy can help identify gaps where more explainability methods
are needed and provide a platform to incorporate them as they are developed.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/zhang2019towards/">Towards a Unified Evaluation of Explanation Methods without Ground Truth</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards a Unified Evaluation of Explanation Methods without Ground Truth' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards a Unified Evaluation of Explanation Methods without Ground Truth' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hao Zhang, Jiayi Chen, Haotian Xue, Quanshi Zhang</td>
	<td></td>
	<td><p>This paper proposes a set of criteria to evaluate the objectiveness of
explanation methods of neural networks, which is crucial for the development of
explainable AI, but it also presents significant challenges. The core challenge
is that people usually cannot obtain ground-truth explanations of the neural
network. To this end, we design four metrics to evaluate explanation results
without ground-truth explanations. Our metrics can be broadly applied to nine
benchmark methods of interpreting neural networks, which provides new insights
of explanation methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/arras2019evaluating/">Evaluating Recurrent Neural Network Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Recurrent Neural Network Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Recurrent Neural Network Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leila Arras, Ahmed Osman, Klaus-Robert Müller, Wojciech Samek</td>
	<td></td>
	<td><p>Recently, several methods have been proposed to explain the predictions of
recurrent neural networks (RNNs), in particular of LSTMs. The goal of these
methods is to understand the network’s decisions by assigning to each input
variable, e.g., a word, a relevance indicating to which extent it contributed
to a particular prediction. In previous works, some of these methods were not
yet compared to one another, or were evaluated only qualitatively. We close
this gap by systematically and quantitatively comparing these methods in
different settings, namely (1) a toy arithmetic task which we use as a sanity
check, (2) a five-class sentiment prediction of movie reviews, and besides (3)
we explore the usefulness of word relevances to build sentence-level
representations. Lastly, using the method that performed best in our
experiments, we show how specific linguistic phenomena such as the negation in
sentiment analysis reflect in terms of relevance patterns, and how the
relevance visualization can help to understand the misclassification of
individual samples.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/arras2019explaining/">Explaining and Interpreting LSTMs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining and Interpreting LSTMs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining and Interpreting LSTMs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leila Arras, Jose A. Arjona-Medina, Michael Widrich, Grégoire Montavon, Michael Gillhofer, Klaus-Robert Müller, Sepp Hochreiter, Wojciech Samek</td>
	<td></td>
	<td><p>While neural networks have acted as a strong unifying force in the design of
modern AI systems, the neural network architectures themselves remain highly
heterogeneous due to the variety of tasks to be solved. In this chapter, we
explore how to adapt the Layer-wise Relevance Propagation (LRP) technique used
for explaining the predictions of feed-forward networks to the LSTM
architecture used for sequential data modeling and forecasting. The special
accumulators and gated interactions present in the LSTM require both a new
propagation scheme and an extension of the underlying theoretical framework to
deliver faithful explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/pruthi2019learning/">Learning to Deceive with Attention-Based Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Deceive with Attention-Based Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Deceive with Attention-Based Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Danish Pruthi, Mansi Gupta, Bhuwan Dhingra, Graham Neubig, Zachary C. Lipton</td>
	<td></td>
	<td><p>Attention mechanisms are ubiquitous components in neural architectures
applied to natural language processing. In addition to yielding gains in
predictive accuracy, attention weights are often claimed to confer
interpretability, purportedly useful both for providing insights to
practitioners and for explaining why a model makes its decisions to
stakeholders. We call the latter use of attention mechanisms into question by
demonstrating a simple method for training models to produce deceptive
attention masks. Our method diminishes the total weight assigned to designated
impermissible tokens, even when the models can be shown to nevertheless rely on
these features to drive predictions. Across multiple models and tasks, our
approach manipulates attention weights while paying surprisingly little cost in
accuracy. Through a human study, we show that our manipulated attention-based
explanations deceive people into thinking that predictions from a model biased
against gender minorities do not rely on the gender. Consequently, our results
cast doubt on attention’s reliability as a tool for auditing algorithms in the
context of fairness and accountability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/rieger2019aggregating/">Aggregating explanation methods for stable and robust explainability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Aggregating explanation methods for stable and robust explainability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Aggregating explanation methods for stable and robust explainability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Laura Rieger, Lars Kai Hansen</td>
	<td></td>
	<td><p>Despite a growing literature on explaining neural networks, no consensus has
been reached on how to explain a neural network decision or how to evaluate an
explanation. Our contributions in this paper are twofold. First, we investigate
schemes to combine explanation methods and reduce model uncertainty to obtain a
single aggregated explanation. We provide evidence that the aggregation is
better at identifying important features, than on individual methods.
Adversarial attacks on explanations is a recent active research topic. As our
second contribution, we present evidence that aggregate explanations are much
more robust to attacks than individual explanation methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/unceta2019copying/">Copying Machine Learning Classifiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Copying Machine Learning Classifiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Copying Machine Learning Classifiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Irene Unceta, Jordi Nin, Oriol Pujol</td>
	<td></td>
	<td><p>We study model-agnostic copies of machine learning classifiers. We develop
the theory behind the problem of copying, highlighting its differences with
that of learning, and propose a framework to copy the functionality of any
classifier using no prior knowledge of its parameters or training data
distribution. We identify the different sources of loss and provide guidelines
on how best to generate synthetic sets for the copying process. We further
introduce a set of metrics to evaluate copies in practice. We validate our
framework through extensive experiments using data from a series of well-known
problems. We demonstrate the value of copies in use cases where desiderata such
as interpretability, fairness or productivization constrains need to be
addressed. Results show that copies can be exploited to enhance existing
solutions and improve them adding new features and characteristics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/warnecke2019evaluating/">Evaluating Explanation Methods for Deep Learning in Security</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating Explanation Methods for Deep Learning in Security' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating Explanation Methods for Deep Learning in Security' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alexander Warnecke, Daniel Arp, Christian Wressnegger, Konrad Rieck</td>
	<td></td>
	<td><p>Deep learning is increasingly used as a building block of security systems.
Unfortunately, neural networks are hard to interpret and typically opaque to
the practitioner. The machine learning community has started to address this
problem by developing methods for explaining the predictions of neural
networks. While several of these approaches have been successfully applied in
the area of computer vision, their application in security has received little
attention so far. It is an open question which explanation methods are
appropriate for computer security and what requirements they need to satisfy.
In this paper, we introduce criteria for comparing and evaluating explanation
methods in the context of computer security. These cover general properties,
such as the accuracy of explanations, as well as security-focused aspects, such
as the completeness, efficiency, and robustness. Based on our criteria, we
investigate six popular explanation methods and assess their utility in
security systems for malware detection and vulnerability discovery. We observe
significant differences between the methods and build on these to derive
general recommendations for selecting and applying explanation methods in
computer security.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/ray2019explain/">Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Can You Explain That? Lucid Explanations Help Human-AI Collaborative Image Retrieval' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Arijit Ray, Yi Yao, Rakesh Kumar, Ajay Divakaran, Giedrius Burachas</td>
	<td>2019 AAAI Conference on Human Computation and Crowdsourcing</td>
	<td><p>While there have been many proposals on making AI algorithms explainable, few
have attempted to evaluate the impact of AI-generated explanations on human
performance in conducting human-AI collaborative tasks. To bridge the gap, we
propose a Twenty-Questions style collaborative image retrieval game,
Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy
of explanations (visual evidence or textual justification) in the context of
Visual Question Answering (VQA). In our proposed ExAG, a human user needs to
guess a secret image picked by the VQA agent by asking natural language
questions to it. We show that overall, when AI explains its answers, users
succeed more often in guessing the secret image correctly. Notably, a few
correct explanations can readily improve human performance when VQA answers are
mostly incorrect as compared to no-explanation games. Furthermore, we also show
that while explanations rated as “helpful” significantly improve human
performance, “incorrect” and “unhelpful” explanations can degrade performance
as compared to no-explanation games. Our experiments, therefore, demonstrate
that ExAG is an effective means to evaluate the efficacy of AI-generated
explanations on a human-AI collaborative task.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/samek2019towards/">Towards Explainable Artificial Intelligence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explainable Artificial Intelligence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explainable Artificial Intelligence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wojciech Samek, Klaus-Robert Müller</td>
	<td></td>
	<td><p>In recent years, machine learning (ML) has become a key enabling technology
for the sciences and industry. Especially through improvements in methodology,
the availability of large databases and increased computational power, today’s
ML algorithms are able to achieve excellent performance (at times even
exceeding the human level) on an increasing number of complex tasks. Deep
learning models are at the forefront of this development. However, due to their
nested non-linear structure, these powerful models have been generally
considered “black boxes”, not providing any information about what exactly
makes them arrive at their predictions. Since in many applications, e.g., in
the medical domain, such lack of transparency may be not acceptable, the
development of methods for visualizing, explaining and interpreting deep
learning models has recently attracted increasing attention. This introductory
paper presents recent developments and applications in this field and makes a
plea for a wider use of explainable learning algorithms in practice.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/mothilal2019explaining/">Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ramaravind Kommiya Mothilal, Amit Sharma, Chenhao Tan</td>
	<td>Conference on Fairness, Accountability, and Transparency (FAT* 2020)</td>
	<td><p>Post-hoc explanations of machine learning models are crucial for people to
understand and act on algorithmic predictions. An intriguing class of
explanations is through counterfactuals, hypothetical examples that show people
how to obtain a different prediction. We posit that effective counterfactual
explanations should satisfy two properties: feasibility of the counterfactual
actions given user context and constraints, and diversity among the
counterfactuals presented. To this end, we propose a framework for generating
and evaluating a diverse set of counterfactual explanations based on
determinantal point processes. To evaluate the actionability of
counterfactuals, we provide metrics that enable comparison of
counterfactual-based methods to other local explanation methods. We further
address necessary tradeoffs and point to causal implications in optimizing for
counterfactuals. Our experiments on four real-world datasets show that our
framework can generate a set of counterfactuals that are diverse and well
approximate local decision boundaries, outperforming prior approaches to
generating diverse counterfactuals. We provide an implementation of the
framework at https://github.com/microsoft/DiCE.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lai2019many/">Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Many Faces of Feature Importance: Comparing Built-in and Post-hoc Feature Importance in Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vivian Lai, Jon Z. Cai, Chenhao Tan</td>
	<td></td>
	<td><p>Feature importance is commonly used to explain machine predictions. While
feature importance can be derived from a machine learning model with a variety
of methods, the consistency of feature importance via different methods remains
understudied. In this work, we systematically compare feature importance from
built-in mechanisms in a model such as attention values and post-hoc methods
that approximate model behavior such as LIME. Using text classification as a
testbed, we find that 1) no matter which method we use, important features from
traditional models such as SVM and XGBoost are more similar with each other,
than with deep learning models; 2) post-hoc methods tend to generate more
similar important features for two models than built-in methods. We further
demonstrate how such similarity varies across instances. Notably, important
features do not always resemble each other better when two models agree on the
predicted label than when they disagree.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lage2019human/">Human Evaluation of Models Built for Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human Evaluation of Models Built for Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human Evaluation of Models Built for Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Isaac Lage, Emily Chen, Jeffrey He, Menaka Narayanan, Been Kim, Sam Gershman, Finale Doshi-Velez</td>
	<td>AAAI Conference on Human Computation and Crowdsourcing</td>
	<td><p>Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others–trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.</p>
</td>
	<td>evaluation </td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lakkaraju2019how/">"How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="How do I fool you?": Manipulating User Trust via Misleading Black Box Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Himabindu Lakkaraju, Osbert Bastani</td>
	<td></td>
	<td><p>As machine learning black boxes are increasingly being deployed in critical
domains such as healthcare and criminal justice, there has been a growing
emphasis on developing techniques for explaining these black boxes in a human
interpretable manner. It has recently become apparent that a high-fidelity
explanation of a black box ML model may not accurately reflect the biases in
the black box. As a consequence, explanations have the potential to mislead
human users into trusting a problematic black box. In this work, we rigorously
explore the notion of misleading explanations and how they influence user trust
in black-box models. More specifically, we propose a novel theoretical
framework for understanding and generating misleading explanations, and carry
out a user study with domain experts to demonstrate how these explanations can
be used to mislead users. Our work is the first to empirically establish how
user trust in black box models can be manipulated via misleading explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/schmidt2019quantifying/">Quantifying Interpretability and Trust in Machine Learning Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Quantifying Interpretability and Trust in Machine Learning Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Quantifying Interpretability and Trust in Machine Learning Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Philipp Schmidt, Felix Biessmann</td>
	<td></td>
	<td><p>Decisions by Machine Learning (ML) models have become ubiquitous. Trusting
these decisions requires understanding how algorithms take them. Hence
interpretability methods for ML are an active focus of research. A central
problem in this context is that both the quality of interpretability methods as
well as trust in ML predictions are difficult to measure. Yet evaluations,
comparisons and improvements of trust and interpretability require quantifiable
measures. Here we propose a quantitative measure for the quality of
interpretability methods. Based on that we derive a quantitative measure of
trust in ML decisions. Building on previous work we propose to measure
intuitive understanding of algorithmic decisions using the information transfer
rate at which humans replicate ML model predictions. We provide empirical
evidence from crowdsourcing experiments that the proposed metric robustly
differentiates interpretability methods. The proposed metric also demonstrates
the value of interpretability for ML assisted human decision making: in our
experiments providing explanations more than doubled productivity in annotation
tasks. However unbiased human judgement is critical for doctors, judges, policy
makers and others. Here we derive a trust metric that identifies when human
decisions are overly biased towards ML predictions. Our results complement
existing qualitative work on trust and interpretability by quantifiable
measures that can serve as objectives for further improving methods in this
field of research.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/kim2019learning/">Learning Interpretable Models with Causal Guarantees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Interpretable Models with Causal Guarantees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Interpretable Models with Causal Guarantees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Carolyn Kim, Osbert Bastani</td>
	<td></td>
	<td><p>Machine learning has shown much promise in helping improve the quality of
medical, legal, and financial decision-making. In these applications, machine
learning models must satisfy two important criteria: (i) they must be causal,
since the goal is typically to predict individual treatment effects, and (ii)
they must be interpretable, so that human decision makers can validate and
trust the model predictions. There has recently been much progress along each
direction independently, yet the state-of-the-art approaches are fundamentally
incompatible. We propose a framework for learning interpretable models from
observational data that can be used to predict individual treatment effects
(ITEs). In particular, our framework converts any supervised learning algorithm
into an algorithm for estimating ITEs. Furthermore, we prove an error bound on
the treatment effects predicted by our model. Finally, in an experiment on
real-world data, we show that the models trained using our framework
significantly outperform a number of baselines.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/keane2019twin-system/">The Twin-System Approach as One Generic Solution for XAI: An Overview of ANN-CBR Twins for Explaining Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Twin-System Approach as One Generic Solution for XAI: An Overview of ANN-CBR Twins for Explaining Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Twin-System Approach as One Generic Solution for XAI: An Overview of ANN-CBR Twins for Explaining Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mark T. Keane, Eoin M. Kenny</td>
	<td>IJCAI 2019 Workshop on Explainable Artificial Intelligence (XAI)</td>
	<td><p>The notion of twin systems is proposed to address the eXplainable AI (XAI)
problem, where an uninterpretable black-box system is mapped to a white-box
‘twin’ that is more interpretable. In this short paper, we overview very recent
work that advances a generic solution to the XAI problem, the so called twin
system approach. The most popular twinning in the literature is that between an
Artificial Neural Networks (ANN ) as a black box and Case Based Reasoning (CBR)
system as a white-box, where the latter acts as an interpretable proxy for the
former. We outline how recent work reviving this idea has applied it to deep
learning methods. Furthermore, we detail the many fruitful directions in which
this work may be taken; such as, determining the most (i) accurate
feature-weighting methods to be used, (ii) appropriate deployments for
explanatory cases, (iii) useful cases of explanatory value to users.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/keane2019case/">How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Case Based Reasoning Explained Neural Networks: An XAI Survey of Post-Hoc Explanation-by-Example in ANN-CBR Twins' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mark T Keane, Eoin M Kenny</td>
	<td>Proceedings of the 27th International Conference on Case Based Reasoning (ICCBR-19), 2019</td>
	<td><p>This paper surveys an approach to the XAI problem, using post-hoc explanation
by example, that hinges on twinning Artificial Neural Networks (ANNs) with
Case-Based Reasoning (CBR) systems, so-called ANN-CBR twins. A systematic
survey of 1100+ papers was carried out to identify the fragmented literature on
this topic and to trace it influence through to more recent work involving Deep
Neural Networks (DNNs). The paper argues that this twin-system approach,
especially using ANN-CBR twins, presents one possible coherent, generic
solution to the XAI problem (and, indeed, XCBR problem). The paper concludes by
road-mapping some future directions for this XAI solution involving (i) further
tests of feature-weighting techniques, (iii) explorations of how explanatory
cases might best be deployed (e.g., in counterfactuals, near-miss cases, a
fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue
of human user evaluation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2019</td>
	<td><a href="/publications/lertvittayakumjorn2019humangrounded/">Human-grounded Evaluations of Explanation Methods for Text Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human-grounded Evaluations of Explanation Methods for Text Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human-grounded Evaluations of Explanation Methods for Text Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Piyawat Lertvittayakumjorn, Francesca Toni</td>
	<td></td>
	<td><p>Due to the black-box nature of deep learning models, methods for explaining
the models’ results are crucial to gain trust from humans and support
collaboration between AIs and humans. In this paper, we consider several
model-agnostic and model-specific explanation methods for CNNs for text
classification and conduct three human-grounded evaluations, focusing on
different purposes of explanations: (1) revealing model behavior, (2)
justifying model predictions, and (3) helping humans investigate uncertain
predictions. The results highlight dissimilar qualities of the various
explanation methods we consider and show the degree to which these methods
could serve for each purpose.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2018</td>
	<td><a href="/publications/hooker2018benchmark/">A Benchmark for Interpretability Methods in Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Benchmark for Interpretability Methods in Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Benchmark for Interpretability Methods in Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim</td>
	<td></td>
	<td><p>We propose an empirical measure of the approximate accuracy of feature
importance estimates in deep neural networks. Our results across several
large-scale image classification datasets show that many popular
interpretability methods produce estimates of feature importance that are not
better than a random designation of feature importance. Only certain ensemble
based approaches—VarGrad and SmoothGrad-Squared—outperform such a random
assignment of importance. The manner of ensembling remains critical, we show
that some approaches do no better then the underlying method but carry a far
higher computational burden.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/hohman2018visual/">Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau</td>
	<td></td>
	<td><p>Deep learning has recently seen rapid development and received significant
attention due to its state-of-the-art performance on previously-thought hard
problems. However, because of the internal complexity and nonlinear structure
of deep neural networks, the underlying decision making processes for why these
models are achieving such performance are challenging and sometimes mystifying
to interpret. As deep learning spreads across domains, it is of paramount
importance that we equip users of deep learning with tools for understanding
when a model works correctly, when it fails, and ultimately how to improve its
performance. Standardized toolkits for building neural networks have helped
democratize deep learning; visual analytics systems have now been developed to
support model explanation, interpretation, debugging, and improvement. We
present a survey of the role of visual analytics in deep learning research,
which highlights its short yet impactful history and thoroughly summarizes the
state-of-the-art using a human-centered interrogative framework, focusing on
the Five W’s and How (Why, Who, What, How, When, and Where). We conclude by
highlighting research directions and open research problems. This survey helps
researchers and practitioners in both visual analytics and deep learning to
quickly learn key aspects of this young and rapidly growing body of research,
whose impact spans a diverse range of domains.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/murdoch2018beyond/">Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Beyond Word Importance: Contextual Decomposition to Extract Interactions from LSTMs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>W. James Murdoch, Peter J. Liu, Bin Yu</td>
	<td></td>
	<td><p>The driving force behind the recent success of LSTMs has been their ability
to learn complex and non-linear relationships. Consequently, our inability to
describe these relationships has led to LSTMs being characterized as black
boxes. To this end, we introduce contextual decomposition (CD), an
interpretation algorithm for analysing individual predictions made by standard
LSTMs, without any changes to the underlying model. By decomposing the output
of a LSTM, CD captures the contributions of combinations of words or variables
to the final prediction of an LSTM. On the task of sentiment analysis with the
Yelp and SST data sets, we show that CD is able to reliably identify words and
phrases of contrasting sentiment, and how they are combined to yield the LSTM’s
final prediction. Using the phrase-level labels in SST, we also demonstrate
that CD is able to successfully extract positive and negative negations from an
LSTM, something which has not previously been done.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/mittelstadt2018explaining/">Explaining Explanations in AI</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Explanations in AI' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Explanations in AI' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Brent Mittelstadt, Chris Russell, Sandra Wachter</td>
	<td></td>
	<td><p>Recent work on interpretability in machine learning and AI has focused on the
building of simplified models that approximate the true criteria used to make
decisions. These models are a useful pedagogical device for teaching trained
professionals how to predict what decisions will be made by the complex system,
and most importantly how the system might break. However, when considering any
such model it’s important to remember Box’s maxim that “All models are wrong
but some are useful.” We focus on the distinction between these models and
explanations in philosophy and sociology. These models can be understood as a
“do it yourself kit” for explanations, allowing a practitioner to directly
answer “what if questions” or generate contrastive explanations without
external assistance. Although a valuable ability, giving these models as
explanations appears more difficult than necessary, and other forms of
explanation may not have the same trade-offs. We contrast the different schools
of thought on what makes an explanation, and suggest that machine learning
might benefit from viewing the problem more broadly.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/craye2018exploring/">Exploring to learn visual saliency: The RL-IAC approach</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploring to learn visual saliency: The RL-IAC approach' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploring to learn visual saliency: The RL-IAC approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Celine Craye, Timothee Lesort, David Filliat, Jean-Francois Goudou</td>
	<td></td>
	<td><p>The problem of object localization and recognition on autonomous mobile
robots is still an active topic. In this context, we tackle the problem of
learning a model of visual saliency directly on a robot. This model, learned
and improved on-the-fly during the robot’s exploration provides an efficient
tool for localizing relevant objects within their environment. The proposed
approach includes two intertwined components. On the one hand, we describe a
method for learning and incrementally updating a model of visual saliency from
a depth-based object detector. This model of saliency can also be exploited to
produce bounding box proposals around objects of interest. On the other hand,
we investigate an autonomous exploration technique to efficiently learn such a
saliency model. The proposed exploration, called Reinforcement
Learning-Intelligent Adaptive Curiosity (RL-IAC) is able to drive the robot’s
exploration so that samples selected by the robot are likely to improve the
current model of saliency. We then demonstrate that such a saliency model
learned directly on a robot outperforms several state-of-the-art saliency
techniques, and that RL-IAC can drastically decrease the required time for
learning a reliable saliency model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/narayanan2018humans/">How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Menaka Narayanan, Emily Chen, Jeffrey He, Been Kim, Sam Gershman, Finale Doshi-Velez</td>
	<td></td>
	<td><p>Recent years have seen a boom in interest in machine learning systems that
can provide a human-understandable rationale for their predictions or
decisions. However, exactly what kinds of explanation are truly
human-interpretable remains poorly understood. This work advances our
understanding of what makes explanations interpretable in the specific context
of verification. Suppose we have a machine learning system that predicts X, and
we provide rationale for this prediction X. Given an input, an explanation, and
an output, is the output consistent with the input and the supposed rationale?
Via a series of user-studies, we identify what kinds of increases in complexity
have the greatest effect on the time it takes for humans to verify the
rationale, and which seem relatively insensitive.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/vivek2018graybox/">Gray-box Adversarial Training</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Gray-box Adversarial Training' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Gray-box Adversarial Training' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vivek B. S., Konda Reddy Mopuri, R. Venkatesh Babu</td>
	<td></td>
	<td><p>Adversarial samples are perturbed inputs crafted to mislead the machine
learning systems. A training mechanism, called adversarial training, which
presents adversarial samples along with clean samples has been introduced to
learn robust models. In order to scale adversarial training for large datasets,
these perturbations can only be crafted using fast and simple methods (e.g.,
gradient ascent). However, it is shown that adversarial training converges to a
degenerate minimum, where the model appears to be robust by generating weaker
adversaries. As a result, the models are vulnerable to simple black-box
attacks. In this paper we, (i) demonstrate the shortcomings of existing
evaluation policy, (ii) introduce novel variants of white-box and black-box
attacks, dubbed gray-box adversarial attacks” based on which we propose novel
evaluation method to assess the robustness of the learned models, and (iii)
propose a novel variant of adversarial training, named Graybox Adversarial
Training” that uses intermediate versions of the models to seed the
adversaries. Experimental evaluation demonstrates that the models trained using
our method exhibit better robustness compared to both undefended and
adversarially trained model</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/ciocan2018interpretable/">Interpretable Optimal Stopping</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Optimal Stopping' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Optimal Stopping' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dragos Florin Ciocan, Velibor V. Mišić</td>
	<td></td>
	<td><p>Optimal stopping is the problem of deciding when to stop a stochastic system
to obtain the greatest reward, arising in numerous application areas such as
finance, healthcare and marketing. State-of-the-art methods for
high-dimensional optimal stopping involve approximating the value function or
the continuation value, and then using that approximation within a greedy
policy. Although such policies can perform very well, they are generally not
guaranteed to be interpretable; that is, a decision maker may not be able to
easily see the link between the current system state and the policy’s action.
In this paper, we propose a new approach to optimal stopping, wherein the
policy is represented as a binary tree, in the spirit of naturally
interpretable tree models commonly used in machine learning. We show that the
class of tree policies is rich enough to approximate the optimal policy. We
formulate the problem of learning such policies from observed trajectories of
the stochastic system as a sample average approximation (SAA) problem. We prove
that the SAA problem converges under mild conditions as the sample size
increases, but that computationally even immediate simplifications of the SAA
problem are theoretically intractable. We thus propose a tractable heuristic
for approximately solving the SAA problem, by greedily constructing the tree
from the top down. We demonstrate the value of our approach by applying it to
the canonical problem of option pricing, using both synthetic instances and
instances using real S&amp;P-500 data. Our method obtains policies that (1)
outperform state-of-the-art non-interpretable methods, based on
simulation-regression and martingale duality, and (2) possess a remarkably
simple and intuitive structure.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/sanakoyeu2018deep/">Deep Unsupervised Learning of Visual Similarities</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Unsupervised Learning of Visual Similarities' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Unsupervised Learning of Visual Similarities' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Artsiom Sanakoyeu, Miguel A. Bautista, Björn Ommer</td>
	<td>Pattern Recognition Volume 78, June 2018, Pages 331-343</td>
	<td><p>Exemplar learning of visual similarities in an unsupervised manner is a
problem of paramount importance to Computer Vision. In this context, however,
the recent breakthrough in deep learning could not yet unfold its full
potential. With only a single positive sample, a great imbalance between one
positive and many negatives, and unreliable relationships between most samples,
training of Convolutional Neural networks is impaired. In this paper we use
weak estimates of local similarities and propose a single optimization problem
to extract batches of samples with mutually consistent relations. Conflicting
relations are distributed over different batches and similar samples are
grouped into compact groups. Learning visual similarities is then framed as a
sequence of categorization tasks. The CNN then consolidates transitivity
relations within and between groups and learns a single representation for all
samples without the need for labels. The proposed unsupervised approach has
shown competitive performance on detailed posture analysis and object
classification.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/teso2018why/">"Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="Why Should I Trust Interactive Learners?" Explaining Interactive Queries of Classifiers to Users' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Stefano Teso, Kristian Kersting</td>
	<td></td>
	<td><p>Although interactive learning puts the user into the loop, the learner
remains mostly a black box for the user. Understanding the reasons behind
queries and predictions is important when assessing how the learner works and,
in turn, trust. Consequently, we propose the novel framework of explanatory
interactive learning: in each step, the learner explains its interactive query
to the user, and she queries of any active classifier for visualizing
explanations of the corresponding predictions. We demonstrate that this can
boost the predictive and explanatory powers of and the trust into the learned
model, using text (e.g. SVMs) and image classification (e.g. neural networks)
experiments as well as a user study.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yadav2018sanity/">Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Check: A Strong Alignment and Information Retrieval Baseline for Question Answering' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Vikas Yadav, Rebecca Sharp, Mihai Surdeanu</td>
	<td></td>
	<td><p>While increasingly complex approaches to question answering (QA) have been
proposed, the true gain of these systems, particularly with respect to their
expensive training requirements, can be inflated when they are not compared to
adequate baselines. Here we propose an unsupervised, simple, and fast alignment
and information retrieval baseline that incorporates two novel contributions: a
\textit{one-to-many alignment} between query and document terms and
\textit{negative alignment} as a proxy for discriminative information. Our
approach not only outperforms all conventional baselines as well as many
supervised recurrent neural networks, but also approaches the state of the art
for supervised systems on three QA datasets. With only three hyperparameters,
we achieve 47\% P@1 on an 8th grade Science QA dataset, 32.9\% P@1 on a Yahoo!
answers QA dataset and 64\% MAP on WikiQA. We also achieve 26.56\% and 58.36\%
on ARC challenge and easy dataset respectively. In addition to including the
additional ARC results in this version of the paper, for the ARC easy set only
we also experimented with one additional parameter – number of justifications
retrieved.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/wang2018comparative/">A Comparative Study of Rule Extraction for Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Comparative Study of Rule Extraction for Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Comparative Study of Rule Extraction for Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Qinglong Wang, Kaixuan Zhang, Alexander G. Ororbia II, Xinyu Xing, Xue Liu, C. Lee Giles</td>
	<td></td>
	<td><p>Understanding recurrent networks through rule extraction has a long history.
This has taken on new interests due to the need for interpreting or verifying
neural networks. One basic form for representing stateful rules is
deterministic finite automata (DFA). Previous research shows that extracting
DFAs from trained second-order recurrent networks is not only possible but also
relatively stable. Recently, several new types of recurrent networks with more
complicated architectures have been introduced. These handle challenging
learning tasks usually involving sequential data. However, it remains an open
problem whether DFAs can be adequately extracted from these models.
Specifically, it is not clear how DFA extraction will be affected when applied
to different recurrent networks trained on data sets with different levels of
complexity. Here, we investigate DFA extraction on several widely adopted
recurrent networks that are trained to learn a set of seven regular Tomita
grammars. We first formally analyze the complexity of Tomita grammars and
categorize these grammars according to that complexity. Then we empirically
evaluate different recurrent networks for their performance of DFA extraction
on all Tomita grammars. Our experiments show that for most recurrent networks,
their extraction performance decreases as the complexity of the underlying
grammar increases. On grammars of lower complexity, most recurrent networks
obtain desirable extraction performance. As for grammars with the highest level
of complexity, while several complicated models fail with only certain
recurrent networks having satisfactory extraction performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/mohseni2018humangrounded/">A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sina Mohseni, Jeremy E. Block, Eric D. Ragan</td>
	<td></td>
	<td><p>Research in interpretable machine learning proposes different computational
and human subject approaches to evaluate model saliency explanations. These
approaches measure different qualities of explanations to achieve diverse goals
in designing interpretable machine learning systems. In this paper, we propose
a human attention benchmark for image and text domains using multi-layer human
attention masks aggregated from multiple human annotators. We then present an
evaluation study to evaluate model saliency explanations obtained using
Grad-cam and LIME techniques. We demonstrate our benchmark’s utility for
quantitative evaluation of model explanations by comparing it with human
subjective ratings and ground-truth single-layer segmentation masks
evaluations. Our study results show that our threshold agnostic evaluation
method with the human attention baseline is more effective than single-layer
object segmentation masks to ground truth. Our experiments also reveal user
biases in the subjective rating of model saliency explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/wang2018multilevel/">Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jingyuan Wang, Ze Wang, Jianfeng Li, Junjie Wu</td>
	<td></td>
	<td><p>Recent years have witnessed the unprecedented rising of time series from
almost all kindes of academic and industrial fields. Various types of deep
neural network models have been introduced to time series analysis, but the
important frequency information is yet lack of effective modeling. In light of
this, in this paper we propose a wavelet-based neural network structure called
multilevel Wavelet Decomposition Network (mWDN) for building frequency-aware
deep learning models for time series analysis. mWDN preserves the advantage of
multilevel discrete wavelet decomposition in frequency learning while enables
the fine-tuning of all parameters under a deep neural network framework. Based
on mWDN, we further propose two deep learning models called Residual
Classification Flow (RCF) and multi-frequecy Long Short-Term Memory (mLSTM) for
time series classification and forecasting, respectively. The two models take
all or partial mWDN decomposed sub-series in different frequencies as input,
and resort to the back propagation algorithm to learn all the parameters
globally, which enables seamless embedding of wavelet-based frequency analysis
into deep learning frameworks. Extensive experiments on 40 UCR datasets and a
real-world user volume dataset demonstrate the excellent performance of our
time series models based on mWDN. In particular, we propose an importance
analysis method to mWDN based models, which successfully identifies those
time-series elements and mWDN layers that are crucially important to time
series analysis. This indeed indicates the interpretability advantage of mWDN,
and can be viewed as an indepth exploration to interpretable deep learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/du2018towards/">Towards Explanation of DNN-based Prediction with Guided Feature Inversion</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explanation of DNN-based Prediction with Guided Feature Inversion' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explanation of DNN-based Prediction with Guided Feature Inversion' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mengnan Du, Ninghao Liu, Qingquan Song, Xia Hu</td>
	<td></td>
	<td><p>While deep neural networks (DNN) have become an effective computational tool,
the prediction results are often criticized by the lack of interpretability,
which is essential in many real-world applications such as health informatics.
Existing attempts based on local interpretations aim to identify relevant
features contributing the most to the prediction of DNN by monitoring the
neighborhood of a given input. They usually simply ignore the intermediate
layers of the DNN that might contain rich information for interpretation. To
bridge the gap, in this paper, we propose to investigate a guided feature
inversion framework for taking advantage of the deep architectures towards
effective interpretation. The proposed framework not only determines the
contribution of each feature in the input but also provides insights into the
decision-making process of DNN models. By further interacting with the neuron
of the target category at the output layer of the DNN, we enforce the
interpretation result to be class-discriminative. We apply the proposed
interpretation model to different CNN architectures to provide explanations for
image data and conduct extensive experiments on ImageNet and PASCAL VOC07
datasets. The interpretation results demonstrate the effectiveness of our
proposed framework in providing class-discriminative interpretation for
DNN-based prediction.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/rudin2018stop/">Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Cynthia Rudin</td>
	<td>Nature Machine Intelligence, Vol 1, May 2019, 206-215</td>
	<td><p>Black box machine learning models are currently being used for high stakes
decision-making throughout society, causing problems throughout healthcare,
criminal justice, and in other domains. People have hoped that creating methods
for explaining these black box models will alleviate some of these problems,
but trying to \textit{explain} black box models, rather than creating models
that are \textit{interpretable} in the first place, is likely to perpetuate bad
practices and can potentially cause catastrophic harm to society. There is a
way forward – it is to design models that are inherently interpretable. This
manuscript clarifies the chasm between explaining black boxes and using
inherently interpretable models, outlines several key reasons why explainable
black boxes should be avoided in high-stakes decisions, identifies challenges
to interpretable machine learning, and provides several example applications
where interpretable models could potentially replace black box models in
criminal justice, healthcare, and computer vision.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/poerner2018evaluating/">Evaluating neural network explanation methods using hybrid documents and morphological agreement</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating neural network explanation methods using hybrid documents and morphological agreement' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating neural network explanation methods using hybrid documents and morphological agreement' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nina Poerner, Benjamin Roth, Hinrich Schütze</td>
	<td></td>
	<td><p>The behavior of deep neural networks (DNNs) is hard to understand. This makes
it necessary to explore post hoc explanation methods. We conduct the first
comprehensive evaluation of explanation methods for NLP. To this end, we design
two novel evaluation paradigms that cover two important classes of NLP
problems: small context and large context problems. Both paradigms require no
manual annotation and are therefore broadly applicable. We also introduce
LIMSSE, an explanation method inspired by LIME that is designed for NLP. We
show empirically that LIMSSE, LRP and DeepLIFT are the most effective
explanation methods and recommend them for explaining DNNs in NLP.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/du2018techniques/">Techniques for Interpretable Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Techniques for Interpretable Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Techniques for Interpretable Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mengnan Du, Ninghao Liu, Xia Hu</td>
	<td></td>
	<td><p>Interpretable machine learning tackles the important problem that humans
cannot understand the behaviors of complex machine learning models and how
these models arrive at a particular decision. Although many approaches have
been proposed, a comprehensive understanding of the achievements and challenges
is still lacking. We provide a survey covering existing techniques to increase
the interpretability of machine learning models. We also discuss crucial issues
that the community should consider in future work such as designing
user-friendly explanations and developing comprehensive evaluation metrics to
further push forward the area of interpretable machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018visual/">Visual Interpretability for Deep Learning: a Survey</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual Interpretability for Deep Learning: a Survey' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual Interpretability for Deep Learning: a Survey' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper reviews recent studies in understanding neural-network
representations and learning neural networks with interpretable/disentangled
middle-layer representations. Although deep neural networks have exhibited
superior performance in various tasks, the interpretability is always the
Achilles’ heel of deep neural networks. At present, deep neural networks obtain
high discrimination power at the cost of low interpretability of their
black-box representations. We believe that high model interpretability may help
people to break several bottlenecks of deep learning, e.g., learning from very
few annotations, learning via human-computer communications at the semantic
level, and semantically debugging network representations. We focus on
convolutional neural networks (CNNs), and we revisit the visualization of CNN
representations, methods of diagnosing representations of pre-trained CNNs,
approaches for disentangling pre-trained CNN representations, learning of CNNs
with disentangled representations, and middle-to-end learning based on model
interpretability. Finally, we discuss prospective trends in explainable
artificial intelligence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/chen2018classifier/">Why Is My Classifier Discriminatory?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Why Is My Classifier Discriminatory?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Why Is My Classifier Discriminatory?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Irene Chen, Fredrik D. Johansson, David Sontag</td>
	<td></td>
	<td><p>Recent attempts to achieve fairness in predictive models focus on the balance
between fairness and accuracy. In sensitive applications such as healthcare or
criminal justice, this trade-off is often undesirable as any increase in
prediction error could have devastating consequences. In this work, we argue
that the fairness of predictions should be evaluated in context of the data,
and that unfairness induced by inadequate samples sizes or unmeasured
predictive variables should be addressed through data collection, rather than
by constraining the model. We decompose cost-based metrics of discrimination
into bias, variance, and noise, and propose actions aimed at estimating and
reducing each term. Finally, we perform case-studies on prediction of income,
mortality, and review ratings, confirming the value of this analysis. We find
that data collection is often a means to reduce discrimination without
sacrificing accuracy.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/chen2018interpretable/">An Interpretable Model with Globally Consistent Explanations for Credit Risk</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=An Interpretable Model with Globally Consistent Explanations for Credit Risk' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=An Interpretable Model with Globally Consistent Explanations for Credit Risk' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chaofan Chen, Kangcheng Lin, Cynthia Rudin, Yaron Shaposhnik, Sijia Wang, Tong Wang</td>
	<td></td>
	<td><p>We propose a possible solution to a public challenge posed by the Fair Isaac
Corporation (FICO), which is to provide an explainable model for credit risk
assessment. Rather than present a black box model and explain it afterwards, we
provide a globally interpretable model that is as accurate as other neural
networks. Our “two-layer additive risk model” is decomposable into subscales,
where each node in the second layer represents a meaningful subscale, and all
of the nonlinearities are transparent. We provide three types of explanations
that are simpler than, but consistent with, the global model. One of these
explanation methods involves solving a minimum set cover problem to find
high-support globally-consistent explanations. We present a new online
visualization tool to allow users to explore the global model and its
explanations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/chen2018looks/">This Looks Like That: Deep Learning for Interpretable Image Recognition</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=This Looks Like That: Deep Learning for Interpretable Image Recognition' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=This Looks Like That: Deep Learning for Interpretable Image Recognition' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su, Cynthia Rudin</td>
	<td>Advances in Neural Information Processing Systems 32 (NeurIPS 2019)</td>
	<td><p>When we are faced with challenging image classification tasks, we often
explain our reasoning by dissecting the image, and pointing out prototypical
aspects of one class or another. The mounting evidence for each of the classes
helps us make our final decision. In this work, we introduce a deep network
architecture – prototypical part network (ProtoPNet), that reasons in a
similar way: the network dissects the image by finding prototypical parts, and
combines evidence from the prototypes to make a final classification. The model
thus reasons in a way that is qualitatively similar to the way ornithologists,
physicians, and others would explain to people on how to solve challenging
image classification tasks. The network uses only image-level labels for
training without any annotations for parts of images. We demonstrate our method
on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show
that ProtoPNet can achieve comparable accuracy with its analogous
non-interpretable counterpart, and when several ProtoPNets are combined into a
larger network, it can achieve an accuracy that is on par with some of the
best-performing deep models. Moreover, ProtoPNet provides a level of
interpretability that is absent in other interpretable deep models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/vaughan2018explainable/">Explainable Neural Networks based on Additive Index Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explainable Neural Networks based on Additive Index Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explainable Neural Networks based on Additive Index Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Joel Vaughan, Agus Sudjianto, Erind Brahimi, Jie Chen, Vijayan N. Nair</td>
	<td></td>
	<td><p>Machine Learning algorithms are increasingly being used in recent years due
to their flexibility in model fitting and increased predictive performance.
However, the complexity of the models makes them hard for the data analyst to
interpret the results and explain them without additional tools. This has led
to much research in developing various approaches to understand the model
behavior. In this paper, we present the Explainable Neural Network (xNN), a
structured neural network designed especially to learn interpretable features.
Unlike fully connected neural networks, the features engineered by the xNN can
be extracted from the network in a relatively straightforward manner and the
results displayed. With appropriate regularization, the xNN provides a
parsimonious explanation of the relationship between the features and the
output. We illustrate this interpretable feature–engineering property on
simulated examples.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/besold2018what/">The What, the Why, and the How of Artificial Explanations in Automated Decision-Making</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The What, the Why, and the How of Artificial Explanations in Automated Decision-Making' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The What, the Why, and the How of Artificial Explanations in Automated Decision-Making' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tarek R. Besold, Sara L. Uckelman</td>
	<td></td>
	<td><p>The increasing incorporation of Artificial Intelligence in the form of
automated systems into decision-making procedures highlights not only the
importance of decision theory for automated systems but also the need for these
decision procedures to be explainable to the people involved in them.
Traditional realist accounts of explanation, wherein explanation is a relation
that holds (or does not hold) eternally between an explanans and an
explanandum, are not adequate to account for the notion of explanation required
for artificial decision procedures. We offer an alternative account of
explanation as used in the context of automated decision-making that makes
explanation an epistemic phenomenon, and one that is dependent on context. This
account of explanation better accounts for the way that we talk about, and use,
explanations and derived concepts, such as `explanatory power’, and also allows
us to differentiate between reasons or causes on the one hand, which do not
need to have an epistemic aspect, and explanations on the other, which do have
such an aspect. Against this theoretical backdrop we then review existing
approaches to explanation in Artificial Intelligence and Machine Learning, and
suggest desiderata which truly explainable decision systems should fulfill.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/ravanelli2018interpretable/">Interpretable Convolutional Filters with SincNet</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Convolutional Filters with SincNet' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Convolutional Filters with SincNet' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mirco Ravanelli, Yoshua Bengio</td>
	<td></td>
	<td><p>Deep learning is currently playing a crucial role toward higher levels of
artificial intelligence. This paradigm allows neural networks to learn complex
and abstract representations, that are progressively obtained by combining
simpler ones. Nevertheless, the internal “black-box” representations
automatically discovered by current neural architectures often suffer from a
lack of interpretability, making of primary interest the study of explainable
machine learning techniques. This paper summarizes our recent efforts to
develop a more interpretable neural model for directly processing speech from
the raw waveform. In particular, we propose SincNet, a novel Convolutional
Neural Network (CNN) that encourages the first layer to discover more
meaningful filters by exploiting parametrized sinc functions. In contrast to
standard CNNs, which learn all the elements of each filter, only low and high
cutoff frequencies of band-pass filters are directly learned from data. This
inductive bias offers a very compact way to derive a customized filter-bank
front-end, that only depends on some parameters with a clear physical meaning.
Our experiments, conducted on both speaker and speech recognition, show that
the proposed architecture converges faster, performs better, and is more
interpretable than standard CNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhu2018exploiting/">Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Exploiting the Value of the Center-dark Channel Prior for Salient Object Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chunbiao Zhu, Wenhao Zhang, Thomas H. Li, Ge Li</td>
	<td></td>
	<td><p>Saliency detection aims to detect the most attractive objects in images and
is widely used as a foundation for various applications. In this paper, we
propose a novel salient object detection algorithm for RGB-D images using
center-dark channel priors. First, we generate an initial saliency map based on
a color saliency map and a depth saliency map of a given RGB-D image. Then, we
generate a center-dark channel map based on center saliency and dark channel
priors. Finally, we fuse the initial saliency map with the center dark channel
map to generate the final saliency map. Extensive evaluations over four
benchmark datasets demonstrate that our proposed method performs favorably
against most of the state-of-the-art approaches. Besides, we further discuss
the application of the proposed algorithm in small target detection and
demonstrate the universal value of center-dark channel priors in the field of
object detection.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/kauffmann2018towards/">Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Explaining Anomalies: A Deep Taylor Decomposition of One-Class Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jacob Kauffmann, Klaus-Robert Müller, Grégoire Montavon</td>
	<td></td>
	<td><p>A common machine learning task is to discriminate between normal and
anomalous data points. In practice, it is not always sufficient to reach high
accuracy at this task, one also would like to understand why a given data point
has been predicted in a certain way. We present a new principled approach for
one-class SVMs that decomposes outlier predictions in terms of input variables.
The method first recomposes the one-class model as a neural network with
distance functions and min-pooling, and then performs a deep Taylor
decomposition (DTD) of the model output. The proposed One-Class DTD is
applicable to a number of common distance-based SVM kernels and is able to
reliably explain a wide set of data anomalies. Furthermore, it outperforms
baselines such as sensitivity analysis, nearest neighbor, or simple edge
detection.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/miller2018contrastive/">Contrastive Explanation: A Structural-Model Approach</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contrastive Explanation: A Structural-Model Approach' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contrastive Explanation: A Structural-Model Approach' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tim Miller</td>
	<td></td>
	<td><p>This paper presents a model of contrastive explanation using structural
casual models. The topic of causal explanation in artificial intelligence has
gathered interest in recent years as researchers and practitioners aim to
increase trust and understanding of intelligent decision-making. While
different sub-fields of artificial intelligence have looked into this problem
with a sub-field-specific view, there are few models that aim to capture
explanation more generally. One general model is based on structural causal
models. It defines an explanation as a fact that, if found to be true, would
constitute an actual cause of a specific event. However, research in philosophy
and social sciences shows that explanations are contrastive: that is, when
people ask for an explanation of an event – the fact – they (sometimes
implicitly) are asking for an explanation relative to some contrast case; that
is, “Why P rather than Q?”. In this paper, we extend the structural causal
model approach to define two complementary notions of contrastive explanation,
and demonstrate them on two classical problems in artificial intelligence:
classification and planning. We believe that this model can help researchers in
subfields of artificial intelligence to better understand contrastive
explanation.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/yeh2018representer/">Representer Point Selection for Explaining Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Representer Point Selection for Explaining Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Representer Point Selection for Explaining Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Chih-Kuan Yeh, Joon Sik Kim, Ian E. H. Yen, Pradeep Ravikumar</td>
	<td></td>
	<td><p>We propose to explain the predictions of a deep neural network, by pointing
to the set of what we call representer points in the training set, for a given
test point prediction. Specifically, we show that we can decompose the
pre-activation prediction of a neural network into a linear combination of
activations of training points, with the weights corresponding to what we call
representer values, which thus capture the importance of that training point on
the learned parameters of the network. But it provides a deeper understanding
of the network than simply training point influence: with positive representer
values corresponding to excitatory training points, and negative values
corresponding to inhibitory points, which as we show provides considerably more
insight. Our method is also much more scalable, allowing for real-time feedback
in a manner not feasible with influence functions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/papernot2018deep/">Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Nicolas Papernot, Patrick McDaniel</td>
	<td></td>
	<td><p>Deep neural networks (DNNs) enable innovative applications of machine
learning like image recognition, machine translation, or malware detection.
However, deep learning is often criticized for its lack of robustness in
adversarial settings (e.g., vulnerability to adversarial inputs) and general
inability to rationalize its predictions. In this work, we exploit the
structure of deep learning to enable new learning-based inference and decision
strategies that achieve desirable properties such as robustness and
interpretability. We take a first step in this direction and introduce the Deep
k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest
neighbors algorithm with representations of the data learned by each layer of
the DNN: a test input is compared to its neighboring training points according
to the distance that separates them in the representations. We show the labels
of these neighboring points afford confidence estimates for inputs outside the
model’s training manifold, including on malicious inputs like adversarial
examples–and therein provides protections against inputs that are outside the
models understanding. This is because the nearest neighbors can be used to
estimate the nonconformity of, i.e., the lack of support for, a prediction in
the training data. The neighbors also constitute human-interpretable
explanations of predictions. We evaluate the DkNN algorithm on several
datasets, and show the confidence estimates accurately identify inputs outside
the model, and that the explanations provided by nearest neighbors are
intuitive and useful in understanding model failures.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lee2018understanding/">Understanding Learned Models by Identifying Important Features at the Right Resolution</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Learned Models by Identifying Important Features at the Right Resolution' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Learned Models by Identifying Important Features at the Right Resolution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Kyubin Lee, Akshay Sood, Mark Craven</td>
	<td></td>
	<td><p>In many application domains, it is important to characterize how complex
learned models make their decisions across the distribution of instances. One
way to do this is to identify the features and interactions among them that
contribute to a model’s predictive accuracy. We present a model-agnostic
approach to this task that makes the following specific contributions. Our
approach (i) tests feature groups, in addition to base features, and tries to
determine the level of resolution at which important features can be
determined, (ii) uses hypothesis testing to rigorously assess the effect of
each feature on the model’s loss, (iii) employs a hierarchical approach to
control the false discovery rate when testing feature groups and individual
base features for importance, and (iv) uses hypothesis testing to identify
important interactions among features and feature groups. We evaluate our
approach by analyzing random forest and LSTM neural network models learned in
two challenging biomedical applications.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/alvarez-melis2018robustness/">On the Robustness of Interpretability Methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=On the Robustness of Interpretability Methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=On the Robustness of Interpretability Methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>David Alvarez-Melis, Tommi S. Jaakkola</td>
	<td></td>
	<td><p>We argue that robustness of explanations—i.e., that similar inputs should
give rise to similar explanations—is a key desideratum for interpretability.
We introduce metrics to quantify robustness and demonstrate that current
methods do not perform well according to these metrics. Finally, we propose
ways that robustness can be enforced on existing interpretability approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/lecue2018semantic/">Semantic Explanations of Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Explanations of Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Explanations of Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Freddy Lecue, Jiewen Wu</td>
	<td></td>
	<td><p>The main objective of explanations is to transmit knowledge to humans. This
work proposes to construct informative explanations for predictions made from
machine learning models. Motivated by the observations from social sciences,
our approach selects data points from the training sample that exhibit special
characteristics crucial for explanation, for instance, ones contrastive to the
classification prediction and ones representative of the models. Subsequently,
semantic concepts are derived from the selected data points through the use of
domain ontologies. These concepts are filtered and ranked to produce
informative explanations that improves human understanding. The main features
of our approach are that (1) knowledge about explanations is captured in the
form of ontological concepts, (2) explanations include contrastive evidences in
addition to normal evidences, and (3) explanations are user relevant.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/roxlo2018opening/">Opening the black box of neural nets: case studies in stop/top discrimination</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Opening the black box of neural nets: case studies in stop/top discrimination' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Opening the black box of neural nets: case studies in stop/top discrimination' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Thomas Roxlo, Matthew Reece</td>
	<td></td>
	<td><p>We introduce techniques for exploring the functionality of a neural network
and extracting simple, human-readable approximations to its performance. By
performing gradient ascent on the input space of the network, we are able to
produce large populations of artificial events which strongly excite a given
classifier. By studying the populations of these events, we then directly
produce what are essentially contour maps of the network’s classification
function. Combined with a suite of tools for identifying the input dimensions
deemed most important by the network, we can utilize these maps to efficiently
interpret the dominant criteria by which the network makes its classification.
  As a test case, we study networks trained to discriminate supersymmetric stop
production in the dilepton channel from Standard Model backgrounds. In the case
of a heavy stop decaying to a light neutralino, we find individual neurons with
large mutual information with $m_{T2}^{\ell\ell}$, a human-designed variable
for optimizing the analysis. The network selects events with significant
missing $p_T$ oriented azimuthally away from both leptons, efficiently
rejecting $t\overline{t}$ background. In the case of a light stop with
three-body decays to $Wb{\widetilde \chi}$ and little phase space, we find
neurons that smoothly interpolate between a similar top-rejection strategy and
an ISR-tagging strategy allowing for more missing momentum. We also find that a
neural network trained on a stealth stop parameter point learns novel angular
correlations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/adebayo2018sanity/">Sanity Checks for Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sanity Checks for Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sanity Checks for Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, Been Kim</td>
	<td></td>
	<td><p>Saliency methods have emerged as a popular tool to highlight features in an
input deemed relevant for the prediction of a learned model. Several saliency
methods have been proposed, often guided by visual appeal on image data. In
this work, we propose an actionable methodology to evaluate what kinds of
explanations a given method can and cannot provide. We find that reliance,
solely, on visual assessment can be misleading. Through extensive experiments
we show that some existing saliency methods are independent both of the model
and of the data generating process. Consequently, methods that fail the
proposed tests are inadequate for tasks that are sensitive to either data or
model, such as, finding outliers in the data, explaining the relationship
between inputs and outputs that the model learned, and debugging the model. We
interpret our findings through an analogy with edge detection in images, a
technique that requires neither training data nor model. Theory in the case of
a linear model and a single-layer convolutional neural network supports our
experimental findings.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/wallace2018interpreting/">Interpreting Neural Networks With Nearest Neighbors</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Neural Networks With Nearest Neighbors' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Neural Networks With Nearest Neighbors' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Eric Wallace, Shi Feng, Jordan Boyd-Graber</td>
	<td></td>
	<td><p>Local model interpretation methods explain individual predictions by
assigning an importance value to each input feature. This value is often
determined by measuring the change in confidence when a feature is removed.
However, the confidence of neural networks is not a robust measure of model
uncertainty. This issue makes reliably judging the importance of the input
features difficult. We address this by changing the test-time behavior of
neural networks using Deep k-Nearest Neighbors. Without harming text
classification accuracy, this algorithm provides a more robust uncertainty
metric which we use to generate feature importance values. The resulting
interpretations better align with human perception than baseline methods.
Finally, we use our interpretation method to analyze model predictions on
dataset annotation artifacts.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/pang2018learning/">Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Guansong Pang, Longbing Cao, Ling Chen, Huan Liu</td>
	<td></td>
	<td><p>Learning expressive low-dimensional representations of ultrahigh-dimensional
data, e.g., data with thousands/millions of features, has been a major way to
enable learning methods to address the curse of dimensionality. However,
existing unsupervised representation learning methods mainly focus on
preserving the data regularity information and learning the representations
independently of subsequent outlier detection methods, which can result in
suboptimal and unstable performance of detecting irregularities (i.e.,
outliers).
  This paper introduces a ranking model-based framework, called RAMODO, to
address this issue. RAMODO unifies representation learning and outlier
detection to learn low-dimensional representations that are tailored for a
state-of-the-art outlier detection approach - the random distance-based
approach. This customized learning yields more optimal and stable
representations for the targeted outlier detectors. Additionally, RAMODO can
leverage little labeled data as prior knowledge to learn more expressive and
application-relevant representations. We instantiate RAMODO to an efficient
method called REPEN to demonstrate the performance of RAMODO.
  Extensive empirical results on eight real-world ultrahigh dimensional data
sets show that REPEN (i) enables a random distance-based detector to obtain
significantly better AUC performance and two orders of magnitude speedup; (ii)
performs substantially better and more stably than four state-of-the-art
representation learning methods; and (iii) leverages less than 1% labeled data
to achieve up to 32% AUC improvement.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018interpretable/">Interpretable Deep Learning under Fire</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Deep Learning under Fire' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Deep Learning under Fire' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xinyang Zhang, Ningfei Wang, Hua Shen, Shouling Ji, Xiapu Luo, Ting Wang</td>
	<td></td>
	<td><p>Providing explanations for deep neural network (DNN) models is crucial for
their use in security-sensitive domains. A plethora of interpretation models
have been proposed to help users understand the inner workings of DNNs: how
does a DNN arrive at a specific decision for a given input? The improved
interpretability is believed to offer a sense of security by involving human in
the decision-making process. Yet, due to its data-driven nature, the
interpretability itself is potentially susceptible to malicious manipulations,
about which little is known thus far.
  Here we bridge this gap by conducting the first systematic study on the
security of interpretable deep learning systems (IDLSes). We show that existing
\imlses are highly vulnerable to adversarial manipulations. Specifically, we
present ADV^2, a new class of attacks that generate adversarial inputs not only
misleading target DNNs but also deceiving their coupled interpretation models.
Through empirical evaluation against four major types of IDLSes on benchmark
datasets and in security-critical applications (e.g., skin cancer diagnosis),
we demonstrate that with ADV^2 the adversary is able to arbitrarily designate
an input’s prediction and interpretation. Further, with both analytical and
empirical evidence, we identify the prediction-interpretation gap as one root
cause of this vulnerability – a DNN and its interpretation model are often
misaligned, resulting in the possibility of exploiting both models
simultaneously. Finally, we explore potential countermeasures against ADV^2,
including leveraging its low transferability and incorporating it in an
adversarial training framework. Our findings shed light on designing and
operating IDLSes in a more secure and informative fashion, leading to several
promising research directions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/jiang2018trust/">To Trust Or Not To Trust A Classifier</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=To Trust Or Not To Trust A Classifier' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=To Trust Or Not To Trust A Classifier' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta</td>
	<td></td>
	<td><p>Knowing when a classifier’s prediction can be trusted is useful in many
applications and critical for safely using AI. While the bulk of the effort in
machine learning research has been towards improving classifier performance,
understanding when a classifier’s predictions should and should not be trusted
has received far less attention. The standard approach is to use the
classifier’s discriminant or confidence score; however, we show there exists an
alternative that is more effective in many situations. We propose a new score,
called the trust score, which measures the agreement between the classifier and
a modified nearest-neighbor classifier on the testing example. We show
empirically that high (low) trust scores produce surprisingly high precision at
identifying correctly (incorrectly) classified examples, consistently
outperforming the classifier’s confidence score as well as many other
baselines. Further, under some mild distributional assumptions, we show that if
the trust score for an example is high (low), the classifier will likely agree
(disagree) with the Bayes-optimal classifier. Our guarantees consist of
non-asymptotic rates of statistical consistency under various nonparametric
settings and build on recent developments in topological data analysis.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/guo2018visualizing/">Visualizing and Understanding Deep Neural Networks in CTR Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Deep Neural Networks in CTR Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Deep Neural Networks in CTR Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lin Guo, Hui Ye, Wenbo Su, Henhuan Liu, Kai Sun, Hang Xiang</td>
	<td></td>
	<td><p>Although deep learning techniques have been successfully applied to many
tasks, interpreting deep neural network models is still a big challenge to us.
Recently, many works have been done on visualizing and analyzing the mechanism
of deep neural networks in the areas of image processing and natural language
processing. In this paper, we present our approaches to visualize and
understand deep neural networks for a very important commercial task–CTR
(Click-through rate) prediction. We conduct experiments on the productive data
from our online advertising system with daily varying distribution. To
understand the mechanism and the performance of the model, we inspect the
model’s inner status at neuron level. Also, a probe approach is implemented to
measure the layer-wise performance of the model. Moreover, to measure the
influence from the input features, we calculate saliency scores based on the
back-propagated gradients. Practical applications are also discussed, for
example, in understanding, monitoring, diagnosing and refining models and
algorithms.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/guidotti2018survey/">A Survey Of Methods For Explaining Black Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Survey Of Methods For Explaining Black Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Survey Of Methods For Explaining Black Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini, Dino Pedreschi, Fosca Giannotti</td>
	<td></td>
	<td><p>In the last years many accurate decision support systems have been
constructed as black boxes, that is as systems that hide their internal logic
to the user. This lack of explanation constitutes both a practical and an
ethical issue. The literature reports many approaches aimed at overcoming this
crucial weakness sometimes at the cost of scarifying accuracy for
interpretability. The applications in which black box decision systems can be
used are various, and each approach is typically developed to provide a
solution for a specific problem and, as a consequence, delineating explicitly
or implicitly its own definition of interpretability and explanation. The aim
of this paper is to provide a classification of the main problems addressed in
the literature with respect to the notion of explanation and the type of black
box system. Given a problem definition, a black box type, and a desired
explanation this survey should help the researcher to find the proposals more
useful for his own work. The proposed classification of approaches to open
black box models should also be useful for putting the many research open
questions in perspective.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/seo2018regional/">Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Regional Multi-scale Approach for Visually Pleasing Explanations of Deep Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dasom Seo, Kanghan Oh, Il-Seok Oh</td>
	<td></td>
	<td><p>Recently, many methods to interpret and visualize deep neural network
predictions have been proposed and significant progress has been made. However,
a more class-discriminative and visually pleasing explanation is required.
Thus, this paper proposes a region-based approach that estimates feature
importance in terms of appropriately segmented regions. By fusing the saliency
maps generated from multi-scale segmentations, a more class-discriminative and
visually pleasing map is obtained. We incorporate this regional multi-scale
concept into a prediction difference method that is model-agnostic. An input
image is segmented in several scales using the super-pixel method, and
exclusion of a region is simulated by sampling a normal distribution
constructed using the boundary prior. The experimental results demonstrate that
the regional multi-scale method produces much more class-discriminative and
visually pleasing saliency maps.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/nushi2018towards/">Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Accountable AI: Hybrid Human-Machine Analyses for Characterizing System Failure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Besmira Nushi, Ece Kamar, Eric Horvitz</td>
	<td>AAAI Conference on Human Computation and Crowdsourcing 2018</td>
	<td><p>As machine learning systems move from computer-science laboratories into the
open world, their accountability becomes a high priority problem.
Accountability requires deep understanding of system behavior and its failures.
Current evaluation methods such as single-score error metrics and confusion
matrices provide aggregate views of system performance that hide important
shortcomings. Understanding details about failures is important for identifying
pathways for refinement, communicating the reliability of systems in different
settings, and for specifying appropriate human oversight and engagement.
Characterization of failures and shortcomings is particularly complex for
systems composed of multiple machine learned components. For such systems,
existing evaluation methods have limited expressiveness in describing and
explaining the relationship among input content, the internal states of system
components, and final output quality. We present Pandora, a set of hybrid
human-machine methods and tools for describing and explaining system failures.
Pandora leverages both human and system-generated observations to summarize
conditions of system malfunction with respect to the input content and system
architecture. We share results of a case study with a machine learning pipeline
for image captioning that show how detailed performance views can be beneficial
for analysis and debugging.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018interpreting/">Interpreting CNNs via Decision Trees</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting CNNs via Decision Trees' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting CNNs via Decision Trees' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Yu Yang, Haotian Ma, Ying Nian Wu</td>
	<td></td>
	<td><p>This paper aims to quantitatively explain rationales of each prediction that
is made by a pre-trained convolutional neural network (CNN). We propose to
learn a decision tree, which clarifies the specific reason for each prediction
made by the CNN at the semantic level. I.e., the decision tree decomposes
feature representations in high conv-layers of the CNN into elementary concepts
of object parts. In this way, the decision tree tells people which object parts
activate which filters for the prediction and how much they contribute to the
prediction score. Such semantic and quantitative explanations for CNN
predictions have specific values beyond the traditional pixel-level analysis of
CNNs. More specifically, our method mines all potential decision modes of the
CNN, where each mode represents a common case of how the CNN uses object parts
for prediction. The decision tree organizes all potential decision modes in a
coarse-to-fine manner to explain CNN predictions at different fine-grained
levels. Experiments have demonstrated the effectiveness of the proposed method.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018non-rigid/">Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Non-rigid Object Tracking via Deep Multi-scale Spatial-temporal Discriminative Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pingping Zhang, Wei Liu, Dong Wang, Yinjie Lei, Hongyu Wang, Chunhua Shen, Huchuan Lu</td>
	<td></td>
	<td><p>In this paper, we propose a novel effective non-rigid object tracking
framework based on the spatial-temporal consistent saliency detection. In
contrast to most existing trackers that utilize a bounding box to specify the
tracked target, the proposed framework can extract accurate regions of the
target as tracking outputs. It achieves a better description of the non-rigid
objects and reduces the background pollution for the tracking model.
Furthermore, our model has several unique features. First, a tailored fully
convolutional neural network (TFCN) is developed to model the local saliency
prior for a given image region, which not only provides the pixel-wise outputs
but also integrates the semantic information. Second, a novel multi-scale
multi-region mechanism is proposed to generate local saliency maps that
effectively consider visual perceptions with different spatial layouts and
scale variations. Subsequently, local saliency maps are fused via a weighted
entropy method, resulting in a final discriminative saliency map. Finally, we
present a non-rigid object tracking algorithm based on the predicted saliency
maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we
conduct target-background classification and use a simple fine-tuning scheme
for online updating. Extensive experiments demonstrate that the proposed
algorithm achieves competitive performance in both saliency detection and
visual tracking, especially outperforming other related trackers on the
non-rigid object tracking datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2018</td>
	<td><a href="/publications/zhang2018explanatory/">Explanatory Graphs for CNNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanatory Graphs for CNNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanatory Graphs for CNNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Xin Wang, Ruiming Cao, Ying Nian Wu, Feng Shi, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper introduces a graphical model, namely an explanatory graph, which
reveals the knowledge hierarchy hidden inside conv-layers of a pre-trained CNN.
Each filter in a conv-layer of a CNN for object classification usually
represents a mixture of object parts. We develop a simple yet effective method
to disentangle object-part pattern components from each filter. We construct an
explanatory graph to organize the mined part patterns, where a node represents
a part pattern, and each edge encodes co-activation relationships and spatial
relationships between patterns. More crucially, given a pre-trained CNN, the
explanatory graph is learned without a need of annotating object parts.
Experiments show that each graph node consistently represented the same object
part through different images, which boosted the transferability of CNN
features. We transferred part patterns in the explanatory graph to the task of
part localization, and our method significantly outperformed other approaches.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2017</td>
	<td><a href="/publications/bau2017network/">Network Dissection: Quantifying Interpretability of Deep Visual Representations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Network Dissection: Quantifying Interpretability of Deep Visual Representations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Network Dissection: Quantifying Interpretability of Deep Visual Representations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>We propose a general framework called Network Dissection for quantifying the
interpretability of latent representations of CNNs by evaluating the alignment
between individual hidden units and a set of semantic concepts. Given any CNN
model, the proposed method draws on a broad data set of visual concepts to
score the semantics of hidden units at each intermediate convolutional layer.
The units with semantics are given labels across a range of objects, parts,
scenes, textures, materials, and colors. We use the proposed method to test the
hypothesis that interpretability of units is equivalent to random linear
combinations of units, then we apply our method to compare the latent
representations of various networks when trained to solve different supervised
and self-supervised training tasks. We further analyze the effect of training
iterations, compare networks trained with different initializations, examine
the impact of network depth and width, and measure the effect of dropout and
batch normalization on the interpretability of deep visual representations. We
demonstrate that the proposed method can shed light on characteristics of CNN
models and training methods that go beyond measurements of their discriminative
power.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ross2017improving/">Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrew Slavin Ross, Finale Doshi-Velez</td>
	<td></td>
	<td><p>Deep neural networks have proven remarkably effective at solving many
classification problems, but have been criticized recently for two major
weaknesses: the reasons behind their predictions are uninterpretable, and the
predictions themselves can often be fooled by small adversarial perturbations.
These problems pose major obstacles for the adoption of neural networks in
domains that require security or transparency. In this work, we evaluate the
effectiveness of defenses that differentiably penalize the degree to which
small changes in inputs can alter model predictions. Across multiple attacks,
architectures, defenses, and datasets, we find that neural networks trained
with this input gradient regularization exhibit robustness to transferred
adversarial examples generated to fool all of the other models. We also find
that adversarial examples generated to fool gradient-regularized models fool
all other models equally well, and actually lead to more “legitimate,”
interpretable misclassifications as rated by people (which we confirm in a
human subject experiment). Finally, we demonstrate that regularizing input
gradients makes them more naturally interpretable as rationales for model
predictions. We conclude by discussing this relationship between
interpretability and robustness in deep neural networks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/shickel2017deep/">Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Benjamin Shickel, Patrick Tighe, Azra Bihorac, Parisa Rashidi</td>
	<td></td>
	<td><p>The past decade has seen an explosion in the amount of digital information
stored in electronic health records (EHR). While primarily designed for
archiving patient clinical information and administrative healthcare tasks,
many researchers have found secondary use of these records for various clinical
informatics tasks. Over the same period, the machine learning community has
seen widespread advances in deep learning techniques, which also have been
successfully applied to the vast amount of EHR data. In this paper, we review
these deep EHR systems, examining architectures, technical aspects, and
clinical applications. We also identify shortcomings of current techniques and
discuss avenues of future research for EHR-based deep learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ross2017right/">Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrew Slavin Ross, Michael C. Hughes, Finale Doshi-Velez</td>
	<td></td>
	<td><p>Neural networks are among the most accurate supervised learning methods in
use today, but their opacity makes them difficult to trust in critical
applications, especially when conditions in training differ from those in test.
Recent work on explanations for black-box models has produced tools (e.g. LIME)
to show the implicit rules behind predictions, which can help us identify when
models are right for the wrong reasons. However, these methods do not scale to
explaining entire datasets and cannot correct the problems they reveal. We
introduce a method for efficiently explaining and regularizing differentiable
models by examining and selectively penalizing their input gradients, which
provide a normal to the decision boundary. We apply these penalties both based
on expert annotation and in an unsupervised fashion that encourages diverse
models with qualitatively different decision boundaries for the same
classification problem. On multiple datasets, we show our approach generates
faithful explanations and models that generalize much better when conditions
differ between training and test.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/krause2017workflow/">A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon Aphinyanaphongs, Enrico Bertini</td>
	<td></td>
	<td><p>Human-in-the-loop data analysis applications necessitate greater transparency
in machine learning models for experts to understand and trust their decisions.
To this end, we propose a visual analytics workflow to help data scientists and
domain experts explore, diagnose, and understand the decisions made by a binary
classifier. The approach leverages “instance-level explanations”, measures of
local feature relevance that explain single instances, and uses them to build a
set of visual representations that guide the users in their investigation. The
workflow is based on three main visual representations and steps: one based on
aggregate statistics to see how data distributes across correct / incorrect
decisions; one based on explanations to understand which features are used to
make these decisions; and one based on raw data, to derive insights on
potential root causes for the observed patterns. The workflow is derived from a
long-term collaboration with a group of machine learning and healthcare
professionals who used our method to make sense of machine learning models they
developed. The case study from this collaboration demonstrates that the
proposed workflow helps experts derive useful knowledge about the model and the
phenomena it describes, thus experts can generate useful hypotheses on how a
model can be improved.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/sabour2017dynamic/">Dynamic Routing Between Capsules</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Dynamic Routing Between Capsules' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Dynamic Routing Between Capsules' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Sara Sabour, Nicholas Frosst, Geoffrey E Hinton</td>
	<td></td>
	<td><p>A capsule is a group of neurons whose activity vector represents the
instantiation parameters of a specific type of entity such as an object or an
object part. We use the length of the activity vector to represent the
probability that the entity exists and its orientation to represent the
instantiation parameters. Active capsules at one level make predictions, via
transformation matrices, for the instantiation parameters of higher-level
capsules. When multiple predictions agree, a higher level capsule becomes
active. We show that a discrimininatively trained, multi-layer capsule system
achieves state-of-the-art performance on MNIST and is considerably better than
a convolutional net at recognizing highly overlapping digits. To achieve these
results we use an iterative routing-by-agreement mechanism: A lower-level
capsule prefers to send its output to higher level capsules whose activity
vectors have a big scalar product with the prediction coming from the
lower-level capsule.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/shrikumar2017learning/">Learning Important Features Through Propagating Activation Differences</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Important Features Through Propagating Activation Differences' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Important Features Through Propagating Activation Differences' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Avanti Shrikumar, Peyton Greenside, Anshul Kundaje</td>
	<td>PMLR 70:3145-3153, 2017</td>
	<td><p>The purported “black box” nature of neural networks is a barrier to adoption
in applications where interpretability is essential. Here we present DeepLIFT
(Deep Learning Important FeaTures), a method for decomposing the output
prediction of a neural network on a specific input by backpropagating the
contributions of all neurons in the network to every feature of the input.
DeepLIFT compares the activation of each neuron to its ‘reference activation’
and assigns contribution scores according to the difference. By optionally
giving separate consideration to positive and negative contributions, DeepLIFT
can also reveal dependencies which are missed by other approaches. Scores can
be computed efficiently in a single backward pass. We apply DeepLIFT to models
trained on MNIST and simulated genomic data, and show significant advantages
over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides:
bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code:
http://goo.gl/RM8jvH.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/senel2017semantic/">Semantic Structure and Interpretability of Word Embeddings</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Semantic Structure and Interpretability of Word Embeddings' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Semantic Structure and Interpretability of Word Embeddings' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lutfi Kerem Senel, Ihsan Utlu, Veysel Yucesoy, Aykut Koc, Tolga Cukur</td>
	<td>L. K. \c{S}enel, \.I. Utlu, V. Y\"ucesoy, A. Ko\c{c} and T. \c{C}ukur, "Semantic Structure and Interpretability of Word Embeddings," in IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 10, pp. 1769-1779, Oct. 2018</td>
	<td><p>Dense word embeddings, which encode semantic meanings of words to low
dimensional vector spaces have become very popular in natural language
processing (NLP) research due to their state-of-the-art performances in many
NLP tasks. Word embeddings are substantially successful in capturing semantic
relations among words, so a meaningful semantic structure must be present in
the respective vector spaces. However, in many cases, this semantic structure
is broadly and heterogeneously distributed across the embedding dimensions,
which makes interpretation a big challenge. In this study, we propose a
statistical method to uncover the latent semantic structure in the dense word
embeddings. To perform our analysis we introduce a new dataset (SEMCAT) that
contains more than 6500 words semantically grouped under 110 categories. We
further propose a method to quantify the interpretability of the word
embeddings; the proposed method is a practical alternative to the classical
word intrusion test that requires human intervention.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lipton2017mythos/">The Mythos of Model Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Mythos of Model Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Mythos of Model Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zachary C. Lipton</td>
	<td>ICML Workshop on Human Interpretability in Machine Learning</td>
	<td><p>Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.</p>
</td>
	<td>understanding </td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/miller2017explanation/">Explanation in Artificial Intelligence: Insights from the Social Sciences</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation in Artificial Intelligence: Insights from the Social Sciences' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation in Artificial Intelligence: Insights from the Social Sciences' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tim Miller</td>
	<td></td>
	<td><p>There has been a recent resurgence in the area of explainable artificial
intelligence as researchers and practitioners seek to make their algorithms
more understandable. Much of this research is focused on explicitly explaining
decisions or actions to a human observer, and it should not be controversial to
say that looking at how humans explain to each other can serve as a useful
starting point for explanation in artificial intelligence. However, it is fair
to say that most work in explainable artificial intelligence uses only the
researchers’ intuition of what constitutes a `good’ explanation. There exists
vast and valuable bodies of research in philosophy, psychology, and cognitive
science of how people define, generate, select, evaluate, and present
explanations, which argues that people employ certain cognitive biases and
social expectations towards the explanation process. This paper argues that the
field of explainable artificial intelligence should build on this existing
research, and reviews relevant papers from philosophy, cognitive
psychology/science, and social psychology, which study these topics. It draws
out some important findings, and discusses ways that these can be infused with
work on explainable artificial intelligence.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhou2017interpreting/">Interpreting Deep Visual Representations via Network Dissection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting Deep Visual Representations via Network Dissection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting Deep Visual Representations via Network Dissection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bolei Zhou, David Bau, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>The success of recent deep convolutional neural networks (CNNs) depends on
learning hidden representations that can summarize the important factors of
variation behind the data. However, CNNs often criticized as being black boxes
that lack interpretability, since they have millions of unexplained model
parameters. In this work, we describe Network Dissection, a method that
interprets networks by providing labels for the units of their deep visual
representations. The proposed method quantifies the interpretability of CNN
representations by evaluating the alignment between individual hidden units and
a set of visual semantic concepts. By identifying the best alignments, units
are given human interpretable labels across a range of objects, parts, scenes,
textures, materials, and colors. The method reveals that deep representations
are more transparent and interpretable than expected: we find that
representations are significantly more interpretable than they would be under a
random equivalently powerful basis. We apply the method to interpret and
compare the latent representations of various network architectures trained to
solve different supervised and self-supervised training tasks. We then examine
factors affecting the network interpretability such as the number of the
training iterations, regularizations, different initializations, and the
network depth and width. Finally we show that the interpreted units can be used
to provide explicit explanations of a prediction given by a CNN for an image.
Our results highlight that interpretability is an important property of deep
neural networks that provides new insights into their hierarchical structure.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wu2017beyond/">Beyond Sparsity: Tree Regularization of Deep Models for Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Beyond Sparsity: Tree Regularization of Deep Models for Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Beyond Sparsity: Tree Regularization of Deep Models for Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mike Wu, Michael C. Hughes, Sonali Parbhoo, Maurizio Zazzi, Volker Roth, Finale Doshi-Velez</td>
	<td></td>
	<td><p>The lack of interpretability remains a key barrier to the adoption of deep
models in many applications. In this work, we explicitly regularize deep models
so human users might step through the process behind their predictions in
little time. Specifically, we train deep time-series models so their
class-probability predictions have high accuracy while being closely modeled by
decision trees with few nodes. Using intuitive toy examples as well as medical
tasks for treating sepsis and HIV, we demonstrate that this new tree
regularization yields models that are easier for humans to simulate than
simpler L1 or L2 penalties without sacrificing predictive power.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/kindermans2017unreliability/">The (Un)reliability of saliency methods</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The (Un)reliability of saliency methods' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The (Un)reliability of saliency methods' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schütt, Sven Dähne, Dumitru Erhan, Been Kim</td>
	<td></td>
	<td><p>Saliency methods aim to explain the predictions of deep neural networks.
These methods lack reliability when the explanation is sensitive to factors
that do not contribute to the model prediction. We use a simple and common
pre-processing step —adding a constant shift to the input data— to show
that a transformation with no effect on the model can cause numerous methods to
incorrectly attribute. In order to guarantee reliability, we posit that methods
should fulfill input invariance, the requirement that a saliency method mirror
the sensitivity of the model with respect to transformations of the input. We
show, through several examples, that saliency methods that do not satisfy input
invariance result in misleading attribution.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/murdoch2017automatic/">Automatic Rule Extraction from Long Short Term Memory Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Automatic Rule Extraction from Long Short Term Memory Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Automatic Rule Extraction from Long Short Term Memory Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>W. James Murdoch, Arthur Szlam</td>
	<td></td>
	<td><p>Although deep learning models have proven effective at solving problems in
natural language processing, the mechanism by which they come to their
conclusions is often unclear. As a result, these models are generally treated
as black boxes, yielding no insight of the underlying learned patterns. In this
paper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new
approach for tracking the importance of a given input to the LSTM for a given
output. By identifying consistently important patterns of words, we are able to
distill state of the art LSTMs on sentiment analysis and question answering
into a set of representative phrases. This representation is then
quantitatively validated by using the extracted phrases to construct a simple,
rule-based classifier which approximates the output of the LSTM.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/gamboa2017deep/">Deep Learning for Time-Series Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Learning for Time-Series Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Learning for Time-Series Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>John Cristian Borges Gamboa</td>
	<td></td>
	<td><p>In many real-world application, e.g., speech recognition or sleep stage
classification, data are captured over the course of time, constituting a
Time-Series. Time-Series often contain temporal dependencies that cause two
otherwise identical points of time to belong to different classes or predict
different behavior. This characteristic generally increases the difficulty of
analysing them. Existing techniques often depended on hand-crafted features
that were expensive to create and required expert knowledge of the field. With
the advent of Deep Learning new models of unsupervised learning of features for
Time-series analysis and forecast have been developed. Such new developments
are the topic of this paper: a review of the main Deep Learning techniques is
presented, and some applications on Time-Series analysis are summaried. The
results make it clear that Deep Learning has a lot to contribute to the field.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/kindermans2017learning/">Learning how to explain neural networks: PatternNet and PatternAttribution</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning how to explain neural networks: PatternNet and PatternAttribution' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning how to explain neural networks: PatternNet and PatternAttribution' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, Sven Dähne</td>
	<td></td>
	<td><p>DeConvNet, Guided BackProp, LRP, were invented to better understand deep
neural networks. We show that these methods do not produce the theoretically
correct explanation for a linear model. Yet they are used on multi-layer
networks with millions of parameters. This is a cause for concern since linear
models are simple neural networks. We argue that explanation methods for neural
nets should work reliably in the limit of simplicity, the linear models. Based
on our analysis of linear models we propose a generalization that yields two
explanation techniques (PatternNet and PatternAttribution) that are
theoretically sound for linear models and produce improved explanations for
deep networks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wang2017deep/">Deep Visual Attention Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Visual Attention Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Visual Attention Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wenguan Wang, Jianbing Shen</td>
	<td>IEEE Transactions on Image Processing, Vol. 27, No. 5, pp 2368-2378, 2018</td>
	<td><p>In this work, we aim to predict human eye fixation with view-free scenes
based on an end-to-end deep learning architecture. Although Convolutional
Neural Networks (CNNs) have made substantial improvement on human attention
prediction, it is still needed to improve CNN based attention models by
efficiently leveraging multi-scale features. Our visual attention network is
proposed to capture hierarchical saliency information from deep, coarse layers
with global saliency information to shallow, fine layers with local saliency
response. Our model is based on a skip-layer network structure, which predicts
human attention from multiple convolutional layers with various reception
fields. Final saliency prediction is achieved via the cooperation of those
global and local predictions. Our model is learned in a deep supervision
manner, where supervision is directly fed into multi-level layers, instead of
previous approaches of providing supervision only at the output layer and
propagating this supervision back to earlier layers. Our model thus
incorporates multi-level saliency predictions within a single network, which
significantly decreases the redundancy of previous approaches of learning
multiple network streams with different input scales. Extensive experimental
analysis on various challenging benchmark datasets demonstrate our method
yields state-of-the-art performance with competitive inference time.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/arras2017explaining/">Explaining Recurrent Neural Network Predictions in Sentiment Analysis</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explaining Recurrent Neural Network Predictions in Sentiment Analysis' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explaining Recurrent Neural Network Predictions in Sentiment Analysis' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Leila Arras, Grégoire Montavon, Klaus-Robert Müller, Wojciech Samek</td>
	<td></td>
	<td><p>Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown
to deliver insightful explanations in the form of input space relevances for
understanding feed-forward neural network classification decisions. In the
present work, we extend the usage of LRP to recurrent neural networks. We
propose a specific propagation rule applicable to multiplicative connections as
they arise in recurrent network architectures such as LSTMs and GRUs. We apply
our technique to a word-based bi-directional LSTM model on a five-class
sentiment prediction task, and evaluate the resulting LRP relevances both
qualitatively and quantitatively, obtaining better results than a
gradient-based related method which was used in previous work.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lakkaraju2017interpretable/">Interpretable & Explorable Approximations of Black Box Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable & Explorable Approximations of Black Box Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable & Explorable Approximations of Black Box Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Himabindu Lakkaraju, Ece Kamar, Rich Caruana, Jure Leskovec</td>
	<td></td>
	<td><p>We propose Black Box Explanations through Transparent Approximations (BETA),
a novel model agnostic framework for explaining the behavior of any black-box
classifier by simultaneously optimizing for fidelity to the original model and
interpretability of the explanation. To this end, we develop a novel objective
function which allows us to learn (with optimality guarantees), a small number
of compact decision sets each of which explains the behavior of the black box
model in unambiguous, well-defined regions of feature space. Furthermore, our
framework also is capable of accepting user input when generating these
approximations, thus allowing users to interactively explore how the black-box
model behaves in different subspaces that are of interest to the user. To the
best of our knowledge, this is the first approach which can produce global
explanations of the behavior of any given black box model through joint
optimization of unambiguity, fidelity, and interpretability, while also
allowing users to explore model behavior based on their preferences.
Experimental evaluation with real-world datasets and user studies demonstrates
that our approach can generate highly compact, easy-to-understand, yet accurate
approximations of various kinds of predictive models compared to
state-of-the-art baselines.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ghosh2017towards/">Towards a New Interpretation of Separable Convolutions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards a New Interpretation of Separable Convolutions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards a New Interpretation of Separable Convolutions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tapabrata Ghosh</td>
	<td></td>
	<td><p>In recent times, the use of separable convolutions in deep convolutional
neural network architectures has been explored. Several researchers, most
notably (Chollet, 2016) and (Ghosh, 2017) have used separable convolutions in
their deep architectures and have demonstrated state of the art or close to
state of the art performance. However, the underlying mechanism of action of
separable convolutions are still not fully understood. Although their
mathematical definition is well understood as a depthwise convolution followed
by a pointwise convolution, deeper interpretations such as the extreme
Inception hypothesis (Chollet, 2016) have failed to provide a thorough
explanation of their efficacy. In this paper, we propose a hybrid
interpretation that we believe is a better model for explaining the efficacy of
separable convolutions.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/herman2017promise/">The Promise and Peril of Human Evaluation for Model Interpretability</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Promise and Peril of Human Evaluation for Model Interpretability' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Promise and Peril of Human Evaluation for Model Interpretability' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bernease Herman</td>
	<td></td>
	<td><p>Transparency, user trust, and human comprehension are popular ethical
motivations for interpretable machine learning. In support of these goals,
researchers evaluate model explanation performance using humans and real world
applications. This alone presents a challenge in many areas of artificial
intelligence. In this position paper, we propose a distinction between
descriptive and persuasive explanations. We discuss reasoning suggesting that
functional interpretability may be correlated with cognitive function and user
preferences. If this is indeed the case, evaluation and optimization using
functional metrics could perpetuate implicit cognitive bias in explanations
that threaten transparency. Finally, we propose two potential research
directions to disambiguate cognitive function and explanation models, retaining
control over the tradeoff between accuracy and interpretability.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/koh2017understanding/">Understanding Black-box Predictions via Influence Functions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Black-box Predictions via Influence Functions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Black-box Predictions via Influence Functions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Pang Wei Koh, Percy Liang</td>
	<td></td>
	<td><p>How can we explain the predictions of a black-box model? In this paper, we
use influence functions – a classic technique from robust statistics – to
trace a model’s prediction through the learning algorithm and back to its
training data, thereby identifying training points most responsible for a given
prediction. To scale up influence functions to modern machine learning
settings, we develop a simple, efficient implementation that requires only
oracle access to gradients and Hessian-vector products. We show that even on
non-convex and non-differentiable models where the theory breaks down,
approximations to influence functions can still provide valuable information.
On linear models and convolutional neural networks, we demonstrate that
influence functions are useful for multiple purposes: understanding model
behavior, debugging models, detecting dataset errors, and even creating
visually-indistinguishable training-set attacks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/sundararajan2017axiomatic/">Axiomatic Attribution for Deep Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Axiomatic Attribution for Deep Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Axiomatic Attribution for Deep Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mukund Sundararajan, Ankur Taly, Qiqi Yan</td>
	<td></td>
	<td><p>We study the problem of attributing the prediction of a deep network to its
input features, a problem previously studied by several other works. We
identify two fundamental axioms—Sensitivity and Implementation Invariance
that attribution methods ought to satisfy. We show that they are not satisfied
by most known attribution methods, which we consider to be a fundamental
weakness of those methods. We use the axioms to guide the design of a new
attribution method called Integrated Gradients. Our method requires no
modification to the original network and is extremely simple to implement; it
just needs a few calls to the standard gradient operator. We apply this method
to a couple of image models, a couple of text models and a chemistry model,
demonstrating its ability to debug networks, to extract rules from a network,
and to enable users to engage with models better.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhang2017interpreting/">Interpreting CNN Knowledge via an Explanatory Graph</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpreting CNN Knowledge via an Explanatory Graph' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpreting CNN Knowledge via an Explanatory Graph' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper learns a graphical model, namely an explanatory graph, which
reveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering
that each filter in a conv-layer of a pre-trained CNN usually represents a
mixture of object parts, we propose a simple yet efficient method to
automatically disentangles different part patterns from each filter, and
construct an explanatory graph. In the explanatory graph, each node represents
a part pattern, and each edge encodes co-activation relationships and spatial
relationships between patterns. More importantly, we learn the explanatory
graph for a pre-trained CNN in an unsupervised manner, i.e., without a need of
annotating object parts. Experiments show that each graph node consistently
represents the same object part through different images. We transfer part
patterns in the explanatory graph to the task of part localization, and our
method significantly outperforms other approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/phillips2017interpretable/">Interpretable Active Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Active Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Active Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Richard L. Phillips, Kyu Hyun Chang, Sorelle A. Friedler</td>
	<td></td>
	<td><p>Active learning has long been a topic of study in machine learning. However,
as increasingly complex and opaque models have become standard practice, the
process of active learning, too, has become more opaque. There has been little
investigation into interpreting what specific trends and patterns an active
learning strategy may be exploring. This work expands on the Local
Interpretable Model-agnostic Explanations framework (LIME) to provide
explanations for active learning recommendations. We demonstrate how LIME can
be used to generate locally faithful explanations for an active learning
strategy, and how these explanations can be used to understand how different
models and datasets explore a problem space over time. In order to quantify the
per-subgroup differences in how an active learning strategy queries spatial
regions, we introduce a notion of uncertainty bias (based on disparate impact)
to measure the discrepancy in the confidence for a model’s predictions between
one subgroup and another. Using the uncertainty bias measure, we show that our
query explanations accurately reflect the subgroup focus of the active learning
queries, allowing for an interpretable explanation of what is being learned as
points with similar sources of uncertainty have their uncertainty bias
resolved. We demonstrate that this technique can be applied to track
uncertainty bias over user-defined clusters or automatically generated clusters
based on the source of uncertainty.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/zhang2017interpretable/">Interpretable Convolutional Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Convolutional Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Convolutional Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper proposes a method to modify traditional convolutional neural
networks (CNNs) into interpretable CNNs, in order to clarify knowledge
representations in high conv-layers of CNNs. In an interpretable CNN, each
filter in a high conv-layer represents a certain object part. We do not need
any annotations of object parts or textures to supervise the learning process.
Instead, the interpretable CNN automatically assigns each filter in a high
conv-layer with an object part during the learning process. Our method can be
applied to different types of CNNs with different structures. The clear
knowledge representation in an interpretable CNN can help people understand the
logics inside a CNN, i.e., based on which patterns the CNN makes the decision.
Experiments showed that filters in an interpretable CNN were more semantically
meaningful than those in traditional CNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/ghorbani2017interpretation/">Interpretation of Neural Networks is Fragile</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretation of Neural Networks is Fragile' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretation of Neural Networks is Fragile' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Amirata Ghorbani, Abubakar Abid, James Zou</td>
	<td></td>
	<td><p>In order for machine learning to be deployed and trusted in many
applications, it is crucial to be able to reliably explain why the machine
learning algorithm makes certain predictions. For example, if an algorithm
classifies a given pathology image to be a malignant tumor, then the doctor may
need to know which parts of the image led the algorithm to this classification.
How to interpret black-box predictors is thus an important and active area of
research. A fundamental question is: how much can we trust the interpretation
itself? In this paper, we show that interpretation of deep learning predictions
is extremely fragile in the following sense: two perceptively indistinguishable
inputs with the same predicted label can be assigned very different
interpretations. We systematically characterize the fragility of several
widely-used feature-importance interpretation methods (saliency maps, relevance
propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that
even small random perturbation can change the feature importance and new
systematic perturbations can lead to dramatically different interpretations
without changing the label. We extend these results to show that
interpretations based on exemplars (e.g. influence functions) are similarly
fragile. Our analysis of the geometry of the Hessian matrix gives insight on
why fragility could be a fundamental challenge to the current interpretation
approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/wu2017towards/">Towards Interpretable R-CNN by Unfolding Latent Structures</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable R-CNN by Unfolding Latent Structures' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable R-CNN by Unfolding Latent Structures' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tianfu Wu, Wei Sun, Xilai Li, Xi Song, Bo Li</td>
	<td></td>
	<td><p>This paper first proposes a method of formulating model interpretability in
visual understanding tasks based on the idea of unfolding latent structures. It
then presents a case study in object detection using popular two-stage
region-based convolutional network (i.e., R-CNN) detection systems. We focus on
weakly-supervised extractive rationale generation, that is learning to unfold
latent discriminative part configurations of object instances automatically and
simultaneously in detection without using any supervision for part
configurations. We utilize a top-down hierarchical and compositional grammar
model embedded in a directed acyclic AND-OR Graph (AOG) to explore and unfold
the space of latent part configurations of regions of interest (RoIs). We
propose an AOGParsing operator to substitute the RoIPooling operator widely
used in R-CNN. In detection, a bounding box is interpreted by the best parse
tree derived from the AOG on-the-fly, which is treated as the qualitatively
extractive rationale generated for interpreting detection. We propose a
folding-unfolding method to train the AOG and convolutional networks
end-to-end. In experiments, we build on R-FCN and test our method on the PASCAL
VOC 2007 and 2012 datasets. We show that the method can unfold promising latent
structures without hurting the performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/hsu2017unsupervised/">Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wei-Ning Hsu, Yu Zhang, James Glass</td>
	<td></td>
	<td><p>We present a factorized hierarchical variational autoencoder, which learns
disentangled and interpretable representations from sequential data without
supervision. Specifically, we exploit the multi-scale nature of information in
sequential data by formulating it explicitly within a factorized hierarchical
graphical model that imposes sequence-dependent priors and sequence-independent
priors to different sets of latent variables. The model is evaluated on two
speech corpora to demonstrate, qualitatively, its ability to transform speakers
or linguistic content by manipulating different sets of latent variables; and
quantitatively, its ability to outperform an i-vector baseline for speaker
verification and reduce the word error rate by as much as 35% in mismatched
train/test scenarios for automatic speech recognition tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/fong2017interpretable/">Interpretable Explanations of Black Boxes by Meaningful Perturbation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Explanations of Black Boxes by Meaningful Perturbation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Explanations of Black Boxes by Meaningful Perturbation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ruth Fong, Andrea Vedaldi</td>
	<td>Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV)</td>
	<td><p>As machine learning algorithms are increasingly applied to high impact yet
high risk tasks, such as medical diagnosis or autonomous driving, it is
critical that researchers can explain how such algorithms arrived at their
predictions. In recent years, a number of image saliency methods have been
developed to summarize where highly complex neural networks “look” in an image
for evidence for their predictions. However, these techniques are limited by
their heuristic nature and architectural constraints. In this paper, we make
two main contributions: First, we propose a general framework for learning
different kinds of explanations for any black box algorithm. Second, we
specialise the framework to find the part of an image most responsible for a
classifier decision. Unlike previous works, our method is model-agnostic and
testable because it is grounded in explicit and interpretable image
perturbations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/doshi-velez2017towards/">Towards A Rigorous Science of Interpretable Machine Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards A Rigorous Science of Interpretable Machine Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards A Rigorous Science of Interpretable Machine Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Finale Doshi-Velez, Been Kim</td>
	<td></td>
	<td><p>As machine learning systems become ubiquitous, there has been a surge of
interest in interpretable machine learning: systems that provide explanation
for their outputs. These explanations are often used to qualitatively assess
other criteria such as safety or non-discrimination. However, despite the
interest in interpretability, there is very little consensus on what
interpretable machine learning is and how it should be measured. In this
position paper, we first define interpretability and describe when
interpretability is needed (and when it is not). Next, we suggest a taxonomy
for rigorous evaluation and expose open questions towards a more rigorous
science of interpretable machine learning.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/su2017one/">One pixel attack for fooling deep neural networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=One pixel attack for fooling deep neural networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=One pixel attack for fooling deep neural networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiawei Su, Danilo Vasconcellos Vargas, Sakurai Kouichi</td>
	<td>IEEE Transactions on Evolutionary Computation</td>
	<td><p>Recent research has revealed that the output of Deep Neural Networks (DNN)
can be easily altered by adding relatively small perturbations to the input
vector. In this paper, we analyze an attack in an extremely limited scenario
where only one pixel can be modified. For that we propose a novel method for
generating one-pixel adversarial perturbations based on differential evolution
(DE). It requires less adversarial information (a black-box attack) and can
fool more types of networks due to the inherent features of DE. The results
show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and
16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least
one target class by modifying just one pixel with 74.03% and 22.91% confidence
on average. We also show the same vulnerability on the original CIFAR-10
dataset. Thus, the proposed attack explores a different take on adversarial
machine learning in an extreme limited scenario, showing that current DNNs are
also vulnerable to such low dimension attacks. Besides, we also illustrate an
important application of DE (or broadly speaking, evolutionary computation) in
the domain of adversarial machine learning: creating tools that can effectively
generate low-cost adversarial attacks against neural networks for evaluating
robustness.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/lundberg2017unified/">A Unified Approach to Interpreting Model Predictions</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=A Unified Approach to Interpreting Model Predictions' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=A Unified Approach to Interpreting Model Predictions' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Scott Lundberg, Su-In Lee</td>
	<td></td>
	<td><p>Understanding why a model makes a certain prediction can be as crucial as the
prediction’s accuracy in many applications. However, the highest accuracy for
large modern datasets is often achieved by complex models that even experts
struggle to interpret, such as ensemble or deep learning models, creating a
tension between accuracy and interpretability. In response, various methods
have recently been proposed to help users interpret the predictions of complex
models, but it is often unclear how these methods are related and when one
method is preferable over another. To address this problem, we present a
unified framework for interpreting predictions, SHAP (SHapley Additive
exPlanations). SHAP assigns each feature an importance value for a particular
prediction. Its novel components include: (1) the identification of a new class
of additive feature importance measures, and (2) theoretical results showing
there is a unique solution in this class with a set of desirable properties.
The new class unifies six existing methods, notable because several recent
methods in the class lack the proposed desirable properties. Based on insights
from this unification, we present new methods that show improved computational
performance and/or better consistency with human intuition than previous
approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/holzinger2017need/">What do we need to build explainable AI systems for the medical domain?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=What do we need to build explainable AI systems for the medical domain?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=What do we need to build explainable AI systems for the medical domain?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andreas Holzinger, Chris Biemann, Constantinos S. Pattichis, Douglas B. Kell</td>
	<td></td>
	<td><p>Artificial intelligence (AI) generally and machine learning (ML) specifically
demonstrate impressive practical success in many different application domains,
e.g. in autonomous driving, speech recognition, or recommender systems. Deep
learning approaches, trained on extremely large data sets or using
reinforcement learning methods have even exceeded human performance in visual
tasks, particularly on playing games such as Atari, or mastering the game of
Go. Even in the medical domain there are remarkable results. The central
problem of such models is that they are regarded as black-box models and even
if we understand the underlying mathematical principles, they lack an explicit
declarative knowledge representation, hence have difficulty in generating the
underlying explanatory structures. This calls for systems enabling to make
decisions transparent, understandable and explainable. A huge motivation for
our approach are rising legal and privacy aspects. The new European General
Data Protection Regulation entering into force on May 25th 2018, will make
black-box approaches difficult to use in business. This does not imply a ban on
automatic learning approaches or an obligation to explain everything all the
time, however, there must be a possibility to make the results re-traceable on
demand. In this paper we outline some of our research topics in the context of
the relatively new area of explainable-AI with a focus on the application in
medicine, which is a very special domain. This is due to the fact that medical
professionals are working mostly with distributed heterogeneous and complex
sources of data. In this paper we concentrate on three sources: images, *omics
data and text. We argue that research in explainable-AI would generally help to
facilitate the implementation of AI/ML in the medical domain, and specifically
help to facilitate transparency and trust.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/liu2017contextual/">Contextual Outlier Interpretation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Contextual Outlier Interpretation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Contextual Outlier Interpretation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Ninghao Liu, Donghwa Shin, Xia Hu</td>
	<td></td>
	<td><p>Outlier detection plays an essential role in many data-driven applications to
identify isolated instances that are different from the majority. While many
statistical learning and data mining techniques have been used for developing
more effective outlier detection algorithms, the interpretation of detected
outliers does not receive much attention. Interpretation is becoming
increasingly important to help people trust and evaluate the developed models
through providing intrinsic reasons why the certain outliers are chosen. It is
difficult, if not impossible, to simply apply feature selection for explaining
outliers due to the distinct characteristics of various detection models,
complicated structures of data in certain applications, and imbalanced
distribution of outliers and normal instances. In addition, the role of
contrastive contexts where outliers locate, as well as the relation between
outliers and contexts, are usually overlooked in interpretation. To tackle the
issues above, in this paper, we propose a novel Contextual Outlier
INterpretation (COIN) method to explain the abnormality of existing outliers
spotted by detectors. The interpretability for an outlier is achieved from
three aspects: outlierness score, attributes that contribute to the
abnormality, and contextual description of its neighborhoods. Experimental
results on various types of datasets demonstrate the flexibility and
effectiveness of the proposed framework compared with existing interpretation
approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/doshi-velez2017accountability/">Accountability of AI Under the Law: The Role of Explanation</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Accountability of AI Under the Law: The Role of Explanation' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Accountability of AI Under the Law: The Role of Explanation' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Finale Doshi-Velez, Mason Kortz, Ryan Budish, Chris Bavitz, Sam Gershman, David O'Brien, Kate Scott, Stuart Schieber, James Waldo, David Weinberger, Adrian Weller, Alexandra Wood</td>
	<td></td>
	<td><p>The ubiquity of systems using artificial intelligence or “AI” has brought
increasing attention to how those systems should be regulated. The choice of
how to regulate AI systems will require care. AI systems have the potential to
synthesize large amounts of data, allowing for greater levels of
personalization and precision than ever before—applications range from
clinical decision support to autonomous driving and predictive policing. That
said, there exist legitimate concerns about the intentional and unintentional
negative consequences of AI systems. There are many ways to hold AI systems
accountable. In this work, we focus on one: explanation. Questions about a
legal right to explanation from AI systems was recently debated in the EU
General Data Protection Regulation, and thus thinking carefully about when and
how explanation from AI systems might improve accountability is timely. In this
work, we review contexts in which explanation is currently required under the
law, and then list the technical considerations that must be considered if we
desired AI systems that could provide kinds of explanations that are currently
required of humans.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/liang2017interpretable/">Interpretable Structure-Evolving LSTM</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Structure-Evolving LSTM' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Structure-Evolving LSTM' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Xiaodan Liang, Liang Lin, Xiaohui Shen, Jiashi Feng, Shuicheng Yan, Eric P. Xing</td>
	<td></td>
	<td><p>This paper develops a general framework for learning interpretable data
representation via Long Short-Term Memory (LSTM) recurrent neural networks over
hierarchal graph structures. Instead of learning LSTM models over the pre-fixed
structures, we propose to further learn the intermediate interpretable
multi-level graph structures in a progressive and stochastic way from data
during the LSTM network optimization. We thus call this model the
structure-evolving LSTM. In particular, starting with an initial element-level
graph representation where each node is a small data element, the
structure-evolving LSTM gradually evolves the multi-level graph representations
by stochastically merging the graph nodes with high compatibilities along the
stacked LSTM layers. In each LSTM layer, we estimate the compatibility of two
connected nodes from their corresponding LSTM gate outputs, which is used to
generate a merging probability. The candidate graph structures are accordingly
generated where the nodes are grouped into cliques with their merging
probabilities. We then produce the new graph structure with a
Metropolis-Hasting algorithm, which alleviates the risk of getting stuck in
local optimums by stochastic sampling with an acceptance probability. Once a
graph structure is accepted, a higher-level graph is then constructed by taking
the partitioned cliques as its nodes. During the evolving process,
representation becomes more abstracted in higher-levels where redundant
information is filtered out, allowing more efficient propagation of long-range
data dependencies. We evaluate the effectiveness of structure-evolving LSTM in
the application of semantic object parsing and demonstrate its advantage over
state-of-the-art LSTM models on standard benchmarks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/dong2017towards/">Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao</td>
	<td></td>
	<td><p>Deep neural networks (DNNs) have demonstrated impressive performance on a
wide array of tasks, but they are usually considered opaque since internal
structure and learned parameters are not interpretable. In this paper, we
re-examine the internal representations of DNNs using adversarial images, which
are generated by an ensemble-optimization algorithm. We find that: (1) the
neurons in DNNs do not truly detect semantic objects/parts, but respond to
objects/parts only as recurrent discriminative patches; (2) deep visual
representations are not robust distributed codes of visual concepts because the
representations of adversarial images are largely not consistent with those of
real images, although they have similar visual appearance, both of which are
different from previous findings. To further improve the interpretability of
DNNs, we propose an adversarial training scheme with a consistent loss such
that the neurons are endowed with human-interpretable concepts. The induced
interpretable representations enable us to trace eventual outcomes back to
influential neurons. Therefore, human users can know how the models make
predictions, as well as when and why they make errors.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2017</td>
	<td><a href="/publications/dong2017improving/">Improving Interpretability of Deep Neural Networks with Semantic Information</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Improving Interpretability of Deep Neural Networks with Semantic Information' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Improving Interpretability of Deep Neural Networks with Semantic Information' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Yinpeng Dong, Hang Su, Jun Zhu, Bo Zhang</td>
	<td></td>
	<td><p>Interpretability of deep neural networks (DNNs) is essential since it enables
users to understand the overall strengths and weaknesses of the models, conveys
an understanding of how the models will behave in the future, and how to
diagnose and correct potential problems. However, it is challenging to reason
about what a DNN actually does due to its opaque or black-box nature. To
address this issue, we propose a novel technique to improve the
interpretability of DNNs by leveraging the rich semantic information embedded
in human descriptions. By concentrating on the video captioning task, we first
extract a set of semantically meaningful topics from the human descriptions
that cover a wide range of visual concepts, and integrate them into the model
with an interpretive loss. We then propose a prediction difference maximization
algorithm to interpret the learned features of each neuron. Experimental
results demonstrate its effectiveness in video captioning using the
interpretable features, which can also be transferred to video action
recognition. By clearly understanding the learned features, users can easily
revise false predictions via a human-in-the-loop procedure.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2016</td>
	<td><a href="/publications/nguyen2016plug/">Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, Jason Yosinski</td>
	<td></td>
	<td><p>Generating high-resolution, photo-realistic images has been a long-standing
goal in machine learning. Recently, Nguyen et al. (2016) showed one interesting
way to synthesize novel images by performing gradient ascent in the latent
space of a generator network to maximize the activations of one or multiple
neurons in a separate classifier network. In this paper we extend this method
by introducing an additional prior on the latent code, improving both sample
quality and sample diversity, leading to a state-of-the-art generative model
that produces high quality images at higher resolutions (227x227) than previous
generative models, and does so for all 1000 ImageNet categories. In addition,
we provide a unified probabilistic interpretation of related activation
maximization methods and call the general class of models “Plug and Play
Generative Networks”. PPGNs are composed of 1) a generator network G that is
capable of drawing a wide range of image types and 2) a replaceable “condition”
network C that tells the generator what to draw. We demonstrate the generation
of images conditioned on a class (when C is an ImageNet or MIT Places
classification network) and also conditioned on a caption (when C is an image
captioning network). Our method also improves the state of the art of
Multifaceted Feature Visualization, which generates the set of synthetic inputs
that activate a neuron in order to better understand how deep neural networks
operate. Finally, we show that our model performs reasonably well at the task
of image inpainting. While image models are used in this paper, the approach is
modality-agnostic and can be applied to many types of data.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/ribeiro2016why/">"Why Should I Trust You?": Explaining the Predictions of Any Classifier</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q="Why Should I Trust You?": Explaining the Predictions of Any Classifier' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q="Why Should I Trust You?": Explaining the Predictions of Any Classifier' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin</td>
	<td></td>
	<td><p>Despite widespread adoption, machine learning models remain mostly black
boxes. Understanding the reasons behind predictions is, however, quite
important in assessing trust, which is fundamental if one plans to take action
based on a prediction, or when choosing whether to deploy a new model. Such
understanding also provides insights into the model, which can be used to
transform an untrustworthy model or prediction into a trustworthy one. In this
work, we propose LIME, a novel explanation technique that explains the
predictions of any classifier in an interpretable and faithful manner, by
learning an interpretable model locally around the prediction. We also propose
a method to explain models by presenting representative individual predictions
and their explanations in a non-redundant way, framing the task as a submodular
optimization problem. We demonstrate the flexibility of these methods by
explaining different models for text (e.g. random forests) and image
classification (e.g. neural networks). We show the utility of explanations via
novel experiments, both simulated and with human subjects, on various scenarios
that require trust: deciding if one should trust a prediction, choosing between
models, improving an untrustworthy classifier, and identifying why a classifier
should not be trusted.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/das2016human/">Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Abhishek Das, Harsh Agrawal, C. Lawrence Zitnick, Devi Parikh, Dhruv Batra</td>
	<td></td>
	<td><p>We conduct large-scale studies on `human attention’ in Visual Question
Answering (VQA) to understand where humans choose to look to answer questions
about images. We design and test multiple game-inspired novel
attention-annotation interfaces that require the subject to sharpen regions of
a blurred image to answer a question. Thus, we introduce the VQA-HAT (Human
ATtention) dataset. We evaluate attention maps generated by state-of-the-art
VQA models against human attention both qualitatively (via visualizations) and
quantitatively (via rank-order correlation). Overall, our experiments show that
current attention models in VQA do not seem to be looking at the same regions
as humans.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zhang2016growing/">Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Quanshi Zhang, Ruiming Cao, Ying Nian Wu, Song-Chun Zhu</td>
	<td></td>
	<td><p>This paper proposes a learning strategy that extracts object-part concepts
from a pre-trained convolutional neural network (CNN), in an attempt to 1)
explore explicit semantics hidden in CNN units and 2) gradually grow a
semantically interpretable graphical model on the pre-trained CNN for
hierarchical object understanding. Given part annotations on very few (e.g.,
3-12) objects, our method mines certain latent patterns from the pre-trained
CNN and associates them with different semantic parts. We use a four-layer
And-Or graph to organize the mined latent patterns, so as to clarify their
internal semantic hierarchy. Our method is guided by a small number of part
annotations, and it achieves superior performance (about 13%-107% improvement)
in part center prediction on the PASCAL VOC and ImageNet datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/leandro2016reverse/">Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz Fuzzy Logics using Linear Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz Fuzzy Logics using Linear Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Reverse Engineering and Symbolic Knowledge Extraction on Łukasiewicz Fuzzy Logics using Linear Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Carlos Leandro</td>
	<td></td>
	<td><p>This work describes a methodology to combine logic-based systems and
connectionist systems. Our approach uses finite truth valued {\L}ukasiewicz
logic, where we take advantage of fact what in this type of logics every
connective can be define by a neuron in an artificial network having by
activation function the identity truncated to zero and one. This allowed the
injection of first-order formulas in a network architecture, and also
simplified symbolic rule extraction.
  Our method trains a neural network using Levenderg-Marquardt algorithm, where
we restrict the knowledge dissemination in the network structure. We show how
this reduces neural networks plasticity without damage drastically the learning
performance. Making the descriptive power of produced neural networks similar
to the descriptive power of {\L}ukasiewicz logic language, simplifying the
translation between symbolic and connectionist structures.
  This method is used in the reverse engineering problem of finding the formula
used on generation of a truth table for a multi-valued {\L}ukasiewicz logic.
For real data sets the method is particularly useful for attribute selection,
on binary classification problems defined using nominal attribute. After
attribute selection and possible data set completion in the resulting
connectionist model: neurons are directly representable using a disjunctive or
conjunctive formulas, in the {\L}ukasiewicz logic, or neurons are
interpretations which can be approximated by symbolic rules. This fact is
exemplified, extracting symbolic knowledge from connectionist models generated
for the data set Mushroom from UCI Machine Learning Repository.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/krakovna2016increasing/">Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Increasing the Interpretability of Recurrent Neural Networks Using Hidden Markov Models' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Viktoriya Krakovna, Finale Doshi-Velez</td>
	<td></td>
	<td><p>As deep neural networks continue to revolutionize various application
domains, there is increasing interest in making these powerful models more
understandable and interpretable, and narrowing down the causes of good and bad
predictions. We focus on recurrent neural networks, state of the art models in
speech recognition and translation. Our approach to increasing interpretability
is by combining a long short-term memory (LSTM) model with a hidden Markov
model (HMM), a simpler and more transparent model. We add the HMM state
probabilities to the output layer of the LSTM, and then train the HMM and LSTM
either sequentially or jointly. The LSTM can make use of the information from
the HMM, and fill in the gaps when the HMM is not performing well. A small
hybrid model usually performs better than a standalone LSTM of the same size,
especially on smaller data sets. We test the algorithms on text data and
medical time series data, and find that the LSTM and HMM learn complementary
information about the features in the text.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/kumar2016understanding/">Understanding Anatomy Classification Through Attentive Response Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Anatomy Classification Through Attentive Response Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Anatomy Classification Through Attentive Response Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Devinder Kumar, Vlado Menkovski, Graham W. Taylor, Alexander Wong</td>
	<td></td>
	<td><p>One of the main challenges for broad adoption of deep learning based models
such as convolutional neural networks (CNN), is the lack of understanding of
their decisions. In many applications, a simpler, less capable model that can
be easily understood is favorable to a black-box model that has superior
performance. In this paper, we present an approach for designing CNNs based on
visualization of the internal activations of the model. We visualize the
model’s response through attentive response maps obtained using a fractional
stride convolution technique and compare the results with known imaging
landmarks from the medical literature. We show that sufficiently deep and
capable models can be successfully trained to use the same medical landmarks a
human expert would use. Our approach allows for communicating the model
decision process well, but also offers insight towards detecting biases.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/hendricks2016generating/">Generating Visual Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Generating Visual Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Generating Visual Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Lisa Anne Hendricks, Zeynep Akata, Marcus Rohrbach, Jeff Donahue, Bernt Schiele, Trevor Darrell</td>
	<td></td>
	<td><p>Clearly explaining a rationale for a classification decision to an end-user
can be as important as the decision itself. Existing approaches for deep visual
recognition are generally opaque and do not output any justification text;
contemporary vision-language models can describe image content but fail to take
into account class-discriminative image aspects which justify visual
predictions. We propose a new model that focuses on the discriminating
properties of the visible object, jointly predicts a class label, and explains
why the predicted label is appropriate for the image. We propose a novel loss
function based on sampling and reinforcement learning that learns to generate
sentences that realize a global sentence property, such as class specificity.
Our results on a fine-grained bird species classification dataset show that our
model is able to generate explanations which are not only consistent with an
image but also more discriminative than descriptions produced by existing
captioning methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wang2016time/">Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhiguang Wang, Weizhong Yan, Tim Oates</td>
	<td></td>
	<td><p>We propose a simple but strong baseline for time series classification from
scratch with deep neural networks. Our proposed baseline models are pure
end-to-end without any heavy preprocessing on the raw data or feature crafting.
The proposed Fully Convolutional Network (FCN) achieves premium performance to
other state-of-the-art approaches and our exploration of the very deep neural
networks with the ResNet structure is also competitive. The global average
pooling in our convolutional model enables the exploitation of the Class
Activation Map (CAM) to find out the contributing region in the raw data for
the specific labels. Our models provides a simple choice for the real world
application and a good starting point for the future research. An overall
analysis is provided to discuss the generalization capability of our models,
learned features, network structures and the classification semantics.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/li2016understanding/">Understanding Neural Networks through Representation Erasure</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Neural Networks through Representation Erasure' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Neural Networks through Representation Erasure' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiwei Li, Will Monroe, Dan Jurafsky</td>
	<td></td>
	<td><p>While neural networks have been successfully applied to many natural language
processing tasks, they come at the cost of interpretability. In this paper, we
propose a general methodology to analyze and interpret decisions from a neural
model by observing the effects on the model of erasing various parts of the
representation, such as input word-vector dimensions, intermediate hidden
units, or input words. We present several approaches to analyzing the effects
of such erasure, from computing the relative difference in evaluation metrics,
to using reinforcement learning to erase the minimum set of input words in
order to flip a neural model’s decision. In a comprehensive analysis of
multiple NLP tasks, including linguistic feature classification, sentence-level
sentiment analysis, and document level sentiment aspect prediction, we show
that the proposed methodology not only offers clear explanations about neural
model decisions, but also provides a way to conduct error analysis on neural
models.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/wisdom2016interpretable/">Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Recurrent Neural Networks Using Sequential Sparse Recovery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Scott Wisdom, Thomas Powers, James Pitton, Les Atlas</td>
	<td></td>
	<td><p>Recurrent neural networks (RNNs) are powerful and effective for processing
sequential data. However, RNNs are usually considered “black box” models whose
internal structure and learned parameters are not interpretable. In this paper,
we propose an interpretable RNN based on the sequential iterative
soft-thresholding algorithm (SISTA) for solving the sequential sparse recovery
problem, which models a sequence of correlated observations with a sequence of
sparse latent vectors. The architecture of the resulting SISTA-RNN is
implicitly defined by the computational structure of SISTA, which results in a
novel stacked RNN architecture. Furthermore, the weights of the SISTA-RNN are
perfectly interpretable as the parameters of a principled statistical model,
which in this case include a sparsifying dictionary, iterative step size, and
regularization parameters. In addition, on a particular sequential compressive
sensing task, the SISTA-RNN trains faster and achieves better performance than
conventional state-of-the-art black box RNNs, including long-short term memory
(LSTM) RNNs.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/pan2016shallow/">Shallow and Deep Convolutional Networks for Saliency Prediction</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Shallow and Deep Convolutional Networks for Saliency Prediction' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Shallow and Deep Convolutional Networks for Saliency Prediction' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Junting Pan, Kevin McGuinness, Elisa Sayrol, Noel O'Connor, Xavier Giro-i-Nieto</td>
	<td></td>
	<td><p>The prediction of salient areas in images has been traditionally addressed
with hand-crafted features based on neuroscience principles. This paper,
however, addresses the problem with a completely data-driven approach by
training a convolutional neural network (convnet). The learning process is
formulated as a minimization of a loss function that measures the Euclidean
distance of the predicted saliency map with the provided ground truth. The
recent publication of large datasets of saliency prediction has provided enough
data to train end-to-end architectures that are both fast and accurate. Two
designs are proposed: a shallow convnet trained from scratch, and a another
deeper solution whose first three layers are adapted from another network
trained for classification. To the authors knowledge, these are the first
end-to-end CNNs trained and tested for the purpose of saliency prediction.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/lee2016going/">Going Deeper with Contextual CNN for Hyperspectral Image Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Going Deeper with Contextual CNN for Hyperspectral Image Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Going Deeper with Contextual CNN for Hyperspectral Image Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hyungtae Lee, Heesung Kwon</td>
	<td></td>
	<td><p>In this paper, we describe a novel deep convolutional neural network (CNN)
that is deeper and wider than other existing deep networks for hyperspectral
image classification. Unlike current state-of-the-art approaches in CNN-based
hyperspectral image classification, the proposed network, called contextual
deep CNN, can optimally explore local contextual interactions by jointly
exploiting local spatio-spectral relationships of neighboring individual pixel
vectors. The joint exploitation of the spatio-spectral information is achieved
by a multi-scale convolutional filter bank used as an initial component of the
proposed CNN pipeline. The initial spatial and spectral feature maps obtained
from the multi-scale filter bank are then combined together to form a joint
spatio-spectral feature map. The joint feature map representing rich spectral
and spatial properties of the hyperspectral image is then fed through a fully
convolutional network that eventually predicts the corresponding label of each
pixel vector. The proposed approach is tested on three benchmark datasets: the
Indian Pines dataset, the Salinas dataset and the University of Pavia dataset.
Performance comparison shows enhanced classification performance of the
proposed approach over the current state-of-the-art on the three datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/alain2016understanding/">Understanding intermediate layers using linear classifier probes</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding intermediate layers using linear classifier probes' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding intermediate layers using linear classifier probes' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Guillaume Alain, Yoshua Bengio</td>
	<td></td>
	<td><p>Neural network models have a reputation for being black boxes. We propose to
monitor the features at every layer of a model and measure how suitable they
are for classification. We use linear classifiers, which we refer to as
“probes”, trained entirely independently of the model itself.
  This helps us better understand the roles and dynamics of the intermediate
layers. We demonstrate how this can be used to develop a better intuition about
models and to diagnose potential problems.
  We apply this technique to the popular models Inception v3 and Resnet-50.
Among other things, we observe experimentally that the linear separability of
features increase monotonically along the depth of the model.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/dong2016characterizing/">Characterizing Driving Styles with Deep Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Characterizing Driving Styles with Deep Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Characterizing Driving Styles with Deep Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Weishan Dong, Jian Li, Renjie Yao, Changsheng Li, Ting Yuan, Lanjun Wang</td>
	<td></td>
	<td><p>Characterizing driving styles of human drivers using vehicle sensor data,
e.g., GPS, is an interesting research problem and an important real-world
requirement from automotive industries. A good representation of driving
features can be highly valuable for autonomous driving, auto insurance, and
many other application scenarios. However, traditional methods mainly rely on
handcrafted features, which limit machine learning algorithms to achieve a
better performance. In this paper, we propose a novel deep learning solution to
this problem, which could be the first attempt of extending deep learning to
driving behavior analysis based on GPS data. The proposed approach can
effectively extract high level and interpretable features describing complex
driving patterns. It also requires significantly less human experience and
work. The power of the learned driving style representations are validated
through the driver identification problem using a large real dataset.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/adler2016auditing/">Auditing Black-box Models for Indirect Influence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Auditing Black-box Models for Indirect Influence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Auditing Black-box Models for Indirect Influence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Philip Adler, Casey Falk, Sorelle A. Friedler, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, Suresh Venkatasubramanian</td>
	<td></td>
	<td><p>Data-trained predictive models see widespread use, but for the most part they
are used as black boxes which output a prediction or score. It is therefore
hard to acquire a deeper understanding of model behavior, and in particular how
different features influence the model prediction. This is important when
interpreting the behavior of complex models, or asserting that certain
problematic attributes (like race or gender) are not unduly influencing
decisions.
  In this paper, we present a technique for auditing black-box models, which
lets us study the extent to which existing models take advantage of particular
features in the dataset, without knowing how the models work. Our work focuses
on the problem of indirect influence: how some features might indirectly
influence outcomes via other, related features. As a result, we can find
attribute influences even in cases where, upon further direct examination of
the model, the attribute is not referred to by the model at all.
  Our approach does not require the black-box model to be retrained. This is
important if (for example) the model is only accessible via an API, and
contrasts our work with other methods that investigate feature influence like
feature selection. We present experimental evidence for the effectiveness of
our procedure using a variety of publicly available datasets and models. We
also validate our procedure using techniques from interpretable learning and
feature selection, as well as against other black-box auditing procedures.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/park2016attentive/">Attentive Explanations: Justifying Decisions and Pointing to the Evidence</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Attentive Explanations: Justifying Decisions and Pointing to the Evidence' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Attentive Explanations: Justifying Decisions and Pointing to the Evidence' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Bernt Schiele, Trevor Darrell, Marcus Rohrbach</td>
	<td></td>
	<td><p>Deep models are the defacto standard in visual decision models due to their
impressive performance on a wide array of visual tasks. However, they are
frequently seen as opaque and are unable to explain their decisions. In
contrast, humans can justify their decisions with natural language and point to
the evidence in the visual world which led to their decisions. We postulate
that deep models can do this as well and propose our Pointing and Justification
(PJ-X) model which can justify its decision with a sentence and point to the
evidence by introspecting its decision and explanation process using an
attention mechanism. Unfortunately there is no dataset available with reference
explanations for visual decision making. We thus collect two datasets in two
domains where it is interesting and challenging to explain decisions. First, we
extend the visual question answering task to not only provide an answer but
also a natural language explanation for the answer. Second, we focus on
explaining human activities which is traditionally more challenging than object
classification. We extensively evaluate our PJ-X model, both on the
justification and pointing tasks, by comparing it to prior models and ablations
using both automatic and human evaluations.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/strobelt2016lstmvis:/">LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Hendrik Strobelt, Sebastian Gehrmann, Hanspeter Pfister, Alexander M. Rush</td>
	<td></td>
	<td><p>Recurrent neural networks, and in particular long short-term memory (LSTM)
networks, are a remarkably effective tool for sequence modeling that learn a
dense black-box hidden representation of their sequential input. Researchers
interested in better understanding these models have studied the changes in
hidden state representations over time and noticed some interpretable patterns
but also significant noise. In this work, we present LSTMVIS, a visual analysis
tool for recurrent neural networks with a focus on understanding these hidden
state dynamics. The tool allows users to select a hypothesis input range to
focus on local state changes, to match these states changes to similar patterns
in a large data set, and to align these results with structural annotations
from their domain. We show several use cases of the tool for analyzing specific
hidden state properties on dataset containing nesting, phrase structure, and
chord progressions, and demonstrate how the tool can be used to isolate
patterns for further statistical analysis. We characterize the domain, the
different stakeholders, and their goals and tasks.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/hu2016harnessing/">Harnessing Deep Neural Networks with Logic Rules</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Harnessing Deep Neural Networks with Logic Rules' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Harnessing Deep Neural Networks with Logic Rules' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, Eric Xing</td>
	<td></td>
	<td><p>Combining deep neural networks with structured logic rules is desirable to
harness flexibility and reduce uninterpretability of the neural models. We
propose a general framework capable of enhancing various types of neural
networks (e.g., CNNs and RNNs) with declarative first-order logic rules.
Specifically, we develop an iterative distillation method that transfers the
structured information of logic rules into the weights of neural networks. We
deploy the framework on a CNN for sentiment analysis, and an RNN for named
entity recognition. With a few highly intuitive rules, we obtain substantial
improvements and achieve state-of-the-art or comparable results to previous
best-performing systems.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/shrikumar2016black/">Not Just a Black Box: Learning Important Features Through Propagating Activation Differences</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Not Just a Black Box: Learning Important Features Through Propagating Activation Differences' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Not Just a Black Box: Learning Important Features Through Propagating Activation Differences' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje</td>
	<td></td>
	<td><p>Note: This paper describes an older version of DeepLIFT. See
https://arxiv.org/abs/1704.02685 for the newer version. Original abstract
follows: The purported “black box” nature of neural networks is a barrier to
adoption in applications where interpretability is essential. Here we present
DeepLIFT (Learning Important FeaTures), an efficient and effective method for
computing importance scores in a neural network. DeepLIFT compares the
activation of each neuron to its ‘reference activation’ and assigns
contribution scores according to the difference. We apply DeepLIFT to models
trained on natural images and genomic data, and show significant advantages
over gradient-based methods.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/vergari2016visualizing/">Visualizing and Understanding Sum-Product Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Sum-Product Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Sum-Product Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Antonio Vergari, Nicola Di Mauro, Floriana Esposito</td>
	<td></td>
	<td><p>Sum-Product Networks (SPNs) are recently introduced deep tractable
probabilistic models by which several kinds of inference queries can be
answered exactly and in a tractable time. Up to now, they have been largely
used as black box density estimators, assessed only by comparing their
likelihood scores only. In this paper we explore and exploit the inner
representations learned by SPNs. We do this with a threefold aim: first we want
to get a better understanding of the inner workings of SPNs; secondly, we seek
additional ways to evaluate one SPN model and compare it against other
probabilistic models, providing diagnostic tools to practitioners; lastly, we
want to empirically evaluate how good and meaningful the extracted
representations are, as in a classic Representation Learning framework. In
order to do so we revise their interpretation as deep neural networks and we
propose to exploit several visualization techniques on their node activations
and network outputs under different types of inference queries. To investigate
these models as feature extractors, we plug some SPNs, learned in a greedy
unsupervised fashion on image datasets, in supervised classification learning
tasks. We extract several embedding types from node activations by filtering
nodes by their type, by their associated feature abstraction level and by their
scope. In a thorough empirical comparison we prove them to be competitive
against those generated from popular feature extractors as Restricted Boltzmann
Machines. Finally, we investigate embeddings generated from random
probabilistic marginal queries as means to compare other tractable
probabilistic models on a common ground, extending our experiments to Mixtures
of Trees.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/sturm2016interpretable/">Interpretable Deep Neural Networks for Single-Trial EEG Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable Deep Neural Networks for Single-Trial EEG Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable Deep Neural Networks for Single-Trial EEG Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Irene Sturm, Sebastian Bach, Wojciech Samek, Klaus-Robert Müller</td>
	<td></td>
	<td><p>Background: In cognitive neuroscience the potential of Deep Neural Networks
(DNNs) for solving complex classification tasks is yet to be fully exploited.
The most limiting factor is that DNNs as notorious ‘black boxes’ do not provide
insight into neurophysiological phenomena underlying a decision. Layer-wise
Relevance Propagation (LRP) has been introduced as a novel method to explain
individual network decisions. New Method: We propose the application of DNNs
with LRP for the first time for EEG data analysis. Through LRP the single-trial
DNN decisions are transformed into heatmaps indicating each data point’s
relevance for the outcome of the decision. Results: DNN achieves classification
accuracies comparable to those of CSP-LDA. In subjects with low performance
subject-to-subject transfer of trained DNNs can improve the results. The
single-trial LRP heatmaps reveal neurophysiologically plausible patterns,
resembling CSP-derived scalp maps. Critically, while CSP patterns represent
class-wise aggregated information, LRP heatmaps pinpoint neural patterns to
single time points in single trials. Comparison with Existing Method(s): We
compare the classification performance of DNNs to that of linear CSP-LDA on two
data sets related to motor-imaginery BCI. Conclusion: We have demonstrated that
DNN is a powerful non-linear tool for EEG analysis. With LRP a new quality of
high-resolution assessment of neural activity can be reached. LRP is a
potential remedy for the lack of interpretability of DNNs that has limited
their utility in neuroscientific applications. The extreme specificity of the
LRP-derived heatmaps opens up new avenues for investigating neural activity
underlying complex perception or decision-related processes.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2016</td>
	<td><a href="/publications/zahavy2016graying/">Graying the black box: Understanding DQNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Graying the black box: Understanding DQNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Graying the black box: Understanding DQNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Tom Zahavy, Nir Ben Zrihem, Shie Mannor</td>
	<td></td>
	<td><p>In recent years there is a growing interest in using deep representations for
reinforcement learning. In this paper, we present a methodology and tools to
analyze Deep Q-networks (DQNs) in a non-blind matter. Moreover, we propose a
new model, the Semi Aggregated Markov Decision Process (SAMDP), and an
algorithm that learns it automatically. The SAMDP model allows us to identify
spatio-temporal abstractions directly from features and may be used as a
sub-goal detector in future work. Using our tools we reveal that the features
learned by DQNs aggregate the state space in a hierarchical fashion, explaining
its success. Moreover, we are able to understand and describe the policies
learned by DQNs for three different Atari2600 games and suggest ways to
interpret, debug and optimize deep neural networks in reinforcement learning.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2015</td>
	<td><a href="/publications/letham2015interpretable/">Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Benjamin Letham, Cynthia Rudin, Tyler H. McCormick, David Madigan</td>
	<td>Annals of Applied Statistics 2015, Vol. 9, No. 3, 1350-1371</td>
	<td><p>We aim to produce predictive models that are not only accurate, but are also
interpretable to human experts. Our models are decision lists, which consist of
a series of if…then… statements (e.g., if high blood pressure, then stroke)
that discretize a high-dimensional, multivariate feature space into a series of
simple, readily interpretable decision statements. We introduce a generative
model called Bayesian Rule Lists that yields a posterior distribution over
possible decision lists. It employs a novel prior structure to encourage
sparsity. Our experiments show that Bayesian Rule Lists has predictive accuracy
on par with the current top algorithms for prediction in machine learning. Our
method is motivated by recent developments in personalized medicine, and can be
used to produce highly accurate and interpretable medical scoring systems. We
demonstrate this by producing an alternative to the CHADS$_2$ score, actively
used in clinical practice for estimating the risk of stroke in patients that
have atrial fibrillation. Our model is as interpretable as CHADS$_2$, but more
accurate.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/li2015visualizing/">Visualizing and Understanding Neural Models in NLP</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Neural Models in NLP' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Neural Models in NLP' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jiwei Li, Xinlei Chen, Eduard Hovy, Dan Jurafsky</td>
	<td></td>
	<td><p>While neural networks have been successfully applied to many NLP tasks the
resulting vector-based models are very difficult to interpret. For example it’s
not clear how they achieve {\em compositionality}, building sentence meaning
from the meanings of words and phrases. In this paper we describe four
strategies for visualizing compositionality in neural models for NLP, inspired
by similar work in computer vision. We first plot unit values to visualize
compositionality of negation, intensification, and concessive clauses, allow us
to see well-known markedness asymmetries in negation. We then introduce three
simple and straightforward methods for visualizing a unit’s {\em salience}, the
amount it contributes to the final composed meaning: (1) gradient
back-propagation, (2) the variance of a token from the average word node, (3)
LSTM-style gates that measure information flow. We test our methods on
sentiment using simple recurrent nets and LSTMs. Our general-purpose methods
may have wide applications for understanding compositionality and other
semantic properties of deep networks , and also shed light on why LSTMs
outperform simple recurrent nets,</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/siddiqui2015sequential/">Sequential Feature Explanations for Anomaly Detection</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Sequential Feature Explanations for Anomaly Detection' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Sequential Feature Explanations for Anomaly Detection' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich, Weng-Keen Wong</td>
	<td></td>
	<td><p>In many applications, an anomaly detection system presents the most anomalous
data instance to a human analyst, who then must determine whether the instance
is truly of interest (e.g. a threat in a security setting). Unfortunately, most
anomaly detectors provide no explanation about why an instance was considered
anomalous, leaving the analyst with no guidance about where to begin the
investigation. To address this issue, we study the problems of computing and
evaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE
of an anomaly is a sequence of features, which are presented to the analyst one
at a time (in order) until the information contained in the highlighted
features is enough for the analyst to make a confident judgement about the
anomaly. Since analyst effort is related to the amount of information that they
consider in an investigation, an explanation’s quality is related to the number
of features that must be revealed to attain confidence. One of our main
contributions is to present a novel framework for large scale quantitative
evaluations of SFEs, where the quality measure is based on analyst effort. To
do this we construct anomaly detection benchmarks from real data sets along
with artificial experts that can be simulated for evaluation. Our second
contribution is to evaluate several novel explanation approaches within the
framework and on traditional anomaly detection benchmarks, offering several
insights into the approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/dosovitskiy2015inverting/">Inverting Visual Representations with Convolutional Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Inverting Visual Representations with Convolutional Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Inverting Visual Representations with Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Alexey Dosovitskiy, Thomas Brox</td>
	<td></td>
	<td><p>Feature representations, both hand-designed and learned ones, are often hard
to analyze and interpret, even when they are extracted from visual data. We
propose a new approach to study image representations by inverting them with an
up-convolutional neural network. We apply the method to shallow representations
(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our
approach provides significantly better reconstructions than existing methods,
revealing that there is surprisingly rich information contained in these
features. Inverting a deep network trained on ImageNet provides several
insights into the properties of the feature representation learned by the
network. Most strikingly, the colors and the rough contours of an image can be
reconstructed from activations in higher network layers and even from the
predicted class probabilities.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/yosinski2015understanding/">Understanding Neural Networks Through Deep Visualization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Neural Networks Through Deep Visualization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Neural Networks Through Deep Visualization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, Hod Lipson</td>
	<td></td>
	<td><p>Recent years have produced great advances in training large, deep neural
networks (DNNs), including notable successes in training convolutional neural
networks (convnets) to recognize natural images. However, our understanding of
how these models work, especially what computations they perform at
intermediate layers, has lagged behind. Progress in the field will be further
accelerated by the development of better tools for visualizing and interpreting
neural nets. We introduce two such tools here. The first is a tool that
visualizes the activations produced on each layer of a trained convnet as it
processes an image or video (e.g. a live webcam stream). We have found that
looking at live activations that change in response to user input helps build
valuable intuitions about how convnets work. The second tool enables
visualizing features at each layer of a DNN via regularized optimization in
image space. Because previous versions of this idea produced less recognizable
images, here we introduce several new regularization methods that combine to
produce qualitatively clearer, more interpretable visualizations. Both tools
are open source and work on a pre-trained convnet with minimal setup.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/graziotin2015feel/">How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How Do You Feel, Developer? An Explanatory Theory of the Impact of Affects on Programming Performance' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Daniel Graziotin, Xiaofeng Wang, Pekka Abrahamsson</td>
	<td>PeerJ Computer Science 1:e18</td>
	<td><p>Affects—emotions and moods—have an impact on cognitive activities and the
working performance of individuals. Development tasks are undertaken through
cognitive processes, yet software engineering research lacks theory on affects
and their impact on software development activities. In this paper, we report
on an interpretive study aimed at broadening our understanding of the
psychology of programming in terms of the experience of affects while
programming, and the impact of affects on programming performance. We conducted
a qualitative interpretive study based on: face-to-face open-ended interviews,
in-field observations, and e-mail exchanges. This enabled us to construct a
novel explanatory theory of the impact of affects on development performance.
The theory is explicated using an established taxonomy framework. The proposed
theory builds upon the concepts of events, affects, attractors, focus, goals,
and performance. Theoretical and practical implications are given.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/karpathy2015visualizing/">Visualizing and Understanding Recurrent Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Recurrent Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Recurrent Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Andrej Karpathy, Justin Johnson, Li Fei-Fei</td>
	<td></td>
	<td><p>Recurrent Neural Networks (RNNs), and specifically a variant with Long
Short-Term Memory (LSTM), are enjoying renewed interest as a result of
successful applications in a wide range of machine learning problems that
involve sequential data. However, while LSTMs provide exceptional results in
practice, the source of their performance and their limitations remain rather
poorly understood. Using character-level language models as an interpretable
testbed, we aim to bridge this gap by providing an analysis of their
representations, predictions and error types. In particular, our experiments
reveal the existence of interpretable cells that keep track of long-range
dependencies such as line lengths, quotes and brackets. Moreover, our
comparative analysis with finite horizon n-gram models traces the source of the
LSTM improvements to long-range structural dependencies. Finally, we provide
analysis of the remaining errors and suggests areas for further study.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/samek2015evaluating/">Evaluating the visualization of what a Deep Neural Network has learned</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Evaluating the visualization of what a Deep Neural Network has learned' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Evaluating the visualization of what a Deep Neural Network has learned' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Wojciech Samek, Alexander Binder, Grégoire Montavon, Sebastian Bach, Klaus-Robert Müller</td>
	<td></td>
	<td><p>Deep Neural Networks (DNNs) have demonstrated impressive performance in
complex machine learning tasks such as image classification or speech
recognition. However, due to their multi-layer nonlinear structure, they are
not transparent, i.e., it is hard to grasp what makes them arrive at a
particular classification or recognition decision given a new unseen data
sample. Recently, several approaches have been proposed enabling one to
understand and interpret the reasoning embodied in a DNN for a single test
image. These methods quantify the ‘‘importance’’ of individual pixels wrt the
classification decision and allow a visualization in terms of a heatmap in
pixel/input space. While the usefulness of heatmaps can be judged subjectively
by a human, an objective quality measure is missing. In this paper we present a
general methodology based on region perturbation for evaluating ordered
collections of pixels such as heatmaps. We compare heatmaps computed by three
different methods on the SUN397, ILSVRC2012 and MIT Places data sets. Our main
result is that the recently proposed Layer-wise Relevance Propagation (LRP)
algorithm qualitatively and quantitatively provides a better explanation of
what made a DNN arrive at a particular classification decision than the
sensitivity-based approach or the deconvolution method. We provide theoretical
arguments to explain this result and discuss its practical implications.
Finally, we investigate the use of heatmaps for unsupervised assessment of
neural network performance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/zhou2015learning/">Learning Deep Features for Discriminative Localization</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning Deep Features for Discriminative Localization' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning Deep Features for Discriminative Localization' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>In this work, we revisit the global average pooling layer proposed in [13],
and shed light on how it explicitly enables the convolutional neural network to
have remarkable localization ability despite being trained on image-level
labels. While this technique was previously proposed as a means for
regularizing training, we find that it actually builds a generic localizable
deep representation that can be applied to a variety of tasks. Despite the
apparent simplicity of global average pooling, we are able to achieve 37.1%
top-5 error for object localization on ILSVRC 2014, which is remarkably close
to the 34.2% top-5 error achieved by a fully supervised CNN approach. We
demonstrate that our network is able to localize the discriminative image
regions on a variety of tasks despite not being trained for them</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/che2015distilling/">Distilling Knowledge from Deep Networks with Applications to Healthcare Domain</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Distilling Knowledge from Deep Networks with Applications to Healthcare Domain' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Distilling Knowledge from Deep Networks with Applications to Healthcare Domain' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zhengping Che, Sanjay Purushotham, Robinder Khemani, Yan Liu</td>
	<td></td>
	<td><p>Exponential growth in Electronic Healthcare Records (EHR) has resulted in new
opportunities and urgent needs for discovery of meaningful data-driven
representations and patterns of diseases in Computational Phenotyping research.
Deep Learning models have shown superior performance for robust prediction in
computational phenotyping tasks, but suffer from the issue of model
interpretability which is crucial for clinicians involved in decision-making.
In this paper, we introduce a novel knowledge-distillation approach called
Interpretable Mimic Learning, to learn interpretable phenotype features for
making robust prediction while mimicking the performance of deep learning
models. Our framework uses Gradient Boosting Trees to learn interpretable
features from deep learning models such as Stacked Denoising Autoencoder and
Long Short-Term Memory. Exhaustive experiments on a real-world clinical
time-series dataset show that our method obtains similar or better performance
than the deep learning models, and it provides interpretable phenotypes for
clinical decision making.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/aubry2015understanding/">Understanding deep features with computer-generated imagery</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding deep features with computer-generated imagery' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding deep features with computer-generated imagery' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Mathieu Aubry, Bryan Russell</td>
	<td></td>
	<td><p>We introduce an approach for analyzing the variation of features generated by
convolutional neural networks (CNNs) with respect to scene factors that occur
in natural images. Such factors may include object style, 3D viewpoint, color,
and scene lighting configuration. Our approach analyzes CNN feature responses
corresponding to different scene factors by controlling for them via rendering
using a large database of 3D CAD models. The rendered images are presented to a
trained CNN and responses for different layers are studied with respect to the
input scene factors. We perform a decomposition of the responses based on
knowledge of the input scene factors and analyze the resulting components. In
particular, we quantify their relative importance in the CNN responses and
visualize them using principal component analysis. We show qualitative and
quantitative results of our study on three CNNs trained on large image
datasets: AlexNet, Places, and Oxford VGG. We observe important differences
across the networks and CNN layers for different scene factors and object
categories. Finally, we demonstrate that our analysis based on
computer-generated imagery translates to the network representation of natural
images.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/lipton2015learning/">Learning to Diagnose with LSTM Recurrent Neural Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Learning to Diagnose with LSTM Recurrent Neural Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Learning to Diagnose with LSTM Recurrent Neural Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Zachary C. Lipton, David C. Kale, Charles Elkan, Randall Wetzel</td>
	<td></td>
	<td><p>Clinical medical data, especially in the intensive care unit (ICU), consist
of multivariate time series of observations. For each patient visit (or
episode), sensor data and lab test results are recorded in the patient’s
Electronic Health Record (EHR). While potentially containing a wealth of
insights, the data is difficult to mine effectively, owing to varying length,
irregular sampling and missing data. Recurrent Neural Networks (RNNs),
particularly those using Long Short-Term Memory (LSTM) hidden units, are
powerful and increasingly popular models for learning from sequence data. They
effectively model varying length sequences and capture long range dependencies.
We present the first study to empirically evaluate the ability of LSTMs to
recognize patterns in multivariate time series of clinical measurements.
Specifically, we consider multilabel classification of diagnoses, training a
model to classify 128 diagnoses given 13 frequently but irregularly sampled
clinical measurements. First, we establish the effectiveness of a simple LSTM
network for modeling clinical data. Then we demonstrate a straightforward and
effective training strategy in which we replicate targets at each sequence
step. Trained only on raw time series, our models outperform several strong
baselines, including a multilayer perceptron trained on hand-engineered
features.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/li2015visual/">Visual Saliency Based on Multiscale Deep Features</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visual Saliency Based on Multiscale Deep Features' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visual Saliency Based on Multiscale Deep Features' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Guanbin Li, Yizhou Yu</td>
	<td></td>
	<td><p>Visual saliency is a fundamental problem in both cognitive and computational
sciences, including computer vision. In this CVPR 2015 paper, we discover that
a high-quality visual saliency model can be trained with multiscale features
extracted using a popular deep learning architecture, convolutional neural
networks (CNNs), which have had many successes in visual recognition tasks. For
learning such saliency models, we introduce a neural network architecture,
which has fully connected layers on top of CNNs responsible for extracting
features at three different scales. We then propose a refinement method to
enhance the spatial coherence of our saliency results. Finally, aggregating
multiple saliency maps computed for different levels of image segmentation can
further boost the performance, yielding saliency maps better than those
generated from a single segmentation. To promote further research and
evaluation of visual saliency models, we also construct a new large database of
4447 challenging images and their pixelwise saliency annotation. Experimental
results demonstrate that our proposed method is capable of achieving
state-of-the-art performance on all public benchmarks, improving the F-Measure
by 5.0% and 13.2% respectively on the MSRA-B dataset and our new dataset
(HKU-IS), and lowering the mean absolute error by 5.7% and 35.1% respectively
on these two datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2015</td>
	<td><a href="/publications/kim2015bayesian/">The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Been Kim, Cynthia Rudin, Julie Shah</td>
	<td>NIPS 2014</td>
	<td><p>We present the Bayesian Case Model (BCM), a general framework for Bayesian
case-based reasoning (CBR) and prototype classification and clustering. BCM
brings the intuitive power of CBR to a Bayesian generative framework. The BCM
learns prototypes, the “quintessential” observations that best represent
clusters in a dataset, by performing joint inference on cluster labels,
prototypes and important features. Simultaneously, BCM pursues sparsity by
learning subspaces, the sets of features that play important roles in the
characterization of the prototypes. The prototype and subspace representation
provides quantitative benefits in interpretability while preserving
classification accuracy. Human subject experiments verify statistically
significant improvements to participants’ understanding when using explanations
produced by BCM, compared to those given by prior art.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2014</td>
	<td><a href="/publications/mahendran2014understanding/">Understanding Deep Image Representations by Inverting Them</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Understanding Deep Image Representations by Inverting Them' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Understanding Deep Image Representations by Inverting Them' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Aravindh Mahendran, Andrea Vedaldi</td>
	<td></td>
	<td><p>Image representations, from SIFT and Bag of Visual Words to Convolutional
Neural Networks (CNNs), are a crucial component of almost any image
understanding system. Nevertheless, our understanding of them remains limited.
In this paper we conduct a direct analysis of the visual information contained
in representations by asking the following question: given an encoding of an
image, to which extent is it possible to reconstruct the image itself? To
answer this question we contribute a general framework to invert
representations. We show that this method can invert representations such as
HOG and SIFT more accurately than recent alternatives while being applicable to
CNNs too. We then use this technique to study the inverse of recent
state-of-the-art CNN image representations for the first time. Among our
findings, we show that several layers in CNNs retain photographically accurate
information about the image, with different degrees of geometric and
photometric invariance.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/yosinski2014transferable/">How transferable are features in deep neural networks?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=How transferable are features in deep neural networks?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=How transferable are features in deep neural networks?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jason Yosinski, Jeff Clune, Yoshua Bengio, Hod Lipson</td>
	<td>Advances in Neural Information Processing Systems 27, pages 3320-3328. Dec. 2014</td>
	<td><p>Many deep neural networks trained on natural images exhibit a curious
phenomenon in common: on the first layer they learn features similar to Gabor
filters and color blobs. Such first-layer features appear not to be specific to
a particular dataset or task, but general in that they are applicable to many
datasets and tasks. Features must eventually transition from general to
specific by the last layer of the network, but this transition has not been
studied extensively. In this paper we experimentally quantify the generality
versus specificity of neurons in each layer of a deep convolutional neural
network and report a few surprising results. Transferability is negatively
affected by two distinct issues: (1) the specialization of higher layer neurons
to their original task at the expense of performance on the target task, which
was expected, and (2) optimization difficulties related to splitting networks
between co-adapted neurons, which was not expected. In an example network
trained on ImageNet, we demonstrate that either of these two issues may
dominate, depending on whether features are transferred from the bottom,
middle, or top of the network. We also document that the transferability of
features decreases as the distance between the base task and target task
increases, but that transferring features even from distant tasks can be better
than using random features. A final surprising result is that initializing a
network with transferred features from almost any number of layers can produce
a boost to generalization that lingers even after fine-tuning to the target
dataset.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/springenberg2014striving/">Striving for Simplicity: The All Convolutional Net</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Striving for Simplicity: The All Convolutional Net' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Striving for Simplicity: The All Convolutional Net' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, Martin Riedmiller</td>
	<td></td>
	<td><p>Most modern convolutional neural networks (CNNs) used for object recognition
are built using the same principles: Alternating convolution and max-pooling
layers followed by a small number of fully connected layers. We re-evaluate the
state of the art for object recognition from small images with convolutional
networks, questioning the necessity of different components in the pipeline. We
find that max-pooling can simply be replaced by a convolutional layer with
increased stride without loss in accuracy on several image recognition
benchmarks. Following this finding – and building on other recent work for
finding simple network structures – we propose a new architecture that
consists solely of convolutional layers and yields competitive or state of the
art performance on several object recognition datasets (CIFAR-10, CIFAR-100,
ImageNet). To analyze the network we introduce a new variant of the
“deconvolution approach” for visualizing features learned by CNNs, which can be
applied to a broader range of network structures than existing approaches.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/zhou2014object/">Object Detectors Emerge in Deep Scene CNNs</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Object Detectors Emerge in Deep Scene CNNs' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Object Detectors Emerge in Deep Scene CNNs' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba</td>
	<td></td>
	<td><p>With the success of new computational architectures for visual processing,
such as convolutional neural networks (CNN) and access to image databases with
millions of labeled examples (e.g., ImageNet, Places), the state of the art in
computer vision is advancing rapidly. One important factor for continued
progress is to understand the representations that are learned by the inner
layers of these deep architectures. Here we show that object detectors emerge
from training CNNs to perform scene classification. As scenes are composed of
objects, the CNN for scene classification automatically discovers meaningful
objects detectors, representative of the learned scene categories. With object
detectors emerging as a result of learning to recognize scenes, our work
demonstrates that the same network can perform both scene recognition and
object localization in a single forward-pass, without ever having been
explicitly taught the notion of objects.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2014</td>
	<td><a href="/publications/zhao2014person/">Person Re-identification by Saliency Learning</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Person Re-identification by Saliency Learning' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Person Re-identification by Saliency Learning' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Rui Zhao, Wanli Ouyang, Xiaogang Wang</td>
	<td></td>
	<td><p>Human eyes can recognize person identities based on small salient regions,
i.e. human saliency is distinctive and reliable in pedestrian matching across
disjoint camera views. However, such valuable information is often hidden when
computing similarities of pedestrian images with existing approaches. Inspired
by our user study result of human perception on human saliency, we propose a
novel perspective for person re-identification based on learning human saliency
and matching saliency distribution. The proposed saliency learning and matching
framework consists of four steps: (1) To handle misalignment caused by drastic
viewpoint change and pose variations, we apply adjacency constrained patch
matching to build dense correspondence between image pairs. (2) We propose two
alternative methods, i.e. K-Nearest Neighbors and One-class SVM, to estimate a
saliency score for each image patch, through which distinctive features stand
out without using identity labels in the training procedure. (3) saliency
matching is proposed based on patch matching. Matching patches with
inconsistent saliency brings penalty, and images of the same identity are
recognized by minimizing the saliency matching cost. (4) Furthermore, saliency
matching is tightly integrated with patch matching in a unified structural
RankSVM learning framework. The effectiveness of our approach is validated on
the VIPeR dataset and the CUHK01 dataset. Our approach outperforms the
state-of-the-art person re-identification methods on both datasets.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2013</td>
	<td><a href="/publications/chajewska2013defining/">Defining Explanation in Probabilistic Systems</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Defining Explanation in Probabilistic Systems' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Defining Explanation in Probabilistic Systems' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Urszula Chajewska, Joseph Y. Halpern</td>
	<td></td>
	<td><p>As probabilistic systems gain popularity and are coming into wider use, the
need for a mechanism that explains the system’s findings and recommendations
becomes more critical. The system will also need a mechanism for ordering
competing explanations. We examine two representative approaches to explanation
in the literature - one due to G"ardenfors and one due to Pearl - and show
that both suffer from significant problems. We propose an approach to defining
a notion of “better explanation” that combines some of the features of both
together with more recent work by Pearl and others on causality.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/zeiler2013visualizing/">Visualizing and Understanding Convolutional Networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Visualizing and Understanding Convolutional Networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Visualizing and Understanding Convolutional Networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Matthew D Zeiler, Rob Fergus</td>
	<td></td>
	<td><p>Large Convolutional Network models have recently demonstrated impressive
classification performance on the ImageNet benchmark. However there is no clear
understanding of why they perform so well, or how they might be improved. In
this paper we address both issues. We introduce a novel visualization technique
that gives insight into the function of intermediate feature layers and the
operation of the classifier. We also perform an ablation study to discover the
performance contribution from different model layers. This enables us to find
model architectures that outperform Krizhevsky \etal on the ImageNet
classification benchmark. We show our ImageNet model generalizes well to other
datasets: when the softmax classifier is retrained, it convincingly beats the
current state-of-the-art results on Caltech-101 and Caltech-256 datasets.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/szegedy2013intriguing/">Intriguing properties of neural networks</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Intriguing properties of neural networks' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Intriguing properties of neural networks' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, Rob Fergus</td>
	<td></td>
	<td><p>Deep neural networks are highly expressive models that have recently achieved
state of the art performance on speech and visual recognition tasks. While
their expressiveness is the reason they succeed, it also causes them to learn
uninterpretable solutions that could have counter-intuitive properties. In this
paper we report two such properties.
  First, we find that there is no distinction between individual high level
units and random linear combinations of high level units, according to various
methods of unit analysis. It suggests that it is the space, rather than the
individual units, that contains of the semantic information in the high layers
of neural networks.
  Second, we find that deep neural networks learn input-output mappings that
are fairly discontinuous to a significant extend. We can cause the network to
misclassify an image by applying a certain imperceptible perturbation, which is
found by maximizing the network’s prediction error. In addition, the specific
nature of these perturbations is not a random artifact of learning: the same
perturbation can cause a different network, that was trained on a different
subset of the dataset, to misclassify the same input.</p>
</td>
	<td></td>
</tr>

<tr>
	<td>2013</td>
	<td><a href="/publications/simonyan2013deep/">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Karen Simonyan, Andrea Vedaldi, Andrew Zisserman</td>
	<td></td>
	<td><p>This paper addresses the visualisation of image classification models, learnt
using deep Convolutional Networks (ConvNets). We consider two visualisation
techniques, based on computing the gradient of the class score with respect to
the input image. The first one generates an image, which maximises the class
score [Erhan et al., 2009], thus visualising the notion of the class, captured
by a ConvNet. The second technique computes a class saliency map, specific to a
given image and class. We show that such maps can be employed for weakly
supervised object segmentation using classification ConvNets. Finally, we
establish the connection between the gradient-based ConvNet visualisation
methods and deconvolutional networks [Zeiler et al., 2013].</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2011</td>
	<td><a href="/publications/fabbri2011explanation-based/">Explanation-Based Auditing</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Explanation-Based Auditing' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Explanation-Based Auditing' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Daniel Fabbri, Kristen LeFevre</td>
	<td>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 1, pp. 1-12 (2011)</td>
	<td><p>To comply with emerging privacy laws and regulations, it has become common
for applications like electronic health records systems (EHRs) to collect
access logs, which record each time a user (e.g., a hospital employee) accesses
a piece of sensitive data (e.g., a patient record). Using the access log, it is
easy to answer simple queries (e.g., Who accessed Alice’s medical record?), but
this often does not provide enough information. In addition to learning who
accessed their medical records, patients will likely want to understand why
each access occurred. In this paper, we introduce the problem of generating
explanations for individual records in an access log. The problem is motivated
by user-centric auditing applications, and it also provides a novel approach to
misuse detection. We develop a framework for modeling explanations which is
based on a fundamental observation: For certain classes of databases, including
EHRs, the reason for most data accesses can be inferred from data stored
elsewhere in the database. For example, if Alice has an appointment with Dr.
Dave, this information is stored in the database, and it explains why Dr. Dave
looked at Alice’s record. Large numbers of data accesses can be explained using
general forms called explanation templates. Rather than requiring an
administrator to manually specify explanation templates, we propose a set of
algorithms for automatically discovering frequent templates from the database
(i.e., those that explain a large number of accesses). We also propose
techniques for inferring collaborative user groups, which can be used to
enhance the quality of the discovered explanations. Finally, we have evaluated
our proposed techniques using an access log and data from the University of
Michigan Health System. Our results demonstrate that in practice we can provide
explanations for over 94% of data accesses in the log.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2004</td>
	<td><a href="/publications/ginsparg2004information/">Information, please... ?</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Information, please... ?' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Information, please... ?' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Paul Ginsparg</td>
	<td>appeared as "The Truth Is Still Out There", New York Times Op-Ed, 3 Aug 2004, http://www.nytimes.com/2004/08/03/opinion/03ginsparg.html</td>
	<td><p>Stephen Hawking’s recent concession that black holes do not irretrievably
eradicate information after all has garnered much attention. It is refreshing
to see the public focused, if just for a moment, on an important conundrum that
has fascinated theoretical physicists for three decades, and prompted much
conceptual progress. The scientific issues, however, remain much less settled
than Dr. Hawking’s celebrated wager on the question.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2002</td>
	<td><a href="/publications/halpern2002causes/">Causes and Explanations: A Structural-Model Approach. Part II: Explanations</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Causes and Explanations: A Structural-Model Approach. Part II: Explanations' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Causes and Explanations: A Structural-Model Approach. Part II: Explanations' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Joseph Y. Halpern, Judea Pearl</td>
	<td></td>
	<td><p>We propose new definitions of (causal) explanation, using structural
equations to model counterfactuals. The definition is based on the notion of
actual cause, as defined and motivated in a companion paper. Essentially, an
explanation is a fact that is not known for certain but, if found to be true,
would constitute an actual cause of the fact to be explained, regardless of the
agent’s initial uncertainty. We show that the definition handles well a number
of problematic examples from the literature.</p>
</td>
	<td></td>
</tr>



<tr>
	<td>2001</td>
	<td><a href="/publications/murata2001magical/">Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition in Japanese and English Sentences</a>
		<span class="externallinks">
			&nbsp;<a href='http://scholar.google.com/scholar?q=Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition in Japanese and English Sentences' target="_blank"><img  style="display: inline; margin: 0;" src="/public/media/google-scholar.png"/></a>
			<a href='https://www.semanticscholar.org/search?q=Magical Number Seven Plus or Minus Two: Syntactic Structure Recognition in Japanese and English Sentences' target="_blank"><img style="display: inline; margin: 0;" src="/public/media/semscholar.png"/></a>
		</span>
	</td>
	<td>Masaki Murata, Kiyotaka Uchimoto, Qing Ma, Hitoshi Isahara</td>
	<td>CICLing'2001, Mexico City, February 2001</td>
	<td><p>George A. Miller said that human beings have only seven chunks in short-term
memory, plus or minus two. We counted the number of bunsetsus (phrases) whose
modifiees are undetermined in each step of an analysis of the dependency
structure of Japanese sentences, and which therefore must be stored in
short-term memory. The number was roughly less than nine, the upper bound of
seven plus or minus two. We also obtained similar results with English
sentences under the assumption that human beings recognize a series of words,
such as a noun phrase (NP), as a unit. This indicates that if we assume that
the human cognitive units in Japanese and English are bunsetsu and NP
respectively, analysis will support Miller’s $7 \pm 2$ theory.</p>
</td>
	<td></td>
</tr>


</tbody></table>

<script>
var datatable;

function searchTable() {
    var hash = decodeURIComponent(window.location.hash.substr(1));
    datatable.search(hash).draw();
}


$(document).ready( function () {
    datatable = $('#allPapers').DataTable({
		paging: false,
		"order": [[ 0, 'desc' ], [ 1, 'asc' ]],
		columnDefs: [
			{
				targets: [3, 4, 5],
				visible: false,
				searchable: true
			}]
		});
    searchTable();
});

$(window).on('hashchange', function() {
  searchTable();
});
</script>


    </div>

  </body>
</html>
